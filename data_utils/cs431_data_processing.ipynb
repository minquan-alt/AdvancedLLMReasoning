{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Oqvpm9jhYGRT"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from collections import defaultdict, deque\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VRTPqnozqYiS"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "910e8cd157a44003bee0955f2ea31c66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "incorrect_solutions/train.jsonl:   0%|          | 0.00/6.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\datasets--nvidia--OpenMathInstruct-1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76ab4417038149d8acd93936f9fb9c6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "correct_solutions/validation.jsonl:   0%|          | 0.00/203M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "281c64795d15477082e60c7a49256302",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "incorrect_solutions/validation.jsonl:   0%|          | 0.00/981M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c1f46572c4f4229bcd0ef72a54a118d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b96bdc9facfe4b7797ece1516881d06b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ds = load_dataset(\"nvidia/OpenMathInstruct-1\", split='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7P5SX6kXQCm",
        "outputId": "f8a61708-1876-44cb-81c0-2ca5ec1d73cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "iter dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7321344/7321344 [05:02<00:00, 24234.87it/s]\n"
          ]
        }
      ],
      "source": [
        "groups = {'gsm8k': defaultdict(list), 'math': defaultdict(list)}\n",
        "\n",
        "for i, ex in enumerate(tqdm(ds, desc='iter dataset')):\n",
        "  dataset_name = ex.get('dataset')\n",
        "  if dataset_name in ('gsm8k', 'math'):\n",
        "    q = ex.get('question')\n",
        "    groups[dataset_name][q].append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2IQGmS9ZbD2",
        "outputId": "b3357fe7-b6b0-42bf-c6ee-7d0e08c3ad64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6500"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(groups['gsm8k'])\n",
        "len(groups['math'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Pg2_JUAAgILS"
      },
      "outputs": [],
      "source": [
        "def get_fair_downsample_subset(q2indices, target, seed=42):\n",
        "  ran = random.Random(seed)\n",
        "  questions = list(q2indices.keys())\n",
        "  ran.shuffle(questions)\n",
        "  result = []\n",
        "  q_deques = {}\n",
        "\n",
        "  for q in questions:\n",
        "    ls_indices = q2indices[q][:]\n",
        "    ran.shuffle(ls_indices)\n",
        "    q_deques[q] = deque(ls_indices)\n",
        "\n",
        "  q_cycle = deque(questions)\n",
        "\n",
        "  while q_cycle and len(result) < target:\n",
        "    q = q_cycle.popleft()\n",
        "    dq = q_deques[q]\n",
        "    if dq:\n",
        "      result.append(dq.popleft())\n",
        "      if dq:\n",
        "        q_cycle.append(q)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P8LmsLsVjg53"
      },
      "outputs": [],
      "source": [
        "def get_any_code_filtering_subset(q2indices, target, seed=42):\n",
        "  result = []\n",
        "  for q, indices in tqdm(q2indices.items(), desc='processing any code filtering'):\n",
        "    code_indices = []\n",
        "    text_indices = []\n",
        "    for i in indices:\n",
        "      em = ds[i].get('error_message')\n",
        "      code_used = (em != '<not_executed>')\n",
        "      if code_used:\n",
        "        code_indices.append(i)\n",
        "      else:\n",
        "        text_indices.append(i)\n",
        "    if code_indices:\n",
        "      result.extend(code_indices)\n",
        "    else:\n",
        "      result.extend(text_indices)\n",
        "\n",
        "  ran = random.Random(seed)\n",
        "  ran.shuffle(result)\n",
        "  return result[:target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b97a093a",
        "outputId": "cd5c016f-9cc3-478d-bd49-ebe2a7bdd336"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing any code filtering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6500/6500 [04:53<00:00, 22.16it/s]\n"
          ]
        }
      ],
      "source": [
        "TARGET_GSM8K = 10000\n",
        "TARGET_MATH = 10000\n",
        "\n",
        "gsm8k_subset = get_fair_downsample_subset(groups['gsm8k'], TARGET_GSM8K)\n",
        "math_subset = get_any_code_filtering_subset(groups['math'], TARGET_MATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nV9XyUD1nYJv"
      },
      "outputs": [],
      "source": [
        "ds_indices = gsm8k_subset + math_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj0C--mS3oFb",
        "outputId": "88d7f56b-cd20-4423-dc79-d63bf21a5481"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "iter dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7321344/7321344 [04:35<00:00, 26581.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 7321344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for sam in tqdm(ds, desc='iter dataset'):\n",
        "  if sam.get('is_correct') == 'false':\n",
        "    count += 1\n",
        "print(count, len(ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lCm20rE64Sul"
      },
      "outputs": [],
      "source": [
        "def save_subset(ds, ds_indices):\n",
        "  subset_data = [ds[i] for i in ds_indices]\n",
        "  subset_data = [{\"question\": ex[\"question\"], \"generated_solution\": ex[\"generated_solution\"]} for ex in subset_data]\n",
        "\n",
        "  subset_ds = Dataset.from_list(subset_data)\n",
        "  subset_ds.save_to_disk(\"data/subset_openmathinstruct_1\")\n",
        "  return subset_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cd75ed7f2b8e4b48ab8613c939bb1e28",
            "817d9f2e3bcb4816bebdf86cd53005a9",
            "7897bd880cbe47d59836bf9f9a4484be",
            "a1026fb56f614cb394b0d5885df0b429",
            "5e147322d5a6452fbb1b79b170bd3709",
            "0c5da37553484c6298b546e1cb90df40",
            "2689a8d2c7384eb2a3cdc669319a4a7b",
            "04a1b32931384fb29284cd4877baa3c6",
            "79c4b32669d241028ead7981e7677fdf",
            "d80f4ffbd20d470dac7b24ca08f0345a",
            "b10d8ac032bd4b328e821d75c4675264"
          ]
        },
        "id": "zDhTADeL56zh",
        "outputId": "e87250b7-8d7a-4594-cebc-1efbf385cc9e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf8e5955a35f40f79c514d6f8fb95d7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/20000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "subset_ds = save_subset(ds, ds_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "eAa7x9M58BYa"
      },
      "outputs": [],
      "source": [
        "class DataPreprocessing:\n",
        "    '''Tiền xử lý dữ liệu gồm làm sạch dữ liệu, tách dữ liệu ra train/dev/test, và tokenize dữ liệu'''\n",
        "    \n",
        "    def __init__(self, dataset, tokenizer, train_ratio=0.8, dev_ratio=0.1):\n",
        "        '''\n",
        "        Khởi tạo với dataset và tokenizer\n",
        "        Args:\n",
        "            dataset: Dataset Huggingface\n",
        "            tokenizer: Tokenizer từ transformers\n",
        "            train_ratio: Tỉ lệ tập train (mặc định 0.8)\n",
        "            dev_ratio: Tỉ lệ tập dev (mặc định 0.1)\n",
        "        '''\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.train_ratio = train_ratio\n",
        "        self.dev_ratio = dev_ratio\n",
        "        \n",
        "    def clean_text(self, text):\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "        \n",
        "        text = text.strip()\n",
        "        text = re.sub(r'[ \\t]+', ' ', text)  # Chỉ xóa space/tab thừa\n",
        "        text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Giữ tối đa 2 newlines\n",
        "        return text\n",
        "    \n",
        "    def prepare_input(self, question, solution):\n",
        "        '''\n",
        "        Chuẩn bị input cho mô hình\n",
        "        Args:\n",
        "            question: Câu hỏi\n",
        "            solution: Lời giải\n",
        "        Returns:\n",
        "            string đã được format theo mẫu cho mô hình\n",
        "        '''\n",
        "        question = self.clean_text(question)\n",
        "        solution = self.clean_text(solution)\n",
        "        return f\"### Question:\\n{question}\\n\\n### Solution:\\n{solution}\"\n",
        "    \n",
        "    def tokenize_data(self, text, max_length=1024):\n",
        "        '''\n",
        "        Tokenize dữ liệu text\n",
        "        Args:\n",
        "            text: string cần tokenize\n",
        "            max_length: độ dài tối đa sau khi tokenize\n",
        "        Returns:\n",
        "            dict chứa input_ids và attention_mask\n",
        "        '''\n",
        "        return self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "    \n",
        "    def split_data(self):\n",
        "        '''\n",
        "        Chia dữ liệu thành các tập train/dev/test\n",
        "        Returns:\n",
        "            tuple (train_data, dev_data, test_data)\n",
        "        '''\n",
        "        total_size = len(self.dataset)\n",
        "        indices = list(range(total_size))\n",
        "        random.shuffle(indices)\n",
        "        \n",
        "        train_size = int(total_size * self.train_ratio)\n",
        "        dev_size = int(total_size * self.dev_ratio)\n",
        "        \n",
        "        train_indices = indices[:train_size]\n",
        "        dev_indices = indices[train_size:train_size + dev_size]\n",
        "        test_indices = indices[train_size + dev_size:]\n",
        "        \n",
        "        return (\n",
        "            self.dataset.select(train_indices),\n",
        "            self.dataset.select(dev_indices),\n",
        "            self.dataset.select(test_indices)\n",
        "        )\n",
        "    \n",
        "    def process_example(self, example, max_length=1024):\n",
        "        '''\n",
        "        Xử lý một mẫu dữ liệu\n",
        "        Args:\n",
        "            example: Một mẫu từ dataset\n",
        "            max_length: độ dài tối đa cho tokenize\n",
        "        Returns:\n",
        "            dict chứa input_ids, attention_mask và labels\n",
        "        '''\n",
        "        text = self.prepare_input(\n",
        "            example['question'],\n",
        "            example['generated_solution']\n",
        "        )\n",
        "        tokenized = self.tokenize_data(text, max_length)\n",
        "        \n",
        "        # Thêm labels cho causal LM\n",
        "        result = {\n",
        "            'input_ids': tokenized['input_ids'].squeeze(),\n",
        "            'attention_mask': tokenized['attention_mask'].squeeze(),\n",
        "            'labels': tokenized['input_ids'].squeeze().clone()\n",
        "        }\n",
        "        return result\n",
        "    \n",
        "    def preprocess(self, max_length=1024):\n",
        "        '''\n",
        "        Tiền xử lý toàn bộ dataset\n",
        "        Args:\n",
        "            max_length: độ dài tối đa cho tokenize\n",
        "        Returns:\n",
        "            tuple (train_data, dev_data, test_data) đã được tiền xử lý\n",
        "        '''\n",
        "        # Chia dữ liệu\n",
        "        train_data, dev_data, test_data = self.split_data()\n",
        "        \n",
        "        # Áp dụng xử lý cho từng tập\n",
        "        train_processed = train_data.map(lambda x: self.process_example(x, max_length))\n",
        "        dev_processed = dev_data.map(lambda x: self.process_example(x, max_length))\n",
        "        test_processed = test_data.map(lambda x: self.process_example(x, max_length))\n",
        "        \n",
        "        return train_processed, dev_processed, test_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a74f9f8286e4b38a3428be8730ea422",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61fe0d39df9d4cafba94fd6629a051fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e13d667477284919b6ce4f392b52efd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dp = DataPreprocessing(dataset=subset_ds, tokenizer=tokenizer, train_ratio=0.8, dev_ratio=0.1)\n",
        "train_processed, dev_processed, test_processed = dp.preprocess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 20000\n",
            "\n",
            "Example from original dataset:\n",
            "{'question': 'Aunt Gemma has four dogs. She feeds them with dog food twice a day. Each dog can consume 250 grams of food per meal. If aunt Gemma bought 2 sacks of dog food, each weighing 50 kilograms, how many days will those food last?', 'generated_solution': 'Let\\'s solve this problem using Python\\'s sympy library.\\n<llm-code>\\nimport sympy as sp\\n\\n# let\\'s denote the number of days in which the food lasts\\n# since the \"number of days\" cannot be a symbol we use q instead\\nq = sp.symbols(\\'q\\')\\n\\n# total number of grams of food per day\\n# a dog consumes 250 grams each day (per meal x 2)\\n# there are 4 dogs\\nfood_per_day = 250 * 4\\n# there are q days\\nfood_total = food_per_day * q\\n\\n# the food is 50 kilograms each in 2 sacks\\n# 1 kilogram is 1000 grams\\n# 50 kilograms is 50000 grams\\ntotal_kg = 50000 * 2\\n# expressed in grams\\ntotal_g = 50000 * 2 * 1000\\n\\n# total grams of food per day is what the dogs ate\\neq = sp.Eq(food_total, total_g)\\n\\n# solving for q the number of days\\nsp.solve(eq, q)[0]\\n</llm-code>\\n<llm-code-output>\\n100000\\n</llm-code-output>\\nThus the food will last for \\\\boxed{100000} days!'}\n",
            "\n",
            "Processed example:\n",
            "\n",
            "input_ids shape: torch.Size([1024])\n",
            "\n",
            "attention_mask shape: torch.Size([1024])\n",
            "\n",
            "labels shape: torch.Size([1024])\n"
          ]
        }
      ],
      "source": [
        "# Check the dataset and processed results\n",
        "print(\"Dataset size:\", len(subset_ds))\n",
        "print(\"\\nExample from original dataset:\")\n",
        "print(subset_ds[0])\n",
        "\n",
        "# Try processing a single example to debug\n",
        "example = dp.process_example(subset_ds[0])\n",
        "print(\"\\nProcessed example:\")\n",
        "for key, value in example.items():\n",
        "    print(f\"\\n{key} shape:\", value.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, hf_dataset):\n",
        "        self.input_ids = [torch.tensor(x['input_ids']) for x in hf_dataset]\n",
        "        self.attention_mask = [torch.tensor(x['attention_mask']) for x in hf_dataset]\n",
        "        self.labels = [torch.tensor(x['labels']) for x in hf_dataset]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(train_processed)\n",
        "dev_dataset = CustomDataset(dev_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supervised Fine-tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login('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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 1. Load base model & tokenizer\n",
        "base_model = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 2. Add LoRA \n",
        "lora_config = LoraConfig(r=8, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.1)\n",
        "model = get_peft_model(model, lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: \"'accelerate\": Expected package name at the start of dependency specifier\n",
            "    'accelerate\n",
            "    ^\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstall \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=0.26.0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./math_tutor_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
            "File \u001b[1;32m<string>:132\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, tp_size, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\training_args.py:1761\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1761\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\training_args.py:2297\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2293\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2294\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2295\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2296\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\utils\\generic.py:67\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     65\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\training_args.py:2167\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2168\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2169\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2170\u001b[0m         )\n\u001b[0;32m   2171\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2172\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
            "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
          ]
        }
      ],
      "source": [
        "%pip install 'accelerate>=0.26.0'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./math_tutor_model\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    warmup_steps=500,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_processed,\n",
        "    eval_dataset=dev_processed,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04a1b32931384fb29284cd4877baa3c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c5da37553484c6298b546e1cb90df40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2689a8d2c7384eb2a3cdc669319a4a7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e147322d5a6452fbb1b79b170bd3709": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7897bd880cbe47d59836bf9f9a4484be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04a1b32931384fb29284cd4877baa3c6",
            "max": 20000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79c4b32669d241028ead7981e7677fdf",
            "value": 20000
          }
        },
        "79c4b32669d241028ead7981e7677fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "817d9f2e3bcb4816bebdf86cd53005a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c5da37553484c6298b546e1cb90df40",
            "placeholder": "​",
            "style": "IPY_MODEL_2689a8d2c7384eb2a3cdc669319a4a7b",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "a1026fb56f614cb394b0d5885df0b429": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d80f4ffbd20d470dac7b24ca08f0345a",
            "placeholder": "​",
            "style": "IPY_MODEL_b10d8ac032bd4b328e821d75c4675264",
            "value": " 20000/20000 [00:00&lt;00:00, 193483.84 examples/s]"
          }
        },
        "b10d8ac032bd4b328e821d75c4675264": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd75ed7f2b8e4b48ab8613c939bb1e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_817d9f2e3bcb4816bebdf86cd53005a9",
              "IPY_MODEL_7897bd880cbe47d59836bf9f9a4484be",
              "IPY_MODEL_a1026fb56f614cb394b0d5885df0b429"
            ],
            "layout": "IPY_MODEL_5e147322d5a6452fbb1b79b170bd3709"
          }
        },
        "d80f4ffbd20d470dac7b24ca08f0345a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

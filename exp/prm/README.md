# PRM Module Structure

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
exp/
â”œâ”€â”€ prm/                           # PRM package
â”‚   â”œâ”€â”€ __init__.py               # Package exports
â”‚   â”œâ”€â”€ config.py                 # PRMConfig dataclass
â”‚   â”œâ”€â”€ utils.py                  # GPU utils, normalize_answer
â”‚   â”œâ”€â”€ parsing.py                # Solution parsing (extract_answer, parse_steps)
â”‚   â”œâ”€â”€ reward.py                 # Reward computation (HE, SE)
â”‚   â”œâ”€â”€ data_generator.py         # PRMDataGenerator class
â”‚   â””â”€â”€ trainer.py                # PRMTrainer class
â”œâ”€â”€ prm_exp_clean.py              # Main training script (CLEAN VERSION)
â”œâ”€â”€ prm_exp.py                    # Old monolithic version (deprecated)
â””â”€â”€ PRM_README.md                 # Usage guide
```

## ğŸ¯ Module Responsibilities

### `config.py`
- `PRMConfig`: Configuration dataclass
  - method (1 hoáº·c 2)
  - reward_type ("HE" hoáº·c "SE")
  - sft_model_path
  - num_rollouts, num_samples_from_mistakes, difficulty_threshold

### `utils.py`
- `stats()`: Check GPU stats
- `wait_for_gpu()`: Wait until GPU available
- `normalize_answer()`: Normalize answers for comparison

### `parsing.py`
- `extract_answer()`: Extract answer from `\boxed{}`
- `parse_solution_into_steps()`: Split solution into steps
- `create_prompt()`: Create generation prompt

### `reward.py`
- `compute_hard_exact_reward()`: HE reward logic
- `compute_soft_exact_reward()`: SE reward logic  
- `compute_reward()`: Main reward dispatcher

### `data_generator.py`
- `PRMDataGenerator`: Generate PRM training data
  - `load_sft_model()`: Load SFT model
  - `load_verifier()`: Load Claude verifier
  - `get_mistake_samples()`: Extract mistakes from OpenMathInstruct-1
  - `generate_with_sft()`: Method 1 generation
  - `generate_with_verifier()`: Method 2 generation
  - `score_solution_with_verifier()`: Claude scoring
  - `generate_prm_dataset()`: Main pipeline

### `trainer.py`
- `PRMTrainer`: Train PRM model
  - `load_prm_model()`: Initialize model with LoRA
  - `prepare_prm_training_data()`: Format dataset
  - `tokenize_function()`: Tokenization
  - `train()`: Main training loop

## ğŸš€ Usage

### Sá»­ dá»¥ng script chÃ­nh (Recommended)

```bash
# Method 1 + HE reward
python exp/prm_exp_clean.py \
    --method 1 \
    --reward HE \
    --sft_model math_tutor_model/math_sft_adapter/v2/final_checkpoint \
    --num_samples 1000 \
    --num_rollouts 5
```

### Sá»­ dá»¥ng nhÆ° Python package

```python
from prm import PRMConfig, PRMDataGenerator, PRMTrainer

# Create config
config = PRMConfig(
    method=1,
    reward_type="HE",
    sft_model_path="path/to/sft",
    num_rollouts=5,
    num_samples_from_mistakes=1000
)

# Generate data
generator = PRMDataGenerator(config, anthropic_api_key)
dataset = generator.generate_prm_dataset()

# Train
trainer = PRMTrainer(config)
trainer.load_prm_model()
trainer.train(train_data, eval_data)
```

### Import individual components

```python
from prm.parsing import extract_answer, parse_solution_into_steps
from prm.reward import compute_reward
from prm.utils import normalize_answer

# Use functions independently
answer = extract_answer(solution_text)
steps = parse_solution_into_steps(solution_text)
rewards = compute_reward("HE", steps, scores, is_correct)
```

## ğŸ”§ Advantages of Modular Structure

### âœ… **Maintainability**
- Má»—i module cÃ³ má»™t trÃ¡ch nhiá»‡m rÃµ rÃ ng
- Dá»… tÃ¬m vÃ  fix bugs
- Code ngáº¯n gá»n hÆ¡n (má»—i file < 300 lines)

### âœ… **Reusability**
- CÃ³ thá»ƒ import tá»«ng function riÃªng láº»
- DÃ¹ng láº¡i parsing/reward logic cho cÃ¡c tasks khÃ¡c
- TÃ¡ch biá»‡t data generation vÃ  training

### âœ… **Testability**
- Dá»… viáº¿t unit tests cho tá»«ng module
- Mock dependencies dá»… dÃ ng
- Test isolated components

### âœ… **Extensibility**
- ThÃªm reward types má»›i: chá»‰ sá»­a `reward.py`
- ThÃªm verifier khÃ¡c: chá»‰ sá»­a `data_generator.py`
- Thay Ä‘á»•i parsing logic: chá»‰ sá»­a `parsing.py`

### âœ… **Readability**
- Code structure rÃµ rÃ ng
- Import statements ngáº¯n gá»n
- Dá»… onboard developers má»›i

## ğŸ“Š Migration from Old Code

### Old (Monolithic):
```python
# prm_exp.py - 810 lines
# Everything in one file:
# - Config
# - Utils
# - Parsing
# - Reward
# - DataGenerator
# - Trainer
# - Main script
```

### New (Modular):
```python
# prm_exp_clean.py - 108 lines (chá»‰ main logic)
# + prm/ package:
#   - config.py: 16 lines
#   - utils.py: 44 lines  
#   - parsing.py: 68 lines
#   - reward.py: 62 lines
#   - data_generator.py: 350 lines
#   - trainer.py: 220 lines
#   - __init__.py: 20 lines
```

**Total**: 888 lines â†’ Organized thÃ nh 7 modules rÃµ rÃ ng!

## ğŸ§ª Testing Examples

```python
# Test parsing
from prm.parsing import parse_solution_into_steps

solution = """
Step 1: Calculate x = 10
<llm-code>
x = 10
</llm-code>
Step 2: Therefore answer is \\boxed{10}
"""

steps = parse_solution_into_steps(solution)
assert len(steps) == 3

# Test reward
from prm.reward import compute_reward

rewards = compute_reward("HE", steps, [1, 1, 1], True)
assert all(r == 1.0 for r in rewards)

# Test utils
from prm.utils import normalize_answer

assert normalize_answer("1,234.56") == "1234.56"
assert normalize_answer("42") == "42.0"
```

## ğŸ“ Next Steps

1. âœ… TÃ¡ch code thÃ nh modules
2. â³ Viáº¿t unit tests cho tá»«ng module
3. â³ Add logging vÃ  error handling
4. â³ Add type hints Ä‘áº§y Ä‘á»§
5. â³ Documentation (docstrings)
6. â³ Performance profiling

---

**Recommendation**: Sá»­ dá»¥ng `prm_exp_clean.py` cho production, giá»¯ `prm_exp.py` cÅ© cho reference.

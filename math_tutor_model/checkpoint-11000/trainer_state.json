{
  "best_global_step": 11000,
  "best_metric": 0.5536175966262817,
  "best_model_checkpoint": "math_tutor_model/checkpoint-11000",
  "epoch": 0.6944444444444444,
  "eval_steps": 1000,
  "global_step": 11000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 6.313131313131313e-05,
      "grad_norm": 0.5092648267745972,
      "learning_rate": 0.0,
      "loss": 1.1781,
      "step": 1
    },
    {
      "epoch": 0.0006313131313131314,
      "grad_norm": 1.0038855075836182,
      "learning_rate": 1.8927444794952682e-06,
      "loss": 1.4598,
      "step": 10
    },
    {
      "epoch": 0.0012626262626262627,
      "grad_norm": 1.1605165004730225,
      "learning_rate": 3.995793901156677e-06,
      "loss": 1.6342,
      "step": 20
    },
    {
      "epoch": 0.001893939393939394,
      "grad_norm": 1.1726101636886597,
      "learning_rate": 6.098843322818087e-06,
      "loss": 1.6964,
      "step": 30
    },
    {
      "epoch": 0.0025252525252525255,
      "grad_norm": 1.453029990196228,
      "learning_rate": 8.201892744479495e-06,
      "loss": 1.7211,
      "step": 40
    },
    {
      "epoch": 0.0031565656565656565,
      "grad_norm": 1.8017903566360474,
      "learning_rate": 1.0304942166140905e-05,
      "loss": 1.7319,
      "step": 50
    },
    {
      "epoch": 0.003787878787878788,
      "grad_norm": 0.5794760584831238,
      "learning_rate": 1.2407991587802314e-05,
      "loss": 1.2407,
      "step": 60
    },
    {
      "epoch": 0.004419191919191919,
      "grad_norm": 0.7006651163101196,
      "learning_rate": 1.4511041009463724e-05,
      "loss": 1.3017,
      "step": 70
    },
    {
      "epoch": 0.005050505050505051,
      "grad_norm": 0.7171847224235535,
      "learning_rate": 1.661409043112513e-05,
      "loss": 1.2685,
      "step": 80
    },
    {
      "epoch": 0.005681818181818182,
      "grad_norm": 0.7153325080871582,
      "learning_rate": 1.871713985278654e-05,
      "loss": 1.178,
      "step": 90
    },
    {
      "epoch": 0.006313131313131313,
      "grad_norm": 1.319507122039795,
      "learning_rate": 2.0820189274447953e-05,
      "loss": 1.1983,
      "step": 100
    },
    {
      "epoch": 0.006944444444444444,
      "grad_norm": 0.4877150058746338,
      "learning_rate": 2.292323869610936e-05,
      "loss": 1.147,
      "step": 110
    },
    {
      "epoch": 0.007575757575757576,
      "grad_norm": 0.4692564010620117,
      "learning_rate": 2.5026288117770768e-05,
      "loss": 1.1314,
      "step": 120
    },
    {
      "epoch": 0.008207070707070708,
      "grad_norm": 0.5800968408584595,
      "learning_rate": 2.7129337539432176e-05,
      "loss": 1.1279,
      "step": 130
    },
    {
      "epoch": 0.008838383838383838,
      "grad_norm": 0.6486352682113647,
      "learning_rate": 2.9232386961093587e-05,
      "loss": 1.0681,
      "step": 140
    },
    {
      "epoch": 0.00946969696969697,
      "grad_norm": 1.0840227603912354,
      "learning_rate": 3.1335436382754995e-05,
      "loss": 1.101,
      "step": 150
    },
    {
      "epoch": 0.010101010101010102,
      "grad_norm": 0.46684345602989197,
      "learning_rate": 3.34384858044164e-05,
      "loss": 1.0765,
      "step": 160
    },
    {
      "epoch": 0.010732323232323232,
      "grad_norm": 0.5504432916641235,
      "learning_rate": 3.554153522607782e-05,
      "loss": 1.1021,
      "step": 170
    },
    {
      "epoch": 0.011363636363636364,
      "grad_norm": 0.5505733489990234,
      "learning_rate": 3.7644584647739225e-05,
      "loss": 1.0749,
      "step": 180
    },
    {
      "epoch": 0.011994949494949494,
      "grad_norm": 0.6061124801635742,
      "learning_rate": 3.974763406940063e-05,
      "loss": 1.0484,
      "step": 190
    },
    {
      "epoch": 0.012626262626262626,
      "grad_norm": 1.1371119022369385,
      "learning_rate": 4.185068349106204e-05,
      "loss": 1.059,
      "step": 200
    },
    {
      "epoch": 0.013257575757575758,
      "grad_norm": 0.5298175811767578,
      "learning_rate": 4.395373291272345e-05,
      "loss": 1.0797,
      "step": 210
    },
    {
      "epoch": 0.013888888888888888,
      "grad_norm": 0.5940582156181335,
      "learning_rate": 4.6056782334384864e-05,
      "loss": 1.0629,
      "step": 220
    },
    {
      "epoch": 0.01452020202020202,
      "grad_norm": 0.6109808683395386,
      "learning_rate": 4.815983175604627e-05,
      "loss": 1.0332,
      "step": 230
    },
    {
      "epoch": 0.015151515151515152,
      "grad_norm": 0.6498274207115173,
      "learning_rate": 5.026288117770768e-05,
      "loss": 1.0109,
      "step": 240
    },
    {
      "epoch": 0.015782828282828284,
      "grad_norm": 1.179053544998169,
      "learning_rate": 5.236593059936909e-05,
      "loss": 0.9896,
      "step": 250
    },
    {
      "epoch": 0.016414141414141416,
      "grad_norm": 0.5732084512710571,
      "learning_rate": 5.44689800210305e-05,
      "loss": 1.0723,
      "step": 260
    },
    {
      "epoch": 0.017045454545454544,
      "grad_norm": 0.559108316898346,
      "learning_rate": 5.657202944269191e-05,
      "loss": 1.0365,
      "step": 270
    },
    {
      "epoch": 0.017676767676767676,
      "grad_norm": 0.5784690976142883,
      "learning_rate": 5.867507886435332e-05,
      "loss": 1.0032,
      "step": 280
    },
    {
      "epoch": 0.018308080808080808,
      "grad_norm": 0.7329161167144775,
      "learning_rate": 6.0778128286014725e-05,
      "loss": 0.9627,
      "step": 290
    },
    {
      "epoch": 0.01893939393939394,
      "grad_norm": 0.9231019616127014,
      "learning_rate": 6.288117770767613e-05,
      "loss": 0.9993,
      "step": 300
    },
    {
      "epoch": 0.019570707070707072,
      "grad_norm": 0.5076531767845154,
      "learning_rate": 6.498422712933754e-05,
      "loss": 1.0374,
      "step": 310
    },
    {
      "epoch": 0.020202020202020204,
      "grad_norm": 0.5314714908599854,
      "learning_rate": 6.708727655099896e-05,
      "loss": 1.0332,
      "step": 320
    },
    {
      "epoch": 0.020833333333333332,
      "grad_norm": 0.5895927548408508,
      "learning_rate": 6.919032597266037e-05,
      "loss": 1.0028,
      "step": 330
    },
    {
      "epoch": 0.021464646464646464,
      "grad_norm": 0.6265683770179749,
      "learning_rate": 7.129337539432177e-05,
      "loss": 0.985,
      "step": 340
    },
    {
      "epoch": 0.022095959595959596,
      "grad_norm": 0.9994589686393738,
      "learning_rate": 7.339642481598317e-05,
      "loss": 0.9914,
      "step": 350
    },
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 0.4712381958961487,
      "learning_rate": 7.549947423764459e-05,
      "loss": 1.0135,
      "step": 360
    },
    {
      "epoch": 0.02335858585858586,
      "grad_norm": 0.5213924646377563,
      "learning_rate": 7.760252365930599e-05,
      "loss": 1.0187,
      "step": 370
    },
    {
      "epoch": 0.023989898989898988,
      "grad_norm": 0.552244246006012,
      "learning_rate": 7.970557308096742e-05,
      "loss": 1.0142,
      "step": 380
    },
    {
      "epoch": 0.02462121212121212,
      "grad_norm": 0.605170488357544,
      "learning_rate": 8.180862250262882e-05,
      "loss": 0.9547,
      "step": 390
    },
    {
      "epoch": 0.025252525252525252,
      "grad_norm": 0.8086289763450623,
      "learning_rate": 8.391167192429022e-05,
      "loss": 0.9541,
      "step": 400
    },
    {
      "epoch": 0.025883838383838384,
      "grad_norm": 0.4483409821987152,
      "learning_rate": 8.601472134595163e-05,
      "loss": 1.0191,
      "step": 410
    },
    {
      "epoch": 0.026515151515151516,
      "grad_norm": 0.4984561502933502,
      "learning_rate": 8.811777076761303e-05,
      "loss": 1.0208,
      "step": 420
    },
    {
      "epoch": 0.027146464646464648,
      "grad_norm": 0.49999457597732544,
      "learning_rate": 9.022082018927446e-05,
      "loss": 0.9828,
      "step": 430
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 0.5872477889060974,
      "learning_rate": 9.232386961093586e-05,
      "loss": 0.9625,
      "step": 440
    },
    {
      "epoch": 0.028409090909090908,
      "grad_norm": 0.9023650288581848,
      "learning_rate": 9.442691903259728e-05,
      "loss": 0.9486,
      "step": 450
    },
    {
      "epoch": 0.02904040404040404,
      "grad_norm": 0.4345221519470215,
      "learning_rate": 9.652996845425868e-05,
      "loss": 1.0117,
      "step": 460
    },
    {
      "epoch": 0.029671717171717172,
      "grad_norm": 0.4707091152667999,
      "learning_rate": 9.863301787592008e-05,
      "loss": 1.0279,
      "step": 470
    },
    {
      "epoch": 0.030303030303030304,
      "grad_norm": 0.47164538502693176,
      "learning_rate": 0.0001007360672975815,
      "loss": 0.9662,
      "step": 480
    },
    {
      "epoch": 0.030934343434343436,
      "grad_norm": 0.5436370968818665,
      "learning_rate": 0.00010283911671924291,
      "loss": 0.939,
      "step": 490
    },
    {
      "epoch": 0.03156565656565657,
      "grad_norm": 0.9068610668182373,
      "learning_rate": 0.00010494216614090431,
      "loss": 0.9748,
      "step": 500
    },
    {
      "epoch": 0.032196969696969696,
      "grad_norm": 0.4267598092556,
      "learning_rate": 0.00010704521556256572,
      "loss": 1.0205,
      "step": 510
    },
    {
      "epoch": 0.03282828282828283,
      "grad_norm": 0.440646231174469,
      "learning_rate": 0.00010914826498422714,
      "loss": 1.0077,
      "step": 520
    },
    {
      "epoch": 0.03345959595959596,
      "grad_norm": 0.4694534242153168,
      "learning_rate": 0.00011125131440588854,
      "loss": 0.9798,
      "step": 530
    },
    {
      "epoch": 0.03409090909090909,
      "grad_norm": 0.5469862818717957,
      "learning_rate": 0.00011335436382754996,
      "loss": 0.9485,
      "step": 540
    },
    {
      "epoch": 0.034722222222222224,
      "grad_norm": 0.873009204864502,
      "learning_rate": 0.00011545741324921136,
      "loss": 0.9576,
      "step": 550
    },
    {
      "epoch": 0.03535353535353535,
      "grad_norm": 0.4294698238372803,
      "learning_rate": 0.00011756046267087277,
      "loss": 0.9826,
      "step": 560
    },
    {
      "epoch": 0.03598484848484849,
      "grad_norm": 0.4311674237251282,
      "learning_rate": 0.00011966351209253419,
      "loss": 0.9763,
      "step": 570
    },
    {
      "epoch": 0.036616161616161616,
      "grad_norm": 0.46870291233062744,
      "learning_rate": 0.00012176656151419559,
      "loss": 0.9651,
      "step": 580
    },
    {
      "epoch": 0.037247474747474744,
      "grad_norm": 0.5374376773834229,
      "learning_rate": 0.000123869610935857,
      "loss": 0.9501,
      "step": 590
    },
    {
      "epoch": 0.03787878787878788,
      "grad_norm": 0.8207476735115051,
      "learning_rate": 0.0001259726603575184,
      "loss": 0.9524,
      "step": 600
    },
    {
      "epoch": 0.03851010101010101,
      "grad_norm": 0.4167443513870239,
      "learning_rate": 0.00012807570977917983,
      "loss": 1.0122,
      "step": 610
    },
    {
      "epoch": 0.039141414141414144,
      "grad_norm": 0.46308737993240356,
      "learning_rate": 0.00013017875920084122,
      "loss": 0.9779,
      "step": 620
    },
    {
      "epoch": 0.03977272727272727,
      "grad_norm": 0.4526059329509735,
      "learning_rate": 0.00013228180862250263,
      "loss": 0.9835,
      "step": 630
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": 0.5314608216285706,
      "learning_rate": 0.00013438485804416405,
      "loss": 0.9387,
      "step": 640
    },
    {
      "epoch": 0.041035353535353536,
      "grad_norm": 0.7409462332725525,
      "learning_rate": 0.00013648790746582546,
      "loss": 0.9155,
      "step": 650
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.39500120282173157,
      "learning_rate": 0.00013859095688748688,
      "loss": 1.045,
      "step": 660
    },
    {
      "epoch": 0.0422979797979798,
      "grad_norm": 0.4251002073287964,
      "learning_rate": 0.00014069400630914826,
      "loss": 1.0002,
      "step": 670
    },
    {
      "epoch": 0.04292929292929293,
      "grad_norm": 0.4866493344306946,
      "learning_rate": 0.00014279705573080968,
      "loss": 0.97,
      "step": 680
    },
    {
      "epoch": 0.043560606060606064,
      "grad_norm": 0.4947895407676697,
      "learning_rate": 0.0001449001051524711,
      "loss": 0.9361,
      "step": 690
    },
    {
      "epoch": 0.04419191919191919,
      "grad_norm": 0.8517065644264221,
      "learning_rate": 0.0001470031545741325,
      "loss": 0.9254,
      "step": 700
    },
    {
      "epoch": 0.04482323232323232,
      "grad_norm": 0.3971863389015198,
      "learning_rate": 0.00014910620399579392,
      "loss": 1.0174,
      "step": 710
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.4516993761062622,
      "learning_rate": 0.0001512092534174553,
      "loss": 0.9711,
      "step": 720
    },
    {
      "epoch": 0.046085858585858584,
      "grad_norm": 0.4635412395000458,
      "learning_rate": 0.00015331230283911672,
      "loss": 0.9625,
      "step": 730
    },
    {
      "epoch": 0.04671717171717172,
      "grad_norm": 0.5018364787101746,
      "learning_rate": 0.00015541535226077814,
      "loss": 0.9047,
      "step": 740
    },
    {
      "epoch": 0.04734848484848485,
      "grad_norm": 0.785271167755127,
      "learning_rate": 0.00015751840168243955,
      "loss": 0.9308,
      "step": 750
    },
    {
      "epoch": 0.047979797979797977,
      "grad_norm": 0.40073466300964355,
      "learning_rate": 0.00015962145110410097,
      "loss": 1.0348,
      "step": 760
    },
    {
      "epoch": 0.04861111111111111,
      "grad_norm": 0.42523932456970215,
      "learning_rate": 0.00016172450052576236,
      "loss": 0.9751,
      "step": 770
    },
    {
      "epoch": 0.04924242424242424,
      "grad_norm": 0.47073236107826233,
      "learning_rate": 0.00016382754994742377,
      "loss": 0.9462,
      "step": 780
    },
    {
      "epoch": 0.049873737373737376,
      "grad_norm": 0.5140103101730347,
      "learning_rate": 0.00016593059936908516,
      "loss": 0.8898,
      "step": 790
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 0.7719871997833252,
      "learning_rate": 0.0001680336487907466,
      "loss": 0.9388,
      "step": 800
    },
    {
      "epoch": 0.05113636363636364,
      "grad_norm": 0.40267521142959595,
      "learning_rate": 0.00017013669821240801,
      "loss": 0.972,
      "step": 810
    },
    {
      "epoch": 0.05176767676767677,
      "grad_norm": 0.4087556004524231,
      "learning_rate": 0.0001722397476340694,
      "loss": 0.9786,
      "step": 820
    },
    {
      "epoch": 0.052398989898989896,
      "grad_norm": 0.45229318737983704,
      "learning_rate": 0.00017434279705573082,
      "loss": 0.9305,
      "step": 830
    },
    {
      "epoch": 0.05303030303030303,
      "grad_norm": 0.5276629328727722,
      "learning_rate": 0.00017644584647739223,
      "loss": 0.9125,
      "step": 840
    },
    {
      "epoch": 0.05366161616161616,
      "grad_norm": 0.7445554733276367,
      "learning_rate": 0.00017854889589905365,
      "loss": 0.9355,
      "step": 850
    },
    {
      "epoch": 0.054292929292929296,
      "grad_norm": 0.3979644775390625,
      "learning_rate": 0.00018065194532071506,
      "loss": 0.9907,
      "step": 860
    },
    {
      "epoch": 0.054924242424242424,
      "grad_norm": 0.44829729199409485,
      "learning_rate": 0.00018275499474237645,
      "loss": 0.9574,
      "step": 870
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 0.44900932908058167,
      "learning_rate": 0.00018485804416403786,
      "loss": 0.9373,
      "step": 880
    },
    {
      "epoch": 0.05618686868686869,
      "grad_norm": 0.4998871982097626,
      "learning_rate": 0.00018696109358569928,
      "loss": 0.9219,
      "step": 890
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 0.7167801856994629,
      "learning_rate": 0.0001890641430073607,
      "loss": 0.8876,
      "step": 900
    },
    {
      "epoch": 0.05744949494949495,
      "grad_norm": 0.4088430106639862,
      "learning_rate": 0.0001911671924290221,
      "loss": 0.9739,
      "step": 910
    },
    {
      "epoch": 0.05808080808080808,
      "grad_norm": 0.4394177496433258,
      "learning_rate": 0.0001932702418506835,
      "loss": 0.9804,
      "step": 920
    },
    {
      "epoch": 0.058712121212121215,
      "grad_norm": 0.4614415168762207,
      "learning_rate": 0.0001953732912723449,
      "loss": 0.9021,
      "step": 930
    },
    {
      "epoch": 0.059343434343434344,
      "grad_norm": 0.5111036896705627,
      "learning_rate": 0.00019747634069400632,
      "loss": 0.8827,
      "step": 940
    },
    {
      "epoch": 0.05997474747474747,
      "grad_norm": 0.7363557815551758,
      "learning_rate": 0.00019957939011566774,
      "loss": 0.8885,
      "step": 950
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": 0.39827895164489746,
      "learning_rate": 0.00019999996655333316,
      "loss": 0.9972,
      "step": 960
    },
    {
      "epoch": 0.061237373737373736,
      "grad_norm": 0.4305219054222107,
      "learning_rate": 0.00019999983067628733,
      "loss": 0.9757,
      "step": 970
    },
    {
      "epoch": 0.06186868686868687,
      "grad_norm": 0.4447081685066223,
      "learning_rate": 0.0001999995902785878,
      "loss": 0.9514,
      "step": 980
    },
    {
      "epoch": 0.0625,
      "grad_norm": 0.48579245805740356,
      "learning_rate": 0.0001999992453604858,
      "loss": 0.8882,
      "step": 990
    },
    {
      "epoch": 0.06313131313131314,
      "grad_norm": 0.7275524139404297,
      "learning_rate": 0.00019999879592234187,
      "loss": 0.9124,
      "step": 1000
    },
    {
      "epoch": 0.06313131313131314,
      "eval_loss": 0.9467018246650696,
      "eval_runtime": 27.2337,
      "eval_samples_per_second": 94.001,
      "eval_steps_per_second": 11.75,
      "step": 1000
    },
    {
      "epoch": 0.06376262626262626,
      "grad_norm": 0.40324586629867554,
      "learning_rate": 0.00019999824196462578,
      "loss": 1.0089,
      "step": 1010
    },
    {
      "epoch": 0.06439393939393939,
      "grad_norm": 0.42818593978881836,
      "learning_rate": 0.00019999758348791647,
      "loss": 0.9418,
      "step": 1020
    },
    {
      "epoch": 0.06502525252525253,
      "grad_norm": 0.43285858631134033,
      "learning_rate": 0.00019999682049290227,
      "loss": 0.9111,
      "step": 1030
    },
    {
      "epoch": 0.06565656565656566,
      "grad_norm": 0.49966171383857727,
      "learning_rate": 0.00019999595298038057,
      "loss": 0.8638,
      "step": 1040
    },
    {
      "epoch": 0.06628787878787878,
      "grad_norm": 0.7247596383094788,
      "learning_rate": 0.00019999498095125818,
      "loss": 0.8962,
      "step": 1050
    },
    {
      "epoch": 0.06691919191919192,
      "grad_norm": 0.3946726322174072,
      "learning_rate": 0.00019999390440655107,
      "loss": 0.9597,
      "step": 1060
    },
    {
      "epoch": 0.06755050505050506,
      "grad_norm": 0.4311758577823639,
      "learning_rate": 0.00019999272334738442,
      "loss": 0.9449,
      "step": 1070
    },
    {
      "epoch": 0.06818181818181818,
      "grad_norm": 0.4666595458984375,
      "learning_rate": 0.0001999914377749927,
      "loss": 0.9137,
      "step": 1080
    },
    {
      "epoch": 0.06881313131313131,
      "grad_norm": 0.4912818372249603,
      "learning_rate": 0.00019999004769071955,
      "loss": 0.8788,
      "step": 1090
    },
    {
      "epoch": 0.06944444444444445,
      "grad_norm": 0.7350306510925293,
      "learning_rate": 0.00019998855309601797,
      "loss": 0.873,
      "step": 1100
    },
    {
      "epoch": 0.07007575757575757,
      "grad_norm": 0.4092772305011749,
      "learning_rate": 0.00019998695399245012,
      "loss": 1.0458,
      "step": 1110
    },
    {
      "epoch": 0.0707070707070707,
      "grad_norm": 0.4206644892692566,
      "learning_rate": 0.00019998525038168735,
      "loss": 0.957,
      "step": 1120
    },
    {
      "epoch": 0.07133838383838384,
      "grad_norm": 0.502644956111908,
      "learning_rate": 0.00019998344226551028,
      "loss": 0.8965,
      "step": 1130
    },
    {
      "epoch": 0.07196969696969698,
      "grad_norm": 0.5151838660240173,
      "learning_rate": 0.00019998152964580883,
      "loss": 0.8693,
      "step": 1140
    },
    {
      "epoch": 0.0726010101010101,
      "grad_norm": 0.7897934317588806,
      "learning_rate": 0.00019997951252458205,
      "loss": 0.9066,
      "step": 1150
    },
    {
      "epoch": 0.07323232323232323,
      "grad_norm": 0.3789682686328888,
      "learning_rate": 0.00019997739090393823,
      "loss": 0.9943,
      "step": 1160
    },
    {
      "epoch": 0.07386363636363637,
      "grad_norm": 0.43282124400138855,
      "learning_rate": 0.00019997516478609498,
      "loss": 0.9281,
      "step": 1170
    },
    {
      "epoch": 0.07449494949494949,
      "grad_norm": 0.4386129677295685,
      "learning_rate": 0.00019997283417337895,
      "loss": 0.8972,
      "step": 1180
    },
    {
      "epoch": 0.07512626262626262,
      "grad_norm": 0.4949673116207123,
      "learning_rate": 0.00019997039906822624,
      "loss": 0.867,
      "step": 1190
    },
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 0.7160463333129883,
      "learning_rate": 0.00019996785947318192,
      "loss": 0.8943,
      "step": 1200
    },
    {
      "epoch": 0.0763888888888889,
      "grad_norm": 0.42326855659484863,
      "learning_rate": 0.00019996521539090046,
      "loss": 1.0329,
      "step": 1210
    },
    {
      "epoch": 0.07702020202020202,
      "grad_norm": 0.416482537984848,
      "learning_rate": 0.00019996246682414548,
      "loss": 0.9281,
      "step": 1220
    },
    {
      "epoch": 0.07765151515151515,
      "grad_norm": 0.48489615321159363,
      "learning_rate": 0.0001999596137757898,
      "loss": 0.9044,
      "step": 1230
    },
    {
      "epoch": 0.07828282828282829,
      "grad_norm": 0.5203187465667725,
      "learning_rate": 0.0001999566562488154,
      "loss": 0.8642,
      "step": 1240
    },
    {
      "epoch": 0.07891414141414141,
      "grad_norm": 0.759473443031311,
      "learning_rate": 0.0001999535942463136,
      "loss": 0.8423,
      "step": 1250
    },
    {
      "epoch": 0.07954545454545454,
      "grad_norm": 0.3947957456111908,
      "learning_rate": 0.00019995042777148477,
      "loss": 0.9748,
      "step": 1260
    },
    {
      "epoch": 0.08017676767676768,
      "grad_norm": 0.447480171918869,
      "learning_rate": 0.00019994715682763854,
      "loss": 0.9358,
      "step": 1270
    },
    {
      "epoch": 0.08080808080808081,
      "grad_norm": 0.45288851857185364,
      "learning_rate": 0.00019994378141819378,
      "loss": 0.8749,
      "step": 1280
    },
    {
      "epoch": 0.08143939393939394,
      "grad_norm": 0.5199007391929626,
      "learning_rate": 0.0001999403015466784,
      "loss": 0.8671,
      "step": 1290
    },
    {
      "epoch": 0.08207070707070707,
      "grad_norm": 0.8685715794563293,
      "learning_rate": 0.0001999367172167297,
      "loss": 0.8648,
      "step": 1300
    },
    {
      "epoch": 0.08270202020202021,
      "grad_norm": 0.379661500453949,
      "learning_rate": 0.00019993302843209393,
      "loss": 0.9641,
      "step": 1310
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.4346831440925598,
      "learning_rate": 0.00019992923519662674,
      "loss": 0.9395,
      "step": 1320
    },
    {
      "epoch": 0.08396464646464646,
      "grad_norm": 0.49057310819625854,
      "learning_rate": 0.00019992533751429282,
      "loss": 0.8662,
      "step": 1330
    },
    {
      "epoch": 0.0845959595959596,
      "grad_norm": 0.5500571727752686,
      "learning_rate": 0.00019992133538916608,
      "loss": 0.8518,
      "step": 1340
    },
    {
      "epoch": 0.08522727272727272,
      "grad_norm": 0.7858998775482178,
      "learning_rate": 0.0001999172288254295,
      "loss": 0.8731,
      "step": 1350
    },
    {
      "epoch": 0.08585858585858586,
      "grad_norm": 0.40742629766464233,
      "learning_rate": 0.00019991301782737537,
      "loss": 0.9738,
      "step": 1360
    },
    {
      "epoch": 0.08648989898989899,
      "grad_norm": 0.44539931416511536,
      "learning_rate": 0.00019990870239940502,
      "loss": 0.926,
      "step": 1370
    },
    {
      "epoch": 0.08712121212121213,
      "grad_norm": 0.5004927515983582,
      "learning_rate": 0.000199904282546029,
      "loss": 0.8581,
      "step": 1380
    },
    {
      "epoch": 0.08775252525252525,
      "grad_norm": 0.5613665580749512,
      "learning_rate": 0.00019989975827186695,
      "loss": 0.835,
      "step": 1390
    },
    {
      "epoch": 0.08838383838383838,
      "grad_norm": 0.7562660574913025,
      "learning_rate": 0.00019989512958164772,
      "loss": 0.8334,
      "step": 1400
    },
    {
      "epoch": 0.08901515151515152,
      "grad_norm": 0.40560752153396606,
      "learning_rate": 0.0001998903964802092,
      "loss": 0.9439,
      "step": 1410
    },
    {
      "epoch": 0.08964646464646464,
      "grad_norm": 0.48586180806159973,
      "learning_rate": 0.0001998855589724985,
      "loss": 0.9081,
      "step": 1420
    },
    {
      "epoch": 0.09027777777777778,
      "grad_norm": 0.5029296875,
      "learning_rate": 0.0001998806170635718,
      "loss": 0.866,
      "step": 1430
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.5713523626327515,
      "learning_rate": 0.0001998755707585945,
      "loss": 0.8891,
      "step": 1440
    },
    {
      "epoch": 0.09154040404040405,
      "grad_norm": 0.8226764798164368,
      "learning_rate": 0.00019987042006284095,
      "loss": 0.8434,
      "step": 1450
    },
    {
      "epoch": 0.09217171717171717,
      "grad_norm": 0.4211214780807495,
      "learning_rate": 0.00019986516498169473,
      "loss": 0.9317,
      "step": 1460
    },
    {
      "epoch": 0.0928030303030303,
      "grad_norm": 0.49744337797164917,
      "learning_rate": 0.0001998598055206485,
      "loss": 0.9114,
      "step": 1470
    },
    {
      "epoch": 0.09343434343434344,
      "grad_norm": 0.5054945945739746,
      "learning_rate": 0.00019985434168530398,
      "loss": 0.873,
      "step": 1480
    },
    {
      "epoch": 0.09406565656565656,
      "grad_norm": 0.5643303394317627,
      "learning_rate": 0.0001998487734813721,
      "loss": 0.8299,
      "step": 1490
    },
    {
      "epoch": 0.0946969696969697,
      "grad_norm": 0.6976872086524963,
      "learning_rate": 0.0001998431009146727,
      "loss": 0.8433,
      "step": 1500
    },
    {
      "epoch": 0.09532828282828283,
      "grad_norm": 0.4267701208591461,
      "learning_rate": 0.00019983732399113483,
      "loss": 0.9523,
      "step": 1510
    },
    {
      "epoch": 0.09595959595959595,
      "grad_norm": 0.4605022072792053,
      "learning_rate": 0.0001998314427167966,
      "loss": 0.9245,
      "step": 1520
    },
    {
      "epoch": 0.09659090909090909,
      "grad_norm": 0.518143355846405,
      "learning_rate": 0.00019982545709780515,
      "loss": 0.8612,
      "step": 1530
    },
    {
      "epoch": 0.09722222222222222,
      "grad_norm": 0.6061258912086487,
      "learning_rate": 0.00019981936714041668,
      "loss": 0.8259,
      "step": 1540
    },
    {
      "epoch": 0.09785353535353536,
      "grad_norm": 0.7591531872749329,
      "learning_rate": 0.00019981317285099653,
      "loss": 0.8042,
      "step": 1550
    },
    {
      "epoch": 0.09848484848484848,
      "grad_norm": 0.3919331133365631,
      "learning_rate": 0.0001998068742360189,
      "loss": 0.9391,
      "step": 1560
    },
    {
      "epoch": 0.09911616161616162,
      "grad_norm": 0.5004214644432068,
      "learning_rate": 0.00019980047130206728,
      "loss": 0.9158,
      "step": 1570
    },
    {
      "epoch": 0.09974747474747475,
      "grad_norm": 0.5116076469421387,
      "learning_rate": 0.000199793964055834,
      "loss": 0.8546,
      "step": 1580
    },
    {
      "epoch": 0.10037878787878787,
      "grad_norm": 0.591215968132019,
      "learning_rate": 0.0001997873525041205,
      "loss": 0.8346,
      "step": 1590
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 0.8753274083137512,
      "learning_rate": 0.00019978063665383725,
      "loss": 0.8634,
      "step": 1600
    },
    {
      "epoch": 0.10164141414141414,
      "grad_norm": 0.43350327014923096,
      "learning_rate": 0.0001997738165120037,
      "loss": 0.9331,
      "step": 1610
    },
    {
      "epoch": 0.10227272727272728,
      "grad_norm": 0.5097168684005737,
      "learning_rate": 0.00019976689208574827,
      "loss": 0.9076,
      "step": 1620
    },
    {
      "epoch": 0.1029040404040404,
      "grad_norm": 0.5247926115989685,
      "learning_rate": 0.00019975986338230853,
      "loss": 0.8207,
      "step": 1630
    },
    {
      "epoch": 0.10353535353535354,
      "grad_norm": 0.6446250677108765,
      "learning_rate": 0.00019975273040903087,
      "loss": 0.8078,
      "step": 1640
    },
    {
      "epoch": 0.10416666666666667,
      "grad_norm": 0.8003672361373901,
      "learning_rate": 0.00019974549317337075,
      "loss": 0.8195,
      "step": 1650
    },
    {
      "epoch": 0.10479797979797979,
      "grad_norm": 0.4129176139831543,
      "learning_rate": 0.00019973815168289257,
      "loss": 0.9432,
      "step": 1660
    },
    {
      "epoch": 0.10542929292929293,
      "grad_norm": 0.45286381244659424,
      "learning_rate": 0.00019973070594526973,
      "loss": 0.9106,
      "step": 1670
    },
    {
      "epoch": 0.10606060606060606,
      "grad_norm": 0.5159316062927246,
      "learning_rate": 0.00019972315596828458,
      "loss": 0.8676,
      "step": 1680
    },
    {
      "epoch": 0.10669191919191919,
      "grad_norm": 0.5524932742118835,
      "learning_rate": 0.0001997155017598284,
      "loss": 0.8116,
      "step": 1690
    },
    {
      "epoch": 0.10732323232323232,
      "grad_norm": 0.8455977439880371,
      "learning_rate": 0.0001997077433279015,
      "loss": 0.8098,
      "step": 1700
    },
    {
      "epoch": 0.10795454545454546,
      "grad_norm": 0.42986762523651123,
      "learning_rate": 0.00019969988068061295,
      "loss": 0.9245,
      "step": 1710
    },
    {
      "epoch": 0.10858585858585859,
      "grad_norm": 0.49853822588920593,
      "learning_rate": 0.00019969191382618095,
      "loss": 0.8926,
      "step": 1720
    },
    {
      "epoch": 0.10921717171717171,
      "grad_norm": 0.5442687273025513,
      "learning_rate": 0.00019968384277293247,
      "loss": 0.8423,
      "step": 1730
    },
    {
      "epoch": 0.10984848484848485,
      "grad_norm": 0.5966998934745789,
      "learning_rate": 0.00019967566752930347,
      "loss": 0.8233,
      "step": 1740
    },
    {
      "epoch": 0.11047979797979798,
      "grad_norm": 0.827552080154419,
      "learning_rate": 0.00019966738810383875,
      "loss": 0.7682,
      "step": 1750
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.43787339329719543,
      "learning_rate": 0.00019965900450519207,
      "loss": 0.9355,
      "step": 1760
    },
    {
      "epoch": 0.11174242424242424,
      "grad_norm": 0.4841938614845276,
      "learning_rate": 0.000199650516742126,
      "loss": 0.8863,
      "step": 1770
    },
    {
      "epoch": 0.11237373737373738,
      "grad_norm": 0.5904253125190735,
      "learning_rate": 0.00019964192482351204,
      "loss": 0.8489,
      "step": 1780
    },
    {
      "epoch": 0.11300505050505051,
      "grad_norm": 0.6222173571586609,
      "learning_rate": 0.00019963322875833056,
      "loss": 0.8154,
      "step": 1790
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 0.8397778868675232,
      "learning_rate": 0.0001996244285556707,
      "loss": 0.7851,
      "step": 1800
    },
    {
      "epoch": 0.11426767676767677,
      "grad_norm": 0.43562033772468567,
      "learning_rate": 0.00019961552422473057,
      "loss": 0.9448,
      "step": 1810
    },
    {
      "epoch": 0.1148989898989899,
      "grad_norm": 0.49730274081230164,
      "learning_rate": 0.000199606515774817,
      "loss": 0.914,
      "step": 1820
    },
    {
      "epoch": 0.11553030303030302,
      "grad_norm": 0.5812826752662659,
      "learning_rate": 0.00019959740321534572,
      "loss": 0.8188,
      "step": 1830
    },
    {
      "epoch": 0.11616161616161616,
      "grad_norm": 0.6531727910041809,
      "learning_rate": 0.00019958818655584125,
      "loss": 0.7757,
      "step": 1840
    },
    {
      "epoch": 0.1167929292929293,
      "grad_norm": 0.7923195362091064,
      "learning_rate": 0.0001995788658059369,
      "loss": 0.7818,
      "step": 1850
    },
    {
      "epoch": 0.11742424242424243,
      "grad_norm": 0.45921364426612854,
      "learning_rate": 0.00019956944097537484,
      "loss": 0.8977,
      "step": 1860
    },
    {
      "epoch": 0.11805555555555555,
      "grad_norm": 0.4942459166049957,
      "learning_rate": 0.00019955991207400595,
      "loss": 0.8633,
      "step": 1870
    },
    {
      "epoch": 0.11868686868686869,
      "grad_norm": 0.550398588180542,
      "learning_rate": 0.0001995502791117899,
      "loss": 0.8287,
      "step": 1880
    },
    {
      "epoch": 0.11931818181818182,
      "grad_norm": 0.5691207647323608,
      "learning_rate": 0.0001995405420987952,
      "loss": 0.7913,
      "step": 1890
    },
    {
      "epoch": 0.11994949494949494,
      "grad_norm": 0.8962611556053162,
      "learning_rate": 0.000199530701045199,
      "loss": 0.7856,
      "step": 1900
    },
    {
      "epoch": 0.12058080808080808,
      "grad_norm": 0.4272255301475525,
      "learning_rate": 0.00019952075596128726,
      "loss": 0.9429,
      "step": 1910
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 0.5055891871452332,
      "learning_rate": 0.0001995107068574547,
      "loss": 0.8689,
      "step": 1920
    },
    {
      "epoch": 0.12184343434343434,
      "grad_norm": 0.5636619925498962,
      "learning_rate": 0.00019950055374420468,
      "loss": 0.8015,
      "step": 1930
    },
    {
      "epoch": 0.12247474747474747,
      "grad_norm": 0.5823515057563782,
      "learning_rate": 0.00019949029663214937,
      "loss": 0.7786,
      "step": 1940
    },
    {
      "epoch": 0.12310606060606061,
      "grad_norm": 0.8790888786315918,
      "learning_rate": 0.00019947993553200955,
      "loss": 0.7672,
      "step": 1950
    },
    {
      "epoch": 0.12373737373737374,
      "grad_norm": 0.4872295558452606,
      "learning_rate": 0.00019946947045461472,
      "loss": 0.9216,
      "step": 1960
    },
    {
      "epoch": 0.12436868686868686,
      "grad_norm": 0.5439364314079285,
      "learning_rate": 0.00019945890141090307,
      "loss": 0.8626,
      "step": 1970
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.6327049732208252,
      "learning_rate": 0.0001994482284119215,
      "loss": 0.8191,
      "step": 1980
    },
    {
      "epoch": 0.12563131313131312,
      "grad_norm": 0.6687145233154297,
      "learning_rate": 0.0001994374514688255,
      "loss": 0.7795,
      "step": 1990
    },
    {
      "epoch": 0.12626262626262627,
      "grad_norm": 0.9320880174636841,
      "learning_rate": 0.00019942657059287918,
      "loss": 0.7792,
      "step": 2000
    },
    {
      "epoch": 0.12626262626262627,
      "eval_loss": 0.8338238000869751,
      "eval_runtime": 27.276,
      "eval_samples_per_second": 93.855,
      "eval_steps_per_second": 11.732,
      "step": 2000
    },
    {
      "epoch": 0.1268939393939394,
      "grad_norm": 0.4458157420158386,
      "learning_rate": 0.00019941558579545534,
      "loss": 0.9383,
      "step": 2010
    },
    {
      "epoch": 0.1275252525252525,
      "grad_norm": 0.5609799027442932,
      "learning_rate": 0.00019940449708803537,
      "loss": 0.8643,
      "step": 2020
    },
    {
      "epoch": 0.12815656565656566,
      "grad_norm": 0.5868834853172302,
      "learning_rate": 0.00019939330448220932,
      "loss": 0.838,
      "step": 2030
    },
    {
      "epoch": 0.12878787878787878,
      "grad_norm": 0.5802881121635437,
      "learning_rate": 0.00019938200798967577,
      "loss": 0.7569,
      "step": 2040
    },
    {
      "epoch": 0.1294191919191919,
      "grad_norm": 0.8532348275184631,
      "learning_rate": 0.00019937060762224192,
      "loss": 0.7822,
      "step": 2050
    },
    {
      "epoch": 0.13005050505050506,
      "grad_norm": 0.44485417008399963,
      "learning_rate": 0.00019935910339182348,
      "loss": 0.9307,
      "step": 2060
    },
    {
      "epoch": 0.13068181818181818,
      "grad_norm": 0.5271552205085754,
      "learning_rate": 0.00019934749531044484,
      "loss": 0.8602,
      "step": 2070
    },
    {
      "epoch": 0.13131313131313133,
      "grad_norm": 0.5830236077308655,
      "learning_rate": 0.0001993357833902388,
      "loss": 0.7825,
      "step": 2080
    },
    {
      "epoch": 0.13194444444444445,
      "grad_norm": 0.6748720407485962,
      "learning_rate": 0.0001993239676434468,
      "loss": 0.7441,
      "step": 2090
    },
    {
      "epoch": 0.13257575757575757,
      "grad_norm": 0.8696335554122925,
      "learning_rate": 0.00019931204808241873,
      "loss": 0.7536,
      "step": 2100
    },
    {
      "epoch": 0.13320707070707072,
      "grad_norm": 0.4964369237422943,
      "learning_rate": 0.00019930002471961301,
      "loss": 0.9012,
      "step": 2110
    },
    {
      "epoch": 0.13383838383838384,
      "grad_norm": 0.5407465696334839,
      "learning_rate": 0.00019928789756759661,
      "loss": 0.8672,
      "step": 2120
    },
    {
      "epoch": 0.13446969696969696,
      "grad_norm": 0.6225587725639343,
      "learning_rate": 0.00019927566663904486,
      "loss": 0.7965,
      "step": 2130
    },
    {
      "epoch": 0.1351010101010101,
      "grad_norm": 0.671110212802887,
      "learning_rate": 0.0001992633319467417,
      "loss": 0.7521,
      "step": 2140
    },
    {
      "epoch": 0.13573232323232323,
      "grad_norm": 0.9717885851860046,
      "learning_rate": 0.00019925089350357938,
      "loss": 0.7638,
      "step": 2150
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 0.4905236065387726,
      "learning_rate": 0.00019923835132255874,
      "loss": 0.9222,
      "step": 2160
    },
    {
      "epoch": 0.1369949494949495,
      "grad_norm": 0.5381935834884644,
      "learning_rate": 0.00019922570541678887,
      "loss": 0.8454,
      "step": 2170
    },
    {
      "epoch": 0.13762626262626262,
      "grad_norm": 0.5398241281509399,
      "learning_rate": 0.00019921295579948748,
      "loss": 0.8094,
      "step": 2180
    },
    {
      "epoch": 0.13825757575757575,
      "grad_norm": 1.054864764213562,
      "learning_rate": 0.00019920010248398052,
      "loss": 0.7368,
      "step": 2190
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 0.9540656805038452,
      "learning_rate": 0.0001991871454837024,
      "loss": 0.7676,
      "step": 2200
    },
    {
      "epoch": 0.13952020202020202,
      "grad_norm": 0.5193989276885986,
      "learning_rate": 0.00019917408481219585,
      "loss": 0.8816,
      "step": 2210
    },
    {
      "epoch": 0.14015151515151514,
      "grad_norm": 0.5837627053260803,
      "learning_rate": 0.00019916092048311205,
      "loss": 0.8781,
      "step": 2220
    },
    {
      "epoch": 0.1407828282828283,
      "grad_norm": 0.6785120368003845,
      "learning_rate": 0.0001991476525102104,
      "loss": 0.8121,
      "step": 2230
    },
    {
      "epoch": 0.1414141414141414,
      "grad_norm": 0.6777743697166443,
      "learning_rate": 0.00019913428090735877,
      "loss": 0.7435,
      "step": 2240
    },
    {
      "epoch": 0.14204545454545456,
      "grad_norm": 1.073117733001709,
      "learning_rate": 0.00019912080568853323,
      "loss": 0.758,
      "step": 2250
    },
    {
      "epoch": 0.14267676767676768,
      "grad_norm": 0.4643658995628357,
      "learning_rate": 0.0001991072268678182,
      "loss": 0.9034,
      "step": 2260
    },
    {
      "epoch": 0.1433080808080808,
      "grad_norm": 0.5185081362724304,
      "learning_rate": 0.00019909354445940634,
      "loss": 0.8541,
      "step": 2270
    },
    {
      "epoch": 0.14393939393939395,
      "grad_norm": 0.6076844930648804,
      "learning_rate": 0.00019907975847759866,
      "loss": 0.7912,
      "step": 2280
    },
    {
      "epoch": 0.14457070707070707,
      "grad_norm": 0.6883999109268188,
      "learning_rate": 0.00019906586893680438,
      "loss": 0.744,
      "step": 2290
    },
    {
      "epoch": 0.1452020202020202,
      "grad_norm": 0.9529832005500793,
      "learning_rate": 0.00019905187585154095,
      "loss": 0.7741,
      "step": 2300
    },
    {
      "epoch": 0.14583333333333334,
      "grad_norm": 0.4924416244029999,
      "learning_rate": 0.00019903777923643406,
      "loss": 0.9186,
      "step": 2310
    },
    {
      "epoch": 0.14646464646464646,
      "grad_norm": 0.5256723165512085,
      "learning_rate": 0.00019902357910621762,
      "loss": 0.8366,
      "step": 2320
    },
    {
      "epoch": 0.14709595959595959,
      "grad_norm": 0.5781369209289551,
      "learning_rate": 0.00019900927547573366,
      "loss": 0.7758,
      "step": 2330
    },
    {
      "epoch": 0.14772727272727273,
      "grad_norm": 0.6617695689201355,
      "learning_rate": 0.00019899486835993257,
      "loss": 0.7545,
      "step": 2340
    },
    {
      "epoch": 0.14835858585858586,
      "grad_norm": 1.0167388916015625,
      "learning_rate": 0.0001989803577738727,
      "loss": 0.7427,
      "step": 2350
    },
    {
      "epoch": 0.14898989898989898,
      "grad_norm": 0.46141016483306885,
      "learning_rate": 0.00019896574373272065,
      "loss": 0.9331,
      "step": 2360
    },
    {
      "epoch": 0.14962121212121213,
      "grad_norm": 0.5681381821632385,
      "learning_rate": 0.00019895102625175118,
      "loss": 0.818,
      "step": 2370
    },
    {
      "epoch": 0.15025252525252525,
      "grad_norm": 0.6212285757064819,
      "learning_rate": 0.00019893620534634706,
      "loss": 0.7782,
      "step": 2380
    },
    {
      "epoch": 0.15088383838383837,
      "grad_norm": 0.7176325917243958,
      "learning_rate": 0.00019892128103199928,
      "loss": 0.7306,
      "step": 2390
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 0.991489827632904,
      "learning_rate": 0.0001989062533243068,
      "loss": 0.7485,
      "step": 2400
    },
    {
      "epoch": 0.15214646464646464,
      "grad_norm": 0.5086601376533508,
      "learning_rate": 0.00019889112223897676,
      "loss": 0.9004,
      "step": 2410
    },
    {
      "epoch": 0.1527777777777778,
      "grad_norm": 0.5726848244667053,
      "learning_rate": 0.0001988758877918243,
      "loss": 0.8408,
      "step": 2420
    },
    {
      "epoch": 0.1534090909090909,
      "grad_norm": 0.637711226940155,
      "learning_rate": 0.0001988605499987725,
      "loss": 0.7949,
      "step": 2430
    },
    {
      "epoch": 0.15404040404040403,
      "grad_norm": 0.71788489818573,
      "learning_rate": 0.00019884510887585263,
      "loss": 0.7077,
      "step": 2440
    },
    {
      "epoch": 0.15467171717171718,
      "grad_norm": 0.9684906005859375,
      "learning_rate": 0.00019882956443920388,
      "loss": 0.7392,
      "step": 2450
    },
    {
      "epoch": 0.1553030303030303,
      "grad_norm": 0.4676392376422882,
      "learning_rate": 0.00019881391670507342,
      "loss": 0.9023,
      "step": 2460
    },
    {
      "epoch": 0.15593434343434343,
      "grad_norm": 0.5791764855384827,
      "learning_rate": 0.00019879816568981636,
      "loss": 0.8158,
      "step": 2470
    },
    {
      "epoch": 0.15656565656565657,
      "grad_norm": 0.674104630947113,
      "learning_rate": 0.0001987823114098958,
      "loss": 0.7772,
      "step": 2480
    },
    {
      "epoch": 0.1571969696969697,
      "grad_norm": 0.7449119091033936,
      "learning_rate": 0.0001987663538818828,
      "loss": 0.7139,
      "step": 2490
    },
    {
      "epoch": 0.15782828282828282,
      "grad_norm": 0.9963280558586121,
      "learning_rate": 0.00019875029312245625,
      "loss": 0.7251,
      "step": 2500
    },
    {
      "epoch": 0.15845959595959597,
      "grad_norm": 0.4839588403701782,
      "learning_rate": 0.00019873412914840304,
      "loss": 0.9355,
      "step": 2510
    },
    {
      "epoch": 0.1590909090909091,
      "grad_norm": 0.5863730311393738,
      "learning_rate": 0.00019871786197661785,
      "loss": 0.8123,
      "step": 2520
    },
    {
      "epoch": 0.1597222222222222,
      "grad_norm": 0.6110161542892456,
      "learning_rate": 0.00019870149162410326,
      "loss": 0.7631,
      "step": 2530
    },
    {
      "epoch": 0.16035353535353536,
      "grad_norm": 0.7272172570228577,
      "learning_rate": 0.00019868501810796975,
      "loss": 0.7259,
      "step": 2540
    },
    {
      "epoch": 0.16098484848484848,
      "grad_norm": 0.9596837759017944,
      "learning_rate": 0.00019866844144543553,
      "loss": 0.7127,
      "step": 2550
    },
    {
      "epoch": 0.16161616161616163,
      "grad_norm": 0.5496588945388794,
      "learning_rate": 0.0001986517616538267,
      "loss": 0.8754,
      "step": 2560
    },
    {
      "epoch": 0.16224747474747475,
      "grad_norm": 0.5752465128898621,
      "learning_rate": 0.00019863497875057705,
      "loss": 0.8076,
      "step": 2570
    },
    {
      "epoch": 0.16287878787878787,
      "grad_norm": 0.6438614726066589,
      "learning_rate": 0.00019861809275322826,
      "loss": 0.7749,
      "step": 2580
    },
    {
      "epoch": 0.16351010101010102,
      "grad_norm": 0.8031302094459534,
      "learning_rate": 0.00019860110367942975,
      "loss": 0.692,
      "step": 2590
    },
    {
      "epoch": 0.16414141414141414,
      "grad_norm": 0.9156016111373901,
      "learning_rate": 0.00019858401154693858,
      "loss": 0.7,
      "step": 2600
    },
    {
      "epoch": 0.16477272727272727,
      "grad_norm": 0.496735155582428,
      "learning_rate": 0.0001985668163736196,
      "loss": 0.9039,
      "step": 2610
    },
    {
      "epoch": 0.16540404040404041,
      "grad_norm": 0.5905826687812805,
      "learning_rate": 0.00019854951817744536,
      "loss": 0.8377,
      "step": 2620
    },
    {
      "epoch": 0.16603535353535354,
      "grad_norm": 0.6868301033973694,
      "learning_rate": 0.00019853211697649608,
      "loss": 0.7556,
      "step": 2630
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.7296802401542664,
      "learning_rate": 0.0001985146127889596,
      "loss": 0.6795,
      "step": 2640
    },
    {
      "epoch": 0.1672979797979798,
      "grad_norm": 1.0678026676177979,
      "learning_rate": 0.00019849700563313153,
      "loss": 0.7152,
      "step": 2650
    },
    {
      "epoch": 0.16792929292929293,
      "grad_norm": 0.4883025586605072,
      "learning_rate": 0.00019847929552741495,
      "loss": 0.8714,
      "step": 2660
    },
    {
      "epoch": 0.16856060606060605,
      "grad_norm": 0.5960721373558044,
      "learning_rate": 0.00019846148249032063,
      "loss": 0.8231,
      "step": 2670
    },
    {
      "epoch": 0.1691919191919192,
      "grad_norm": 0.64022296667099,
      "learning_rate": 0.00019844356654046688,
      "loss": 0.7734,
      "step": 2680
    },
    {
      "epoch": 0.16982323232323232,
      "grad_norm": 0.7293738126754761,
      "learning_rate": 0.00019842554769657962,
      "loss": 0.6963,
      "step": 2690
    },
    {
      "epoch": 0.17045454545454544,
      "grad_norm": 1.0133296251296997,
      "learning_rate": 0.0001984074259774923,
      "loss": 0.7068,
      "step": 2700
    },
    {
      "epoch": 0.1710858585858586,
      "grad_norm": 0.4781786799430847,
      "learning_rate": 0.00019838920140214587,
      "loss": 0.8699,
      "step": 2710
    },
    {
      "epoch": 0.1717171717171717,
      "grad_norm": 0.5547852516174316,
      "learning_rate": 0.00019837087398958883,
      "loss": 0.817,
      "step": 2720
    },
    {
      "epoch": 0.17234848484848486,
      "grad_norm": 0.6851038932800293,
      "learning_rate": 0.00019835244375897713,
      "loss": 0.7674,
      "step": 2730
    },
    {
      "epoch": 0.17297979797979798,
      "grad_norm": 0.8076152205467224,
      "learning_rate": 0.00019833391072957422,
      "loss": 0.7121,
      "step": 2740
    },
    {
      "epoch": 0.1736111111111111,
      "grad_norm": 1.0549875497817993,
      "learning_rate": 0.00019831527492075092,
      "loss": 0.6929,
      "step": 2750
    },
    {
      "epoch": 0.17424242424242425,
      "grad_norm": 0.5288456082344055,
      "learning_rate": 0.00019829653635198563,
      "loss": 0.8682,
      "step": 2760
    },
    {
      "epoch": 0.17487373737373738,
      "grad_norm": 0.6235291361808777,
      "learning_rate": 0.00019827769504286396,
      "loss": 0.8012,
      "step": 2770
    },
    {
      "epoch": 0.1755050505050505,
      "grad_norm": 0.6597782969474792,
      "learning_rate": 0.00019825875101307905,
      "loss": 0.766,
      "step": 2780
    },
    {
      "epoch": 0.17613636363636365,
      "grad_norm": 0.7904951572418213,
      "learning_rate": 0.00019823970428243135,
      "loss": 0.712,
      "step": 2790
    },
    {
      "epoch": 0.17676767676767677,
      "grad_norm": 1.2259427309036255,
      "learning_rate": 0.00019822055487082866,
      "loss": 0.7038,
      "step": 2800
    },
    {
      "epoch": 0.1773989898989899,
      "grad_norm": 0.5086365342140198,
      "learning_rate": 0.00019820130279828613,
      "loss": 0.8943,
      "step": 2810
    },
    {
      "epoch": 0.17803030303030304,
      "grad_norm": 0.5791149139404297,
      "learning_rate": 0.00019818194808492615,
      "loss": 0.8077,
      "step": 2820
    },
    {
      "epoch": 0.17866161616161616,
      "grad_norm": 0.7340189218521118,
      "learning_rate": 0.00019816249075097845,
      "loss": 0.7248,
      "step": 2830
    },
    {
      "epoch": 0.17929292929292928,
      "grad_norm": 0.8152987360954285,
      "learning_rate": 0.00019814293081677994,
      "loss": 0.677,
      "step": 2840
    },
    {
      "epoch": 0.17992424242424243,
      "grad_norm": 1.044371485710144,
      "learning_rate": 0.0001981232683027749,
      "loss": 0.6892,
      "step": 2850
    },
    {
      "epoch": 0.18055555555555555,
      "grad_norm": 0.49501755833625793,
      "learning_rate": 0.00019810350322951474,
      "loss": 0.8991,
      "step": 2860
    },
    {
      "epoch": 0.18118686868686867,
      "grad_norm": 0.602576732635498,
      "learning_rate": 0.00019808363561765806,
      "loss": 0.7898,
      "step": 2870
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.6292011737823486,
      "learning_rate": 0.00019806366548797067,
      "loss": 0.756,
      "step": 2880
    },
    {
      "epoch": 0.18244949494949494,
      "grad_norm": 0.834084153175354,
      "learning_rate": 0.00019804359286132548,
      "loss": 0.6917,
      "step": 2890
    },
    {
      "epoch": 0.1830808080808081,
      "grad_norm": 0.9863002896308899,
      "learning_rate": 0.0001980234177587026,
      "loss": 0.7159,
      "step": 2900
    },
    {
      "epoch": 0.18371212121212122,
      "grad_norm": 0.5218273401260376,
      "learning_rate": 0.00019800314020118918,
      "loss": 0.8608,
      "step": 2910
    },
    {
      "epoch": 0.18434343434343434,
      "grad_norm": 0.5750924944877625,
      "learning_rate": 0.00019798276020997952,
      "loss": 0.7912,
      "step": 2920
    },
    {
      "epoch": 0.1849747474747475,
      "grad_norm": 0.6217162013053894,
      "learning_rate": 0.00019796227780637498,
      "loss": 0.7214,
      "step": 2930
    },
    {
      "epoch": 0.1856060606060606,
      "grad_norm": 0.7451394200325012,
      "learning_rate": 0.0001979416930117839,
      "loss": 0.6686,
      "step": 2940
    },
    {
      "epoch": 0.18623737373737373,
      "grad_norm": 1.078221321105957,
      "learning_rate": 0.00019792100584772166,
      "loss": 0.6891,
      "step": 2950
    },
    {
      "epoch": 0.18686868686868688,
      "grad_norm": 0.5199106335639954,
      "learning_rate": 0.00019790021633581071,
      "loss": 0.8727,
      "step": 2960
    },
    {
      "epoch": 0.1875,
      "grad_norm": 0.6874793767929077,
      "learning_rate": 0.0001978793244977804,
      "loss": 0.7838,
      "step": 2970
    },
    {
      "epoch": 0.18813131313131312,
      "grad_norm": 0.7335466742515564,
      "learning_rate": 0.00019785833035546702,
      "loss": 0.7198,
      "step": 2980
    },
    {
      "epoch": 0.18876262626262627,
      "grad_norm": 0.7366064786911011,
      "learning_rate": 0.00019783723393081387,
      "loss": 0.6815,
      "step": 2990
    },
    {
      "epoch": 0.1893939393939394,
      "grad_norm": 1.0605926513671875,
      "learning_rate": 0.00019781603524587107,
      "loss": 0.6666,
      "step": 3000
    },
    {
      "epoch": 0.1893939393939394,
      "eval_loss": 0.745069682598114,
      "eval_runtime": 27.261,
      "eval_samples_per_second": 93.907,
      "eval_steps_per_second": 11.738,
      "step": 3000
    },
    {
      "epoch": 0.1900252525252525,
      "grad_norm": 0.5447154641151428,
      "learning_rate": 0.00019779473432279568,
      "loss": 0.902,
      "step": 3010
    },
    {
      "epoch": 0.19065656565656566,
      "grad_norm": 0.6627722382545471,
      "learning_rate": 0.00019777333118385163,
      "loss": 0.7788,
      "step": 3020
    },
    {
      "epoch": 0.19128787878787878,
      "grad_norm": 0.6715835332870483,
      "learning_rate": 0.00019775182585140958,
      "loss": 0.7167,
      "step": 3030
    },
    {
      "epoch": 0.1919191919191919,
      "grad_norm": 0.7608147859573364,
      "learning_rate": 0.0001977302183479472,
      "loss": 0.6747,
      "step": 3040
    },
    {
      "epoch": 0.19255050505050506,
      "grad_norm": 1.015800952911377,
      "learning_rate": 0.00019770850869604872,
      "loss": 0.6537,
      "step": 3050
    },
    {
      "epoch": 0.19318181818181818,
      "grad_norm": 0.5336358547210693,
      "learning_rate": 0.0001976866969184053,
      "loss": 0.8782,
      "step": 3060
    },
    {
      "epoch": 0.19381313131313133,
      "grad_norm": 0.6922327280044556,
      "learning_rate": 0.00019766478303781475,
      "loss": 0.7973,
      "step": 3070
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 0.6918286681175232,
      "learning_rate": 0.0001976427670771817,
      "loss": 0.721,
      "step": 3080
    },
    {
      "epoch": 0.19507575757575757,
      "grad_norm": 0.7608218789100647,
      "learning_rate": 0.0001976206490595174,
      "loss": 0.6621,
      "step": 3090
    },
    {
      "epoch": 0.19570707070707072,
      "grad_norm": 1.1376641988754272,
      "learning_rate": 0.00019759842900793974,
      "loss": 0.6817,
      "step": 3100
    },
    {
      "epoch": 0.19633838383838384,
      "grad_norm": 0.5511743426322937,
      "learning_rate": 0.00019757610694567338,
      "loss": 0.8621,
      "step": 3110
    },
    {
      "epoch": 0.19696969696969696,
      "grad_norm": 0.6059994101524353,
      "learning_rate": 0.00019755368289604944,
      "loss": 0.7733,
      "step": 3120
    },
    {
      "epoch": 0.1976010101010101,
      "grad_norm": 0.6792340278625488,
      "learning_rate": 0.0001975311568825058,
      "loss": 0.7338,
      "step": 3130
    },
    {
      "epoch": 0.19823232323232323,
      "grad_norm": 0.7740210294723511,
      "learning_rate": 0.00019750852892858677,
      "loss": 0.6456,
      "step": 3140
    },
    {
      "epoch": 0.19886363636363635,
      "grad_norm": 0.9879257678985596,
      "learning_rate": 0.00019748579905794333,
      "loss": 0.6095,
      "step": 3150
    },
    {
      "epoch": 0.1994949494949495,
      "grad_norm": 0.5154120922088623,
      "learning_rate": 0.0001974629672943329,
      "loss": 0.8742,
      "step": 3160
    },
    {
      "epoch": 0.20012626262626262,
      "grad_norm": 0.6250808238983154,
      "learning_rate": 0.00019744003366161942,
      "loss": 0.791,
      "step": 3170
    },
    {
      "epoch": 0.20075757575757575,
      "grad_norm": 0.6933863162994385,
      "learning_rate": 0.00019741699818377333,
      "loss": 0.7154,
      "step": 3180
    },
    {
      "epoch": 0.2013888888888889,
      "grad_norm": 0.8763179779052734,
      "learning_rate": 0.0001973938608848715,
      "loss": 0.6208,
      "step": 3190
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 1.149739146232605,
      "learning_rate": 0.00019737062178909725,
      "loss": 0.659,
      "step": 3200
    },
    {
      "epoch": 0.20265151515151514,
      "grad_norm": 0.5820778012275696,
      "learning_rate": 0.00019734728092074024,
      "loss": 0.8547,
      "step": 3210
    },
    {
      "epoch": 0.2032828282828283,
      "grad_norm": 0.655501127243042,
      "learning_rate": 0.00019732383830419658,
      "loss": 0.7587,
      "step": 3220
    },
    {
      "epoch": 0.2039141414141414,
      "grad_norm": 0.695408284664154,
      "learning_rate": 0.00019730029396396862,
      "loss": 0.7177,
      "step": 3230
    },
    {
      "epoch": 0.20454545454545456,
      "grad_norm": 0.7577276825904846,
      "learning_rate": 0.00019727664792466515,
      "loss": 0.6552,
      "step": 3240
    },
    {
      "epoch": 0.20517676767676768,
      "grad_norm": 0.9298124313354492,
      "learning_rate": 0.0001972529002110012,
      "loss": 0.6579,
      "step": 3250
    },
    {
      "epoch": 0.2058080808080808,
      "grad_norm": 0.5072958469390869,
      "learning_rate": 0.00019722905084779808,
      "loss": 0.8608,
      "step": 3260
    },
    {
      "epoch": 0.20643939393939395,
      "grad_norm": 0.5833141803741455,
      "learning_rate": 0.0001972050998599833,
      "loss": 0.7793,
      "step": 3270
    },
    {
      "epoch": 0.20707070707070707,
      "grad_norm": 0.6601421236991882,
      "learning_rate": 0.00019718104727259073,
      "loss": 0.7038,
      "step": 3280
    },
    {
      "epoch": 0.2077020202020202,
      "grad_norm": 0.8299331665039062,
      "learning_rate": 0.00019715689311076024,
      "loss": 0.6527,
      "step": 3290
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 1.1553014516830444,
      "learning_rate": 0.000197132637399738,
      "loss": 0.6747,
      "step": 3300
    },
    {
      "epoch": 0.20896464646464646,
      "grad_norm": 0.6042424440383911,
      "learning_rate": 0.00019710828016487628,
      "loss": 0.9149,
      "step": 3310
    },
    {
      "epoch": 0.20959595959595959,
      "grad_norm": 0.6279816031455994,
      "learning_rate": 0.00019708382143163343,
      "loss": 0.751,
      "step": 3320
    },
    {
      "epoch": 0.21022727272727273,
      "grad_norm": 0.7412064075469971,
      "learning_rate": 0.000197059261225574,
      "loss": 0.7027,
      "step": 3330
    },
    {
      "epoch": 0.21085858585858586,
      "grad_norm": 0.8499515652656555,
      "learning_rate": 0.00019703459957236844,
      "loss": 0.6421,
      "step": 3340
    },
    {
      "epoch": 0.21148989898989898,
      "grad_norm": 1.0128357410430908,
      "learning_rate": 0.00019700983649779334,
      "loss": 0.5927,
      "step": 3350
    },
    {
      "epoch": 0.21212121212121213,
      "grad_norm": 0.5070518851280212,
      "learning_rate": 0.0001969849720277313,
      "loss": 0.9043,
      "step": 3360
    },
    {
      "epoch": 0.21275252525252525,
      "grad_norm": 0.6357104778289795,
      "learning_rate": 0.0001969600061881708,
      "loss": 0.7667,
      "step": 3370
    },
    {
      "epoch": 0.21338383838383837,
      "grad_norm": 0.6981188058853149,
      "learning_rate": 0.00019693493900520644,
      "loss": 0.7097,
      "step": 3380
    },
    {
      "epoch": 0.21401515151515152,
      "grad_norm": 0.8290572166442871,
      "learning_rate": 0.0001969097705050386,
      "loss": 0.6576,
      "step": 3390
    },
    {
      "epoch": 0.21464646464646464,
      "grad_norm": 0.9649792313575745,
      "learning_rate": 0.00019688450071397357,
      "loss": 0.635,
      "step": 3400
    },
    {
      "epoch": 0.2152777777777778,
      "grad_norm": 0.5470774173736572,
      "learning_rate": 0.00019685912965842359,
      "loss": 0.8291,
      "step": 3410
    },
    {
      "epoch": 0.2159090909090909,
      "grad_norm": 0.6061720252037048,
      "learning_rate": 0.00019683365736490672,
      "loss": 0.744,
      "step": 3420
    },
    {
      "epoch": 0.21654040404040403,
      "grad_norm": 0.7339971661567688,
      "learning_rate": 0.00019680808386004673,
      "loss": 0.6862,
      "step": 3430
    },
    {
      "epoch": 0.21717171717171718,
      "grad_norm": 0.742709219455719,
      "learning_rate": 0.00019678240917057335,
      "loss": 0.6256,
      "step": 3440
    },
    {
      "epoch": 0.2178030303030303,
      "grad_norm": 1.038543701171875,
      "learning_rate": 0.0001967566333233219,
      "loss": 0.632,
      "step": 3450
    },
    {
      "epoch": 0.21843434343434343,
      "grad_norm": 0.6171247959136963,
      "learning_rate": 0.0001967307563452336,
      "loss": 0.8551,
      "step": 3460
    },
    {
      "epoch": 0.21906565656565657,
      "grad_norm": 0.6691005229949951,
      "learning_rate": 0.00019670477826335517,
      "loss": 0.7501,
      "step": 3470
    },
    {
      "epoch": 0.2196969696969697,
      "grad_norm": 0.7246281504631042,
      "learning_rate": 0.00019667869910483922,
      "loss": 0.7102,
      "step": 3480
    },
    {
      "epoch": 0.22032828282828282,
      "grad_norm": 0.8003141283988953,
      "learning_rate": 0.00019665251889694387,
      "loss": 0.6457,
      "step": 3490
    },
    {
      "epoch": 0.22095959595959597,
      "grad_norm": 1.2344048023223877,
      "learning_rate": 0.00019662623766703285,
      "loss": 0.6386,
      "step": 3500
    },
    {
      "epoch": 0.2215909090909091,
      "grad_norm": 0.5584490895271301,
      "learning_rate": 0.00019659985544257557,
      "loss": 0.8473,
      "step": 3510
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.7575691342353821,
      "learning_rate": 0.00019657337225114692,
      "loss": 0.7638,
      "step": 3520
    },
    {
      "epoch": 0.22285353535353536,
      "grad_norm": 0.6677626371383667,
      "learning_rate": 0.0001965467881204274,
      "loss": 0.6938,
      "step": 3530
    },
    {
      "epoch": 0.22348484848484848,
      "grad_norm": 0.8051018714904785,
      "learning_rate": 0.00019652010307820287,
      "loss": 0.625,
      "step": 3540
    },
    {
      "epoch": 0.22411616161616163,
      "grad_norm": 1.150392770767212,
      "learning_rate": 0.00019649331715236484,
      "loss": 0.6314,
      "step": 3550
    },
    {
      "epoch": 0.22474747474747475,
      "grad_norm": 0.5936555862426758,
      "learning_rate": 0.00019646643037091017,
      "loss": 0.8472,
      "step": 3560
    },
    {
      "epoch": 0.22537878787878787,
      "grad_norm": 0.7444565296173096,
      "learning_rate": 0.00019643944276194112,
      "loss": 0.7804,
      "step": 3570
    },
    {
      "epoch": 0.22601010101010102,
      "grad_norm": 0.6727052330970764,
      "learning_rate": 0.0001964123543536654,
      "loss": 0.6928,
      "step": 3580
    },
    {
      "epoch": 0.22664141414141414,
      "grad_norm": 0.7139135003089905,
      "learning_rate": 0.00019638516517439598,
      "loss": 0.5986,
      "step": 3590
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 1.0766022205352783,
      "learning_rate": 0.0001963578752525513,
      "loss": 0.611,
      "step": 3600
    },
    {
      "epoch": 0.22790404040404041,
      "grad_norm": 0.5333102941513062,
      "learning_rate": 0.00019633048461665492,
      "loss": 0.8331,
      "step": 3610
    },
    {
      "epoch": 0.22853535353535354,
      "grad_norm": 0.6479005813598633,
      "learning_rate": 0.00019630299329533583,
      "loss": 0.7427,
      "step": 3620
    },
    {
      "epoch": 0.22916666666666666,
      "grad_norm": 0.7891281247138977,
      "learning_rate": 0.00019627540131732815,
      "loss": 0.6872,
      "step": 3630
    },
    {
      "epoch": 0.2297979797979798,
      "grad_norm": 0.744083821773529,
      "learning_rate": 0.0001962477087114713,
      "loss": 0.6245,
      "step": 3640
    },
    {
      "epoch": 0.23042929292929293,
      "grad_norm": 1.1561157703399658,
      "learning_rate": 0.00019621991550670975,
      "loss": 0.646,
      "step": 3650
    },
    {
      "epoch": 0.23106060606060605,
      "grad_norm": 0.548424482345581,
      "learning_rate": 0.0001961920217320932,
      "loss": 0.8397,
      "step": 3660
    },
    {
      "epoch": 0.2316919191919192,
      "grad_norm": 0.6671515107154846,
      "learning_rate": 0.0001961640274167765,
      "loss": 0.7579,
      "step": 3670
    },
    {
      "epoch": 0.23232323232323232,
      "grad_norm": 0.697683572769165,
      "learning_rate": 0.0001961359325900195,
      "loss": 0.6858,
      "step": 3680
    },
    {
      "epoch": 0.23295454545454544,
      "grad_norm": 0.8648586869239807,
      "learning_rate": 0.0001961077372811872,
      "loss": 0.6409,
      "step": 3690
    },
    {
      "epoch": 0.2335858585858586,
      "grad_norm": 0.9259229302406311,
      "learning_rate": 0.0001960794415197495,
      "loss": 0.635,
      "step": 3700
    },
    {
      "epoch": 0.2342171717171717,
      "grad_norm": 0.5201980471611023,
      "learning_rate": 0.00019605104533528138,
      "loss": 0.8651,
      "step": 3710
    },
    {
      "epoch": 0.23484848484848486,
      "grad_norm": 0.7483479380607605,
      "learning_rate": 0.00019602254875746283,
      "loss": 0.749,
      "step": 3720
    },
    {
      "epoch": 0.23547979797979798,
      "grad_norm": 0.6824462413787842,
      "learning_rate": 0.00019599395181607864,
      "loss": 0.6461,
      "step": 3730
    },
    {
      "epoch": 0.2361111111111111,
      "grad_norm": 0.8410384654998779,
      "learning_rate": 0.00019596525454101856,
      "loss": 0.6197,
      "step": 3740
    },
    {
      "epoch": 0.23674242424242425,
      "grad_norm": 1.0816935300827026,
      "learning_rate": 0.0001959364569622773,
      "loss": 0.6296,
      "step": 3750
    },
    {
      "epoch": 0.23737373737373738,
      "grad_norm": 0.583977460861206,
      "learning_rate": 0.00019590755910995426,
      "loss": 0.8402,
      "step": 3760
    },
    {
      "epoch": 0.2380050505050505,
      "grad_norm": 0.6941959857940674,
      "learning_rate": 0.00019587856101425377,
      "loss": 0.7222,
      "step": 3770
    },
    {
      "epoch": 0.23863636363636365,
      "grad_norm": 0.7717662453651428,
      "learning_rate": 0.00019584946270548482,
      "loss": 0.6863,
      "step": 3780
    },
    {
      "epoch": 0.23926767676767677,
      "grad_norm": 0.8349329829216003,
      "learning_rate": 0.00019582026421406125,
      "loss": 0.5904,
      "step": 3790
    },
    {
      "epoch": 0.2398989898989899,
      "grad_norm": 1.1655611991882324,
      "learning_rate": 0.00019579096557050156,
      "loss": 0.623,
      "step": 3800
    },
    {
      "epoch": 0.24053030303030304,
      "grad_norm": 0.5716366767883301,
      "learning_rate": 0.0001957615668054289,
      "loss": 0.8073,
      "step": 3810
    },
    {
      "epoch": 0.24116161616161616,
      "grad_norm": 0.7168810963630676,
      "learning_rate": 0.00019573206794957116,
      "loss": 0.7278,
      "step": 3820
    },
    {
      "epoch": 0.24179292929292928,
      "grad_norm": 0.7578932642936707,
      "learning_rate": 0.00019570246903376071,
      "loss": 0.6611,
      "step": 3830
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 0.7495874166488647,
      "learning_rate": 0.00019567277008893466,
      "loss": 0.6034,
      "step": 3840
    },
    {
      "epoch": 0.24305555555555555,
      "grad_norm": 1.0915462970733643,
      "learning_rate": 0.0001956429711461346,
      "loss": 0.6146,
      "step": 3850
    },
    {
      "epoch": 0.24368686868686867,
      "grad_norm": 0.6142016053199768,
      "learning_rate": 0.00019561307223650654,
      "loss": 0.8049,
      "step": 3860
    },
    {
      "epoch": 0.24431818181818182,
      "grad_norm": 0.7064968943595886,
      "learning_rate": 0.00019558307339130116,
      "loss": 0.7363,
      "step": 3870
    },
    {
      "epoch": 0.24494949494949494,
      "grad_norm": 0.785517156124115,
      "learning_rate": 0.00019555297464187343,
      "loss": 0.6719,
      "step": 3880
    },
    {
      "epoch": 0.2455808080808081,
      "grad_norm": 0.7869837284088135,
      "learning_rate": 0.0001955227760196829,
      "loss": 0.5953,
      "step": 3890
    },
    {
      "epoch": 0.24621212121212122,
      "grad_norm": 1.0495184659957886,
      "learning_rate": 0.00019549247755629333,
      "loss": 0.6269,
      "step": 3900
    },
    {
      "epoch": 0.24684343434343434,
      "grad_norm": 0.5724454522132874,
      "learning_rate": 0.00019546207928337298,
      "loss": 0.8508,
      "step": 3910
    },
    {
      "epoch": 0.2474747474747475,
      "grad_norm": 0.7092358469963074,
      "learning_rate": 0.00019543158123269439,
      "loss": 0.7218,
      "step": 3920
    },
    {
      "epoch": 0.2481060606060606,
      "grad_norm": 0.8124663233757019,
      "learning_rate": 0.00019540098343613435,
      "loss": 0.6789,
      "step": 3930
    },
    {
      "epoch": 0.24873737373737373,
      "grad_norm": 0.8590080142021179,
      "learning_rate": 0.0001953702859256739,
      "loss": 0.6081,
      "step": 3940
    },
    {
      "epoch": 0.24936868686868688,
      "grad_norm": 1.1656885147094727,
      "learning_rate": 0.00019533948873339836,
      "loss": 0.5964,
      "step": 3950
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.5682050585746765,
      "learning_rate": 0.00019530859189149723,
      "loss": 0.8059,
      "step": 3960
    },
    {
      "epoch": 0.25063131313131315,
      "grad_norm": 0.7570399045944214,
      "learning_rate": 0.00019527759543226414,
      "loss": 0.7325,
      "step": 3970
    },
    {
      "epoch": 0.25126262626262624,
      "grad_norm": 0.7688345909118652,
      "learning_rate": 0.00019524649938809681,
      "loss": 0.6367,
      "step": 3980
    },
    {
      "epoch": 0.2518939393939394,
      "grad_norm": 0.8515782952308655,
      "learning_rate": 0.00019521530379149716,
      "loss": 0.6034,
      "step": 3990
    },
    {
      "epoch": 0.25252525252525254,
      "grad_norm": 1.068707823753357,
      "learning_rate": 0.00019518400867507102,
      "loss": 0.6117,
      "step": 4000
    },
    {
      "epoch": 0.25252525252525254,
      "eval_loss": 0.6772322654724121,
      "eval_runtime": 27.3052,
      "eval_samples_per_second": 93.755,
      "eval_steps_per_second": 11.719,
      "step": 4000
    },
    {
      "epoch": 0.25315656565656564,
      "grad_norm": 0.5219240784645081,
      "learning_rate": 0.0001951526140715283,
      "loss": 0.889,
      "step": 4010
    },
    {
      "epoch": 0.2537878787878788,
      "grad_norm": 0.7831730842590332,
      "learning_rate": 0.00019512112001368297,
      "loss": 0.7098,
      "step": 4020
    },
    {
      "epoch": 0.25441919191919193,
      "grad_norm": 0.7411784529685974,
      "learning_rate": 0.0001950895265344528,
      "loss": 0.6524,
      "step": 4030
    },
    {
      "epoch": 0.255050505050505,
      "grad_norm": 0.764510989189148,
      "learning_rate": 0.00019505783366685958,
      "loss": 0.6029,
      "step": 4040
    },
    {
      "epoch": 0.2556818181818182,
      "grad_norm": 1.09827721118927,
      "learning_rate": 0.00019502604144402903,
      "loss": 0.5922,
      "step": 4050
    },
    {
      "epoch": 0.2563131313131313,
      "grad_norm": 0.5714800953865051,
      "learning_rate": 0.00019499414989919054,
      "loss": 0.8305,
      "step": 4060
    },
    {
      "epoch": 0.2569444444444444,
      "grad_norm": 0.792681097984314,
      "learning_rate": 0.00019496215906567748,
      "loss": 0.706,
      "step": 4070
    },
    {
      "epoch": 0.25757575757575757,
      "grad_norm": 0.7366827726364136,
      "learning_rate": 0.0001949300689769269,
      "loss": 0.6661,
      "step": 4080
    },
    {
      "epoch": 0.2582070707070707,
      "grad_norm": 0.830299973487854,
      "learning_rate": 0.0001948978796664797,
      "loss": 0.5909,
      "step": 4090
    },
    {
      "epoch": 0.2588383838383838,
      "grad_norm": 1.0126402378082275,
      "learning_rate": 0.00019486559116798028,
      "loss": 0.6126,
      "step": 4100
    },
    {
      "epoch": 0.25946969696969696,
      "grad_norm": 0.582241952419281,
      "learning_rate": 0.00019483320351517698,
      "loss": 0.8272,
      "step": 4110
    },
    {
      "epoch": 0.2601010101010101,
      "grad_norm": 0.6968902945518494,
      "learning_rate": 0.00019480071674192158,
      "loss": 0.7031,
      "step": 4120
    },
    {
      "epoch": 0.26073232323232326,
      "grad_norm": 0.727138340473175,
      "learning_rate": 0.00019476813088216955,
      "loss": 0.6279,
      "step": 4130
    },
    {
      "epoch": 0.26136363636363635,
      "grad_norm": 0.8846218585968018,
      "learning_rate": 0.00019473544596997986,
      "loss": 0.604,
      "step": 4140
    },
    {
      "epoch": 0.2619949494949495,
      "grad_norm": 1.0743659734725952,
      "learning_rate": 0.0001947026620395151,
      "loss": 0.5853,
      "step": 4150
    },
    {
      "epoch": 0.26262626262626265,
      "grad_norm": 0.5679904222488403,
      "learning_rate": 0.00019466977912504127,
      "loss": 0.8055,
      "step": 4160
    },
    {
      "epoch": 0.26325757575757575,
      "grad_norm": 0.6614130735397339,
      "learning_rate": 0.00019463679726092791,
      "loss": 0.7486,
      "step": 4170
    },
    {
      "epoch": 0.2638888888888889,
      "grad_norm": 0.7180368900299072,
      "learning_rate": 0.0001946037164816479,
      "loss": 0.6361,
      "step": 4180
    },
    {
      "epoch": 0.26452020202020204,
      "grad_norm": 0.7655189633369446,
      "learning_rate": 0.00019457053682177754,
      "loss": 0.5966,
      "step": 4190
    },
    {
      "epoch": 0.26515151515151514,
      "grad_norm": 0.9534170627593994,
      "learning_rate": 0.00019453725831599652,
      "loss": 0.5772,
      "step": 4200
    },
    {
      "epoch": 0.2657828282828283,
      "grad_norm": 0.5781427621841431,
      "learning_rate": 0.0001945038809990878,
      "loss": 0.8491,
      "step": 4210
    },
    {
      "epoch": 0.26641414141414144,
      "grad_norm": 0.6894657611846924,
      "learning_rate": 0.0001944704049059376,
      "loss": 0.7078,
      "step": 4220
    },
    {
      "epoch": 0.26704545454545453,
      "grad_norm": 0.7195695042610168,
      "learning_rate": 0.00019443683007153544,
      "loss": 0.6465,
      "step": 4230
    },
    {
      "epoch": 0.2676767676767677,
      "grad_norm": 0.7929624915122986,
      "learning_rate": 0.00019440315653097398,
      "loss": 0.5807,
      "step": 4240
    },
    {
      "epoch": 0.26830808080808083,
      "grad_norm": 1.247113585472107,
      "learning_rate": 0.00019436938431944916,
      "loss": 0.5965,
      "step": 4250
    },
    {
      "epoch": 0.2689393939393939,
      "grad_norm": 0.5307172536849976,
      "learning_rate": 0.0001943355134722599,
      "loss": 0.8116,
      "step": 4260
    },
    {
      "epoch": 0.26957070707070707,
      "grad_norm": 0.673069417476654,
      "learning_rate": 0.00019430154402480832,
      "loss": 0.7112,
      "step": 4270
    },
    {
      "epoch": 0.2702020202020202,
      "grad_norm": 0.7150174379348755,
      "learning_rate": 0.0001942674760125996,
      "loss": 0.6553,
      "step": 4280
    },
    {
      "epoch": 0.2708333333333333,
      "grad_norm": 0.8584343194961548,
      "learning_rate": 0.00019423330947124183,
      "loss": 0.5793,
      "step": 4290
    },
    {
      "epoch": 0.27146464646464646,
      "grad_norm": 1.003257393836975,
      "learning_rate": 0.00019419904443644624,
      "loss": 0.6084,
      "step": 4300
    },
    {
      "epoch": 0.2720959595959596,
      "grad_norm": 0.6080260872840881,
      "learning_rate": 0.00019416468094402687,
      "loss": 0.8042,
      "step": 4310
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.7347806096076965,
      "learning_rate": 0.00019413021902990078,
      "loss": 0.7033,
      "step": 4320
    },
    {
      "epoch": 0.27335858585858586,
      "grad_norm": 0.7934287190437317,
      "learning_rate": 0.00019409565873008782,
      "loss": 0.6367,
      "step": 4330
    },
    {
      "epoch": 0.273989898989899,
      "grad_norm": 0.8434942960739136,
      "learning_rate": 0.0001940610000807107,
      "loss": 0.5901,
      "step": 4340
    },
    {
      "epoch": 0.2746212121212121,
      "grad_norm": 1.2004021406173706,
      "learning_rate": 0.00019402624311799495,
      "loss": 0.6021,
      "step": 4350
    },
    {
      "epoch": 0.27525252525252525,
      "grad_norm": 0.5505009293556213,
      "learning_rate": 0.00019399138787826883,
      "loss": 0.8679,
      "step": 4360
    },
    {
      "epoch": 0.2758838383838384,
      "grad_norm": 0.6756744384765625,
      "learning_rate": 0.0001939564343979633,
      "loss": 0.7198,
      "step": 4370
    },
    {
      "epoch": 0.2765151515151515,
      "grad_norm": 0.7191880941390991,
      "learning_rate": 0.00019392138271361205,
      "loss": 0.6194,
      "step": 4380
    },
    {
      "epoch": 0.27714646464646464,
      "grad_norm": 0.7674593329429626,
      "learning_rate": 0.00019388623286185138,
      "loss": 0.5826,
      "step": 4390
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 1.0534693002700806,
      "learning_rate": 0.00019385098487942023,
      "loss": 0.6015,
      "step": 4400
    },
    {
      "epoch": 0.2784090909090909,
      "grad_norm": 0.5751586556434631,
      "learning_rate": 0.00019381563880316004,
      "loss": 0.8411,
      "step": 4410
    },
    {
      "epoch": 0.27904040404040403,
      "grad_norm": 0.7208975553512573,
      "learning_rate": 0.0001937801946700149,
      "loss": 0.6886,
      "step": 4420
    },
    {
      "epoch": 0.2796717171717172,
      "grad_norm": 0.7053536772727966,
      "learning_rate": 0.00019374465251703122,
      "loss": 0.6338,
      "step": 4430
    },
    {
      "epoch": 0.2803030303030303,
      "grad_norm": 0.7985022068023682,
      "learning_rate": 0.00019370901238135804,
      "loss": 0.5766,
      "step": 4440
    },
    {
      "epoch": 0.2809343434343434,
      "grad_norm": 1.1297340393066406,
      "learning_rate": 0.00019367327430024663,
      "loss": 0.5979,
      "step": 4450
    },
    {
      "epoch": 0.2815656565656566,
      "grad_norm": 0.604833722114563,
      "learning_rate": 0.00019363743831105081,
      "loss": 0.8053,
      "step": 4460
    },
    {
      "epoch": 0.2821969696969697,
      "grad_norm": 0.7244064211845398,
      "learning_rate": 0.00019360150445122665,
      "loss": 0.7158,
      "step": 4470
    },
    {
      "epoch": 0.2828282828282828,
      "grad_norm": 0.8539688587188721,
      "learning_rate": 0.0001935654727583325,
      "loss": 0.6034,
      "step": 4480
    },
    {
      "epoch": 0.28345959595959597,
      "grad_norm": 0.8163095116615295,
      "learning_rate": 0.00019352934327002892,
      "loss": 0.569,
      "step": 4490
    },
    {
      "epoch": 0.2840909090909091,
      "grad_norm": 1.0585854053497314,
      "learning_rate": 0.00019349311602407884,
      "loss": 0.59,
      "step": 4500
    },
    {
      "epoch": 0.2847222222222222,
      "grad_norm": 0.5627225041389465,
      "learning_rate": 0.00019345679105834727,
      "loss": 0.792,
      "step": 4510
    },
    {
      "epoch": 0.28535353535353536,
      "grad_norm": 0.6342817544937134,
      "learning_rate": 0.00019342036841080132,
      "loss": 0.6857,
      "step": 4520
    },
    {
      "epoch": 0.2859848484848485,
      "grad_norm": 0.8010412454605103,
      "learning_rate": 0.00019338384811951027,
      "loss": 0.6352,
      "step": 4530
    },
    {
      "epoch": 0.2866161616161616,
      "grad_norm": 0.9310699701309204,
      "learning_rate": 0.00019334723022264544,
      "loss": 0.5966,
      "step": 4540
    },
    {
      "epoch": 0.28724747474747475,
      "grad_norm": 0.9700534343719482,
      "learning_rate": 0.00019331051475848018,
      "loss": 0.5774,
      "step": 4550
    },
    {
      "epoch": 0.2878787878787879,
      "grad_norm": 0.5599976181983948,
      "learning_rate": 0.0001932737017653897,
      "loss": 0.8336,
      "step": 4560
    },
    {
      "epoch": 0.288510101010101,
      "grad_norm": 0.6462521553039551,
      "learning_rate": 0.00019323679128185135,
      "loss": 0.6961,
      "step": 4570
    },
    {
      "epoch": 0.28914141414141414,
      "grad_norm": 0.7330341935157776,
      "learning_rate": 0.00019319978334644426,
      "loss": 0.6463,
      "step": 4580
    },
    {
      "epoch": 0.2897727272727273,
      "grad_norm": 0.8686737418174744,
      "learning_rate": 0.00019316267799784938,
      "loss": 0.5649,
      "step": 4590
    },
    {
      "epoch": 0.2904040404040404,
      "grad_norm": 1.0145928859710693,
      "learning_rate": 0.00019312547527484958,
      "loss": 0.5921,
      "step": 4600
    },
    {
      "epoch": 0.29103535353535354,
      "grad_norm": 0.6002787351608276,
      "learning_rate": 0.00019308817521632943,
      "loss": 0.8042,
      "step": 4610
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.6557075381278992,
      "learning_rate": 0.00019305077786127526,
      "loss": 0.681,
      "step": 4620
    },
    {
      "epoch": 0.2922979797979798,
      "grad_norm": 0.726617693901062,
      "learning_rate": 0.00019301328324877512,
      "loss": 0.6145,
      "step": 4630
    },
    {
      "epoch": 0.29292929292929293,
      "grad_norm": 0.8333576321601868,
      "learning_rate": 0.00019297569141801867,
      "loss": 0.5771,
      "step": 4640
    },
    {
      "epoch": 0.2935606060606061,
      "grad_norm": 1.144715428352356,
      "learning_rate": 0.00019293800240829717,
      "loss": 0.5777,
      "step": 4650
    },
    {
      "epoch": 0.29419191919191917,
      "grad_norm": 0.5324021577835083,
      "learning_rate": 0.00019290021625900354,
      "loss": 0.7965,
      "step": 4660
    },
    {
      "epoch": 0.2948232323232323,
      "grad_norm": 0.7485097646713257,
      "learning_rate": 0.00019286233300963218,
      "loss": 0.7075,
      "step": 4670
    },
    {
      "epoch": 0.29545454545454547,
      "grad_norm": 0.668709933757782,
      "learning_rate": 0.00019282435269977894,
      "loss": 0.6295,
      "step": 4680
    },
    {
      "epoch": 0.29608585858585856,
      "grad_norm": 0.7289692759513855,
      "learning_rate": 0.00019278627536914117,
      "loss": 0.565,
      "step": 4690
    },
    {
      "epoch": 0.2967171717171717,
      "grad_norm": 1.241212010383606,
      "learning_rate": 0.00019274810105751762,
      "loss": 0.5714,
      "step": 4700
    },
    {
      "epoch": 0.29734848484848486,
      "grad_norm": 0.5874489545822144,
      "learning_rate": 0.0001927098298048084,
      "loss": 0.7991,
      "step": 4710
    },
    {
      "epoch": 0.29797979797979796,
      "grad_norm": 0.6207696199417114,
      "learning_rate": 0.00019267146165101491,
      "loss": 0.727,
      "step": 4720
    },
    {
      "epoch": 0.2986111111111111,
      "grad_norm": 0.6582732796669006,
      "learning_rate": 0.0001926329966362399,
      "loss": 0.6191,
      "step": 4730
    },
    {
      "epoch": 0.29924242424242425,
      "grad_norm": 0.7452285289764404,
      "learning_rate": 0.0001925944348006873,
      "loss": 0.5626,
      "step": 4740
    },
    {
      "epoch": 0.29987373737373735,
      "grad_norm": 1.1364666223526,
      "learning_rate": 0.00019255577618466227,
      "loss": 0.5828,
      "step": 4750
    },
    {
      "epoch": 0.3005050505050505,
      "grad_norm": 0.5877828001976013,
      "learning_rate": 0.0001925170208285711,
      "loss": 0.7884,
      "step": 4760
    },
    {
      "epoch": 0.30113636363636365,
      "grad_norm": 0.7253538370132446,
      "learning_rate": 0.00019247816877292125,
      "loss": 0.6988,
      "step": 4770
    },
    {
      "epoch": 0.30176767676767674,
      "grad_norm": 0.7966983914375305,
      "learning_rate": 0.0001924392200583212,
      "loss": 0.6379,
      "step": 4780
    },
    {
      "epoch": 0.3023989898989899,
      "grad_norm": 0.7604701519012451,
      "learning_rate": 0.00019240017472548044,
      "loss": 0.5318,
      "step": 4790
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 1.0633622407913208,
      "learning_rate": 0.0001923610328152095,
      "loss": 0.5949,
      "step": 4800
    },
    {
      "epoch": 0.3036616161616162,
      "grad_norm": 0.5759729743003845,
      "learning_rate": 0.00019232179436841983,
      "loss": 0.7733,
      "step": 4810
    },
    {
      "epoch": 0.3042929292929293,
      "grad_norm": 0.7012568116188049,
      "learning_rate": 0.00019228245942612374,
      "loss": 0.7112,
      "step": 4820
    },
    {
      "epoch": 0.30492424242424243,
      "grad_norm": 0.7440206408500671,
      "learning_rate": 0.0001922430280294345,
      "loss": 0.6041,
      "step": 4830
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 0.8046401143074036,
      "learning_rate": 0.0001922035002195661,
      "loss": 0.555,
      "step": 4840
    },
    {
      "epoch": 0.3061868686868687,
      "grad_norm": 1.043587565422058,
      "learning_rate": 0.00019216387603783334,
      "loss": 0.5325,
      "step": 4850
    },
    {
      "epoch": 0.3068181818181818,
      "grad_norm": 0.5920006632804871,
      "learning_rate": 0.00019212415552565174,
      "loss": 0.8003,
      "step": 4860
    },
    {
      "epoch": 0.307449494949495,
      "grad_norm": 0.6786617636680603,
      "learning_rate": 0.00019208433872453754,
      "loss": 0.696,
      "step": 4870
    },
    {
      "epoch": 0.30808080808080807,
      "grad_norm": 0.8274611830711365,
      "learning_rate": 0.00019204442567610756,
      "loss": 0.6497,
      "step": 4880
    },
    {
      "epoch": 0.3087121212121212,
      "grad_norm": 0.9361644983291626,
      "learning_rate": 0.00019200441642207923,
      "loss": 0.5727,
      "step": 4890
    },
    {
      "epoch": 0.30934343434343436,
      "grad_norm": 0.9931752681732178,
      "learning_rate": 0.0001919643110042706,
      "loss": 0.5839,
      "step": 4900
    },
    {
      "epoch": 0.30997474747474746,
      "grad_norm": 0.5650510191917419,
      "learning_rate": 0.00019192410946460015,
      "loss": 0.7536,
      "step": 4910
    },
    {
      "epoch": 0.3106060606060606,
      "grad_norm": 0.7109998464584351,
      "learning_rate": 0.00019188381184508688,
      "loss": 0.6898,
      "step": 4920
    },
    {
      "epoch": 0.31123737373737376,
      "grad_norm": 0.6318044662475586,
      "learning_rate": 0.0001918434181878502,
      "loss": 0.6105,
      "step": 4930
    },
    {
      "epoch": 0.31186868686868685,
      "grad_norm": 0.8525688648223877,
      "learning_rate": 0.00019180292853510992,
      "loss": 0.5463,
      "step": 4940
    },
    {
      "epoch": 0.3125,
      "grad_norm": 1.1676466464996338,
      "learning_rate": 0.00019176234292918608,
      "loss": 0.6016,
      "step": 4950
    },
    {
      "epoch": 0.31313131313131315,
      "grad_norm": 0.5535843968391418,
      "learning_rate": 0.00019172166141249915,
      "loss": 0.7952,
      "step": 4960
    },
    {
      "epoch": 0.31376262626262624,
      "grad_norm": 0.6939905285835266,
      "learning_rate": 0.00019168088402756985,
      "loss": 0.6837,
      "step": 4970
    },
    {
      "epoch": 0.3143939393939394,
      "grad_norm": 0.7587629556655884,
      "learning_rate": 0.0001916400108170189,
      "loss": 0.6063,
      "step": 4980
    },
    {
      "epoch": 0.31502525252525254,
      "grad_norm": 0.8732277154922485,
      "learning_rate": 0.00019159904182356746,
      "loss": 0.5479,
      "step": 4990
    },
    {
      "epoch": 0.31565656565656564,
      "grad_norm": 1.0229947566986084,
      "learning_rate": 0.00019155797709003656,
      "loss": 0.5747,
      "step": 5000
    },
    {
      "epoch": 0.31565656565656564,
      "eval_loss": 0.6354244947433472,
      "eval_runtime": 28.38,
      "eval_samples_per_second": 90.204,
      "eval_steps_per_second": 11.276,
      "step": 5000
    },
    {
      "epoch": 0.3162878787878788,
      "grad_norm": 0.6578483581542969,
      "learning_rate": 0.00019151681665934746,
      "loss": 0.8051,
      "step": 5010
    },
    {
      "epoch": 0.31691919191919193,
      "grad_norm": 0.6700741052627563,
      "learning_rate": 0.00019147556057452135,
      "loss": 0.6832,
      "step": 5020
    },
    {
      "epoch": 0.317550505050505,
      "grad_norm": 0.6822919845581055,
      "learning_rate": 0.00019143420887867945,
      "loss": 0.6239,
      "step": 5030
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 0.8895336389541626,
      "learning_rate": 0.00019139276161504287,
      "loss": 0.5422,
      "step": 5040
    },
    {
      "epoch": 0.3188131313131313,
      "grad_norm": 1.083667278289795,
      "learning_rate": 0.00019135121882693268,
      "loss": 0.5851,
      "step": 5050
    },
    {
      "epoch": 0.3194444444444444,
      "grad_norm": 0.5383729338645935,
      "learning_rate": 0.00019130958055776969,
      "loss": 0.7796,
      "step": 5060
    },
    {
      "epoch": 0.32007575757575757,
      "grad_norm": 0.7698951363563538,
      "learning_rate": 0.00019126784685107463,
      "loss": 0.6807,
      "step": 5070
    },
    {
      "epoch": 0.3207070707070707,
      "grad_norm": 0.8183860778808594,
      "learning_rate": 0.00019122601775046789,
      "loss": 0.6165,
      "step": 5080
    },
    {
      "epoch": 0.3213383838383838,
      "grad_norm": 0.7684093117713928,
      "learning_rate": 0.00019118409329966956,
      "loss": 0.562,
      "step": 5090
    },
    {
      "epoch": 0.32196969696969696,
      "grad_norm": 0.9316547513008118,
      "learning_rate": 0.00019114207354249948,
      "loss": 0.5644,
      "step": 5100
    },
    {
      "epoch": 0.3226010101010101,
      "grad_norm": 0.5924767255783081,
      "learning_rate": 0.00019109995852287698,
      "loss": 0.7886,
      "step": 5110
    },
    {
      "epoch": 0.32323232323232326,
      "grad_norm": 0.7118251323699951,
      "learning_rate": 0.0001910577482848211,
      "loss": 0.6822,
      "step": 5120
    },
    {
      "epoch": 0.32386363636363635,
      "grad_norm": 0.7546359300613403,
      "learning_rate": 0.0001910154428724503,
      "loss": 0.6195,
      "step": 5130
    },
    {
      "epoch": 0.3244949494949495,
      "grad_norm": 0.8794420957565308,
      "learning_rate": 0.00019097304232998255,
      "loss": 0.5468,
      "step": 5140
    },
    {
      "epoch": 0.32512626262626265,
      "grad_norm": 0.9442066550254822,
      "learning_rate": 0.00019093054670173522,
      "loss": 0.5689,
      "step": 5150
    },
    {
      "epoch": 0.32575757575757575,
      "grad_norm": 0.6172857880592346,
      "learning_rate": 0.00019088795603212517,
      "loss": 0.8134,
      "step": 5160
    },
    {
      "epoch": 0.3263888888888889,
      "grad_norm": 0.650283932685852,
      "learning_rate": 0.00019084527036566847,
      "loss": 0.6556,
      "step": 5170
    },
    {
      "epoch": 0.32702020202020204,
      "grad_norm": 0.7220401763916016,
      "learning_rate": 0.0001908024897469805,
      "loss": 0.6053,
      "step": 5180
    },
    {
      "epoch": 0.32765151515151514,
      "grad_norm": 0.8287461996078491,
      "learning_rate": 0.00019075961422077597,
      "loss": 0.5468,
      "step": 5190
    },
    {
      "epoch": 0.3282828282828283,
      "grad_norm": 1.1781922578811646,
      "learning_rate": 0.00019071664383186874,
      "loss": 0.5837,
      "step": 5200
    },
    {
      "epoch": 0.32891414141414144,
      "grad_norm": 0.6113929152488708,
      "learning_rate": 0.00019067357862517177,
      "loss": 0.7827,
      "step": 5210
    },
    {
      "epoch": 0.32954545454545453,
      "grad_norm": 0.7292771339416504,
      "learning_rate": 0.00019063041864569722,
      "loss": 0.6508,
      "step": 5220
    },
    {
      "epoch": 0.3301767676767677,
      "grad_norm": 0.6693084239959717,
      "learning_rate": 0.00019058716393855624,
      "loss": 0.6278,
      "step": 5230
    },
    {
      "epoch": 0.33080808080808083,
      "grad_norm": 0.8752071261405945,
      "learning_rate": 0.000190543814548959,
      "loss": 0.5606,
      "step": 5240
    },
    {
      "epoch": 0.3314393939393939,
      "grad_norm": 0.9859668016433716,
      "learning_rate": 0.00019050037052221463,
      "loss": 0.5579,
      "step": 5250
    },
    {
      "epoch": 0.33207070707070707,
      "grad_norm": 0.5765371918678284,
      "learning_rate": 0.00019045683190373124,
      "loss": 0.805,
      "step": 5260
    },
    {
      "epoch": 0.3327020202020202,
      "grad_norm": 0.7346866130828857,
      "learning_rate": 0.00019041319873901573,
      "loss": 0.6739,
      "step": 5270
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.718723714351654,
      "learning_rate": 0.00019036947107367382,
      "loss": 0.5789,
      "step": 5280
    },
    {
      "epoch": 0.33396464646464646,
      "grad_norm": 0.8555094599723816,
      "learning_rate": 0.00019032564895341006,
      "loss": 0.577,
      "step": 5290
    },
    {
      "epoch": 0.3345959595959596,
      "grad_norm": 1.1615906953811646,
      "learning_rate": 0.00019028173242402767,
      "loss": 0.5765,
      "step": 5300
    },
    {
      "epoch": 0.3352272727272727,
      "grad_norm": 0.6241433620452881,
      "learning_rate": 0.0001902377215314286,
      "loss": 0.7686,
      "step": 5310
    },
    {
      "epoch": 0.33585858585858586,
      "grad_norm": 0.7163758873939514,
      "learning_rate": 0.00019019361632161336,
      "loss": 0.677,
      "step": 5320
    },
    {
      "epoch": 0.336489898989899,
      "grad_norm": 0.7867381572723389,
      "learning_rate": 0.00019014941684068114,
      "loss": 0.596,
      "step": 5330
    },
    {
      "epoch": 0.3371212121212121,
      "grad_norm": 0.839661717414856,
      "learning_rate": 0.00019010512313482955,
      "loss": 0.5634,
      "step": 5340
    },
    {
      "epoch": 0.33775252525252525,
      "grad_norm": 1.001997709274292,
      "learning_rate": 0.00019006073525035477,
      "loss": 0.5794,
      "step": 5350
    },
    {
      "epoch": 0.3383838383838384,
      "grad_norm": 0.6121259331703186,
      "learning_rate": 0.0001900162532336514,
      "loss": 0.7722,
      "step": 5360
    },
    {
      "epoch": 0.3390151515151515,
      "grad_norm": 0.6729865670204163,
      "learning_rate": 0.00018997167713121236,
      "loss": 0.6536,
      "step": 5370
    },
    {
      "epoch": 0.33964646464646464,
      "grad_norm": 0.7232084274291992,
      "learning_rate": 0.000189927006989629,
      "loss": 0.5816,
      "step": 5380
    },
    {
      "epoch": 0.3402777777777778,
      "grad_norm": 0.8457463383674622,
      "learning_rate": 0.0001898822428555909,
      "loss": 0.5316,
      "step": 5390
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 1.1690783500671387,
      "learning_rate": 0.00018983738477588595,
      "loss": 0.564,
      "step": 5400
    },
    {
      "epoch": 0.34154040404040403,
      "grad_norm": 0.6278010010719299,
      "learning_rate": 0.00018979243279740015,
      "loss": 0.7921,
      "step": 5410
    },
    {
      "epoch": 0.3421717171717172,
      "grad_norm": 0.6660869121551514,
      "learning_rate": 0.00018974738696711768,
      "loss": 0.6724,
      "step": 5420
    },
    {
      "epoch": 0.3428030303030303,
      "grad_norm": 0.7201115489006042,
      "learning_rate": 0.00018970224733212083,
      "loss": 0.5853,
      "step": 5430
    },
    {
      "epoch": 0.3434343434343434,
      "grad_norm": 0.9530907273292542,
      "learning_rate": 0.0001896570139395899,
      "loss": 0.5394,
      "step": 5440
    },
    {
      "epoch": 0.3440656565656566,
      "grad_norm": 0.9547246694564819,
      "learning_rate": 0.00018961168683680326,
      "loss": 0.5302,
      "step": 5450
    },
    {
      "epoch": 0.3446969696969697,
      "grad_norm": 0.6238282322883606,
      "learning_rate": 0.0001895662660711371,
      "loss": 0.7731,
      "step": 5460
    },
    {
      "epoch": 0.3453282828282828,
      "grad_norm": 0.6518881320953369,
      "learning_rate": 0.00018952075169006568,
      "loss": 0.6653,
      "step": 5470
    },
    {
      "epoch": 0.34595959595959597,
      "grad_norm": 0.7803547978401184,
      "learning_rate": 0.00018947514374116089,
      "loss": 0.6125,
      "step": 5480
    },
    {
      "epoch": 0.3465909090909091,
      "grad_norm": 0.7857193946838379,
      "learning_rate": 0.00018942944227209264,
      "loss": 0.5539,
      "step": 5490
    },
    {
      "epoch": 0.3472222222222222,
      "grad_norm": 1.122987985610962,
      "learning_rate": 0.0001893836473306284,
      "loss": 0.5603,
      "step": 5500
    },
    {
      "epoch": 0.34785353535353536,
      "grad_norm": 0.5832096934318542,
      "learning_rate": 0.00018933775896463347,
      "loss": 0.7777,
      "step": 5510
    },
    {
      "epoch": 0.3484848484848485,
      "grad_norm": 0.6410543918609619,
      "learning_rate": 0.00018929177722207076,
      "loss": 0.6832,
      "step": 5520
    },
    {
      "epoch": 0.3491161616161616,
      "grad_norm": 0.7499108910560608,
      "learning_rate": 0.00018924570215100075,
      "loss": 0.5925,
      "step": 5530
    },
    {
      "epoch": 0.34974747474747475,
      "grad_norm": 0.7321015000343323,
      "learning_rate": 0.0001891995337995815,
      "loss": 0.5456,
      "step": 5540
    },
    {
      "epoch": 0.3503787878787879,
      "grad_norm": 1.081998348236084,
      "learning_rate": 0.00018915327221606854,
      "loss": 0.5583,
      "step": 5550
    },
    {
      "epoch": 0.351010101010101,
      "grad_norm": 0.5806440711021423,
      "learning_rate": 0.0001891069174488149,
      "loss": 0.7725,
      "step": 5560
    },
    {
      "epoch": 0.35164141414141414,
      "grad_norm": 0.6845955848693848,
      "learning_rate": 0.0001890604695462709,
      "loss": 0.6665,
      "step": 5570
    },
    {
      "epoch": 0.3522727272727273,
      "grad_norm": 0.7361495494842529,
      "learning_rate": 0.0001890139285569843,
      "loss": 0.589,
      "step": 5580
    },
    {
      "epoch": 0.3529040404040404,
      "grad_norm": 0.7890605926513672,
      "learning_rate": 0.00018896729452960015,
      "loss": 0.542,
      "step": 5590
    },
    {
      "epoch": 0.35353535353535354,
      "grad_norm": 1.4187060594558716,
      "learning_rate": 0.00018892056751286073,
      "loss": 0.5633,
      "step": 5600
    },
    {
      "epoch": 0.3541666666666667,
      "grad_norm": 0.573908269405365,
      "learning_rate": 0.0001888737475556055,
      "loss": 0.7618,
      "step": 5610
    },
    {
      "epoch": 0.3547979797979798,
      "grad_norm": 0.6761958003044128,
      "learning_rate": 0.00018882683470677103,
      "loss": 0.6585,
      "step": 5620
    },
    {
      "epoch": 0.35542929292929293,
      "grad_norm": 0.7293843030929565,
      "learning_rate": 0.00018877982901539103,
      "loss": 0.6277,
      "step": 5630
    },
    {
      "epoch": 0.3560606060606061,
      "grad_norm": 0.8363537192344666,
      "learning_rate": 0.00018873273053059627,
      "loss": 0.5663,
      "step": 5640
    },
    {
      "epoch": 0.35669191919191917,
      "grad_norm": 1.3587478399276733,
      "learning_rate": 0.00018868553930161447,
      "loss": 0.5326,
      "step": 5650
    },
    {
      "epoch": 0.3573232323232323,
      "grad_norm": 0.6028680801391602,
      "learning_rate": 0.00018863825537777026,
      "loss": 0.7792,
      "step": 5660
    },
    {
      "epoch": 0.35795454545454547,
      "grad_norm": 0.6457834839820862,
      "learning_rate": 0.00018859087880848525,
      "loss": 0.661,
      "step": 5670
    },
    {
      "epoch": 0.35858585858585856,
      "grad_norm": 0.7663736939430237,
      "learning_rate": 0.0001885434096432778,
      "loss": 0.5807,
      "step": 5680
    },
    {
      "epoch": 0.3592171717171717,
      "grad_norm": 0.8832402229309082,
      "learning_rate": 0.00018849584793176303,
      "loss": 0.5543,
      "step": 5690
    },
    {
      "epoch": 0.35984848484848486,
      "grad_norm": 1.1062904596328735,
      "learning_rate": 0.00018844819372365286,
      "loss": 0.5572,
      "step": 5700
    },
    {
      "epoch": 0.36047979797979796,
      "grad_norm": 0.5973621606826782,
      "learning_rate": 0.0001884004470687559,
      "loss": 0.7711,
      "step": 5710
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 0.6439493298530579,
      "learning_rate": 0.00018835260801697734,
      "loss": 0.6446,
      "step": 5720
    },
    {
      "epoch": 0.36174242424242425,
      "grad_norm": 0.6515389680862427,
      "learning_rate": 0.00018830467661831891,
      "loss": 0.5828,
      "step": 5730
    },
    {
      "epoch": 0.36237373737373735,
      "grad_norm": 0.797014057636261,
      "learning_rate": 0.00018825665292287894,
      "loss": 0.5391,
      "step": 5740
    },
    {
      "epoch": 0.3630050505050505,
      "grad_norm": 0.9291923642158508,
      "learning_rate": 0.0001882085369808522,
      "loss": 0.5817,
      "step": 5750
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.5489802360534668,
      "learning_rate": 0.00018816032884252988,
      "loss": 0.7893,
      "step": 5760
    },
    {
      "epoch": 0.36426767676767674,
      "grad_norm": 0.7280834317207336,
      "learning_rate": 0.0001881120285582995,
      "loss": 0.6722,
      "step": 5770
    },
    {
      "epoch": 0.3648989898989899,
      "grad_norm": 0.861813485622406,
      "learning_rate": 0.00018806363617864493,
      "loss": 0.5882,
      "step": 5780
    },
    {
      "epoch": 0.36553030303030304,
      "grad_norm": 0.8888502717018127,
      "learning_rate": 0.00018801515175414629,
      "loss": 0.5327,
      "step": 5790
    },
    {
      "epoch": 0.3661616161616162,
      "grad_norm": 1.318236231803894,
      "learning_rate": 0.00018796657533547988,
      "loss": 0.5337,
      "step": 5800
    },
    {
      "epoch": 0.3667929292929293,
      "grad_norm": 0.5998983979225159,
      "learning_rate": 0.0001879179069734182,
      "loss": 0.8128,
      "step": 5810
    },
    {
      "epoch": 0.36742424242424243,
      "grad_norm": 0.6808114647865295,
      "learning_rate": 0.00018786914671882983,
      "loss": 0.668,
      "step": 5820
    },
    {
      "epoch": 0.3680555555555556,
      "grad_norm": 0.7133130431175232,
      "learning_rate": 0.0001878202946226794,
      "loss": 0.5761,
      "step": 5830
    },
    {
      "epoch": 0.3686868686868687,
      "grad_norm": 0.8375285267829895,
      "learning_rate": 0.00018777135073602748,
      "loss": 0.5452,
      "step": 5840
    },
    {
      "epoch": 0.3693181818181818,
      "grad_norm": 0.9664574265480042,
      "learning_rate": 0.00018772231511003068,
      "loss": 0.5618,
      "step": 5850
    },
    {
      "epoch": 0.369949494949495,
      "grad_norm": 0.5635879039764404,
      "learning_rate": 0.0001876731877959414,
      "loss": 0.7609,
      "step": 5860
    },
    {
      "epoch": 0.37058080808080807,
      "grad_norm": 0.66936194896698,
      "learning_rate": 0.00018762396884510797,
      "loss": 0.6753,
      "step": 5870
    },
    {
      "epoch": 0.3712121212121212,
      "grad_norm": 0.7512418031692505,
      "learning_rate": 0.0001875746583089744,
      "loss": 0.5749,
      "step": 5880
    },
    {
      "epoch": 0.37184343434343436,
      "grad_norm": 0.9039126038551331,
      "learning_rate": 0.0001875252562390805,
      "loss": 0.5294,
      "step": 5890
    },
    {
      "epoch": 0.37247474747474746,
      "grad_norm": 1.146936297416687,
      "learning_rate": 0.00018747576268706172,
      "loss": 0.557,
      "step": 5900
    },
    {
      "epoch": 0.3731060606060606,
      "grad_norm": 0.5891149044036865,
      "learning_rate": 0.0001874261777046491,
      "loss": 0.787,
      "step": 5910
    },
    {
      "epoch": 0.37373737373737376,
      "grad_norm": 0.6318565011024475,
      "learning_rate": 0.00018737650134366927,
      "loss": 0.6267,
      "step": 5920
    },
    {
      "epoch": 0.37436868686868685,
      "grad_norm": 0.7535669207572937,
      "learning_rate": 0.00018732673365604447,
      "loss": 0.57,
      "step": 5930
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.7772605419158936,
      "learning_rate": 0.0001872768746937922,
      "loss": 0.5439,
      "step": 5940
    },
    {
      "epoch": 0.37563131313131315,
      "grad_norm": 1.2650660276412964,
      "learning_rate": 0.00018722692450902551,
      "loss": 0.5343,
      "step": 5950
    },
    {
      "epoch": 0.37626262626262624,
      "grad_norm": 0.5863489508628845,
      "learning_rate": 0.0001871768831539527,
      "loss": 0.7943,
      "step": 5960
    },
    {
      "epoch": 0.3768939393939394,
      "grad_norm": 0.6829921007156372,
      "learning_rate": 0.00018712675068087746,
      "loss": 0.6401,
      "step": 5970
    },
    {
      "epoch": 0.37752525252525254,
      "grad_norm": 0.9169061779975891,
      "learning_rate": 0.00018707652714219868,
      "loss": 0.59,
      "step": 5980
    },
    {
      "epoch": 0.37815656565656564,
      "grad_norm": 0.7714381814002991,
      "learning_rate": 0.00018702621259041036,
      "loss": 0.5238,
      "step": 5990
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 0.9657641053199768,
      "learning_rate": 0.00018697580707810173,
      "loss": 0.5391,
      "step": 6000
    },
    {
      "epoch": 0.3787878787878788,
      "eval_loss": 0.6083608865737915,
      "eval_runtime": 27.409,
      "eval_samples_per_second": 93.4,
      "eval_steps_per_second": 11.675,
      "step": 6000
    },
    {
      "epoch": 0.37941919191919193,
      "grad_norm": 0.5433776378631592,
      "learning_rate": 0.00018692531065795702,
      "loss": 0.7675,
      "step": 6010
    },
    {
      "epoch": 0.380050505050505,
      "grad_norm": 0.6648391485214233,
      "learning_rate": 0.00018687472338275557,
      "loss": 0.6569,
      "step": 6020
    },
    {
      "epoch": 0.3806818181818182,
      "grad_norm": 0.6911960244178772,
      "learning_rate": 0.00018682404530537155,
      "loss": 0.5856,
      "step": 6030
    },
    {
      "epoch": 0.3813131313131313,
      "grad_norm": 0.7883504033088684,
      "learning_rate": 0.00018677327647877412,
      "loss": 0.5385,
      "step": 6040
    },
    {
      "epoch": 0.3819444444444444,
      "grad_norm": 1.1119394302368164,
      "learning_rate": 0.00018672241695602733,
      "loss": 0.5584,
      "step": 6050
    },
    {
      "epoch": 0.38257575757575757,
      "grad_norm": 0.5715657472610474,
      "learning_rate": 0.0001866714667902899,
      "loss": 0.7979,
      "step": 6060
    },
    {
      "epoch": 0.3832070707070707,
      "grad_norm": 0.6526241898536682,
      "learning_rate": 0.00018662042603481542,
      "loss": 0.6476,
      "step": 6070
    },
    {
      "epoch": 0.3838383838383838,
      "grad_norm": 0.7770770192146301,
      "learning_rate": 0.00018656929474295209,
      "loss": 0.5941,
      "step": 6080
    },
    {
      "epoch": 0.38446969696969696,
      "grad_norm": 0.9009290933609009,
      "learning_rate": 0.00018651807296814278,
      "loss": 0.529,
      "step": 6090
    },
    {
      "epoch": 0.3851010101010101,
      "grad_norm": 1.033205270767212,
      "learning_rate": 0.0001864667607639249,
      "loss": 0.5541,
      "step": 6100
    },
    {
      "epoch": 0.38573232323232326,
      "grad_norm": 0.5942338705062866,
      "learning_rate": 0.0001864153581839304,
      "loss": 0.7525,
      "step": 6110
    },
    {
      "epoch": 0.38636363636363635,
      "grad_norm": 0.7236207127571106,
      "learning_rate": 0.00018636386528188568,
      "loss": 0.6637,
      "step": 6120
    },
    {
      "epoch": 0.3869949494949495,
      "grad_norm": 0.7482415437698364,
      "learning_rate": 0.00018631228211161152,
      "loss": 0.6017,
      "step": 6130
    },
    {
      "epoch": 0.38762626262626265,
      "grad_norm": 0.8524355292320251,
      "learning_rate": 0.00018626060872702313,
      "loss": 0.5475,
      "step": 6140
    },
    {
      "epoch": 0.38825757575757575,
      "grad_norm": 1.0196795463562012,
      "learning_rate": 0.00018620884518212995,
      "loss": 0.5287,
      "step": 6150
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.5488223433494568,
      "learning_rate": 0.00018615699153103562,
      "loss": 0.7936,
      "step": 6160
    },
    {
      "epoch": 0.38952020202020204,
      "grad_norm": 0.7334930896759033,
      "learning_rate": 0.00018610504782793808,
      "loss": 0.6859,
      "step": 6170
    },
    {
      "epoch": 0.39015151515151514,
      "grad_norm": 0.825121283531189,
      "learning_rate": 0.00018605301412712922,
      "loss": 0.5811,
      "step": 6180
    },
    {
      "epoch": 0.3907828282828283,
      "grad_norm": 0.8506012558937073,
      "learning_rate": 0.0001860008904829952,
      "loss": 0.5203,
      "step": 6190
    },
    {
      "epoch": 0.39141414141414144,
      "grad_norm": 1.061792254447937,
      "learning_rate": 0.00018594867695001605,
      "loss": 0.5837,
      "step": 6200
    },
    {
      "epoch": 0.39204545454545453,
      "grad_norm": 0.5806285738945007,
      "learning_rate": 0.00018589637358276578,
      "loss": 0.77,
      "step": 6210
    },
    {
      "epoch": 0.3926767676767677,
      "grad_norm": 0.6656145453453064,
      "learning_rate": 0.0001858439804359123,
      "loss": 0.6648,
      "step": 6220
    },
    {
      "epoch": 0.39330808080808083,
      "grad_norm": 0.7021386027336121,
      "learning_rate": 0.00018579149756421735,
      "loss": 0.592,
      "step": 6230
    },
    {
      "epoch": 0.3939393939393939,
      "grad_norm": 0.9116240739822388,
      "learning_rate": 0.0001857389250225365,
      "loss": 0.5052,
      "step": 6240
    },
    {
      "epoch": 0.39457070707070707,
      "grad_norm": 1.116866111755371,
      "learning_rate": 0.00018568626286581897,
      "loss": 0.5371,
      "step": 6250
    },
    {
      "epoch": 0.3952020202020202,
      "grad_norm": 0.6154735088348389,
      "learning_rate": 0.0001856335111491077,
      "loss": 0.7853,
      "step": 6260
    },
    {
      "epoch": 0.3958333333333333,
      "grad_norm": 0.6758896112442017,
      "learning_rate": 0.00018558066992753925,
      "loss": 0.6464,
      "step": 6270
    },
    {
      "epoch": 0.39646464646464646,
      "grad_norm": 0.7455413937568665,
      "learning_rate": 0.00018552773925634367,
      "loss": 0.5678,
      "step": 6280
    },
    {
      "epoch": 0.3970959595959596,
      "grad_norm": 0.7612907886505127,
      "learning_rate": 0.00018547471919084453,
      "loss": 0.5226,
      "step": 6290
    },
    {
      "epoch": 0.3977272727272727,
      "grad_norm": 1.1246331930160522,
      "learning_rate": 0.00018542160978645886,
      "loss": 0.5358,
      "step": 6300
    },
    {
      "epoch": 0.39835858585858586,
      "grad_norm": 0.6579055190086365,
      "learning_rate": 0.00018536841109869704,
      "loss": 0.7494,
      "step": 6310
    },
    {
      "epoch": 0.398989898989899,
      "grad_norm": 0.7879274487495422,
      "learning_rate": 0.00018531512318316283,
      "loss": 0.6677,
      "step": 6320
    },
    {
      "epoch": 0.3996212121212121,
      "grad_norm": 0.6887534856796265,
      "learning_rate": 0.0001852617460955531,
      "loss": 0.5894,
      "step": 6330
    },
    {
      "epoch": 0.40025252525252525,
      "grad_norm": 0.7911218404769897,
      "learning_rate": 0.00018520827989165813,
      "loss": 0.5258,
      "step": 6340
    },
    {
      "epoch": 0.4008838383838384,
      "grad_norm": 1.0992769002914429,
      "learning_rate": 0.0001851547246273612,
      "loss": 0.5665,
      "step": 6350
    },
    {
      "epoch": 0.4015151515151515,
      "grad_norm": 0.581439733505249,
      "learning_rate": 0.00018510108035863868,
      "loss": 0.7713,
      "step": 6360
    },
    {
      "epoch": 0.40214646464646464,
      "grad_norm": 0.6784172654151917,
      "learning_rate": 0.00018504734714156008,
      "loss": 0.6303,
      "step": 6370
    },
    {
      "epoch": 0.4027777777777778,
      "grad_norm": 0.7228202223777771,
      "learning_rate": 0.00018499352503228774,
      "loss": 0.5847,
      "step": 6380
    },
    {
      "epoch": 0.4034090909090909,
      "grad_norm": 0.8072268962860107,
      "learning_rate": 0.000184939614087077,
      "loss": 0.508,
      "step": 6390
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 0.9974951148033142,
      "learning_rate": 0.00018488561436227603,
      "loss": 0.5387,
      "step": 6400
    },
    {
      "epoch": 0.4046717171717172,
      "grad_norm": 0.6997259259223938,
      "learning_rate": 0.0001848315259143258,
      "loss": 0.8198,
      "step": 6410
    },
    {
      "epoch": 0.4053030303030303,
      "grad_norm": 0.7578186988830566,
      "learning_rate": 0.00018477734879976,
      "loss": 0.6636,
      "step": 6420
    },
    {
      "epoch": 0.4059343434343434,
      "grad_norm": 0.6908935904502869,
      "learning_rate": 0.00018472308307520497,
      "loss": 0.5877,
      "step": 6430
    },
    {
      "epoch": 0.4065656565656566,
      "grad_norm": 0.823662519454956,
      "learning_rate": 0.0001846687287973797,
      "loss": 0.5326,
      "step": 6440
    },
    {
      "epoch": 0.4071969696969697,
      "grad_norm": 1.0582275390625,
      "learning_rate": 0.0001846142860230958,
      "loss": 0.5284,
      "step": 6450
    },
    {
      "epoch": 0.4078282828282828,
      "grad_norm": 0.611077070236206,
      "learning_rate": 0.00018455975480925722,
      "loss": 0.8242,
      "step": 6460
    },
    {
      "epoch": 0.40845959595959597,
      "grad_norm": 0.7780469655990601,
      "learning_rate": 0.0001845051352128605,
      "loss": 0.6677,
      "step": 6470
    },
    {
      "epoch": 0.4090909090909091,
      "grad_norm": 0.7325640320777893,
      "learning_rate": 0.00018445042729099445,
      "loss": 0.5845,
      "step": 6480
    },
    {
      "epoch": 0.4097222222222222,
      "grad_norm": 0.7049621343612671,
      "learning_rate": 0.00018439563110084033,
      "loss": 0.5487,
      "step": 6490
    },
    {
      "epoch": 0.41035353535353536,
      "grad_norm": 1.2103140354156494,
      "learning_rate": 0.00018434074669967148,
      "loss": 0.5398,
      "step": 6500
    },
    {
      "epoch": 0.4109848484848485,
      "grad_norm": 0.594965398311615,
      "learning_rate": 0.00018428577414485357,
      "loss": 0.7614,
      "step": 6510
    },
    {
      "epoch": 0.4116161616161616,
      "grad_norm": 0.623036801815033,
      "learning_rate": 0.00018423071349384435,
      "loss": 0.6692,
      "step": 6520
    },
    {
      "epoch": 0.41224747474747475,
      "grad_norm": 0.7474129796028137,
      "learning_rate": 0.00018417556480419372,
      "loss": 0.5522,
      "step": 6530
    },
    {
      "epoch": 0.4128787878787879,
      "grad_norm": 0.7588824033737183,
      "learning_rate": 0.00018412032813354347,
      "loss": 0.5217,
      "step": 6540
    },
    {
      "epoch": 0.413510101010101,
      "grad_norm": 0.9659335613250732,
      "learning_rate": 0.0001840650035396275,
      "loss": 0.5261,
      "step": 6550
    },
    {
      "epoch": 0.41414141414141414,
      "grad_norm": 0.5676929950714111,
      "learning_rate": 0.0001840095910802715,
      "loss": 0.7588,
      "step": 6560
    },
    {
      "epoch": 0.4147727272727273,
      "grad_norm": 0.6372143626213074,
      "learning_rate": 0.00018395409081339305,
      "loss": 0.6365,
      "step": 6570
    },
    {
      "epoch": 0.4154040404040404,
      "grad_norm": 0.7602266669273376,
      "learning_rate": 0.00018389850279700148,
      "loss": 0.5762,
      "step": 6580
    },
    {
      "epoch": 0.41603535353535354,
      "grad_norm": 0.8911465406417847,
      "learning_rate": 0.00018384282708919784,
      "loss": 0.5141,
      "step": 6590
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.9443477988243103,
      "learning_rate": 0.00018378706374817485,
      "loss": 0.5335,
      "step": 6600
    },
    {
      "epoch": 0.4172979797979798,
      "grad_norm": 0.6008862257003784,
      "learning_rate": 0.00018373121283221682,
      "loss": 0.7393,
      "step": 6610
    },
    {
      "epoch": 0.41792929292929293,
      "grad_norm": 0.7743738889694214,
      "learning_rate": 0.00018367527439969958,
      "loss": 0.6205,
      "step": 6620
    },
    {
      "epoch": 0.4185606060606061,
      "grad_norm": 0.7291122674942017,
      "learning_rate": 0.00018361924850909044,
      "loss": 0.567,
      "step": 6630
    },
    {
      "epoch": 0.41919191919191917,
      "grad_norm": 0.7600089311599731,
      "learning_rate": 0.00018356313521894816,
      "loss": 0.5141,
      "step": 6640
    },
    {
      "epoch": 0.4198232323232323,
      "grad_norm": 1.0716909170150757,
      "learning_rate": 0.00018350693458792279,
      "loss": 0.5423,
      "step": 6650
    },
    {
      "epoch": 0.42045454545454547,
      "grad_norm": 0.6891409754753113,
      "learning_rate": 0.0001834506466747557,
      "loss": 0.7655,
      "step": 6660
    },
    {
      "epoch": 0.42108585858585856,
      "grad_norm": 0.6870277523994446,
      "learning_rate": 0.0001833942715382795,
      "loss": 0.64,
      "step": 6670
    },
    {
      "epoch": 0.4217171717171717,
      "grad_norm": 0.6293309926986694,
      "learning_rate": 0.00018333780923741788,
      "loss": 0.5451,
      "step": 6680
    },
    {
      "epoch": 0.42234848484848486,
      "grad_norm": 0.8664209246635437,
      "learning_rate": 0.0001832812598311858,
      "loss": 0.5201,
      "step": 6690
    },
    {
      "epoch": 0.42297979797979796,
      "grad_norm": 0.9683524966239929,
      "learning_rate": 0.00018322462337868914,
      "loss": 0.5334,
      "step": 6700
    },
    {
      "epoch": 0.4236111111111111,
      "grad_norm": 0.6050415635108948,
      "learning_rate": 0.00018316789993912477,
      "loss": 0.7712,
      "step": 6710
    },
    {
      "epoch": 0.42424242424242425,
      "grad_norm": 0.7022687792778015,
      "learning_rate": 0.0001831110895717805,
      "loss": 0.6321,
      "step": 6720
    },
    {
      "epoch": 0.42487373737373735,
      "grad_norm": 0.6981886029243469,
      "learning_rate": 0.00018305419233603508,
      "loss": 0.5857,
      "step": 6730
    },
    {
      "epoch": 0.4255050505050505,
      "grad_norm": 0.8014115691184998,
      "learning_rate": 0.00018299720829135786,
      "loss": 0.5373,
      "step": 6740
    },
    {
      "epoch": 0.42613636363636365,
      "grad_norm": 0.9578883647918701,
      "learning_rate": 0.00018294013749730904,
      "loss": 0.5681,
      "step": 6750
    },
    {
      "epoch": 0.42676767676767674,
      "grad_norm": 0.6189029216766357,
      "learning_rate": 0.00018288298001353957,
      "loss": 0.7301,
      "step": 6760
    },
    {
      "epoch": 0.4273989898989899,
      "grad_norm": 0.6787287592887878,
      "learning_rate": 0.00018282573589979085,
      "loss": 0.6215,
      "step": 6770
    },
    {
      "epoch": 0.42803030303030304,
      "grad_norm": 0.8055798411369324,
      "learning_rate": 0.00018276840521589497,
      "loss": 0.566,
      "step": 6780
    },
    {
      "epoch": 0.4286616161616162,
      "grad_norm": 0.9095854759216309,
      "learning_rate": 0.0001827109880217744,
      "loss": 0.5247,
      "step": 6790
    },
    {
      "epoch": 0.4292929292929293,
      "grad_norm": 0.9432606101036072,
      "learning_rate": 0.0001826534843774421,
      "loss": 0.5338,
      "step": 6800
    },
    {
      "epoch": 0.42992424242424243,
      "grad_norm": 0.5806302428245544,
      "learning_rate": 0.0001825958943430013,
      "loss": 0.718,
      "step": 6810
    },
    {
      "epoch": 0.4305555555555556,
      "grad_norm": 0.7472066283226013,
      "learning_rate": 0.00018253821797864562,
      "loss": 0.6416,
      "step": 6820
    },
    {
      "epoch": 0.4311868686868687,
      "grad_norm": 0.7274330258369446,
      "learning_rate": 0.00018248045534465884,
      "loss": 0.5518,
      "step": 6830
    },
    {
      "epoch": 0.4318181818181818,
      "grad_norm": 0.777045488357544,
      "learning_rate": 0.00018242260650141502,
      "loss": 0.5344,
      "step": 6840
    },
    {
      "epoch": 0.432449494949495,
      "grad_norm": 1.1557681560516357,
      "learning_rate": 0.0001823646715093782,
      "loss": 0.5489,
      "step": 6850
    },
    {
      "epoch": 0.43308080808080807,
      "grad_norm": 0.5989468693733215,
      "learning_rate": 0.00018230665042910248,
      "loss": 0.7759,
      "step": 6860
    },
    {
      "epoch": 0.4337121212121212,
      "grad_norm": 0.6763079166412354,
      "learning_rate": 0.00018224854332123206,
      "loss": 0.6468,
      "step": 6870
    },
    {
      "epoch": 0.43434343434343436,
      "grad_norm": 0.8058540225028992,
      "learning_rate": 0.0001821903502465009,
      "loss": 0.5782,
      "step": 6880
    },
    {
      "epoch": 0.43497474747474746,
      "grad_norm": 0.8007886409759521,
      "learning_rate": 0.00018213207126573292,
      "loss": 0.5087,
      "step": 6890
    },
    {
      "epoch": 0.4356060606060606,
      "grad_norm": 1.3041198253631592,
      "learning_rate": 0.00018207370643984178,
      "loss": 0.5437,
      "step": 6900
    },
    {
      "epoch": 0.43623737373737376,
      "grad_norm": 0.6729303002357483,
      "learning_rate": 0.0001820152558298309,
      "loss": 0.7478,
      "step": 6910
    },
    {
      "epoch": 0.43686868686868685,
      "grad_norm": 0.7168794274330139,
      "learning_rate": 0.00018195671949679333,
      "loss": 0.6327,
      "step": 6920
    },
    {
      "epoch": 0.4375,
      "grad_norm": 0.7596614956855774,
      "learning_rate": 0.0001818980975019117,
      "loss": 0.5482,
      "step": 6930
    },
    {
      "epoch": 0.43813131313131315,
      "grad_norm": 0.8045967817306519,
      "learning_rate": 0.00018183938990645827,
      "loss": 0.5066,
      "step": 6940
    },
    {
      "epoch": 0.43876262626262624,
      "grad_norm": 1.0041868686676025,
      "learning_rate": 0.00018178059677179467,
      "loss": 0.5669,
      "step": 6950
    },
    {
      "epoch": 0.4393939393939394,
      "grad_norm": 0.6689700484275818,
      "learning_rate": 0.00018172171815937195,
      "loss": 0.7325,
      "step": 6960
    },
    {
      "epoch": 0.44002525252525254,
      "grad_norm": 0.7198066115379333,
      "learning_rate": 0.00018166275413073062,
      "loss": 0.6622,
      "step": 6970
    },
    {
      "epoch": 0.44065656565656564,
      "grad_norm": 0.8159673810005188,
      "learning_rate": 0.00018160370474750023,
      "loss": 0.5726,
      "step": 6980
    },
    {
      "epoch": 0.4412878787878788,
      "grad_norm": 0.6863812804222107,
      "learning_rate": 0.0001815445700713998,
      "loss": 0.5362,
      "step": 6990
    },
    {
      "epoch": 0.44191919191919193,
      "grad_norm": 0.9821881651878357,
      "learning_rate": 0.00018148535016423734,
      "loss": 0.5441,
      "step": 7000
    },
    {
      "epoch": 0.44191919191919193,
      "eval_loss": 0.5908827781677246,
      "eval_runtime": 27.3746,
      "eval_samples_per_second": 93.517,
      "eval_steps_per_second": 11.69,
      "step": 7000
    },
    {
      "epoch": 0.442550505050505,
      "grad_norm": 0.6582397818565369,
      "learning_rate": 0.00018142604508791,
      "loss": 0.719,
      "step": 7010
    },
    {
      "epoch": 0.4431818181818182,
      "grad_norm": 0.6876886487007141,
      "learning_rate": 0.00018136665490440393,
      "loss": 0.6558,
      "step": 7020
    },
    {
      "epoch": 0.4438131313131313,
      "grad_norm": 0.6641818881034851,
      "learning_rate": 0.00018130717967579423,
      "loss": 0.564,
      "step": 7030
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.7168505191802979,
      "learning_rate": 0.00018124761946424492,
      "loss": 0.5157,
      "step": 7040
    },
    {
      "epoch": 0.44507575757575757,
      "grad_norm": 0.944426417350769,
      "learning_rate": 0.00018118797433200882,
      "loss": 0.5181,
      "step": 7050
    },
    {
      "epoch": 0.4457070707070707,
      "grad_norm": 0.660180926322937,
      "learning_rate": 0.00018112824434142753,
      "loss": 0.831,
      "step": 7060
    },
    {
      "epoch": 0.4463383838383838,
      "grad_norm": 0.6853012442588806,
      "learning_rate": 0.00018106842955493133,
      "loss": 0.6381,
      "step": 7070
    },
    {
      "epoch": 0.44696969696969696,
      "grad_norm": 0.6295440793037415,
      "learning_rate": 0.00018100853003503916,
      "loss": 0.5765,
      "step": 7080
    },
    {
      "epoch": 0.4476010101010101,
      "grad_norm": 0.7571380734443665,
      "learning_rate": 0.00018094854584435843,
      "loss": 0.493,
      "step": 7090
    },
    {
      "epoch": 0.44823232323232326,
      "grad_norm": 0.8866646885871887,
      "learning_rate": 0.00018088847704558517,
      "loss": 0.545,
      "step": 7100
    },
    {
      "epoch": 0.44886363636363635,
      "grad_norm": 0.6081569790840149,
      "learning_rate": 0.00018082832370150374,
      "loss": 0.7195,
      "step": 7110
    },
    {
      "epoch": 0.4494949494949495,
      "grad_norm": 0.7159175872802734,
      "learning_rate": 0.00018076808587498696,
      "loss": 0.6225,
      "step": 7120
    },
    {
      "epoch": 0.45012626262626265,
      "grad_norm": 0.6093949675559998,
      "learning_rate": 0.00018070776362899587,
      "loss": 0.5654,
      "step": 7130
    },
    {
      "epoch": 0.45075757575757575,
      "grad_norm": 0.8400552868843079,
      "learning_rate": 0.0001806473570265798,
      "loss": 0.5062,
      "step": 7140
    },
    {
      "epoch": 0.4513888888888889,
      "grad_norm": 0.963050127029419,
      "learning_rate": 0.00018058686613087624,
      "loss": 0.5249,
      "step": 7150
    },
    {
      "epoch": 0.45202020202020204,
      "grad_norm": 0.5821149349212646,
      "learning_rate": 0.00018052629100511077,
      "loss": 0.7557,
      "step": 7160
    },
    {
      "epoch": 0.45265151515151514,
      "grad_norm": 0.644615888595581,
      "learning_rate": 0.00018046563171259701,
      "loss": 0.667,
      "step": 7170
    },
    {
      "epoch": 0.4532828282828283,
      "grad_norm": 0.6924688220024109,
      "learning_rate": 0.00018040488831673655,
      "loss": 0.5744,
      "step": 7180
    },
    {
      "epoch": 0.45391414141414144,
      "grad_norm": 0.8942870497703552,
      "learning_rate": 0.00018034406088101893,
      "loss": 0.5427,
      "step": 7190
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.9686787128448486,
      "learning_rate": 0.00018028314946902144,
      "loss": 0.5296,
      "step": 7200
    },
    {
      "epoch": 0.4551767676767677,
      "grad_norm": 0.5707201361656189,
      "learning_rate": 0.00018022215414440924,
      "loss": 0.7414,
      "step": 7210
    },
    {
      "epoch": 0.45580808080808083,
      "grad_norm": 0.618965744972229,
      "learning_rate": 0.00018016107497093514,
      "loss": 0.6463,
      "step": 7220
    },
    {
      "epoch": 0.4564393939393939,
      "grad_norm": 0.7574859261512756,
      "learning_rate": 0.00018009991201243955,
      "loss": 0.5733,
      "step": 7230
    },
    {
      "epoch": 0.45707070707070707,
      "grad_norm": 0.6828228235244751,
      "learning_rate": 0.00018003866533285054,
      "loss": 0.4784,
      "step": 7240
    },
    {
      "epoch": 0.4577020202020202,
      "grad_norm": 1.163494348526001,
      "learning_rate": 0.00017997733499618365,
      "loss": 0.5776,
      "step": 7250
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.5958918333053589,
      "learning_rate": 0.00017991592106654186,
      "loss": 0.7468,
      "step": 7260
    },
    {
      "epoch": 0.45896464646464646,
      "grad_norm": 0.6819302439689636,
      "learning_rate": 0.00017985442360811553,
      "loss": 0.6071,
      "step": 7270
    },
    {
      "epoch": 0.4595959595959596,
      "grad_norm": 0.8120288252830505,
      "learning_rate": 0.00017979284268518228,
      "loss": 0.5528,
      "step": 7280
    },
    {
      "epoch": 0.4602272727272727,
      "grad_norm": 0.7200056910514832,
      "learning_rate": 0.00017973117836210702,
      "loss": 0.5421,
      "step": 7290
    },
    {
      "epoch": 0.46085858585858586,
      "grad_norm": 1.1278009414672852,
      "learning_rate": 0.00017966943070334184,
      "loss": 0.5113,
      "step": 7300
    },
    {
      "epoch": 0.461489898989899,
      "grad_norm": 0.6265709400177002,
      "learning_rate": 0.00017960759977342586,
      "loss": 0.7388,
      "step": 7310
    },
    {
      "epoch": 0.4621212121212121,
      "grad_norm": 0.6382631063461304,
      "learning_rate": 0.0001795456856369853,
      "loss": 0.6246,
      "step": 7320
    },
    {
      "epoch": 0.46275252525252525,
      "grad_norm": 0.7256936430931091,
      "learning_rate": 0.00017948368835873332,
      "loss": 0.5802,
      "step": 7330
    },
    {
      "epoch": 0.4633838383838384,
      "grad_norm": 0.755691647529602,
      "learning_rate": 0.00017942160800347,
      "loss": 0.5068,
      "step": 7340
    },
    {
      "epoch": 0.4640151515151515,
      "grad_norm": 1.0924580097198486,
      "learning_rate": 0.00017935944463608227,
      "loss": 0.5494,
      "step": 7350
    },
    {
      "epoch": 0.46464646464646464,
      "grad_norm": 0.669704794883728,
      "learning_rate": 0.00017929719832154376,
      "loss": 0.7261,
      "step": 7360
    },
    {
      "epoch": 0.4652777777777778,
      "grad_norm": 0.6547823548316956,
      "learning_rate": 0.00017923486912491482,
      "loss": 0.6524,
      "step": 7370
    },
    {
      "epoch": 0.4659090909090909,
      "grad_norm": 0.692944347858429,
      "learning_rate": 0.0001791724571113425,
      "loss": 0.5674,
      "step": 7380
    },
    {
      "epoch": 0.46654040404040403,
      "grad_norm": 0.6874088048934937,
      "learning_rate": 0.0001791099623460603,
      "loss": 0.4861,
      "step": 7390
    },
    {
      "epoch": 0.4671717171717172,
      "grad_norm": 1.0708526372909546,
      "learning_rate": 0.00017904738489438836,
      "loss": 0.5307,
      "step": 7400
    },
    {
      "epoch": 0.4678030303030303,
      "grad_norm": 0.6836604475975037,
      "learning_rate": 0.00017898472482173302,
      "loss": 0.7085,
      "step": 7410
    },
    {
      "epoch": 0.4684343434343434,
      "grad_norm": 0.6248346567153931,
      "learning_rate": 0.0001789219821935872,
      "loss": 0.6257,
      "step": 7420
    },
    {
      "epoch": 0.4690656565656566,
      "grad_norm": 0.7317250370979309,
      "learning_rate": 0.00017885915707552998,
      "loss": 0.5405,
      "step": 7430
    },
    {
      "epoch": 0.4696969696969697,
      "grad_norm": 0.862128734588623,
      "learning_rate": 0.0001787962495332267,
      "loss": 0.501,
      "step": 7440
    },
    {
      "epoch": 0.4703282828282828,
      "grad_norm": 0.889229416847229,
      "learning_rate": 0.00017873325963242888,
      "loss": 0.5304,
      "step": 7450
    },
    {
      "epoch": 0.47095959595959597,
      "grad_norm": 0.582894504070282,
      "learning_rate": 0.00017867018743897406,
      "loss": 0.7615,
      "step": 7460
    },
    {
      "epoch": 0.4715909090909091,
      "grad_norm": 0.6623352766036987,
      "learning_rate": 0.0001786070330187858,
      "loss": 0.63,
      "step": 7470
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 0.7342755794525146,
      "learning_rate": 0.00017854379643787363,
      "loss": 0.5656,
      "step": 7480
    },
    {
      "epoch": 0.47285353535353536,
      "grad_norm": 0.8783890604972839,
      "learning_rate": 0.000178480477762333,
      "loss": 0.5225,
      "step": 7490
    },
    {
      "epoch": 0.4734848484848485,
      "grad_norm": 1.0121986865997314,
      "learning_rate": 0.00017841707705834505,
      "loss": 0.5334,
      "step": 7500
    },
    {
      "epoch": 0.4741161616161616,
      "grad_norm": 0.572057843208313,
      "learning_rate": 0.00017835359439217677,
      "loss": 0.7659,
      "step": 7510
    },
    {
      "epoch": 0.47474747474747475,
      "grad_norm": 0.6950221061706543,
      "learning_rate": 0.00017829002983018075,
      "loss": 0.6317,
      "step": 7520
    },
    {
      "epoch": 0.4753787878787879,
      "grad_norm": 0.654387891292572,
      "learning_rate": 0.0001782263834387952,
      "loss": 0.6124,
      "step": 7530
    },
    {
      "epoch": 0.476010101010101,
      "grad_norm": 0.7840420007705688,
      "learning_rate": 0.00017816265528454382,
      "loss": 0.5061,
      "step": 7540
    },
    {
      "epoch": 0.47664141414141414,
      "grad_norm": 1.003827452659607,
      "learning_rate": 0.0001780988454340359,
      "loss": 0.5301,
      "step": 7550
    },
    {
      "epoch": 0.4772727272727273,
      "grad_norm": 0.6165429353713989,
      "learning_rate": 0.00017803495395396593,
      "loss": 0.7529,
      "step": 7560
    },
    {
      "epoch": 0.4779040404040404,
      "grad_norm": 0.6123965382575989,
      "learning_rate": 0.0001779709809111139,
      "loss": 0.6411,
      "step": 7570
    },
    {
      "epoch": 0.47853535353535354,
      "grad_norm": 0.7383981347084045,
      "learning_rate": 0.00017790692637234488,
      "loss": 0.5393,
      "step": 7580
    },
    {
      "epoch": 0.4791666666666667,
      "grad_norm": 0.6301671266555786,
      "learning_rate": 0.00017784279040460924,
      "loss": 0.5123,
      "step": 7590
    },
    {
      "epoch": 0.4797979797979798,
      "grad_norm": 0.9712590575218201,
      "learning_rate": 0.00017777857307494247,
      "loss": 0.5312,
      "step": 7600
    },
    {
      "epoch": 0.48042929292929293,
      "grad_norm": 0.6300521492958069,
      "learning_rate": 0.000177714274450465,
      "loss": 0.7522,
      "step": 7610
    },
    {
      "epoch": 0.4810606060606061,
      "grad_norm": 0.7592799067497253,
      "learning_rate": 0.00017764989459838232,
      "loss": 0.6261,
      "step": 7620
    },
    {
      "epoch": 0.48169191919191917,
      "grad_norm": 0.7232942581176758,
      "learning_rate": 0.00017758543358598476,
      "loss": 0.5591,
      "step": 7630
    },
    {
      "epoch": 0.4823232323232323,
      "grad_norm": 0.813723623752594,
      "learning_rate": 0.00017752089148064752,
      "loss": 0.5076,
      "step": 7640
    },
    {
      "epoch": 0.48295454545454547,
      "grad_norm": 1.0034205913543701,
      "learning_rate": 0.00017745626834983055,
      "loss": 0.5091,
      "step": 7650
    },
    {
      "epoch": 0.48358585858585856,
      "grad_norm": 0.6554344296455383,
      "learning_rate": 0.00017739156426107845,
      "loss": 0.7771,
      "step": 7660
    },
    {
      "epoch": 0.4842171717171717,
      "grad_norm": 0.6211978793144226,
      "learning_rate": 0.00017732677928202053,
      "loss": 0.6419,
      "step": 7670
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 0.7142544388771057,
      "learning_rate": 0.00017726191348037054,
      "loss": 0.5554,
      "step": 7680
    },
    {
      "epoch": 0.48547979797979796,
      "grad_norm": 0.7731212377548218,
      "learning_rate": 0.00017719696692392677,
      "loss": 0.5322,
      "step": 7690
    },
    {
      "epoch": 0.4861111111111111,
      "grad_norm": 1.2062458992004395,
      "learning_rate": 0.0001771319396805719,
      "loss": 0.5178,
      "step": 7700
    },
    {
      "epoch": 0.48674242424242425,
      "grad_norm": 0.606095016002655,
      "learning_rate": 0.00017706683181827295,
      "loss": 0.7375,
      "step": 7710
    },
    {
      "epoch": 0.48737373737373735,
      "grad_norm": 0.6507496237754822,
      "learning_rate": 0.00017700164340508117,
      "loss": 0.6423,
      "step": 7720
    },
    {
      "epoch": 0.4880050505050505,
      "grad_norm": 0.689579963684082,
      "learning_rate": 0.0001769363745091321,
      "loss": 0.5534,
      "step": 7730
    },
    {
      "epoch": 0.48863636363636365,
      "grad_norm": 0.7607659101486206,
      "learning_rate": 0.00017687102519864525,
      "loss": 0.5178,
      "step": 7740
    },
    {
      "epoch": 0.48926767676767674,
      "grad_norm": 0.9309207797050476,
      "learning_rate": 0.0001768055955419243,
      "loss": 0.5524,
      "step": 7750
    },
    {
      "epoch": 0.4898989898989899,
      "grad_norm": 0.6069095134735107,
      "learning_rate": 0.0001767400856073569,
      "loss": 0.7089,
      "step": 7760
    },
    {
      "epoch": 0.49053030303030304,
      "grad_norm": 0.6496742367744446,
      "learning_rate": 0.00017667449546341453,
      "loss": 0.6324,
      "step": 7770
    },
    {
      "epoch": 0.4911616161616162,
      "grad_norm": 0.6555066704750061,
      "learning_rate": 0.00017660882517865254,
      "loss": 0.548,
      "step": 7780
    },
    {
      "epoch": 0.4917929292929293,
      "grad_norm": 0.764835000038147,
      "learning_rate": 0.00017654307482171014,
      "loss": 0.5077,
      "step": 7790
    },
    {
      "epoch": 0.49242424242424243,
      "grad_norm": 1.2813174724578857,
      "learning_rate": 0.00017647724446131005,
      "loss": 0.5318,
      "step": 7800
    },
    {
      "epoch": 0.4930555555555556,
      "grad_norm": 0.5997570753097534,
      "learning_rate": 0.00017641133416625878,
      "loss": 0.7279,
      "step": 7810
    },
    {
      "epoch": 0.4936868686868687,
      "grad_norm": 0.691418468952179,
      "learning_rate": 0.00017634534400544631,
      "loss": 0.6005,
      "step": 7820
    },
    {
      "epoch": 0.4943181818181818,
      "grad_norm": 0.7245383262634277,
      "learning_rate": 0.00017627927404784607,
      "loss": 0.5336,
      "step": 7830
    },
    {
      "epoch": 0.494949494949495,
      "grad_norm": 0.728950023651123,
      "learning_rate": 0.00017621312436251496,
      "loss": 0.517,
      "step": 7840
    },
    {
      "epoch": 0.49558080808080807,
      "grad_norm": 0.8812928795814514,
      "learning_rate": 0.00017614689501859316,
      "loss": 0.5181,
      "step": 7850
    },
    {
      "epoch": 0.4962121212121212,
      "grad_norm": 0.5982614159584045,
      "learning_rate": 0.00017608058608530413,
      "loss": 0.7642,
      "step": 7860
    },
    {
      "epoch": 0.49684343434343436,
      "grad_norm": 0.7403950095176697,
      "learning_rate": 0.00017601419763195453,
      "loss": 0.6235,
      "step": 7870
    },
    {
      "epoch": 0.49747474747474746,
      "grad_norm": 0.712908923625946,
      "learning_rate": 0.0001759477297279341,
      "loss": 0.5889,
      "step": 7880
    },
    {
      "epoch": 0.4981060606060606,
      "grad_norm": 0.804527997970581,
      "learning_rate": 0.00017588118244271568,
      "loss": 0.5289,
      "step": 7890
    },
    {
      "epoch": 0.49873737373737376,
      "grad_norm": 0.8421348333358765,
      "learning_rate": 0.00017581455584585507,
      "loss": 0.5182,
      "step": 7900
    },
    {
      "epoch": 0.49936868686868685,
      "grad_norm": 0.6108242869377136,
      "learning_rate": 0.00017574785000699084,
      "loss": 0.7236,
      "step": 7910
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.6591709852218628,
      "learning_rate": 0.0001756810649958446,
      "loss": 0.6464,
      "step": 7920
    },
    {
      "epoch": 0.5006313131313131,
      "grad_norm": 0.6823734641075134,
      "learning_rate": 0.00017561420088222054,
      "loss": 0.5448,
      "step": 7930
    },
    {
      "epoch": 0.5012626262626263,
      "grad_norm": 0.7356827259063721,
      "learning_rate": 0.0001755472577360056,
      "loss": 0.511,
      "step": 7940
    },
    {
      "epoch": 0.5018939393939394,
      "grad_norm": 1.0888158082962036,
      "learning_rate": 0.0001754802356271693,
      "loss": 0.5342,
      "step": 7950
    },
    {
      "epoch": 0.5025252525252525,
      "grad_norm": 0.5929847955703735,
      "learning_rate": 0.00017541313462576368,
      "loss": 0.7431,
      "step": 7960
    },
    {
      "epoch": 0.5031565656565656,
      "grad_norm": 0.6430888175964355,
      "learning_rate": 0.0001753459548019233,
      "loss": 0.6264,
      "step": 7970
    },
    {
      "epoch": 0.5037878787878788,
      "grad_norm": 0.7002361416816711,
      "learning_rate": 0.0001752786962258651,
      "loss": 0.5578,
      "step": 7980
    },
    {
      "epoch": 0.5044191919191919,
      "grad_norm": 0.7845878601074219,
      "learning_rate": 0.00017521135896788828,
      "loss": 0.5156,
      "step": 7990
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 1.0626643896102905,
      "learning_rate": 0.00017514394309837424,
      "loss": 0.54,
      "step": 8000
    },
    {
      "epoch": 0.5050505050505051,
      "eval_loss": 0.578924834728241,
      "eval_runtime": 27.148,
      "eval_samples_per_second": 94.298,
      "eval_steps_per_second": 11.787,
      "step": 8000
    },
    {
      "epoch": 0.5056818181818182,
      "grad_norm": 0.5671148896217346,
      "learning_rate": 0.0001750764486877867,
      "loss": 0.7472,
      "step": 8010
    },
    {
      "epoch": 0.5063131313131313,
      "grad_norm": 0.6959950923919678,
      "learning_rate": 0.0001750088758066713,
      "loss": 0.6157,
      "step": 8020
    },
    {
      "epoch": 0.5069444444444444,
      "grad_norm": 0.700547993183136,
      "learning_rate": 0.00017494122452565582,
      "loss": 0.5707,
      "step": 8030
    },
    {
      "epoch": 0.5075757575757576,
      "grad_norm": 0.8388614058494568,
      "learning_rate": 0.00017487349491544996,
      "loss": 0.5223,
      "step": 8040
    },
    {
      "epoch": 0.5082070707070707,
      "grad_norm": 1.037367820739746,
      "learning_rate": 0.00017480568704684521,
      "loss": 0.4973,
      "step": 8050
    },
    {
      "epoch": 0.5088383838383839,
      "grad_norm": 0.6163510084152222,
      "learning_rate": 0.00017473780099071498,
      "loss": 0.7425,
      "step": 8060
    },
    {
      "epoch": 0.509469696969697,
      "grad_norm": 0.6549428701400757,
      "learning_rate": 0.0001746698368180143,
      "loss": 0.6162,
      "step": 8070
    },
    {
      "epoch": 0.51010101010101,
      "grad_norm": 0.7457553744316101,
      "learning_rate": 0.0001746017945997799,
      "loss": 0.5671,
      "step": 8080
    },
    {
      "epoch": 0.5107323232323232,
      "grad_norm": 0.746569812297821,
      "learning_rate": 0.00017453367440713007,
      "loss": 0.5014,
      "step": 8090
    },
    {
      "epoch": 0.5113636363636364,
      "grad_norm": 0.9903282523155212,
      "learning_rate": 0.00017446547631126463,
      "loss": 0.5287,
      "step": 8100
    },
    {
      "epoch": 0.5119949494949495,
      "grad_norm": 0.6073612570762634,
      "learning_rate": 0.00017439720038346472,
      "loss": 0.7083,
      "step": 8110
    },
    {
      "epoch": 0.5126262626262627,
      "grad_norm": 0.6428342461585999,
      "learning_rate": 0.00017432884669509299,
      "loss": 0.6185,
      "step": 8120
    },
    {
      "epoch": 0.5132575757575758,
      "grad_norm": 0.6305633187294006,
      "learning_rate": 0.0001742604153175932,
      "loss": 0.5426,
      "step": 8130
    },
    {
      "epoch": 0.5138888888888888,
      "grad_norm": 0.7640303373336792,
      "learning_rate": 0.00017419190632249053,
      "loss": 0.5176,
      "step": 8140
    },
    {
      "epoch": 0.514520202020202,
      "grad_norm": 1.0350719690322876,
      "learning_rate": 0.000174123319781391,
      "loss": 0.5282,
      "step": 8150
    },
    {
      "epoch": 0.5151515151515151,
      "grad_norm": 0.5674313306808472,
      "learning_rate": 0.0001740546557659819,
      "loss": 0.7461,
      "step": 8160
    },
    {
      "epoch": 0.5157828282828283,
      "grad_norm": 0.6418477296829224,
      "learning_rate": 0.00017398591434803143,
      "loss": 0.5668,
      "step": 8170
    },
    {
      "epoch": 0.5164141414141414,
      "grad_norm": 0.7291843295097351,
      "learning_rate": 0.0001739170955993887,
      "loss": 0.5473,
      "step": 8180
    },
    {
      "epoch": 0.5170454545454546,
      "grad_norm": 0.7252343893051147,
      "learning_rate": 0.0001738481995919836,
      "loss": 0.4877,
      "step": 8190
    },
    {
      "epoch": 0.5176767676767676,
      "grad_norm": 0.9164720773696899,
      "learning_rate": 0.00017377922639782685,
      "loss": 0.5385,
      "step": 8200
    },
    {
      "epoch": 0.5183080808080808,
      "grad_norm": 0.6269335150718689,
      "learning_rate": 0.00017371017608900982,
      "loss": 0.7553,
      "step": 8210
    },
    {
      "epoch": 0.5189393939393939,
      "grad_norm": 0.7063830494880676,
      "learning_rate": 0.0001736410487377044,
      "loss": 0.6116,
      "step": 8220
    },
    {
      "epoch": 0.5195707070707071,
      "grad_norm": 0.6731562614440918,
      "learning_rate": 0.0001735718444161631,
      "loss": 0.5472,
      "step": 8230
    },
    {
      "epoch": 0.5202020202020202,
      "grad_norm": 0.790433406829834,
      "learning_rate": 0.00017350256319671888,
      "loss": 0.499,
      "step": 8240
    },
    {
      "epoch": 0.5208333333333334,
      "grad_norm": 0.9409703016281128,
      "learning_rate": 0.000173433205151785,
      "loss": 0.516,
      "step": 8250
    },
    {
      "epoch": 0.5214646464646465,
      "grad_norm": 0.621878445148468,
      "learning_rate": 0.0001733637703538551,
      "loss": 0.7558,
      "step": 8260
    },
    {
      "epoch": 0.5220959595959596,
      "grad_norm": 0.6025867462158203,
      "learning_rate": 0.000173294258875503,
      "loss": 0.6437,
      "step": 8270
    },
    {
      "epoch": 0.5227272727272727,
      "grad_norm": 0.6046354174613953,
      "learning_rate": 0.0001732246707893827,
      "loss": 0.5597,
      "step": 8280
    },
    {
      "epoch": 0.5233585858585859,
      "grad_norm": 0.8065158128738403,
      "learning_rate": 0.0001731550061682282,
      "loss": 0.494,
      "step": 8290
    },
    {
      "epoch": 0.523989898989899,
      "grad_norm": 1.2688246965408325,
      "learning_rate": 0.00017308526508485352,
      "loss": 0.5417,
      "step": 8300
    },
    {
      "epoch": 0.5246212121212122,
      "grad_norm": 0.5910103917121887,
      "learning_rate": 0.0001730154476121527,
      "loss": 0.7436,
      "step": 8310
    },
    {
      "epoch": 0.5252525252525253,
      "grad_norm": 0.6910600662231445,
      "learning_rate": 0.00017294555382309947,
      "loss": 0.6249,
      "step": 8320
    },
    {
      "epoch": 0.5258838383838383,
      "grad_norm": 0.7428235411643982,
      "learning_rate": 0.00017287558379074747,
      "loss": 0.5597,
      "step": 8330
    },
    {
      "epoch": 0.5265151515151515,
      "grad_norm": 0.770065426826477,
      "learning_rate": 0.0001728055375882299,
      "loss": 0.4918,
      "step": 8340
    },
    {
      "epoch": 0.5271464646464646,
      "grad_norm": 1.0323365926742554,
      "learning_rate": 0.00017273541528875966,
      "loss": 0.5268,
      "step": 8350
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 0.6374819874763489,
      "learning_rate": 0.00017266521696562913,
      "loss": 0.7487,
      "step": 8360
    },
    {
      "epoch": 0.5284090909090909,
      "grad_norm": 0.6149225234985352,
      "learning_rate": 0.0001725949426922102,
      "loss": 0.6105,
      "step": 8370
    },
    {
      "epoch": 0.5290404040404041,
      "grad_norm": 0.806321918964386,
      "learning_rate": 0.00017252459254195413,
      "loss": 0.5591,
      "step": 8380
    },
    {
      "epoch": 0.5296717171717171,
      "grad_norm": 0.7144362330436707,
      "learning_rate": 0.00017245416658839152,
      "loss": 0.5154,
      "step": 8390
    },
    {
      "epoch": 0.5303030303030303,
      "grad_norm": 1.1171679496765137,
      "learning_rate": 0.00017238366490513206,
      "loss": 0.5103,
      "step": 8400
    },
    {
      "epoch": 0.5309343434343434,
      "grad_norm": 0.6402524709701538,
      "learning_rate": 0.00017231308756586477,
      "loss": 0.7245,
      "step": 8410
    },
    {
      "epoch": 0.5315656565656566,
      "grad_norm": 0.6471856832504272,
      "learning_rate": 0.00017224243464435766,
      "loss": 0.6019,
      "step": 8420
    },
    {
      "epoch": 0.5321969696969697,
      "grad_norm": 0.7572510242462158,
      "learning_rate": 0.00017217170621445775,
      "loss": 0.5278,
      "step": 8430
    },
    {
      "epoch": 0.5328282828282829,
      "grad_norm": 0.813193678855896,
      "learning_rate": 0.00017210090235009098,
      "loss": 0.4922,
      "step": 8440
    },
    {
      "epoch": 0.5334595959595959,
      "grad_norm": 0.9495216608047485,
      "learning_rate": 0.00017203002312526214,
      "loss": 0.5123,
      "step": 8450
    },
    {
      "epoch": 0.5340909090909091,
      "grad_norm": 0.5641337633132935,
      "learning_rate": 0.00017195906861405477,
      "loss": 0.71,
      "step": 8460
    },
    {
      "epoch": 0.5347222222222222,
      "grad_norm": 0.6673982739448547,
      "learning_rate": 0.00017188803889063112,
      "loss": 0.626,
      "step": 8470
    },
    {
      "epoch": 0.5353535353535354,
      "grad_norm": 0.7623387575149536,
      "learning_rate": 0.00017181693402923206,
      "loss": 0.5514,
      "step": 8480
    },
    {
      "epoch": 0.5359848484848485,
      "grad_norm": 0.7562404274940491,
      "learning_rate": 0.00017174575410417697,
      "loss": 0.4833,
      "step": 8490
    },
    {
      "epoch": 0.5366161616161617,
      "grad_norm": 0.9768145084381104,
      "learning_rate": 0.0001716744991898637,
      "loss": 0.5138,
      "step": 8500
    },
    {
      "epoch": 0.5372474747474747,
      "grad_norm": 0.607541024684906,
      "learning_rate": 0.00017160316936076848,
      "loss": 0.7708,
      "step": 8510
    },
    {
      "epoch": 0.5378787878787878,
      "grad_norm": 0.5920299291610718,
      "learning_rate": 0.00017153176469144585,
      "loss": 0.6266,
      "step": 8520
    },
    {
      "epoch": 0.538510101010101,
      "grad_norm": 0.6860084533691406,
      "learning_rate": 0.00017146028525652856,
      "loss": 0.5409,
      "step": 8530
    },
    {
      "epoch": 0.5391414141414141,
      "grad_norm": 0.8015006184577942,
      "learning_rate": 0.0001713887311307275,
      "loss": 0.5071,
      "step": 8540
    },
    {
      "epoch": 0.5397727272727273,
      "grad_norm": 0.9435932636260986,
      "learning_rate": 0.00017131710238883164,
      "loss": 0.5027,
      "step": 8550
    },
    {
      "epoch": 0.5404040404040404,
      "grad_norm": 0.5997703671455383,
      "learning_rate": 0.00017124539910570792,
      "loss": 0.7374,
      "step": 8560
    },
    {
      "epoch": 0.5410353535353535,
      "grad_norm": 0.6288837194442749,
      "learning_rate": 0.0001711736213563012,
      "loss": 0.6025,
      "step": 8570
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.7528879642486572,
      "learning_rate": 0.00017110176921563425,
      "loss": 0.5339,
      "step": 8580
    },
    {
      "epoch": 0.5422979797979798,
      "grad_norm": 0.7620011568069458,
      "learning_rate": 0.00017102984275880746,
      "loss": 0.5084,
      "step": 8590
    },
    {
      "epoch": 0.5429292929292929,
      "grad_norm": 0.9728717803955078,
      "learning_rate": 0.00017095784206099896,
      "loss": 0.524,
      "step": 8600
    },
    {
      "epoch": 0.5435606060606061,
      "grad_norm": 0.5647729635238647,
      "learning_rate": 0.00017088576719746453,
      "loss": 0.7584,
      "step": 8610
    },
    {
      "epoch": 0.5441919191919192,
      "grad_norm": 0.7118963003158569,
      "learning_rate": 0.00017081361824353736,
      "loss": 0.6467,
      "step": 8620
    },
    {
      "epoch": 0.5448232323232324,
      "grad_norm": 0.664786159992218,
      "learning_rate": 0.00017074139527462818,
      "loss": 0.5284,
      "step": 8630
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.8275582790374756,
      "learning_rate": 0.000170669098366225,
      "loss": 0.4749,
      "step": 8640
    },
    {
      "epoch": 0.5460858585858586,
      "grad_norm": 1.2185934782028198,
      "learning_rate": 0.00017059672759389317,
      "loss": 0.5081,
      "step": 8650
    },
    {
      "epoch": 0.5467171717171717,
      "grad_norm": 0.5786290764808655,
      "learning_rate": 0.0001705242830332752,
      "loss": 0.7057,
      "step": 8660
    },
    {
      "epoch": 0.5473484848484849,
      "grad_norm": 0.6911237239837646,
      "learning_rate": 0.00017045176476009074,
      "loss": 0.6291,
      "step": 8670
    },
    {
      "epoch": 0.547979797979798,
      "grad_norm": 0.6757599115371704,
      "learning_rate": 0.00017037917285013654,
      "loss": 0.5353,
      "step": 8680
    },
    {
      "epoch": 0.5486111111111112,
      "grad_norm": 0.7431146502494812,
      "learning_rate": 0.00017030650737928627,
      "loss": 0.4639,
      "step": 8690
    },
    {
      "epoch": 0.5492424242424242,
      "grad_norm": 0.941423237323761,
      "learning_rate": 0.00017023376842349041,
      "loss": 0.5136,
      "step": 8700
    },
    {
      "epoch": 0.5498737373737373,
      "grad_norm": 0.5689589977264404,
      "learning_rate": 0.00017016095605877637,
      "loss": 0.7326,
      "step": 8710
    },
    {
      "epoch": 0.5505050505050505,
      "grad_norm": 0.6286278367042542,
      "learning_rate": 0.00017008807036124828,
      "loss": 0.6207,
      "step": 8720
    },
    {
      "epoch": 0.5511363636363636,
      "grad_norm": 0.7922292947769165,
      "learning_rate": 0.0001700151114070868,
      "loss": 0.5461,
      "step": 8730
    },
    {
      "epoch": 0.5517676767676768,
      "grad_norm": 0.8212761878967285,
      "learning_rate": 0.00016994207927254924,
      "loss": 0.4865,
      "step": 8740
    },
    {
      "epoch": 0.55239898989899,
      "grad_norm": 1.0219744443893433,
      "learning_rate": 0.00016986897403396944,
      "loss": 0.5152,
      "step": 8750
    },
    {
      "epoch": 0.553030303030303,
      "grad_norm": 0.6037240624427795,
      "learning_rate": 0.00016979579576775758,
      "loss": 0.7185,
      "step": 8760
    },
    {
      "epoch": 0.5536616161616161,
      "grad_norm": 0.6580737233161926,
      "learning_rate": 0.00016972254455040021,
      "loss": 0.6025,
      "step": 8770
    },
    {
      "epoch": 0.5542929292929293,
      "grad_norm": 0.6297159194946289,
      "learning_rate": 0.0001696492204584601,
      "loss": 0.5651,
      "step": 8780
    },
    {
      "epoch": 0.5549242424242424,
      "grad_norm": 0.894807755947113,
      "learning_rate": 0.00016957582356857617,
      "loss": 0.4631,
      "step": 8790
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.0152623653411865,
      "learning_rate": 0.00016950235395746347,
      "loss": 0.5219,
      "step": 8800
    },
    {
      "epoch": 0.5561868686868687,
      "grad_norm": 0.6217525005340576,
      "learning_rate": 0.0001694288117019131,
      "loss": 0.7262,
      "step": 8810
    },
    {
      "epoch": 0.5568181818181818,
      "grad_norm": 0.6197936534881592,
      "learning_rate": 0.000169355196878792,
      "loss": 0.6071,
      "step": 8820
    },
    {
      "epoch": 0.5574494949494949,
      "grad_norm": 0.7360895872116089,
      "learning_rate": 0.00016928150956504293,
      "loss": 0.5464,
      "step": 8830
    },
    {
      "epoch": 0.5580808080808081,
      "grad_norm": 0.7620795369148254,
      "learning_rate": 0.0001692077498376846,
      "loss": 0.4894,
      "step": 8840
    },
    {
      "epoch": 0.5587121212121212,
      "grad_norm": 0.9170685410499573,
      "learning_rate": 0.00016913391777381124,
      "loss": 0.5227,
      "step": 8850
    },
    {
      "epoch": 0.5593434343434344,
      "grad_norm": 0.5853192806243896,
      "learning_rate": 0.00016906001345059273,
      "loss": 0.7288,
      "step": 8860
    },
    {
      "epoch": 0.5599747474747475,
      "grad_norm": 0.628969132900238,
      "learning_rate": 0.00016898603694527443,
      "loss": 0.6257,
      "step": 8870
    },
    {
      "epoch": 0.5606060606060606,
      "grad_norm": 0.7234898209571838,
      "learning_rate": 0.00016891198833517729,
      "loss": 0.5436,
      "step": 8880
    },
    {
      "epoch": 0.5612373737373737,
      "grad_norm": 0.7205088138580322,
      "learning_rate": 0.00016883786769769752,
      "loss": 0.5183,
      "step": 8890
    },
    {
      "epoch": 0.5618686868686869,
      "grad_norm": 1.038011908531189,
      "learning_rate": 0.00016876367511030655,
      "loss": 0.531,
      "step": 8900
    },
    {
      "epoch": 0.5625,
      "grad_norm": 0.6014278531074524,
      "learning_rate": 0.00016868941065055116,
      "loss": 0.7461,
      "step": 8910
    },
    {
      "epoch": 0.5631313131313131,
      "grad_norm": 0.6033473014831543,
      "learning_rate": 0.00016861507439605317,
      "loss": 0.625,
      "step": 8920
    },
    {
      "epoch": 0.5637626262626263,
      "grad_norm": 0.6435050368309021,
      "learning_rate": 0.00016854066642450942,
      "loss": 0.5486,
      "step": 8930
    },
    {
      "epoch": 0.5643939393939394,
      "grad_norm": 0.7559719085693359,
      "learning_rate": 0.00016846618681369178,
      "loss": 0.4738,
      "step": 8940
    },
    {
      "epoch": 0.5650252525252525,
      "grad_norm": 0.8655114769935608,
      "learning_rate": 0.00016839163564144694,
      "loss": 0.524,
      "step": 8950
    },
    {
      "epoch": 0.5656565656565656,
      "grad_norm": 0.5815051794052124,
      "learning_rate": 0.0001683170129856964,
      "loss": 0.7318,
      "step": 8960
    },
    {
      "epoch": 0.5662878787878788,
      "grad_norm": 0.6429320573806763,
      "learning_rate": 0.00016824231892443635,
      "loss": 0.6209,
      "step": 8970
    },
    {
      "epoch": 0.5669191919191919,
      "grad_norm": 0.6432978510856628,
      "learning_rate": 0.0001681675535357377,
      "loss": 0.54,
      "step": 8980
    },
    {
      "epoch": 0.5675505050505051,
      "grad_norm": 0.8134156465530396,
      "learning_rate": 0.00016809271689774584,
      "loss": 0.5091,
      "step": 8990
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 0.9552921652793884,
      "learning_rate": 0.00016801780908868062,
      "loss": 0.4853,
      "step": 9000
    },
    {
      "epoch": 0.5681818181818182,
      "eval_loss": 0.5697404146194458,
      "eval_runtime": 28.5065,
      "eval_samples_per_second": 89.804,
      "eval_steps_per_second": 11.225,
      "step": 9000
    },
    {
      "epoch": 0.5688131313131313,
      "grad_norm": 0.5849342942237854,
      "learning_rate": 0.00016794283018683632,
      "loss": 0.7374,
      "step": 9010
    },
    {
      "epoch": 0.5694444444444444,
      "grad_norm": 0.7163851857185364,
      "learning_rate": 0.00016786778027058153,
      "loss": 0.6094,
      "step": 9020
    },
    {
      "epoch": 0.5700757575757576,
      "grad_norm": 0.6832115054130554,
      "learning_rate": 0.00016779265941835897,
      "loss": 0.5534,
      "step": 9030
    },
    {
      "epoch": 0.5707070707070707,
      "grad_norm": 0.7660689949989319,
      "learning_rate": 0.00016771746770868567,
      "loss": 0.4896,
      "step": 9040
    },
    {
      "epoch": 0.5713383838383839,
      "grad_norm": 1.063767671585083,
      "learning_rate": 0.00016764220522015263,
      "loss": 0.5359,
      "step": 9050
    },
    {
      "epoch": 0.571969696969697,
      "grad_norm": 0.5511501431465149,
      "learning_rate": 0.0001675668720314248,
      "loss": 0.7329,
      "step": 9060
    },
    {
      "epoch": 0.57260101010101,
      "grad_norm": 0.6513356566429138,
      "learning_rate": 0.00016749146822124097,
      "loss": 0.6245,
      "step": 9070
    },
    {
      "epoch": 0.5732323232323232,
      "grad_norm": 0.5953903198242188,
      "learning_rate": 0.00016741599386841397,
      "loss": 0.547,
      "step": 9080
    },
    {
      "epoch": 0.5738636363636364,
      "grad_norm": 0.7221503257751465,
      "learning_rate": 0.00016734044905183012,
      "loss": 0.4586,
      "step": 9090
    },
    {
      "epoch": 0.5744949494949495,
      "grad_norm": 1.1477705240249634,
      "learning_rate": 0.00016726483385044958,
      "loss": 0.5489,
      "step": 9100
    },
    {
      "epoch": 0.5751262626262627,
      "grad_norm": 0.5694388151168823,
      "learning_rate": 0.0001671891483433059,
      "loss": 0.7114,
      "step": 9110
    },
    {
      "epoch": 0.5757575757575758,
      "grad_norm": 0.6186876893043518,
      "learning_rate": 0.00016711339260950622,
      "loss": 0.5986,
      "step": 9120
    },
    {
      "epoch": 0.5763888888888888,
      "grad_norm": 0.6753848195075989,
      "learning_rate": 0.0001670375667282311,
      "loss": 0.5232,
      "step": 9130
    },
    {
      "epoch": 0.577020202020202,
      "grad_norm": 0.702938973903656,
      "learning_rate": 0.00016696167077873435,
      "loss": 0.4887,
      "step": 9140
    },
    {
      "epoch": 0.5776515151515151,
      "grad_norm": 1.0157321691513062,
      "learning_rate": 0.00016688570484034307,
      "loss": 0.519,
      "step": 9150
    },
    {
      "epoch": 0.5782828282828283,
      "grad_norm": 0.5609768033027649,
      "learning_rate": 0.00016680966899245748,
      "loss": 0.7024,
      "step": 9160
    },
    {
      "epoch": 0.5789141414141414,
      "grad_norm": 0.569517970085144,
      "learning_rate": 0.00016673356331455084,
      "loss": 0.6071,
      "step": 9170
    },
    {
      "epoch": 0.5795454545454546,
      "grad_norm": 0.699206531047821,
      "learning_rate": 0.00016665738788616946,
      "loss": 0.5404,
      "step": 9180
    },
    {
      "epoch": 0.5801767676767676,
      "grad_norm": 0.6626868844032288,
      "learning_rate": 0.0001665811427869326,
      "loss": 0.4741,
      "step": 9190
    },
    {
      "epoch": 0.5808080808080808,
      "grad_norm": 1.057674765586853,
      "learning_rate": 0.00016650482809653217,
      "loss": 0.514,
      "step": 9200
    },
    {
      "epoch": 0.5814393939393939,
      "grad_norm": 0.6003877520561218,
      "learning_rate": 0.000166428443894733,
      "loss": 0.7363,
      "step": 9210
    },
    {
      "epoch": 0.5820707070707071,
      "grad_norm": 0.5828176140785217,
      "learning_rate": 0.00016635199026137243,
      "loss": 0.6037,
      "step": 9220
    },
    {
      "epoch": 0.5827020202020202,
      "grad_norm": 0.747687816619873,
      "learning_rate": 0.00016627546727636044,
      "loss": 0.5642,
      "step": 9230
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.7262148857116699,
      "learning_rate": 0.00016619887501967954,
      "loss": 0.4979,
      "step": 9240
    },
    {
      "epoch": 0.5839646464646465,
      "grad_norm": 0.9663421511650085,
      "learning_rate": 0.00016612221357138453,
      "loss": 0.5092,
      "step": 9250
    },
    {
      "epoch": 0.5845959595959596,
      "grad_norm": 0.6312105059623718,
      "learning_rate": 0.00016604548301160264,
      "loss": 0.8135,
      "step": 9260
    },
    {
      "epoch": 0.5852272727272727,
      "grad_norm": 0.6083599925041199,
      "learning_rate": 0.00016596868342053325,
      "loss": 0.616,
      "step": 9270
    },
    {
      "epoch": 0.5858585858585859,
      "grad_norm": 0.7082657814025879,
      "learning_rate": 0.000165891814878448,
      "loss": 0.532,
      "step": 9280
    },
    {
      "epoch": 0.586489898989899,
      "grad_norm": 0.761286199092865,
      "learning_rate": 0.00016581487746569043,
      "loss": 0.4988,
      "step": 9290
    },
    {
      "epoch": 0.5871212121212122,
      "grad_norm": 1.0681735277175903,
      "learning_rate": 0.0001657378712626762,
      "loss": 0.5055,
      "step": 9300
    },
    {
      "epoch": 0.5877525252525253,
      "grad_norm": 0.6230157613754272,
      "learning_rate": 0.00016566079634989289,
      "loss": 0.7132,
      "step": 9310
    },
    {
      "epoch": 0.5883838383838383,
      "grad_norm": 0.6026455163955688,
      "learning_rate": 0.00016558365280789977,
      "loss": 0.5804,
      "step": 9320
    },
    {
      "epoch": 0.5890151515151515,
      "grad_norm": 0.646434485912323,
      "learning_rate": 0.00016550644071732794,
      "loss": 0.534,
      "step": 9330
    },
    {
      "epoch": 0.5896464646464646,
      "grad_norm": 0.66474449634552,
      "learning_rate": 0.0001654291601588801,
      "loss": 0.496,
      "step": 9340
    },
    {
      "epoch": 0.5902777777777778,
      "grad_norm": 0.8743157386779785,
      "learning_rate": 0.00016535181121333058,
      "loss": 0.5129,
      "step": 9350
    },
    {
      "epoch": 0.5909090909090909,
      "grad_norm": 0.5828522443771362,
      "learning_rate": 0.00016527439396152508,
      "loss": 0.7057,
      "step": 9360
    },
    {
      "epoch": 0.5915404040404041,
      "grad_norm": 0.6677644848823547,
      "learning_rate": 0.0001651969084843808,
      "loss": 0.6276,
      "step": 9370
    },
    {
      "epoch": 0.5921717171717171,
      "grad_norm": 0.7089948654174805,
      "learning_rate": 0.00016511935486288618,
      "loss": 0.523,
      "step": 9380
    },
    {
      "epoch": 0.5928030303030303,
      "grad_norm": 0.7703858613967896,
      "learning_rate": 0.0001650417331781009,
      "loss": 0.4543,
      "step": 9390
    },
    {
      "epoch": 0.5934343434343434,
      "grad_norm": 1.0024311542510986,
      "learning_rate": 0.0001649640435111558,
      "loss": 0.5082,
      "step": 9400
    },
    {
      "epoch": 0.5940656565656566,
      "grad_norm": 0.5712293982505798,
      "learning_rate": 0.00016488628594325277,
      "loss": 0.7182,
      "step": 9410
    },
    {
      "epoch": 0.5946969696969697,
      "grad_norm": 0.7216587662696838,
      "learning_rate": 0.0001648084605556647,
      "loss": 0.6093,
      "step": 9420
    },
    {
      "epoch": 0.5953282828282829,
      "grad_norm": 0.5914806723594666,
      "learning_rate": 0.00016473056742973526,
      "loss": 0.5013,
      "step": 9430
    },
    {
      "epoch": 0.5959595959595959,
      "grad_norm": 0.7751336693763733,
      "learning_rate": 0.00016465260664687902,
      "loss": 0.4687,
      "step": 9440
    },
    {
      "epoch": 0.5965909090909091,
      "grad_norm": 1.0089976787567139,
      "learning_rate": 0.0001645745782885813,
      "loss": 0.5082,
      "step": 9450
    },
    {
      "epoch": 0.5972222222222222,
      "grad_norm": 0.5451928973197937,
      "learning_rate": 0.00016449648243639788,
      "loss": 0.7138,
      "step": 9460
    },
    {
      "epoch": 0.5978535353535354,
      "grad_norm": 0.6148408055305481,
      "learning_rate": 0.0001644183191719553,
      "loss": 0.6298,
      "step": 9470
    },
    {
      "epoch": 0.5984848484848485,
      "grad_norm": 0.6403055191040039,
      "learning_rate": 0.00016434008857695037,
      "loss": 0.5521,
      "step": 9480
    },
    {
      "epoch": 0.5991161616161617,
      "grad_norm": 0.7750465273857117,
      "learning_rate": 0.0001642617907331504,
      "loss": 0.4663,
      "step": 9490
    },
    {
      "epoch": 0.5997474747474747,
      "grad_norm": 1.1629151105880737,
      "learning_rate": 0.00016418342572239292,
      "loss": 0.5095,
      "step": 9500
    },
    {
      "epoch": 0.6003787878787878,
      "grad_norm": 0.5770272016525269,
      "learning_rate": 0.0001641049936265857,
      "loss": 0.7128,
      "step": 9510
    },
    {
      "epoch": 0.601010101010101,
      "grad_norm": 0.6337581872940063,
      "learning_rate": 0.00016402649452770666,
      "loss": 0.6233,
      "step": 9520
    },
    {
      "epoch": 0.6016414141414141,
      "grad_norm": 0.6559898257255554,
      "learning_rate": 0.00016394792850780364,
      "loss": 0.5374,
      "step": 9530
    },
    {
      "epoch": 0.6022727272727273,
      "grad_norm": 0.7238504886627197,
      "learning_rate": 0.00016386929564899457,
      "loss": 0.4797,
      "step": 9540
    },
    {
      "epoch": 0.6029040404040404,
      "grad_norm": 0.9948258399963379,
      "learning_rate": 0.0001637905960334671,
      "loss": 0.5225,
      "step": 9550
    },
    {
      "epoch": 0.6035353535353535,
      "grad_norm": 0.6306954622268677,
      "learning_rate": 0.00016371182974347876,
      "loss": 0.7366,
      "step": 9560
    },
    {
      "epoch": 0.6041666666666666,
      "grad_norm": 0.6813060641288757,
      "learning_rate": 0.0001636329968613567,
      "loss": 0.6037,
      "step": 9570
    },
    {
      "epoch": 0.6047979797979798,
      "grad_norm": 0.6454476714134216,
      "learning_rate": 0.00016355409746949778,
      "loss": 0.5347,
      "step": 9580
    },
    {
      "epoch": 0.6054292929292929,
      "grad_norm": 0.8190798759460449,
      "learning_rate": 0.0001634751316503682,
      "loss": 0.4916,
      "step": 9590
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.1511801481246948,
      "learning_rate": 0.00016339609948650376,
      "loss": 0.4965,
      "step": 9600
    },
    {
      "epoch": 0.6066919191919192,
      "grad_norm": 0.6270524859428406,
      "learning_rate": 0.0001633170010605095,
      "loss": 0.7242,
      "step": 9610
    },
    {
      "epoch": 0.6073232323232324,
      "grad_norm": 0.6262259483337402,
      "learning_rate": 0.00016323783645505975,
      "loss": 0.6118,
      "step": 9620
    },
    {
      "epoch": 0.6079545454545454,
      "grad_norm": 0.6198704242706299,
      "learning_rate": 0.000163158605752898,
      "loss": 0.5308,
      "step": 9630
    },
    {
      "epoch": 0.6085858585858586,
      "grad_norm": 0.7219989895820618,
      "learning_rate": 0.0001630793090368369,
      "loss": 0.4594,
      "step": 9640
    },
    {
      "epoch": 0.6092171717171717,
      "grad_norm": 0.8701320290565491,
      "learning_rate": 0.00016299994638975797,
      "loss": 0.532,
      "step": 9650
    },
    {
      "epoch": 0.6098484848484849,
      "grad_norm": 0.5924901962280273,
      "learning_rate": 0.0001629205178946118,
      "loss": 0.7544,
      "step": 9660
    },
    {
      "epoch": 0.610479797979798,
      "grad_norm": 0.6924330592155457,
      "learning_rate": 0.00016284102363441758,
      "loss": 0.6051,
      "step": 9670
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.5664271116256714,
      "learning_rate": 0.00016276146369226347,
      "loss": 0.5428,
      "step": 9680
    },
    {
      "epoch": 0.6117424242424242,
      "grad_norm": 0.7779406309127808,
      "learning_rate": 0.00016268183815130614,
      "loss": 0.4939,
      "step": 9690
    },
    {
      "epoch": 0.6123737373737373,
      "grad_norm": 0.9932751655578613,
      "learning_rate": 0.00016260214709477088,
      "loss": 0.5183,
      "step": 9700
    },
    {
      "epoch": 0.6130050505050505,
      "grad_norm": 0.624411404132843,
      "learning_rate": 0.00016252239060595146,
      "loss": 0.6997,
      "step": 9710
    },
    {
      "epoch": 0.6136363636363636,
      "grad_norm": 0.6652482151985168,
      "learning_rate": 0.00016244256876821,
      "loss": 0.6061,
      "step": 9720
    },
    {
      "epoch": 0.6142676767676768,
      "grad_norm": 0.7317819595336914,
      "learning_rate": 0.00016236268166497695,
      "loss": 0.5343,
      "step": 9730
    },
    {
      "epoch": 0.61489898989899,
      "grad_norm": 0.7673357129096985,
      "learning_rate": 0.000162282729379751,
      "loss": 0.4947,
      "step": 9740
    },
    {
      "epoch": 0.615530303030303,
      "grad_norm": 0.9375787973403931,
      "learning_rate": 0.0001622027119960989,
      "loss": 0.5405,
      "step": 9750
    },
    {
      "epoch": 0.6161616161616161,
      "grad_norm": 0.6180186867713928,
      "learning_rate": 0.0001621226295976555,
      "loss": 0.73,
      "step": 9760
    },
    {
      "epoch": 0.6167929292929293,
      "grad_norm": 0.639901876449585,
      "learning_rate": 0.00016204248226812365,
      "loss": 0.6033,
      "step": 9770
    },
    {
      "epoch": 0.6174242424242424,
      "grad_norm": 0.8158813118934631,
      "learning_rate": 0.0001619622700912739,
      "loss": 0.5281,
      "step": 9780
    },
    {
      "epoch": 0.6180555555555556,
      "grad_norm": 0.6952929496765137,
      "learning_rate": 0.00016188199315094473,
      "loss": 0.4966,
      "step": 9790
    },
    {
      "epoch": 0.6186868686868687,
      "grad_norm": 1.1230703592300415,
      "learning_rate": 0.0001618016515310423,
      "loss": 0.5465,
      "step": 9800
    },
    {
      "epoch": 0.6193181818181818,
      "grad_norm": 0.5628705024719238,
      "learning_rate": 0.0001617212453155403,
      "loss": 0.6899,
      "step": 9810
    },
    {
      "epoch": 0.6199494949494949,
      "grad_norm": 0.5926527380943298,
      "learning_rate": 0.00016164077458847995,
      "loss": 0.5677,
      "step": 9820
    },
    {
      "epoch": 0.6205808080808081,
      "grad_norm": 0.6544759273529053,
      "learning_rate": 0.00016156023943396998,
      "loss": 0.5603,
      "step": 9830
    },
    {
      "epoch": 0.6212121212121212,
      "grad_norm": 0.722484827041626,
      "learning_rate": 0.0001614796399361864,
      "loss": 0.5152,
      "step": 9840
    },
    {
      "epoch": 0.6218434343434344,
      "grad_norm": 1.075011134147644,
      "learning_rate": 0.00016139897617937238,
      "loss": 0.5327,
      "step": 9850
    },
    {
      "epoch": 0.6224747474747475,
      "grad_norm": 0.6388146281242371,
      "learning_rate": 0.00016131824824783847,
      "loss": 0.694,
      "step": 9860
    },
    {
      "epoch": 0.6231060606060606,
      "grad_norm": 0.6580607891082764,
      "learning_rate": 0.00016123745622596212,
      "loss": 0.6024,
      "step": 9870
    },
    {
      "epoch": 0.6237373737373737,
      "grad_norm": 0.6531664133071899,
      "learning_rate": 0.0001611566001981878,
      "loss": 0.5339,
      "step": 9880
    },
    {
      "epoch": 0.6243686868686869,
      "grad_norm": 0.9387347102165222,
      "learning_rate": 0.00016107568024902697,
      "loss": 0.465,
      "step": 9890
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.9779948592185974,
      "learning_rate": 0.00016099469646305777,
      "loss": 0.5205,
      "step": 9900
    },
    {
      "epoch": 0.6256313131313131,
      "grad_norm": 0.5690833330154419,
      "learning_rate": 0.00016091364892492516,
      "loss": 0.7166,
      "step": 9910
    },
    {
      "epoch": 0.6262626262626263,
      "grad_norm": 0.618030309677124,
      "learning_rate": 0.00016083253771934065,
      "loss": 0.5875,
      "step": 9920
    },
    {
      "epoch": 0.6268939393939394,
      "grad_norm": 0.6901810765266418,
      "learning_rate": 0.00016075136293108245,
      "loss": 0.5289,
      "step": 9930
    },
    {
      "epoch": 0.6275252525252525,
      "grad_norm": 0.7188361287117004,
      "learning_rate": 0.000160670124644995,
      "loss": 0.4809,
      "step": 9940
    },
    {
      "epoch": 0.6281565656565656,
      "grad_norm": 1.02596914768219,
      "learning_rate": 0.00016058882294598932,
      "loss": 0.5085,
      "step": 9950
    },
    {
      "epoch": 0.6287878787878788,
      "grad_norm": 0.5997493267059326,
      "learning_rate": 0.00016050745791904256,
      "loss": 0.6954,
      "step": 9960
    },
    {
      "epoch": 0.6294191919191919,
      "grad_norm": 0.6015306711196899,
      "learning_rate": 0.00016042602964919816,
      "loss": 0.6095,
      "step": 9970
    },
    {
      "epoch": 0.6300505050505051,
      "grad_norm": 0.7047749757766724,
      "learning_rate": 0.0001603445382215656,
      "loss": 0.5287,
      "step": 9980
    },
    {
      "epoch": 0.6306818181818182,
      "grad_norm": 0.6820768117904663,
      "learning_rate": 0.00016026298372132046,
      "loss": 0.4862,
      "step": 9990
    },
    {
      "epoch": 0.6313131313131313,
      "grad_norm": 0.9814989566802979,
      "learning_rate": 0.00016018136623370408,
      "loss": 0.5095,
      "step": 10000
    },
    {
      "epoch": 0.6313131313131313,
      "eval_loss": 0.5610395669937134,
      "eval_runtime": 29.0275,
      "eval_samples_per_second": 88.192,
      "eval_steps_per_second": 11.024,
      "step": 10000
    },
    {
      "epoch": 0.6319444444444444,
      "grad_norm": 0.5336341261863708,
      "learning_rate": 0.00016009968584402383,
      "loss": 0.7298,
      "step": 10010
    },
    {
      "epoch": 0.6325757575757576,
      "grad_norm": 0.7084565758705139,
      "learning_rate": 0.00016001794263765265,
      "loss": 0.6319,
      "step": 10020
    },
    {
      "epoch": 0.6332070707070707,
      "grad_norm": 0.6133972406387329,
      "learning_rate": 0.0001599361367000293,
      "loss": 0.5176,
      "step": 10030
    },
    {
      "epoch": 0.6338383838383839,
      "grad_norm": 0.7794643044471741,
      "learning_rate": 0.000159854268116658,
      "loss": 0.4928,
      "step": 10040
    },
    {
      "epoch": 0.634469696969697,
      "grad_norm": 0.8817728161811829,
      "learning_rate": 0.00015977233697310838,
      "loss": 0.5165,
      "step": 10050
    },
    {
      "epoch": 0.63510101010101,
      "grad_norm": 0.5256279110908508,
      "learning_rate": 0.00015969034335501568,
      "loss": 0.7081,
      "step": 10060
    },
    {
      "epoch": 0.6357323232323232,
      "grad_norm": 0.6603567004203796,
      "learning_rate": 0.00015960828734808027,
      "loss": 0.6066,
      "step": 10070
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.7120043635368347,
      "learning_rate": 0.00015952616903806775,
      "loss": 0.5262,
      "step": 10080
    },
    {
      "epoch": 0.6369949494949495,
      "grad_norm": 0.7762681841850281,
      "learning_rate": 0.00015944398851080885,
      "loss": 0.5054,
      "step": 10090
    },
    {
      "epoch": 0.6376262626262627,
      "grad_norm": 1.0158042907714844,
      "learning_rate": 0.00015936174585219937,
      "loss": 0.4961,
      "step": 10100
    },
    {
      "epoch": 0.6382575757575758,
      "grad_norm": 0.5520707368850708,
      "learning_rate": 0.00015927944114820005,
      "loss": 0.7019,
      "step": 10110
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 0.623985230922699,
      "learning_rate": 0.00015919707448483638,
      "loss": 0.6219,
      "step": 10120
    },
    {
      "epoch": 0.639520202020202,
      "grad_norm": 0.6146858334541321,
      "learning_rate": 0.0001591146459481987,
      "loss": 0.5262,
      "step": 10130
    },
    {
      "epoch": 0.6401515151515151,
      "grad_norm": 0.8028244972229004,
      "learning_rate": 0.00015903215562444202,
      "loss": 0.4579,
      "step": 10140
    },
    {
      "epoch": 0.6407828282828283,
      "grad_norm": 1.1887600421905518,
      "learning_rate": 0.00015894960359978593,
      "loss": 0.5283,
      "step": 10150
    },
    {
      "epoch": 0.6414141414141414,
      "grad_norm": 0.6019163131713867,
      "learning_rate": 0.00015886698996051446,
      "loss": 0.7141,
      "step": 10160
    },
    {
      "epoch": 0.6420454545454546,
      "grad_norm": 0.612490713596344,
      "learning_rate": 0.00015878431479297603,
      "loss": 0.6034,
      "step": 10170
    },
    {
      "epoch": 0.6426767676767676,
      "grad_norm": 0.6465587019920349,
      "learning_rate": 0.0001587015781835835,
      "loss": 0.5416,
      "step": 10180
    },
    {
      "epoch": 0.6433080808080808,
      "grad_norm": 0.7560814619064331,
      "learning_rate": 0.00015861878021881384,
      "loss": 0.5156,
      "step": 10190
    },
    {
      "epoch": 0.6439393939393939,
      "grad_norm": 1.0162643194198608,
      "learning_rate": 0.0001585359209852081,
      "loss": 0.5106,
      "step": 10200
    },
    {
      "epoch": 0.6445707070707071,
      "grad_norm": 0.5484115481376648,
      "learning_rate": 0.00015845300056937153,
      "loss": 0.6798,
      "step": 10210
    },
    {
      "epoch": 0.6452020202020202,
      "grad_norm": 0.6867944002151489,
      "learning_rate": 0.0001583700190579732,
      "loss": 0.5895,
      "step": 10220
    },
    {
      "epoch": 0.6458333333333334,
      "grad_norm": 0.6822202801704407,
      "learning_rate": 0.00015828697653774606,
      "loss": 0.5187,
      "step": 10230
    },
    {
      "epoch": 0.6464646464646465,
      "grad_norm": 0.6740832328796387,
      "learning_rate": 0.00015820387309548685,
      "loss": 0.4817,
      "step": 10240
    },
    {
      "epoch": 0.6470959595959596,
      "grad_norm": 1.0034559965133667,
      "learning_rate": 0.00015812070881805602,
      "loss": 0.4928,
      "step": 10250
    },
    {
      "epoch": 0.6477272727272727,
      "grad_norm": 0.5202574729919434,
      "learning_rate": 0.00015803748379237747,
      "loss": 0.7267,
      "step": 10260
    },
    {
      "epoch": 0.6483585858585859,
      "grad_norm": 0.6594498157501221,
      "learning_rate": 0.00015795419810543882,
      "loss": 0.6229,
      "step": 10270
    },
    {
      "epoch": 0.648989898989899,
      "grad_norm": 0.6646615266799927,
      "learning_rate": 0.00015787085184429086,
      "loss": 0.5175,
      "step": 10280
    },
    {
      "epoch": 0.6496212121212122,
      "grad_norm": 0.6308152079582214,
      "learning_rate": 0.0001577874450960478,
      "loss": 0.4874,
      "step": 10290
    },
    {
      "epoch": 0.6502525252525253,
      "grad_norm": 0.9893712401390076,
      "learning_rate": 0.00015770397794788706,
      "loss": 0.5198,
      "step": 10300
    },
    {
      "epoch": 0.6508838383838383,
      "grad_norm": 0.5140767097473145,
      "learning_rate": 0.00015762045048704927,
      "loss": 0.7319,
      "step": 10310
    },
    {
      "epoch": 0.6515151515151515,
      "grad_norm": 0.6555214524269104,
      "learning_rate": 0.00015753686280083797,
      "loss": 0.6023,
      "step": 10320
    },
    {
      "epoch": 0.6521464646464646,
      "grad_norm": 0.6270751357078552,
      "learning_rate": 0.00015745321497661973,
      "loss": 0.5599,
      "step": 10330
    },
    {
      "epoch": 0.6527777777777778,
      "grad_norm": 0.6615898609161377,
      "learning_rate": 0.00015736950710182392,
      "loss": 0.4872,
      "step": 10340
    },
    {
      "epoch": 0.6534090909090909,
      "grad_norm": 1.1644892692565918,
      "learning_rate": 0.00015728573926394271,
      "loss": 0.518,
      "step": 10350
    },
    {
      "epoch": 0.6540404040404041,
      "grad_norm": 0.5682229995727539,
      "learning_rate": 0.00015720191155053098,
      "loss": 0.6618,
      "step": 10360
    },
    {
      "epoch": 0.6546717171717171,
      "grad_norm": 0.628243625164032,
      "learning_rate": 0.0001571180240492061,
      "loss": 0.6051,
      "step": 10370
    },
    {
      "epoch": 0.6553030303030303,
      "grad_norm": 0.6722463965415955,
      "learning_rate": 0.00015703407684764802,
      "loss": 0.5316,
      "step": 10380
    },
    {
      "epoch": 0.6559343434343434,
      "grad_norm": 0.7588673233985901,
      "learning_rate": 0.00015695007003359908,
      "loss": 0.4777,
      "step": 10390
    },
    {
      "epoch": 0.6565656565656566,
      "grad_norm": 0.9397499561309814,
      "learning_rate": 0.0001568660036948638,
      "loss": 0.5357,
      "step": 10400
    },
    {
      "epoch": 0.6571969696969697,
      "grad_norm": 0.5240222811698914,
      "learning_rate": 0.0001567818779193091,
      "loss": 0.6932,
      "step": 10410
    },
    {
      "epoch": 0.6578282828282829,
      "grad_norm": 0.6276808977127075,
      "learning_rate": 0.0001566976927948639,
      "loss": 0.6016,
      "step": 10420
    },
    {
      "epoch": 0.6584595959595959,
      "grad_norm": 0.6330903768539429,
      "learning_rate": 0.0001566134484095192,
      "loss": 0.5264,
      "step": 10430
    },
    {
      "epoch": 0.6590909090909091,
      "grad_norm": 0.8061608672142029,
      "learning_rate": 0.00015652914485132796,
      "loss": 0.4742,
      "step": 10440
    },
    {
      "epoch": 0.6597222222222222,
      "grad_norm": 0.8812556862831116,
      "learning_rate": 0.00015644478220840492,
      "loss": 0.503,
      "step": 10450
    },
    {
      "epoch": 0.6603535353535354,
      "grad_norm": 0.6100068688392639,
      "learning_rate": 0.00015636036056892663,
      "loss": 0.7505,
      "step": 10460
    },
    {
      "epoch": 0.6609848484848485,
      "grad_norm": 0.6305376291275024,
      "learning_rate": 0.00015627588002113134,
      "loss": 0.6018,
      "step": 10470
    },
    {
      "epoch": 0.6616161616161617,
      "grad_norm": 0.6960409283638,
      "learning_rate": 0.00015619134065331873,
      "loss": 0.5286,
      "step": 10480
    },
    {
      "epoch": 0.6622474747474747,
      "grad_norm": 0.6642200350761414,
      "learning_rate": 0.0001561067425538501,
      "loss": 0.4955,
      "step": 10490
    },
    {
      "epoch": 0.6628787878787878,
      "grad_norm": 1.1106765270233154,
      "learning_rate": 0.00015602208581114808,
      "loss": 0.5066,
      "step": 10500
    },
    {
      "epoch": 0.663510101010101,
      "grad_norm": 0.5917830467224121,
      "learning_rate": 0.00015593737051369655,
      "loss": 0.6852,
      "step": 10510
    },
    {
      "epoch": 0.6641414141414141,
      "grad_norm": 0.5990296006202698,
      "learning_rate": 0.00015585259675004076,
      "loss": 0.617,
      "step": 10520
    },
    {
      "epoch": 0.6647727272727273,
      "grad_norm": 0.6183884143829346,
      "learning_rate": 0.00015576776460878686,
      "loss": 0.5169,
      "step": 10530
    },
    {
      "epoch": 0.6654040404040404,
      "grad_norm": 0.8281865119934082,
      "learning_rate": 0.0001556828741786021,
      "loss": 0.4973,
      "step": 10540
    },
    {
      "epoch": 0.6660353535353535,
      "grad_norm": 1.2316936254501343,
      "learning_rate": 0.00015559792554821472,
      "loss": 0.494,
      "step": 10550
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.5816935896873474,
      "learning_rate": 0.00015551291880641372,
      "loss": 0.6913,
      "step": 10560
    },
    {
      "epoch": 0.6672979797979798,
      "grad_norm": 0.6800694465637207,
      "learning_rate": 0.00015542785404204883,
      "loss": 0.6264,
      "step": 10570
    },
    {
      "epoch": 0.6679292929292929,
      "grad_norm": 0.6122955083847046,
      "learning_rate": 0.00015534273134403047,
      "loss": 0.5368,
      "step": 10580
    },
    {
      "epoch": 0.6685606060606061,
      "grad_norm": 0.7313991785049438,
      "learning_rate": 0.00015525755080132955,
      "loss": 0.4949,
      "step": 10590
    },
    {
      "epoch": 0.6691919191919192,
      "grad_norm": 1.056698203086853,
      "learning_rate": 0.0001551723125029775,
      "loss": 0.5143,
      "step": 10600
    },
    {
      "epoch": 0.6698232323232324,
      "grad_norm": 0.6068392992019653,
      "learning_rate": 0.00015508701653806615,
      "loss": 0.6976,
      "step": 10610
    },
    {
      "epoch": 0.6704545454545454,
      "grad_norm": 0.6738645434379578,
      "learning_rate": 0.0001550016629957475,
      "loss": 0.6286,
      "step": 10620
    },
    {
      "epoch": 0.6710858585858586,
      "grad_norm": 0.6442766785621643,
      "learning_rate": 0.0001549162519652338,
      "loss": 0.5254,
      "step": 10630
    },
    {
      "epoch": 0.6717171717171717,
      "grad_norm": 0.6953415870666504,
      "learning_rate": 0.00015483078353579734,
      "loss": 0.5102,
      "step": 10640
    },
    {
      "epoch": 0.6723484848484849,
      "grad_norm": 1.2428793907165527,
      "learning_rate": 0.00015474525779677047,
      "loss": 0.5192,
      "step": 10650
    },
    {
      "epoch": 0.672979797979798,
      "grad_norm": 0.5626111030578613,
      "learning_rate": 0.00015465967483754538,
      "loss": 0.7237,
      "step": 10660
    },
    {
      "epoch": 0.6736111111111112,
      "grad_norm": 0.6394316554069519,
      "learning_rate": 0.00015457403474757405,
      "loss": 0.5996,
      "step": 10670
    },
    {
      "epoch": 0.6742424242424242,
      "grad_norm": 0.6563475131988525,
      "learning_rate": 0.0001544883376163683,
      "loss": 0.533,
      "step": 10680
    },
    {
      "epoch": 0.6748737373737373,
      "grad_norm": 0.6722785830497742,
      "learning_rate": 0.00015440258353349945,
      "loss": 0.4636,
      "step": 10690
    },
    {
      "epoch": 0.6755050505050505,
      "grad_norm": 1.0582404136657715,
      "learning_rate": 0.00015431677258859837,
      "loss": 0.5013,
      "step": 10700
    },
    {
      "epoch": 0.6761363636363636,
      "grad_norm": 0.5443518161773682,
      "learning_rate": 0.00015423090487135535,
      "loss": 0.6876,
      "step": 10710
    },
    {
      "epoch": 0.6767676767676768,
      "grad_norm": 0.6639506816864014,
      "learning_rate": 0.00015414498047152007,
      "loss": 0.5983,
      "step": 10720
    },
    {
      "epoch": 0.67739898989899,
      "grad_norm": 0.6784919500350952,
      "learning_rate": 0.00015405899947890146,
      "loss": 0.5219,
      "step": 10730
    },
    {
      "epoch": 0.678030303030303,
      "grad_norm": 0.7205167412757874,
      "learning_rate": 0.0001539729619833675,
      "loss": 0.4646,
      "step": 10740
    },
    {
      "epoch": 0.6786616161616161,
      "grad_norm": 0.9414646029472351,
      "learning_rate": 0.00015388686807484532,
      "loss": 0.5044,
      "step": 10750
    },
    {
      "epoch": 0.6792929292929293,
      "grad_norm": 0.6037731766700745,
      "learning_rate": 0.000153800717843321,
      "loss": 0.7222,
      "step": 10760
    },
    {
      "epoch": 0.6799242424242424,
      "grad_norm": 0.6716630458831787,
      "learning_rate": 0.00015371451137883945,
      "loss": 0.5928,
      "step": 10770
    },
    {
      "epoch": 0.6805555555555556,
      "grad_norm": 0.7048764824867249,
      "learning_rate": 0.00015362824877150443,
      "loss": 0.5107,
      "step": 10780
    },
    {
      "epoch": 0.6811868686868687,
      "grad_norm": 0.7398087978363037,
      "learning_rate": 0.0001535419301114783,
      "loss": 0.4836,
      "step": 10790
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 1.035003900527954,
      "learning_rate": 0.00015345555548898206,
      "loss": 0.4798,
      "step": 10800
    },
    {
      "epoch": 0.6824494949494949,
      "grad_norm": 0.5533728003501892,
      "learning_rate": 0.00015336912499429515,
      "loss": 0.6955,
      "step": 10810
    },
    {
      "epoch": 0.6830808080808081,
      "grad_norm": 0.6299276947975159,
      "learning_rate": 0.00015328263871775544,
      "loss": 0.5719,
      "step": 10820
    },
    {
      "epoch": 0.6837121212121212,
      "grad_norm": 0.7040059566497803,
      "learning_rate": 0.0001531960967497592,
      "loss": 0.541,
      "step": 10830
    },
    {
      "epoch": 0.6843434343434344,
      "grad_norm": 0.6807965040206909,
      "learning_rate": 0.00015310949918076068,
      "loss": 0.4852,
      "step": 10840
    },
    {
      "epoch": 0.6849747474747475,
      "grad_norm": 1.003161072731018,
      "learning_rate": 0.00015302284610127246,
      "loss": 0.505,
      "step": 10850
    },
    {
      "epoch": 0.6856060606060606,
      "grad_norm": 0.6331389546394348,
      "learning_rate": 0.00015293613760186502,
      "loss": 0.6787,
      "step": 10860
    },
    {
      "epoch": 0.6862373737373737,
      "grad_norm": 0.5756049752235413,
      "learning_rate": 0.00015284937377316684,
      "loss": 0.5905,
      "step": 10870
    },
    {
      "epoch": 0.6868686868686869,
      "grad_norm": 0.6996668577194214,
      "learning_rate": 0.00015276255470586417,
      "loss": 0.523,
      "step": 10880
    },
    {
      "epoch": 0.6875,
      "grad_norm": 0.6445791721343994,
      "learning_rate": 0.00015267568049070102,
      "loss": 0.4926,
      "step": 10890
    },
    {
      "epoch": 0.6881313131313131,
      "grad_norm": 0.9679751396179199,
      "learning_rate": 0.00015258875121847902,
      "loss": 0.5181,
      "step": 10900
    },
    {
      "epoch": 0.6887626262626263,
      "grad_norm": 0.5702676177024841,
      "learning_rate": 0.00015250176698005744,
      "loss": 0.7372,
      "step": 10910
    },
    {
      "epoch": 0.6893939393939394,
      "grad_norm": 0.5687804222106934,
      "learning_rate": 0.00015241472786635288,
      "loss": 0.5875,
      "step": 10920
    },
    {
      "epoch": 0.6900252525252525,
      "grad_norm": 0.6807616353034973,
      "learning_rate": 0.0001523276339683393,
      "loss": 0.5296,
      "step": 10930
    },
    {
      "epoch": 0.6906565656565656,
      "grad_norm": 0.761805534362793,
      "learning_rate": 0.0001522404853770481,
      "loss": 0.4721,
      "step": 10940
    },
    {
      "epoch": 0.6912878787878788,
      "grad_norm": 0.8681610822677612,
      "learning_rate": 0.00015215328218356756,
      "loss": 0.5146,
      "step": 10950
    },
    {
      "epoch": 0.6919191919191919,
      "grad_norm": 0.534610390663147,
      "learning_rate": 0.00015206602447904327,
      "loss": 0.7234,
      "step": 10960
    },
    {
      "epoch": 0.6925505050505051,
      "grad_norm": 0.7093101739883423,
      "learning_rate": 0.00015197871235467765,
      "loss": 0.5945,
      "step": 10970
    },
    {
      "epoch": 0.6931818181818182,
      "grad_norm": 0.5722554922103882,
      "learning_rate": 0.00015189134590173014,
      "loss": 0.503,
      "step": 10980
    },
    {
      "epoch": 0.6938131313131313,
      "grad_norm": 0.626319944858551,
      "learning_rate": 0.00015180392521151677,
      "loss": 0.4936,
      "step": 10990
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 0.9075873494148254,
      "learning_rate": 0.0001517164503754105,
      "loss": 0.5148,
      "step": 11000
    },
    {
      "epoch": 0.6944444444444444,
      "eval_loss": 0.5536175966262817,
      "eval_runtime": 27.2874,
      "eval_samples_per_second": 93.816,
      "eval_steps_per_second": 11.727,
      "step": 11000
    }
  ],
  "logging_steps": 10,
  "max_steps": 31680,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.0465219936256e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3282d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "HF_AUTH_TOKEN = os.getenv('HF_AUTH_TOKEN')\n",
    "login(HF_AUTH_TOKEN)\n",
    "\n",
    "ADAPTER_PATH = \"/home/guest/AdvancedLLMReasoning/math_tutor_model/math_sft_adapter/v3/final_checkpoint\"\n",
    "BASE_MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "def load_model():\n",
    "    print(\"‚è≥ ƒêang load Base Model (4-bit)...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)\n",
    "    tokenizer.padding_side = \"left\"  # left for inference\n",
    "    \n",
    "    print(f\"ƒêang gh√©p LoRA Adapter t·ª´: {ADAPTER_PATH}\")\n",
    "    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def execute_python_code(code_str):\n",
    "    \"\"\"Execute Python code and return the output.\"\"\"\n",
    "    try:\n",
    "        # Capture stdout\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = StringIO()\n",
    "        \n",
    "        # Execute code\n",
    "        exec_globals = {}\n",
    "        exec(code_str, exec_globals)\n",
    "        \n",
    "        # Get output\n",
    "        output = sys.stdout.getvalue()\n",
    "        sys.stdout = old_stdout\n",
    "        \n",
    "        # If no print output, try to get the last expression value\n",
    "        if not output.strip():\n",
    "            # Try to get the last variable or expression result\n",
    "            code_lines = code_str.strip().split('\\n')\n",
    "            if code_lines:\n",
    "                last_line = code_lines[-1].strip()\n",
    "                # If last line is not an assignment or import\n",
    "                if '=' not in last_line and not last_line.startswith('import'):\n",
    "                    try:\n",
    "                        result = eval(last_line, exec_globals)\n",
    "                        output = str(result)\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        return output.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "def post_process_solution(generated_text):\n",
    "    \"\"\"\n",
    "    Post-process the generated solution:\n",
    "    1. Trim to line containing \\\\boxed{...}\n",
    "    2. If code is present, execute it and replace result in boxed\n",
    "    \"\"\"\n",
    "    match = re.search(r'^.*\\\\boxed\\{[^}]+\\}.*$', generated_text, re.MULTILINE)\n",
    "    if match:\n",
    "        trimmed_text = generated_text[:match.end()]\n",
    "    else:\n",
    "        trimmed_text = generated_text\n",
    "    code_match = re.search(r'```python\\s*\\n(.*?)\\n```', trimmed_text, re.DOTALL)\n",
    "    if code_match:\n",
    "        code_str = code_match.group(1)\n",
    "        # Remove <llm></llm> or <llm-code-output></llm-code-output> patterns\n",
    "        trimmed_text = re.sub(r'<llm>.*?</llm>', '', trimmed_text, flags=re.DOTALL)\n",
    "        trimmed_text = re.sub(r'<llm-code-output>.*?</llm-code-output>', '', trimmed_text, flags=re.DOTALL)\n",
    "        \n",
    "        # Execute code\n",
    "        result = execute_python_code(code_str)\n",
    "        \n",
    "        # Replace result to \\boxed{}\n",
    "        if result:\n",
    "            boxed_match = re.search(r'\\\\boxed\\{([^}]+)\\}', trimmed_text)\n",
    "            if boxed_match:\n",
    "                trimmed_text = re.sub(r'\\\\boxed\\{[^}]+\\}', f'\\\\\\\\boxed{{{result}}}', trimmed_text)\n",
    "            else:\n",
    "                trimmed_text += f'\\n\\nTherefore, the answer is \\\\boxed{{{result}}}.'\n",
    "    \n",
    "    return trimmed_text\n",
    "\n",
    "def solve_math_problem(model, tokenizer, question, max_length=1024):\n",
    "    system_prompt = (\n",
    "            \"You are a math reasoning assistant.\\n\"\n",
    "            \"Solve the problem step by step.\\n\"\n",
    "            \"You can use Python code if needed.\\n\"\n",
    "            \"If you write code, put it inside a Python code block:\\n\"\n",
    "            \"```python\\n\"\n",
    "            \"...\\n\"\n",
    "            \"```\\n\"\n",
    "            \"Output ONLY the final number inside \\\\boxed{}.\"\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "            prompt, \n",
    "            padding=False, \n",
    "            truncation=True, \n",
    "            max_length=max_length, \n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    print(\"\\nü§ñ Model ƒëang suy nghƒ©...\\n\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    processed_solution = post_process_solution(generated_text)\n",
    "    \n",
    "    return processed_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d20fe925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang load Base Model (4-bit)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang gh√©p LoRA Adapter t·ª´: /home/guest/AdvancedLLMReasoning/math_tutor_model/math_sft_adapter/v3/final_checkpoint\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Model ƒëang suy nghƒ©...\n",
      "\n",
      "--------------------------------------------------\n",
      "Let's use sympy to solve the equation.\n",
      "```python\n",
      "import sympy as sp\n",
      "\n",
      "# define the symbols\n",
      "x = sp.symbols('x')\n",
      "\n",
      "# define the equation\n",
      "equation = sp.Eq(2*x + 3, 7)\n",
      "\n",
      "# solve the equation\n",
      "solution = sp.solve(equation, x)\n",
      "\n",
      "# print the solution\n",
      "print(solution)\n",
      "```\n",
      "\n",
      "So the solution is $\\boxed{[2]}$.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "question = \"Solve the equation: 2x + 3 = 7\"  # V√≠ d·ª• c√¢u h·ªèi\n",
    "solution = solve_math_problem(model, tokenizer, question)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a78b8716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12000/12000 [00:00<00:00, 765244.30 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 303935.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "gsm8k_ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "math_ds = load_dataset(\"nlile/hendrycks-MATH-benchmark\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ec9d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_answer(text):\n",
    "    if \"\\\\boxed{\" in text:\n",
    "        idx = text.rfind(\"\\\\boxed{\")\n",
    "        content = \"\"\n",
    "        count = 0\n",
    "        started = False\n",
    "        for char in text[idx:]:\n",
    "            if char == \"{\":\n",
    "                count += 1\n",
    "                started = True\n",
    "                if count == 1: continue \n",
    "            elif char == \"}\":\n",
    "                count -= 1\n",
    "            if started:\n",
    "                if count == 0: break\n",
    "                content += char\n",
    "        return content.strip()\n",
    "    \n",
    "    match = re.search(r'[Tt]he answer is[:\\s]+(-?[\\d,\\.]+)', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c277301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 50\n",
    "gsm8k_answer = []\n",
    "math_answer = []\n",
    "for i in range(limit):\n",
    "    gsm8k_truth = gsm8k_ds[i]['answer'].split(\"####\")[-1].strip()\n",
    "    gsm8k_answer.append(gsm8k_truth)\n",
    "    \n",
    "    math_truth = extract_answer(math_ds[i]['solution'])\n",
    "    math_answer.append(math_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fff906f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18',\n",
       " '3',\n",
       " '70000',\n",
       " '540',\n",
       " '20',\n",
       " '64',\n",
       " '260',\n",
       " '160',\n",
       " '45',\n",
       " '460',\n",
       " '366',\n",
       " '694',\n",
       " '13',\n",
       " '18',\n",
       " '60',\n",
       " '125',\n",
       " '230',\n",
       " '57500',\n",
       " '7',\n",
       " '6',\n",
       " '15',\n",
       " '14',\n",
       " '7',\n",
       " '8',\n",
       " '26',\n",
       " '2',\n",
       " '243',\n",
       " '16',\n",
       " '25',\n",
       " '104',\n",
       " '109',\n",
       " '80',\n",
       " '35',\n",
       " '70',\n",
       " '23',\n",
       " '9',\n",
       " '75',\n",
       " '2',\n",
       " '10',\n",
       " '18',\n",
       " '8',\n",
       " '200',\n",
       " '26',\n",
       " '48',\n",
       " '20',\n",
       " '104',\n",
       " '163',\n",
       " '800',\n",
       " '8',\n",
       " '30']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24c21720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)',\n",
       " 'p - q',\n",
       " '\\\\frac{14}{3}',\n",
       " '9',\n",
       " '\\\\text{Evelyn}',\n",
       " '42',\n",
       " '27',\n",
       " '90^\\\\circ',\n",
       " '3\\\\sqrt{13}',\n",
       " '4',\n",
       " '2220',\n",
       " '\\\\frac{3}{56}',\n",
       " '284',\n",
       " '5',\n",
       " '\\\\sqrt{51}',\n",
       " '6 - 5i',\n",
       " '-50',\n",
       " '\\\\pi',\n",
       " '28',\n",
       " '3',\n",
       " '6+9i',\n",
       " '13535',\n",
       " '5',\n",
       " 'x=5',\n",
       " '10',\n",
       " '1,-2',\n",
       " '144',\n",
       " '78',\n",
       " '-2 + 7i',\n",
       " '225',\n",
       " '52_8',\n",
       " '11\\\\sqrt2',\n",
       " '720',\n",
       " '\\\\frac{243}{625}',\n",
       " '-125',\n",
       " '3',\n",
       " '3, 5, 7',\n",
       " '72',\n",
       " '2000',\n",
       " '23',\n",
       " '12',\n",
       " '17',\n",
       " '4',\n",
       " '70 \\\\sqrt{2}',\n",
       " '1.25',\n",
       " '2',\n",
       " '6',\n",
       " '5',\n",
       " '\\\\frac{3}{2}',\n",
       " '83']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1202dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "best_global_step": 11000,
  "best_metric": 0.648476243019104,
  "best_model_checkpoint": "math_tutor_model/math_sft_adapter/v1/checkpoint-11000",
  "epoch": 0.6944444444444444,
  "eval_steps": 1000,
  "global_step": 11000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "eval_loss": 1.5935035943984985,
      "eval_runtime": 44.2224,
      "eval_samples_per_second": 57.889,
      "eval_steps_per_second": 7.236,
      "step": 0
    },
    {
      "epoch": 6.313131313131313e-05,
      "grad_norm": 0.5231262445449829,
      "learning_rate": 0.0,
      "loss": 1.4109,
      "step": 1
    },
    {
      "epoch": 0.0006313131313131314,
      "grad_norm": 1.0744287967681885,
      "learning_rate": 1.8927444794952682e-06,
      "loss": 1.354,
      "step": 10
    },
    {
      "epoch": 0.0012626262626262627,
      "grad_norm": 1.2428052425384521,
      "learning_rate": 3.995793901156677e-06,
      "loss": 1.4351,
      "step": 20
    },
    {
      "epoch": 0.001893939393939394,
      "grad_norm": 1.5835915803909302,
      "learning_rate": 6.098843322818087e-06,
      "loss": 1.4633,
      "step": 30
    },
    {
      "epoch": 0.0025252525252525255,
      "grad_norm": 1.6409869194030762,
      "learning_rate": 8.201892744479495e-06,
      "loss": 1.4833,
      "step": 40
    },
    {
      "epoch": 0.0031565656565656565,
      "grad_norm": 2.525390625,
      "learning_rate": 1.0304942166140905e-05,
      "loss": 1.4602,
      "step": 50
    },
    {
      "epoch": 0.003787878787878788,
      "grad_norm": 0.6554961204528809,
      "learning_rate": 1.2407991587802314e-05,
      "loss": 1.1293,
      "step": 60
    },
    {
      "epoch": 0.004419191919191919,
      "grad_norm": 0.8337051272392273,
      "learning_rate": 1.4511041009463724e-05,
      "loss": 1.0853,
      "step": 70
    },
    {
      "epoch": 0.005050505050505051,
      "grad_norm": 0.9777280688285828,
      "learning_rate": 1.661409043112513e-05,
      "loss": 0.9349,
      "step": 80
    },
    {
      "epoch": 0.005681818181818182,
      "grad_norm": 0.795711100101471,
      "learning_rate": 1.871713985278654e-05,
      "loss": 0.8178,
      "step": 90
    },
    {
      "epoch": 0.006313131313131313,
      "grad_norm": 1.8887639045715332,
      "learning_rate": 2.0820189274447953e-05,
      "loss": 0.8303,
      "step": 100
    },
    {
      "epoch": 0.006944444444444444,
      "grad_norm": 0.5561102032661438,
      "learning_rate": 2.292323869610936e-05,
      "loss": 1.0339,
      "step": 110
    },
    {
      "epoch": 0.007575757575757576,
      "grad_norm": 0.5732130408287048,
      "learning_rate": 2.5026288117770768e-05,
      "loss": 0.8946,
      "step": 120
    },
    {
      "epoch": 0.008207070707070708,
      "grad_norm": 0.7393359541893005,
      "learning_rate": 2.7129337539432176e-05,
      "loss": 0.7833,
      "step": 130
    },
    {
      "epoch": 0.008838383838383838,
      "grad_norm": 0.7587074637413025,
      "learning_rate": 2.9232386961093587e-05,
      "loss": 0.7421,
      "step": 140
    },
    {
      "epoch": 0.00946969696969697,
      "grad_norm": 1.5055650472640991,
      "learning_rate": 3.1335436382754995e-05,
      "loss": 0.7842,
      "step": 150
    },
    {
      "epoch": 0.010101010101010102,
      "grad_norm": 0.5556084513664246,
      "learning_rate": 3.34384858044164e-05,
      "loss": 0.9686,
      "step": 160
    },
    {
      "epoch": 0.010732323232323232,
      "grad_norm": 0.5722646117210388,
      "learning_rate": 3.554153522607782e-05,
      "loss": 0.8657,
      "step": 170
    },
    {
      "epoch": 0.011363636363636364,
      "grad_norm": 0.6355533003807068,
      "learning_rate": 3.7644584647739225e-05,
      "loss": 0.7828,
      "step": 180
    },
    {
      "epoch": 0.011994949494949494,
      "grad_norm": 0.6747802495956421,
      "learning_rate": 3.974763406940063e-05,
      "loss": 0.7015,
      "step": 190
    },
    {
      "epoch": 0.012626262626262626,
      "grad_norm": 1.6578633785247803,
      "learning_rate": 4.185068349106204e-05,
      "loss": 0.7456,
      "step": 200
    },
    {
      "epoch": 0.013257575757575758,
      "grad_norm": 0.5622562766075134,
      "learning_rate": 4.395373291272345e-05,
      "loss": 1.0254,
      "step": 210
    },
    {
      "epoch": 0.013888888888888888,
      "grad_norm": 0.645758867263794,
      "learning_rate": 4.6056782334384864e-05,
      "loss": 0.8607,
      "step": 220
    },
    {
      "epoch": 0.01452020202020202,
      "grad_norm": 0.6761602163314819,
      "learning_rate": 4.815983175604627e-05,
      "loss": 0.7305,
      "step": 230
    },
    {
      "epoch": 0.015151515151515152,
      "grad_norm": 0.7268579602241516,
      "learning_rate": 5.026288117770768e-05,
      "loss": 0.6928,
      "step": 240
    },
    {
      "epoch": 0.015782828282828284,
      "grad_norm": 1.3678913116455078,
      "learning_rate": 5.236593059936909e-05,
      "loss": 0.7286,
      "step": 250
    },
    {
      "epoch": 0.016414141414141416,
      "grad_norm": 0.5411850810050964,
      "learning_rate": 5.44689800210305e-05,
      "loss": 0.9273,
      "step": 260
    },
    {
      "epoch": 0.017045454545454544,
      "grad_norm": 0.5664272308349609,
      "learning_rate": 5.657202944269191e-05,
      "loss": 0.7935,
      "step": 270
    },
    {
      "epoch": 0.017676767676767676,
      "grad_norm": 0.6389761567115784,
      "learning_rate": 5.867507886435332e-05,
      "loss": 0.712,
      "step": 280
    },
    {
      "epoch": 0.018308080808080808,
      "grad_norm": 0.7394918203353882,
      "learning_rate": 6.0778128286014725e-05,
      "loss": 0.6766,
      "step": 290
    },
    {
      "epoch": 0.01893939393939394,
      "grad_norm": 1.2056524753570557,
      "learning_rate": 6.288117770767613e-05,
      "loss": 0.7475,
      "step": 300
    },
    {
      "epoch": 0.019570707070707072,
      "grad_norm": 0.5564950704574585,
      "learning_rate": 6.498422712933754e-05,
      "loss": 0.9394,
      "step": 310
    },
    {
      "epoch": 0.020202020202020204,
      "grad_norm": 0.597830057144165,
      "learning_rate": 6.708727655099896e-05,
      "loss": 0.8343,
      "step": 320
    },
    {
      "epoch": 0.020833333333333332,
      "grad_norm": 0.5914454460144043,
      "learning_rate": 6.919032597266037e-05,
      "loss": 0.6985,
      "step": 330
    },
    {
      "epoch": 0.021464646464646464,
      "grad_norm": 0.7423954010009766,
      "learning_rate": 7.129337539432177e-05,
      "loss": 0.683,
      "step": 340
    },
    {
      "epoch": 0.022095959595959596,
      "grad_norm": 1.2033611536026,
      "learning_rate": 7.339642481598317e-05,
      "loss": 0.6986,
      "step": 350
    },
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 0.5604282021522522,
      "learning_rate": 7.549947423764459e-05,
      "loss": 0.8819,
      "step": 360
    },
    {
      "epoch": 0.02335858585858586,
      "grad_norm": 0.5426417589187622,
      "learning_rate": 7.760252365930599e-05,
      "loss": 0.8029,
      "step": 370
    },
    {
      "epoch": 0.023989898989898988,
      "grad_norm": 0.6001778841018677,
      "learning_rate": 7.970557308096742e-05,
      "loss": 0.7269,
      "step": 380
    },
    {
      "epoch": 0.02462121212121212,
      "grad_norm": 0.5661148428916931,
      "learning_rate": 8.180862250262882e-05,
      "loss": 0.6606,
      "step": 390
    },
    {
      "epoch": 0.025252525252525252,
      "grad_norm": 0.8704198598861694,
      "learning_rate": 8.391167192429022e-05,
      "loss": 0.6411,
      "step": 400
    },
    {
      "epoch": 0.025883838383838384,
      "grad_norm": 0.5042587518692017,
      "learning_rate": 8.601472134595163e-05,
      "loss": 0.9531,
      "step": 410
    },
    {
      "epoch": 0.026515151515151516,
      "grad_norm": 0.5312213897705078,
      "learning_rate": 8.811777076761303e-05,
      "loss": 0.7935,
      "step": 420
    },
    {
      "epoch": 0.027146464646464648,
      "grad_norm": 0.49941182136535645,
      "learning_rate": 9.022082018927446e-05,
      "loss": 0.7261,
      "step": 430
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 0.6098383665084839,
      "learning_rate": 9.232386961093586e-05,
      "loss": 0.6301,
      "step": 440
    },
    {
      "epoch": 0.028409090909090908,
      "grad_norm": 0.9564092755317688,
      "learning_rate": 9.442691903259728e-05,
      "loss": 0.6809,
      "step": 450
    },
    {
      "epoch": 0.02904040404040404,
      "grad_norm": 0.45313307642936707,
      "learning_rate": 9.652996845425868e-05,
      "loss": 0.9415,
      "step": 460
    },
    {
      "epoch": 0.029671717171717172,
      "grad_norm": 0.5039793252944946,
      "learning_rate": 9.863301787592008e-05,
      "loss": 0.8089,
      "step": 470
    },
    {
      "epoch": 0.030303030303030304,
      "grad_norm": 0.5561915636062622,
      "learning_rate": 0.0001007360672975815,
      "loss": 0.6986,
      "step": 480
    },
    {
      "epoch": 0.030934343434343436,
      "grad_norm": 0.5862769484519958,
      "learning_rate": 0.00010283911671924291,
      "loss": 0.6232,
      "step": 490
    },
    {
      "epoch": 0.03156565656565657,
      "grad_norm": 0.9825224876403809,
      "learning_rate": 0.00010494216614090431,
      "loss": 0.71,
      "step": 500
    },
    {
      "epoch": 0.032196969696969696,
      "grad_norm": 0.42280808091163635,
      "learning_rate": 0.00010704521556256572,
      "loss": 0.9162,
      "step": 510
    },
    {
      "epoch": 0.03282828282828283,
      "grad_norm": 0.4278038442134857,
      "learning_rate": 0.00010914826498422714,
      "loss": 0.7871,
      "step": 520
    },
    {
      "epoch": 0.03345959595959596,
      "grad_norm": 0.5541549324989319,
      "learning_rate": 0.00011125131440588854,
      "loss": 0.7126,
      "step": 530
    },
    {
      "epoch": 0.03409090909090909,
      "grad_norm": 0.6073480248451233,
      "learning_rate": 0.00011335436382754996,
      "loss": 0.6652,
      "step": 540
    },
    {
      "epoch": 0.034722222222222224,
      "grad_norm": 0.9696308374404907,
      "learning_rate": 0.00011545741324921136,
      "loss": 0.6711,
      "step": 550
    },
    {
      "epoch": 0.03535353535353535,
      "grad_norm": 0.40793925523757935,
      "learning_rate": 0.00011756046267087277,
      "loss": 0.957,
      "step": 560
    },
    {
      "epoch": 0.03598484848484849,
      "grad_norm": 0.4499455392360687,
      "learning_rate": 0.00011966351209253419,
      "loss": 0.7665,
      "step": 570
    },
    {
      "epoch": 0.036616161616161616,
      "grad_norm": 0.5129889249801636,
      "learning_rate": 0.00012176656151419559,
      "loss": 0.668,
      "step": 580
    },
    {
      "epoch": 0.037247474747474744,
      "grad_norm": 0.5738191604614258,
      "learning_rate": 0.000123869610935857,
      "loss": 0.6332,
      "step": 590
    },
    {
      "epoch": 0.03787878787878788,
      "grad_norm": 0.9415767192840576,
      "learning_rate": 0.0001259726603575184,
      "loss": 0.68,
      "step": 600
    },
    {
      "epoch": 0.03851010101010101,
      "grad_norm": 0.4069151282310486,
      "learning_rate": 0.00012807570977917983,
      "loss": 0.8525,
      "step": 610
    },
    {
      "epoch": 0.039141414141414144,
      "grad_norm": 0.447846382856369,
      "learning_rate": 0.00013017875920084122,
      "loss": 0.7986,
      "step": 620
    },
    {
      "epoch": 0.03977272727272727,
      "grad_norm": 0.4828910827636719,
      "learning_rate": 0.00013228180862250263,
      "loss": 0.703,
      "step": 630
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": 0.5003986954689026,
      "learning_rate": 0.00013438485804416405,
      "loss": 0.6528,
      "step": 640
    },
    {
      "epoch": 0.041035353535353536,
      "grad_norm": 1.0431395769119263,
      "learning_rate": 0.00013648790746582546,
      "loss": 0.6587,
      "step": 650
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.40883415937423706,
      "learning_rate": 0.00013859095688748688,
      "loss": 0.8451,
      "step": 660
    },
    {
      "epoch": 0.0422979797979798,
      "grad_norm": 0.41671615839004517,
      "learning_rate": 0.00014069400630914826,
      "loss": 0.7863,
      "step": 670
    },
    {
      "epoch": 0.04292929292929293,
      "grad_norm": 0.45995432138442993,
      "learning_rate": 0.00014279705573080968,
      "loss": 0.6979,
      "step": 680
    },
    {
      "epoch": 0.043560606060606064,
      "grad_norm": 0.5524851679801941,
      "learning_rate": 0.0001449001051524711,
      "loss": 0.6402,
      "step": 690
    },
    {
      "epoch": 0.04419191919191919,
      "grad_norm": 0.8536369204521179,
      "learning_rate": 0.0001470031545741325,
      "loss": 0.6776,
      "step": 700
    },
    {
      "epoch": 0.04482323232323232,
      "grad_norm": 0.43402841687202454,
      "learning_rate": 0.00014910620399579392,
      "loss": 0.9286,
      "step": 710
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.44479241967201233,
      "learning_rate": 0.0001512092534174553,
      "loss": 0.7664,
      "step": 720
    },
    {
      "epoch": 0.046085858585858584,
      "grad_norm": 0.5016056895256042,
      "learning_rate": 0.00015331230283911672,
      "loss": 0.6945,
      "step": 730
    },
    {
      "epoch": 0.04671717171717172,
      "grad_norm": 0.5506412982940674,
      "learning_rate": 0.00015541535226077814,
      "loss": 0.6222,
      "step": 740
    },
    {
      "epoch": 0.04734848484848485,
      "grad_norm": 0.9175305962562561,
      "learning_rate": 0.00015751840168243955,
      "loss": 0.6545,
      "step": 750
    },
    {
      "epoch": 0.047979797979797977,
      "grad_norm": 0.41651400923728943,
      "learning_rate": 0.00015962145110410097,
      "loss": 0.9326,
      "step": 760
    },
    {
      "epoch": 0.04861111111111111,
      "grad_norm": 0.43066680431365967,
      "learning_rate": 0.00016172450052576236,
      "loss": 0.7381,
      "step": 770
    },
    {
      "epoch": 0.04924242424242424,
      "grad_norm": 0.4925856590270996,
      "learning_rate": 0.00016382754994742377,
      "loss": 0.6926,
      "step": 780
    },
    {
      "epoch": 0.049873737373737376,
      "grad_norm": 0.5217193365097046,
      "learning_rate": 0.00016593059936908516,
      "loss": 0.6227,
      "step": 790
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 0.88702392578125,
      "learning_rate": 0.0001680336487907466,
      "loss": 0.6958,
      "step": 800
    },
    {
      "epoch": 0.05113636363636364,
      "grad_norm": 0.3806374967098236,
      "learning_rate": 0.00017013669821240801,
      "loss": 0.8871,
      "step": 810
    },
    {
      "epoch": 0.05176767676767677,
      "grad_norm": 0.4745800495147705,
      "learning_rate": 0.0001722397476340694,
      "loss": 0.7985,
      "step": 820
    },
    {
      "epoch": 0.052398989898989896,
      "grad_norm": 0.47345954179763794,
      "learning_rate": 0.00017434279705573082,
      "loss": 0.7019,
      "step": 830
    },
    {
      "epoch": 0.05303030303030303,
      "grad_norm": 0.5221117734909058,
      "learning_rate": 0.00017644584647739223,
      "loss": 0.6599,
      "step": 840
    },
    {
      "epoch": 0.05366161616161616,
      "grad_norm": 0.7917218208312988,
      "learning_rate": 0.00017854889589905365,
      "loss": 0.6472,
      "step": 850
    },
    {
      "epoch": 0.054292929292929296,
      "grad_norm": 0.37973278760910034,
      "learning_rate": 0.00018065194532071506,
      "loss": 0.9645,
      "step": 860
    },
    {
      "epoch": 0.054924242424242424,
      "grad_norm": 0.41031450033187866,
      "learning_rate": 0.00018275499474237645,
      "loss": 0.7891,
      "step": 870
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 0.4644099473953247,
      "learning_rate": 0.00018485804416403786,
      "loss": 0.6885,
      "step": 880
    },
    {
      "epoch": 0.05618686868686869,
      "grad_norm": 0.47953516244888306,
      "learning_rate": 0.00018696109358569928,
      "loss": 0.6449,
      "step": 890
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 0.7553716897964478,
      "learning_rate": 0.0001890641430073607,
      "loss": 0.6532,
      "step": 900
    },
    {
      "epoch": 0.05744949494949495,
      "grad_norm": 0.3853500485420227,
      "learning_rate": 0.0001911671924290221,
      "loss": 0.8799,
      "step": 910
    },
    {
      "epoch": 0.05808080808080808,
      "grad_norm": 0.4862810969352722,
      "learning_rate": 0.0001932702418506835,
      "loss": 0.8063,
      "step": 920
    },
    {
      "epoch": 0.058712121212121215,
      "grad_norm": 0.4684811532497406,
      "learning_rate": 0.0001953732912723449,
      "loss": 0.6911,
      "step": 930
    },
    {
      "epoch": 0.059343434343434344,
      "grad_norm": 0.5311151146888733,
      "learning_rate": 0.00019747634069400632,
      "loss": 0.679,
      "step": 940
    },
    {
      "epoch": 0.05997474747474747,
      "grad_norm": 0.8532497882843018,
      "learning_rate": 0.00019957939011566774,
      "loss": 0.6517,
      "step": 950
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": 0.40760210156440735,
      "learning_rate": 0.00019999996655333316,
      "loss": 0.8703,
      "step": 960
    },
    {
      "epoch": 0.061237373737373736,
      "grad_norm": 0.42941901087760925,
      "learning_rate": 0.00019999983067628733,
      "loss": 0.7981,
      "step": 970
    },
    {
      "epoch": 0.06186868686868687,
      "grad_norm": 0.4922045171260834,
      "learning_rate": 0.0001999995902785878,
      "loss": 0.7272,
      "step": 980
    },
    {
      "epoch": 0.0625,
      "grad_norm": 0.4542674124240875,
      "learning_rate": 0.0001999992453604858,
      "loss": 0.5965,
      "step": 990
    },
    {
      "epoch": 0.06313131313131314,
      "grad_norm": 0.7866173982620239,
      "learning_rate": 0.00019999879592234187,
      "loss": 0.6616,
      "step": 1000
    },
    {
      "epoch": 0.06313131313131314,
      "eval_loss": 0.7408784627914429,
      "eval_runtime": 43.6849,
      "eval_samples_per_second": 58.601,
      "eval_steps_per_second": 7.325,
      "step": 1000
    },
    {
      "epoch": 0.06376262626262626,
      "grad_norm": 0.36401039361953735,
      "learning_rate": 0.00019999824196462578,
      "loss": 0.913,
      "step": 1010
    },
    {
      "epoch": 0.06439393939393939,
      "grad_norm": 0.4623726010322571,
      "learning_rate": 0.00019999758348791647,
      "loss": 0.7689,
      "step": 1020
    },
    {
      "epoch": 0.06502525252525253,
      "grad_norm": 0.4357050061225891,
      "learning_rate": 0.00019999682049290227,
      "loss": 0.6501,
      "step": 1030
    },
    {
      "epoch": 0.06565656565656566,
      "grad_norm": 0.47543665766716003,
      "learning_rate": 0.00019999595298038057,
      "loss": 0.5807,
      "step": 1040
    },
    {
      "epoch": 0.06628787878787878,
      "grad_norm": 0.7970326542854309,
      "learning_rate": 0.00019999498095125818,
      "loss": 0.6169,
      "step": 1050
    },
    {
      "epoch": 0.06691919191919192,
      "grad_norm": 0.3858082890510559,
      "learning_rate": 0.00019999390440655107,
      "loss": 0.8714,
      "step": 1060
    },
    {
      "epoch": 0.06755050505050506,
      "grad_norm": 0.4667547643184662,
      "learning_rate": 0.00019999272334738442,
      "loss": 0.7816,
      "step": 1070
    },
    {
      "epoch": 0.06818181818181818,
      "grad_norm": 0.44944727420806885,
      "learning_rate": 0.0001999914377749927,
      "loss": 0.6535,
      "step": 1080
    },
    {
      "epoch": 0.06881313131313131,
      "grad_norm": 0.46745064854621887,
      "learning_rate": 0.00019999004769071955,
      "loss": 0.637,
      "step": 1090
    },
    {
      "epoch": 0.06944444444444445,
      "grad_norm": 0.7340429425239563,
      "learning_rate": 0.00019998855309601797,
      "loss": 0.6312,
      "step": 1100
    },
    {
      "epoch": 0.07007575757575757,
      "grad_norm": 0.3948884606361389,
      "learning_rate": 0.00019998695399245012,
      "loss": 0.8667,
      "step": 1110
    },
    {
      "epoch": 0.0707070707070707,
      "grad_norm": 0.4422679543495178,
      "learning_rate": 0.00019998525038168735,
      "loss": 0.7636,
      "step": 1120
    },
    {
      "epoch": 0.07133838383838384,
      "grad_norm": 0.43747594952583313,
      "learning_rate": 0.00019998344226551028,
      "loss": 0.6872,
      "step": 1130
    },
    {
      "epoch": 0.07196969696969698,
      "grad_norm": 0.5347137451171875,
      "learning_rate": 0.00019998152964580883,
      "loss": 0.6326,
      "step": 1140
    },
    {
      "epoch": 0.0726010101010101,
      "grad_norm": 0.7890794277191162,
      "learning_rate": 0.00019997951252458205,
      "loss": 0.6612,
      "step": 1150
    },
    {
      "epoch": 0.07323232323232323,
      "grad_norm": 0.38595354557037354,
      "learning_rate": 0.00019997739090393823,
      "loss": 0.8888,
      "step": 1160
    },
    {
      "epoch": 0.07386363636363637,
      "grad_norm": 0.45106688141822815,
      "learning_rate": 0.00019997516478609498,
      "loss": 0.7631,
      "step": 1170
    },
    {
      "epoch": 0.07449494949494949,
      "grad_norm": 0.4829006791114807,
      "learning_rate": 0.00019997283417337895,
      "loss": 0.6871,
      "step": 1180
    },
    {
      "epoch": 0.07512626262626262,
      "grad_norm": 0.5265364050865173,
      "learning_rate": 0.00019997039906822624,
      "loss": 0.6209,
      "step": 1190
    },
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 0.7572271823883057,
      "learning_rate": 0.00019996785947318192,
      "loss": 0.6744,
      "step": 1200
    },
    {
      "epoch": 0.0763888888888889,
      "grad_norm": 0.39621466398239136,
      "learning_rate": 0.00019996521539090046,
      "loss": 0.8927,
      "step": 1210
    },
    {
      "epoch": 0.07702020202020202,
      "grad_norm": 0.43013980984687805,
      "learning_rate": 0.00019996246682414548,
      "loss": 0.763,
      "step": 1220
    },
    {
      "epoch": 0.07765151515151515,
      "grad_norm": 0.4166487455368042,
      "learning_rate": 0.0001999596137757898,
      "loss": 0.6721,
      "step": 1230
    },
    {
      "epoch": 0.07828282828282829,
      "grad_norm": 0.5304975509643555,
      "learning_rate": 0.0001999566562488154,
      "loss": 0.6253,
      "step": 1240
    },
    {
      "epoch": 0.07891414141414141,
      "grad_norm": 0.8720878958702087,
      "learning_rate": 0.0001999535942463136,
      "loss": 0.6463,
      "step": 1250
    },
    {
      "epoch": 0.07954545454545454,
      "grad_norm": 0.37620532512664795,
      "learning_rate": 0.00019995042777148477,
      "loss": 0.9525,
      "step": 1260
    },
    {
      "epoch": 0.08017676767676768,
      "grad_norm": 0.42094913125038147,
      "learning_rate": 0.00019994715682763854,
      "loss": 0.7416,
      "step": 1270
    },
    {
      "epoch": 0.08080808080808081,
      "grad_norm": 0.43714022636413574,
      "learning_rate": 0.00019994378141819378,
      "loss": 0.6814,
      "step": 1280
    },
    {
      "epoch": 0.08143939393939394,
      "grad_norm": 0.4799962341785431,
      "learning_rate": 0.0001999403015466784,
      "loss": 0.6186,
      "step": 1290
    },
    {
      "epoch": 0.08207070707070707,
      "grad_norm": 0.7517330646514893,
      "learning_rate": 0.0001999367172167297,
      "loss": 0.6394,
      "step": 1300
    },
    {
      "epoch": 0.08270202020202021,
      "grad_norm": 0.37433353066444397,
      "learning_rate": 0.00019993302843209393,
      "loss": 0.8798,
      "step": 1310
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.38659605383872986,
      "learning_rate": 0.00019992923519662674,
      "loss": 0.7325,
      "step": 1320
    },
    {
      "epoch": 0.08396464646464646,
      "grad_norm": 0.435062438249588,
      "learning_rate": 0.00019992533751429282,
      "loss": 0.6436,
      "step": 1330
    },
    {
      "epoch": 0.0845959595959596,
      "grad_norm": 0.44114160537719727,
      "learning_rate": 0.00019992133538916608,
      "loss": 0.6157,
      "step": 1340
    },
    {
      "epoch": 0.08522727272727272,
      "grad_norm": 0.6788908839225769,
      "learning_rate": 0.0001999172288254295,
      "loss": 0.6272,
      "step": 1350
    },
    {
      "epoch": 0.08585858585858586,
      "grad_norm": 0.3562941253185272,
      "learning_rate": 0.00019991301782737537,
      "loss": 0.9052,
      "step": 1360
    },
    {
      "epoch": 0.08648989898989899,
      "grad_norm": 0.372789204120636,
      "learning_rate": 0.00019990870239940502,
      "loss": 0.7484,
      "step": 1370
    },
    {
      "epoch": 0.08712121212121213,
      "grad_norm": 0.44719839096069336,
      "learning_rate": 0.000199904282546029,
      "loss": 0.711,
      "step": 1380
    },
    {
      "epoch": 0.08775252525252525,
      "grad_norm": 0.4805004894733429,
      "learning_rate": 0.00019989975827186695,
      "loss": 0.6269,
      "step": 1390
    },
    {
      "epoch": 0.08838383838383838,
      "grad_norm": 0.8108805418014526,
      "learning_rate": 0.00019989512958164772,
      "loss": 0.6373,
      "step": 1400
    },
    {
      "epoch": 0.08901515151515152,
      "grad_norm": 0.35549214482307434,
      "learning_rate": 0.0001998903964802092,
      "loss": 0.9255,
      "step": 1410
    },
    {
      "epoch": 0.08964646464646464,
      "grad_norm": 0.43658778071403503,
      "learning_rate": 0.0001998855589724985,
      "loss": 0.7508,
      "step": 1420
    },
    {
      "epoch": 0.09027777777777778,
      "grad_norm": 0.42971107363700867,
      "learning_rate": 0.0001998806170635718,
      "loss": 0.6889,
      "step": 1430
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.4325684607028961,
      "learning_rate": 0.0001998755707585945,
      "loss": 0.5878,
      "step": 1440
    },
    {
      "epoch": 0.09154040404040405,
      "grad_norm": 0.8253008723258972,
      "learning_rate": 0.00019987042006284095,
      "loss": 0.6483,
      "step": 1450
    },
    {
      "epoch": 0.09217171717171717,
      "grad_norm": 0.3694595992565155,
      "learning_rate": 0.00019986516498169473,
      "loss": 0.8924,
      "step": 1460
    },
    {
      "epoch": 0.0928030303030303,
      "grad_norm": 0.4225056767463684,
      "learning_rate": 0.0001998598055206485,
      "loss": 0.7886,
      "step": 1470
    },
    {
      "epoch": 0.09343434343434344,
      "grad_norm": 0.4516845643520355,
      "learning_rate": 0.00019985434168530398,
      "loss": 0.6779,
      "step": 1480
    },
    {
      "epoch": 0.09406565656565656,
      "grad_norm": 0.4686310291290283,
      "learning_rate": 0.0001998487734813721,
      "loss": 0.6218,
      "step": 1490
    },
    {
      "epoch": 0.0946969696969697,
      "grad_norm": 0.8352633714675903,
      "learning_rate": 0.0001998431009146727,
      "loss": 0.6271,
      "step": 1500
    },
    {
      "epoch": 0.09532828282828283,
      "grad_norm": 0.397965669631958,
      "learning_rate": 0.00019983732399113483,
      "loss": 0.8337,
      "step": 1510
    },
    {
      "epoch": 0.09595959595959595,
      "grad_norm": 0.4450063705444336,
      "learning_rate": 0.0001998314427167966,
      "loss": 0.7458,
      "step": 1520
    },
    {
      "epoch": 0.09659090909090909,
      "grad_norm": 0.44464510679244995,
      "learning_rate": 0.00019982545709780515,
      "loss": 0.675,
      "step": 1530
    },
    {
      "epoch": 0.09722222222222222,
      "grad_norm": 0.46707382798194885,
      "learning_rate": 0.00019981936714041668,
      "loss": 0.5858,
      "step": 1540
    },
    {
      "epoch": 0.09785353535353536,
      "grad_norm": 0.714739203453064,
      "learning_rate": 0.00019981317285099653,
      "loss": 0.6565,
      "step": 1550
    },
    {
      "epoch": 0.09848484848484848,
      "grad_norm": 0.40981924533843994,
      "learning_rate": 0.0001998068742360189,
      "loss": 0.8859,
      "step": 1560
    },
    {
      "epoch": 0.09911616161616162,
      "grad_norm": 0.4260037839412689,
      "learning_rate": 0.00019980047130206728,
      "loss": 0.7662,
      "step": 1570
    },
    {
      "epoch": 0.09974747474747475,
      "grad_norm": 0.43289273977279663,
      "learning_rate": 0.000199793964055834,
      "loss": 0.668,
      "step": 1580
    },
    {
      "epoch": 0.10037878787878787,
      "grad_norm": 0.4764469861984253,
      "learning_rate": 0.0001997873525041205,
      "loss": 0.6036,
      "step": 1590
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 0.8135440945625305,
      "learning_rate": 0.00019978063665383725,
      "loss": 0.6567,
      "step": 1600
    },
    {
      "epoch": 0.10164141414141414,
      "grad_norm": 0.3650861978530884,
      "learning_rate": 0.0001997738165120037,
      "loss": 0.911,
      "step": 1610
    },
    {
      "epoch": 0.10227272727272728,
      "grad_norm": 0.4177100360393524,
      "learning_rate": 0.00019976689208574827,
      "loss": 0.7725,
      "step": 1620
    },
    {
      "epoch": 0.1029040404040404,
      "grad_norm": 0.478882372379303,
      "learning_rate": 0.00019975986338230853,
      "loss": 0.6738,
      "step": 1630
    },
    {
      "epoch": 0.10353535353535354,
      "grad_norm": 0.4678427577018738,
      "learning_rate": 0.00019975273040903087,
      "loss": 0.579,
      "step": 1640
    },
    {
      "epoch": 0.10416666666666667,
      "grad_norm": 0.7243507504463196,
      "learning_rate": 0.00019974549317337075,
      "loss": 0.637,
      "step": 1650
    },
    {
      "epoch": 0.10479797979797979,
      "grad_norm": 0.3980304002761841,
      "learning_rate": 0.00019973815168289257,
      "loss": 0.8986,
      "step": 1660
    },
    {
      "epoch": 0.10542929292929293,
      "grad_norm": 0.4167799651622772,
      "learning_rate": 0.00019973070594526973,
      "loss": 0.7509,
      "step": 1670
    },
    {
      "epoch": 0.10606060606060606,
      "grad_norm": 0.45270684361457825,
      "learning_rate": 0.00019972315596828458,
      "loss": 0.6488,
      "step": 1680
    },
    {
      "epoch": 0.10669191919191919,
      "grad_norm": 0.4703749120235443,
      "learning_rate": 0.0001997155017598284,
      "loss": 0.6097,
      "step": 1690
    },
    {
      "epoch": 0.10732323232323232,
      "grad_norm": 0.6967815160751343,
      "learning_rate": 0.0001997077433279015,
      "loss": 0.618,
      "step": 1700
    },
    {
      "epoch": 0.10795454545454546,
      "grad_norm": 0.3769453763961792,
      "learning_rate": 0.00019969988068061295,
      "loss": 0.8388,
      "step": 1710
    },
    {
      "epoch": 0.10858585858585859,
      "grad_norm": 0.40468305349349976,
      "learning_rate": 0.00019969191382618095,
      "loss": 0.7456,
      "step": 1720
    },
    {
      "epoch": 0.10921717171717171,
      "grad_norm": 0.39710918068885803,
      "learning_rate": 0.00019968384277293247,
      "loss": 0.6929,
      "step": 1730
    },
    {
      "epoch": 0.10984848484848485,
      "grad_norm": 0.4509437084197998,
      "learning_rate": 0.00019967566752930347,
      "loss": 0.6275,
      "step": 1740
    },
    {
      "epoch": 0.11047979797979798,
      "grad_norm": 0.7590951323509216,
      "learning_rate": 0.00019966738810383875,
      "loss": 0.6019,
      "step": 1750
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.35539889335632324,
      "learning_rate": 0.00019965900450519207,
      "loss": 0.8978,
      "step": 1760
    },
    {
      "epoch": 0.11174242424242424,
      "grad_norm": 0.4299410283565521,
      "learning_rate": 0.000199650516742126,
      "loss": 0.753,
      "step": 1770
    },
    {
      "epoch": 0.11237373737373738,
      "grad_norm": 0.42306286096572876,
      "learning_rate": 0.00019964192482351204,
      "loss": 0.6712,
      "step": 1780
    },
    {
      "epoch": 0.11300505050505051,
      "grad_norm": 0.5238969922065735,
      "learning_rate": 0.00019963322875833056,
      "loss": 0.6219,
      "step": 1790
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 0.7755720019340515,
      "learning_rate": 0.0001996244285556707,
      "loss": 0.606,
      "step": 1800
    },
    {
      "epoch": 0.11426767676767677,
      "grad_norm": 0.3691238760948181,
      "learning_rate": 0.00019961552422473057,
      "loss": 0.8836,
      "step": 1810
    },
    {
      "epoch": 0.1148989898989899,
      "grad_norm": 0.44038835167884827,
      "learning_rate": 0.000199606515774817,
      "loss": 0.7555,
      "step": 1820
    },
    {
      "epoch": 0.11553030303030302,
      "grad_norm": 0.4313177466392517,
      "learning_rate": 0.00019959740321534572,
      "loss": 0.6767,
      "step": 1830
    },
    {
      "epoch": 0.11616161616161616,
      "grad_norm": 0.4617747962474823,
      "learning_rate": 0.00019958818655584125,
      "loss": 0.5749,
      "step": 1840
    },
    {
      "epoch": 0.1167929292929293,
      "grad_norm": 0.8045175075531006,
      "learning_rate": 0.0001995788658059369,
      "loss": 0.6362,
      "step": 1850
    },
    {
      "epoch": 0.11742424242424243,
      "grad_norm": 0.36529091000556946,
      "learning_rate": 0.00019956944097537484,
      "loss": 0.9324,
      "step": 1860
    },
    {
      "epoch": 0.11805555555555555,
      "grad_norm": 0.38397401571273804,
      "learning_rate": 0.00019955991207400595,
      "loss": 0.739,
      "step": 1870
    },
    {
      "epoch": 0.11868686868686869,
      "grad_norm": 0.42217332124710083,
      "learning_rate": 0.0001995502791117899,
      "loss": 0.6625,
      "step": 1880
    },
    {
      "epoch": 0.11931818181818182,
      "grad_norm": 0.4794639050960541,
      "learning_rate": 0.0001995405420987952,
      "loss": 0.5898,
      "step": 1890
    },
    {
      "epoch": 0.11994949494949494,
      "grad_norm": 0.708035409450531,
      "learning_rate": 0.000199530701045199,
      "loss": 0.6336,
      "step": 1900
    },
    {
      "epoch": 0.12058080808080808,
      "grad_norm": 0.3729591965675354,
      "learning_rate": 0.00019952075596128726,
      "loss": 0.86,
      "step": 1910
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 0.4287702739238739,
      "learning_rate": 0.0001995107068574547,
      "loss": 0.7415,
      "step": 1920
    },
    {
      "epoch": 0.12184343434343434,
      "grad_norm": 0.42662420868873596,
      "learning_rate": 0.00019950055374420468,
      "loss": 0.6315,
      "step": 1930
    },
    {
      "epoch": 0.12247474747474747,
      "grad_norm": 0.46767657995224,
      "learning_rate": 0.00019949029663214937,
      "loss": 0.5772,
      "step": 1940
    },
    {
      "epoch": 0.12310606060606061,
      "grad_norm": 0.721224844455719,
      "learning_rate": 0.00019947993553200955,
      "loss": 0.6199,
      "step": 1950
    },
    {
      "epoch": 0.12373737373737374,
      "grad_norm": 0.39447125792503357,
      "learning_rate": 0.00019946947045461472,
      "loss": 0.9029,
      "step": 1960
    },
    {
      "epoch": 0.12436868686868686,
      "grad_norm": 0.4112243950366974,
      "learning_rate": 0.00019945890141090307,
      "loss": 0.7406,
      "step": 1970
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.4835154712200165,
      "learning_rate": 0.0001994482284119215,
      "loss": 0.6735,
      "step": 1980
    },
    {
      "epoch": 0.12563131313131312,
      "grad_norm": 0.47351589798927307,
      "learning_rate": 0.0001994374514688255,
      "loss": 0.6295,
      "step": 1990
    },
    {
      "epoch": 0.12626262626262627,
      "grad_norm": 0.6989267468452454,
      "learning_rate": 0.00019942657059287918,
      "loss": 0.6006,
      "step": 2000
    },
    {
      "epoch": 0.12626262626262627,
      "eval_loss": 0.7102093696594238,
      "eval_runtime": 43.201,
      "eval_samples_per_second": 59.258,
      "eval_steps_per_second": 7.407,
      "step": 2000
    },
    {
      "epoch": 0.1268939393939394,
      "grad_norm": 0.38398998975753784,
      "learning_rate": 0.00019941558579545534,
      "loss": 0.8531,
      "step": 2010
    },
    {
      "epoch": 0.1275252525252525,
      "grad_norm": 0.4126354455947876,
      "learning_rate": 0.00019940449708803537,
      "loss": 0.7542,
      "step": 2020
    },
    {
      "epoch": 0.12815656565656566,
      "grad_norm": 0.43430760502815247,
      "learning_rate": 0.00019939330448220932,
      "loss": 0.6668,
      "step": 2030
    },
    {
      "epoch": 0.12878787878787878,
      "grad_norm": 0.4472557306289673,
      "learning_rate": 0.00019938200798967577,
      "loss": 0.6023,
      "step": 2040
    },
    {
      "epoch": 0.1294191919191919,
      "grad_norm": 0.7205840945243835,
      "learning_rate": 0.00019937060762224192,
      "loss": 0.6488,
      "step": 2050
    },
    {
      "epoch": 0.13005050505050506,
      "grad_norm": 0.3752710521221161,
      "learning_rate": 0.00019935910339182348,
      "loss": 0.9529,
      "step": 2060
    },
    {
      "epoch": 0.13068181818181818,
      "grad_norm": 0.40621161460876465,
      "learning_rate": 0.00019934749531044484,
      "loss": 0.7398,
      "step": 2070
    },
    {
      "epoch": 0.13131313131313133,
      "grad_norm": 0.44346189498901367,
      "learning_rate": 0.0001993357833902388,
      "loss": 0.644,
      "step": 2080
    },
    {
      "epoch": 0.13194444444444445,
      "grad_norm": 0.4760226905345917,
      "learning_rate": 0.0001993239676434468,
      "loss": 0.5829,
      "step": 2090
    },
    {
      "epoch": 0.13257575757575757,
      "grad_norm": 0.7512854933738708,
      "learning_rate": 0.00019931204808241873,
      "loss": 0.6023,
      "step": 2100
    },
    {
      "epoch": 0.13320707070707072,
      "grad_norm": 0.36616045236587524,
      "learning_rate": 0.00019930002471961301,
      "loss": 0.8909,
      "step": 2110
    },
    {
      "epoch": 0.13383838383838384,
      "grad_norm": 0.39439329504966736,
      "learning_rate": 0.00019928789756759661,
      "loss": 0.6958,
      "step": 2120
    },
    {
      "epoch": 0.13446969696969696,
      "grad_norm": 0.4313907325267792,
      "learning_rate": 0.00019927566663904486,
      "loss": 0.6619,
      "step": 2130
    },
    {
      "epoch": 0.1351010101010101,
      "grad_norm": 0.4927107095718384,
      "learning_rate": 0.0001992633319467417,
      "loss": 0.6133,
      "step": 2140
    },
    {
      "epoch": 0.13573232323232323,
      "grad_norm": 0.7209543585777283,
      "learning_rate": 0.00019925089350357938,
      "loss": 0.6427,
      "step": 2150
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 0.40085354447364807,
      "learning_rate": 0.00019923835132255874,
      "loss": 0.9204,
      "step": 2160
    },
    {
      "epoch": 0.1369949494949495,
      "grad_norm": 0.41591277718544006,
      "learning_rate": 0.00019922570541678887,
      "loss": 0.7569,
      "step": 2170
    },
    {
      "epoch": 0.13762626262626262,
      "grad_norm": 0.4353790581226349,
      "learning_rate": 0.00019921295579948748,
      "loss": 0.6439,
      "step": 2180
    },
    {
      "epoch": 0.13825757575757575,
      "grad_norm": 0.4944901168346405,
      "learning_rate": 0.00019920010248398052,
      "loss": 0.5715,
      "step": 2190
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 0.7295461297035217,
      "learning_rate": 0.0001991871454837024,
      "loss": 0.6354,
      "step": 2200
    },
    {
      "epoch": 0.13952020202020202,
      "grad_norm": 0.3912712335586548,
      "learning_rate": 0.00019917408481219585,
      "loss": 0.8821,
      "step": 2210
    },
    {
      "epoch": 0.14015151515151514,
      "grad_norm": 0.44148242473602295,
      "learning_rate": 0.00019916092048311205,
      "loss": 0.7067,
      "step": 2220
    },
    {
      "epoch": 0.1407828282828283,
      "grad_norm": 0.4530971944332123,
      "learning_rate": 0.0001991476525102104,
      "loss": 0.6644,
      "step": 2230
    },
    {
      "epoch": 0.1414141414141414,
      "grad_norm": 0.4648737907409668,
      "learning_rate": 0.00019913428090735877,
      "loss": 0.5908,
      "step": 2240
    },
    {
      "epoch": 0.14204545454545456,
      "grad_norm": 0.7213729023933411,
      "learning_rate": 0.00019912080568853323,
      "loss": 0.6397,
      "step": 2250
    },
    {
      "epoch": 0.14267676767676768,
      "grad_norm": 0.4028184413909912,
      "learning_rate": 0.0001991072268678182,
      "loss": 0.8703,
      "step": 2260
    },
    {
      "epoch": 0.1433080808080808,
      "grad_norm": 0.4417440891265869,
      "learning_rate": 0.00019909354445940634,
      "loss": 0.7617,
      "step": 2270
    },
    {
      "epoch": 0.14393939393939395,
      "grad_norm": 0.39569923281669617,
      "learning_rate": 0.00019907975847759866,
      "loss": 0.672,
      "step": 2280
    },
    {
      "epoch": 0.14457070707070707,
      "grad_norm": 0.48962852358818054,
      "learning_rate": 0.00019906586893680438,
      "loss": 0.5786,
      "step": 2290
    },
    {
      "epoch": 0.1452020202020202,
      "grad_norm": 0.7266977429389954,
      "learning_rate": 0.00019905187585154095,
      "loss": 0.6296,
      "step": 2300
    },
    {
      "epoch": 0.14583333333333334,
      "grad_norm": 0.375555157661438,
      "learning_rate": 0.00019903777923643406,
      "loss": 0.8916,
      "step": 2310
    },
    {
      "epoch": 0.14646464646464646,
      "grad_norm": 0.41925907135009766,
      "learning_rate": 0.00019902357910621762,
      "loss": 0.7421,
      "step": 2320
    },
    {
      "epoch": 0.14709595959595959,
      "grad_norm": 0.4378170371055603,
      "learning_rate": 0.00019900927547573366,
      "loss": 0.6463,
      "step": 2330
    },
    {
      "epoch": 0.14772727272727273,
      "grad_norm": 0.5034804344177246,
      "learning_rate": 0.00019899486835993257,
      "loss": 0.612,
      "step": 2340
    },
    {
      "epoch": 0.14835858585858586,
      "grad_norm": 0.7034321427345276,
      "learning_rate": 0.0001989803577738727,
      "loss": 0.6385,
      "step": 2350
    },
    {
      "epoch": 0.14898989898989898,
      "grad_norm": 0.367694616317749,
      "learning_rate": 0.00019896574373272065,
      "loss": 0.9192,
      "step": 2360
    },
    {
      "epoch": 0.14962121212121213,
      "grad_norm": 0.42062458395957947,
      "learning_rate": 0.00019895102625175118,
      "loss": 0.7543,
      "step": 2370
    },
    {
      "epoch": 0.15025252525252525,
      "grad_norm": 0.42876091599464417,
      "learning_rate": 0.00019893620534634706,
      "loss": 0.6554,
      "step": 2380
    },
    {
      "epoch": 0.15088383838383837,
      "grad_norm": 0.5555932521820068,
      "learning_rate": 0.00019892128103199928,
      "loss": 0.6247,
      "step": 2390
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 0.6495023369789124,
      "learning_rate": 0.0001989062533243068,
      "loss": 0.6187,
      "step": 2400
    },
    {
      "epoch": 0.15214646464646464,
      "grad_norm": 0.38661274313926697,
      "learning_rate": 0.00019889112223897676,
      "loss": 0.9365,
      "step": 2410
    },
    {
      "epoch": 0.1527777777777778,
      "grad_norm": 0.42024537920951843,
      "learning_rate": 0.0001988758877918243,
      "loss": 0.7207,
      "step": 2420
    },
    {
      "epoch": 0.1534090909090909,
      "grad_norm": 0.4088040590286255,
      "learning_rate": 0.0001988605499987725,
      "loss": 0.6192,
      "step": 2430
    },
    {
      "epoch": 0.15404040404040403,
      "grad_norm": 0.5075106620788574,
      "learning_rate": 0.00019884510887585263,
      "loss": 0.6143,
      "step": 2440
    },
    {
      "epoch": 0.15467171717171718,
      "grad_norm": 0.7757822871208191,
      "learning_rate": 0.00019882956443920388,
      "loss": 0.6314,
      "step": 2450
    },
    {
      "epoch": 0.1553030303030303,
      "grad_norm": 0.38490667939186096,
      "learning_rate": 0.00019881391670507342,
      "loss": 0.8601,
      "step": 2460
    },
    {
      "epoch": 0.15593434343434343,
      "grad_norm": 0.4422278106212616,
      "learning_rate": 0.00019879816568981636,
      "loss": 0.7506,
      "step": 2470
    },
    {
      "epoch": 0.15656565656565657,
      "grad_norm": 0.4420340955257416,
      "learning_rate": 0.0001987823114098958,
      "loss": 0.6533,
      "step": 2480
    },
    {
      "epoch": 0.1571969696969697,
      "grad_norm": 0.45500168204307556,
      "learning_rate": 0.0001987663538818828,
      "loss": 0.5736,
      "step": 2490
    },
    {
      "epoch": 0.15782828282828282,
      "grad_norm": 0.7859130501747131,
      "learning_rate": 0.00019875029312245625,
      "loss": 0.6071,
      "step": 2500
    },
    {
      "epoch": 0.15845959595959597,
      "grad_norm": 0.4214460551738739,
      "learning_rate": 0.00019873412914840304,
      "loss": 0.8472,
      "step": 2510
    },
    {
      "epoch": 0.1590909090909091,
      "grad_norm": 0.3863556385040283,
      "learning_rate": 0.00019871786197661785,
      "loss": 0.7032,
      "step": 2520
    },
    {
      "epoch": 0.1597222222222222,
      "grad_norm": 0.43008968234062195,
      "learning_rate": 0.00019870149162410326,
      "loss": 0.6535,
      "step": 2530
    },
    {
      "epoch": 0.16035353535353536,
      "grad_norm": 0.5112194418907166,
      "learning_rate": 0.00019868501810796975,
      "loss": 0.6057,
      "step": 2540
    },
    {
      "epoch": 0.16098484848484848,
      "grad_norm": 0.8072705864906311,
      "learning_rate": 0.00019866844144543553,
      "loss": 0.6353,
      "step": 2550
    },
    {
      "epoch": 0.16161616161616163,
      "grad_norm": 0.37552520632743835,
      "learning_rate": 0.0001986517616538267,
      "loss": 0.8857,
      "step": 2560
    },
    {
      "epoch": 0.16224747474747475,
      "grad_norm": 0.415007621049881,
      "learning_rate": 0.00019863497875057705,
      "loss": 0.7328,
      "step": 2570
    },
    {
      "epoch": 0.16287878787878787,
      "grad_norm": 0.45991581678390503,
      "learning_rate": 0.00019861809275322826,
      "loss": 0.6375,
      "step": 2580
    },
    {
      "epoch": 0.16351010101010102,
      "grad_norm": 0.5026717782020569,
      "learning_rate": 0.00019860110367942975,
      "loss": 0.6117,
      "step": 2590
    },
    {
      "epoch": 0.16414141414141414,
      "grad_norm": 0.8464011549949646,
      "learning_rate": 0.00019858401154693858,
      "loss": 0.6099,
      "step": 2600
    },
    {
      "epoch": 0.16477272727272727,
      "grad_norm": 0.3989109992980957,
      "learning_rate": 0.0001985668163736196,
      "loss": 0.957,
      "step": 2610
    },
    {
      "epoch": 0.16540404040404041,
      "grad_norm": 0.4367481768131256,
      "learning_rate": 0.00019854951817744536,
      "loss": 0.7394,
      "step": 2620
    },
    {
      "epoch": 0.16603535353535354,
      "grad_norm": 0.43486663699150085,
      "learning_rate": 0.00019853211697649608,
      "loss": 0.6636,
      "step": 2630
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.47072291374206543,
      "learning_rate": 0.0001985146127889596,
      "loss": 0.6049,
      "step": 2640
    },
    {
      "epoch": 0.1672979797979798,
      "grad_norm": 0.7002814412117004,
      "learning_rate": 0.00019849700563313153,
      "loss": 0.6312,
      "step": 2650
    },
    {
      "epoch": 0.16792929292929293,
      "grad_norm": 0.3991352915763855,
      "learning_rate": 0.00019847929552741495,
      "loss": 0.8802,
      "step": 2660
    },
    {
      "epoch": 0.16856060606060605,
      "grad_norm": 0.3926878571510315,
      "learning_rate": 0.00019846148249032063,
      "loss": 0.7101,
      "step": 2670
    },
    {
      "epoch": 0.1691919191919192,
      "grad_norm": 0.43340063095092773,
      "learning_rate": 0.00019844356654046688,
      "loss": 0.6762,
      "step": 2680
    },
    {
      "epoch": 0.16982323232323232,
      "grad_norm": 0.5315001606941223,
      "learning_rate": 0.00019842554769657962,
      "loss": 0.568,
      "step": 2690
    },
    {
      "epoch": 0.17045454545454544,
      "grad_norm": 0.6893368363380432,
      "learning_rate": 0.0001984074259774923,
      "loss": 0.6257,
      "step": 2700
    },
    {
      "epoch": 0.1710858585858586,
      "grad_norm": 0.37899598479270935,
      "learning_rate": 0.00019838920140214587,
      "loss": 0.8601,
      "step": 2710
    },
    {
      "epoch": 0.1717171717171717,
      "grad_norm": 0.44036245346069336,
      "learning_rate": 0.00019837087398958883,
      "loss": 0.774,
      "step": 2720
    },
    {
      "epoch": 0.17234848484848486,
      "grad_norm": 0.4360364079475403,
      "learning_rate": 0.00019835244375897713,
      "loss": 0.6695,
      "step": 2730
    },
    {
      "epoch": 0.17297979797979798,
      "grad_norm": 0.48525476455688477,
      "learning_rate": 0.00019833391072957422,
      "loss": 0.5788,
      "step": 2740
    },
    {
      "epoch": 0.1736111111111111,
      "grad_norm": 0.7466248869895935,
      "learning_rate": 0.00019831527492075092,
      "loss": 0.6091,
      "step": 2750
    },
    {
      "epoch": 0.17424242424242425,
      "grad_norm": 0.37810850143432617,
      "learning_rate": 0.00019829653635198563,
      "loss": 0.8644,
      "step": 2760
    },
    {
      "epoch": 0.17487373737373738,
      "grad_norm": 0.45031797885894775,
      "learning_rate": 0.00019827769504286396,
      "loss": 0.7271,
      "step": 2770
    },
    {
      "epoch": 0.1755050505050505,
      "grad_norm": 0.4908962547779083,
      "learning_rate": 0.00019825875101307905,
      "loss": 0.6512,
      "step": 2780
    },
    {
      "epoch": 0.17613636363636365,
      "grad_norm": 0.4761175513267517,
      "learning_rate": 0.00019823970428243135,
      "loss": 0.6089,
      "step": 2790
    },
    {
      "epoch": 0.17676767676767677,
      "grad_norm": 0.7022927403450012,
      "learning_rate": 0.00019822055487082866,
      "loss": 0.612,
      "step": 2800
    },
    {
      "epoch": 0.1773989898989899,
      "grad_norm": 0.39960235357284546,
      "learning_rate": 0.00019820130279828613,
      "loss": 0.8709,
      "step": 2810
    },
    {
      "epoch": 0.17803030303030304,
      "grad_norm": 0.42103078961372375,
      "learning_rate": 0.00019818194808492615,
      "loss": 0.7449,
      "step": 2820
    },
    {
      "epoch": 0.17866161616161616,
      "grad_norm": 0.46219736337661743,
      "learning_rate": 0.00019816249075097845,
      "loss": 0.6835,
      "step": 2830
    },
    {
      "epoch": 0.17929292929292928,
      "grad_norm": 0.45632466673851013,
      "learning_rate": 0.00019814293081677994,
      "loss": 0.5981,
      "step": 2840
    },
    {
      "epoch": 0.17992424242424243,
      "grad_norm": 0.6064882278442383,
      "learning_rate": 0.0001981232683027749,
      "loss": 0.5868,
      "step": 2850
    },
    {
      "epoch": 0.18055555555555555,
      "grad_norm": 0.37274813652038574,
      "learning_rate": 0.00019810350322951474,
      "loss": 0.8686,
      "step": 2860
    },
    {
      "epoch": 0.18118686868686867,
      "grad_norm": 0.41627806425094604,
      "learning_rate": 0.00019808363561765806,
      "loss": 0.7286,
      "step": 2870
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.43914487957954407,
      "learning_rate": 0.00019806366548797067,
      "loss": 0.6334,
      "step": 2880
    },
    {
      "epoch": 0.18244949494949494,
      "grad_norm": 0.4713365137577057,
      "learning_rate": 0.00019804359286132548,
      "loss": 0.5664,
      "step": 2890
    },
    {
      "epoch": 0.1830808080808081,
      "grad_norm": 0.7646815180778503,
      "learning_rate": 0.0001980234177587026,
      "loss": 0.6081,
      "step": 2900
    },
    {
      "epoch": 0.18371212121212122,
      "grad_norm": 0.4062012732028961,
      "learning_rate": 0.00019800314020118918,
      "loss": 0.8534,
      "step": 2910
    },
    {
      "epoch": 0.18434343434343434,
      "grad_norm": 0.4134324789047241,
      "learning_rate": 0.00019798276020997952,
      "loss": 0.7245,
      "step": 2920
    },
    {
      "epoch": 0.1849747474747475,
      "grad_norm": 0.41381025314331055,
      "learning_rate": 0.00019796227780637498,
      "loss": 0.6349,
      "step": 2930
    },
    {
      "epoch": 0.1856060606060606,
      "grad_norm": 0.4569241404533386,
      "learning_rate": 0.0001979416930117839,
      "loss": 0.5558,
      "step": 2940
    },
    {
      "epoch": 0.18623737373737373,
      "grad_norm": 0.7092502117156982,
      "learning_rate": 0.00019792100584772166,
      "loss": 0.6075,
      "step": 2950
    },
    {
      "epoch": 0.18686868686868688,
      "grad_norm": 0.41287681460380554,
      "learning_rate": 0.00019790021633581071,
      "loss": 0.8668,
      "step": 2960
    },
    {
      "epoch": 0.1875,
      "grad_norm": 0.37278586626052856,
      "learning_rate": 0.0001978793244977804,
      "loss": 0.7265,
      "step": 2970
    },
    {
      "epoch": 0.18813131313131312,
      "grad_norm": 0.4289104640483856,
      "learning_rate": 0.00019785833035546702,
      "loss": 0.6574,
      "step": 2980
    },
    {
      "epoch": 0.18876262626262627,
      "grad_norm": 0.49847957491874695,
      "learning_rate": 0.00019783723393081387,
      "loss": 0.5805,
      "step": 2990
    },
    {
      "epoch": 0.1893939393939394,
      "grad_norm": 0.9003369808197021,
      "learning_rate": 0.00019781603524587107,
      "loss": 0.6198,
      "step": 3000
    },
    {
      "epoch": 0.1893939393939394,
      "eval_loss": 0.6947613954544067,
      "eval_runtime": 44.2612,
      "eval_samples_per_second": 57.838,
      "eval_steps_per_second": 7.23,
      "step": 3000
    },
    {
      "epoch": 0.1900252525252525,
      "grad_norm": 0.374426007270813,
      "learning_rate": 0.00019779473432279568,
      "loss": 0.8617,
      "step": 3010
    },
    {
      "epoch": 0.19065656565656566,
      "grad_norm": 0.4181010127067566,
      "learning_rate": 0.00019777333118385163,
      "loss": 0.7127,
      "step": 3020
    },
    {
      "epoch": 0.19128787878787878,
      "grad_norm": 0.43361616134643555,
      "learning_rate": 0.00019775182585140958,
      "loss": 0.6917,
      "step": 3030
    },
    {
      "epoch": 0.1919191919191919,
      "grad_norm": 0.4582299590110779,
      "learning_rate": 0.0001977302183479472,
      "loss": 0.6191,
      "step": 3040
    },
    {
      "epoch": 0.19255050505050506,
      "grad_norm": 0.7791195511817932,
      "learning_rate": 0.00019770850869604872,
      "loss": 0.6333,
      "step": 3050
    },
    {
      "epoch": 0.19318181818181818,
      "grad_norm": 0.3939934968948364,
      "learning_rate": 0.0001976866969184053,
      "loss": 0.8596,
      "step": 3060
    },
    {
      "epoch": 0.19381313131313133,
      "grad_norm": 0.4123491644859314,
      "learning_rate": 0.00019766478303781475,
      "loss": 0.7332,
      "step": 3070
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 0.4319055676460266,
      "learning_rate": 0.0001976427670771817,
      "loss": 0.6557,
      "step": 3080
    },
    {
      "epoch": 0.19507575757575757,
      "grad_norm": 0.4677248001098633,
      "learning_rate": 0.0001976206490595174,
      "loss": 0.6019,
      "step": 3090
    },
    {
      "epoch": 0.19570707070707072,
      "grad_norm": 0.7245080471038818,
      "learning_rate": 0.00019759842900793974,
      "loss": 0.6287,
      "step": 3100
    },
    {
      "epoch": 0.19633838383838384,
      "grad_norm": 0.37030619382858276,
      "learning_rate": 0.00019757610694567338,
      "loss": 0.8609,
      "step": 3110
    },
    {
      "epoch": 0.19696969696969696,
      "grad_norm": 0.4220130443572998,
      "learning_rate": 0.00019755368289604944,
      "loss": 0.7255,
      "step": 3120
    },
    {
      "epoch": 0.1976010101010101,
      "grad_norm": 0.4558398425579071,
      "learning_rate": 0.0001975311568825058,
      "loss": 0.6701,
      "step": 3130
    },
    {
      "epoch": 0.19823232323232323,
      "grad_norm": 0.49365347623825073,
      "learning_rate": 0.00019750852892858677,
      "loss": 0.566,
      "step": 3140
    },
    {
      "epoch": 0.19886363636363635,
      "grad_norm": 0.7322593331336975,
      "learning_rate": 0.00019748579905794333,
      "loss": 0.6165,
      "step": 3150
    },
    {
      "epoch": 0.1994949494949495,
      "grad_norm": 0.4016718864440918,
      "learning_rate": 0.0001974629672943329,
      "loss": 0.8841,
      "step": 3160
    },
    {
      "epoch": 0.20012626262626262,
      "grad_norm": 0.41031792759895325,
      "learning_rate": 0.00019744003366161942,
      "loss": 0.6924,
      "step": 3170
    },
    {
      "epoch": 0.20075757575757575,
      "grad_norm": 0.45528778433799744,
      "learning_rate": 0.00019741699818377333,
      "loss": 0.6368,
      "step": 3180
    },
    {
      "epoch": 0.2013888888888889,
      "grad_norm": 0.4902435839176178,
      "learning_rate": 0.0001973938608848715,
      "loss": 0.5693,
      "step": 3190
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 0.6676995754241943,
      "learning_rate": 0.00019737062178909725,
      "loss": 0.6013,
      "step": 3200
    },
    {
      "epoch": 0.20265151515151514,
      "grad_norm": 0.35717493295669556,
      "learning_rate": 0.00019734728092074024,
      "loss": 0.8257,
      "step": 3210
    },
    {
      "epoch": 0.2032828282828283,
      "grad_norm": 0.407542884349823,
      "learning_rate": 0.00019732383830419658,
      "loss": 0.7129,
      "step": 3220
    },
    {
      "epoch": 0.2039141414141414,
      "grad_norm": 0.43723610043525696,
      "learning_rate": 0.00019730029396396862,
      "loss": 0.667,
      "step": 3230
    },
    {
      "epoch": 0.20454545454545456,
      "grad_norm": 0.41983848810195923,
      "learning_rate": 0.00019727664792466515,
      "loss": 0.5601,
      "step": 3240
    },
    {
      "epoch": 0.20517676767676768,
      "grad_norm": 0.8276674151420593,
      "learning_rate": 0.0001972529002110012,
      "loss": 0.625,
      "step": 3250
    },
    {
      "epoch": 0.2058080808080808,
      "grad_norm": 0.36047762632369995,
      "learning_rate": 0.00019722905084779808,
      "loss": 0.8328,
      "step": 3260
    },
    {
      "epoch": 0.20643939393939395,
      "grad_norm": 0.44098591804504395,
      "learning_rate": 0.0001972050998599833,
      "loss": 0.7368,
      "step": 3270
    },
    {
      "epoch": 0.20707070707070707,
      "grad_norm": 0.45419272780418396,
      "learning_rate": 0.00019718104727259073,
      "loss": 0.6579,
      "step": 3280
    },
    {
      "epoch": 0.2077020202020202,
      "grad_norm": 0.44818294048309326,
      "learning_rate": 0.00019715689311076024,
      "loss": 0.5467,
      "step": 3290
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 1.0136107206344604,
      "learning_rate": 0.000197132637399738,
      "loss": 0.6334,
      "step": 3300
    },
    {
      "epoch": 0.20896464646464646,
      "grad_norm": 0.43933311104774475,
      "learning_rate": 0.00019710828016487628,
      "loss": 0.8359,
      "step": 3310
    },
    {
      "epoch": 0.20959595959595959,
      "grad_norm": 0.4378514587879181,
      "learning_rate": 0.00019708382143163343,
      "loss": 0.7327,
      "step": 3320
    },
    {
      "epoch": 0.21022727272727273,
      "grad_norm": 0.4329286813735962,
      "learning_rate": 0.000197059261225574,
      "loss": 0.6894,
      "step": 3330
    },
    {
      "epoch": 0.21085858585858586,
      "grad_norm": 0.4728871285915375,
      "learning_rate": 0.00019703459957236844,
      "loss": 0.5686,
      "step": 3340
    },
    {
      "epoch": 0.21148989898989898,
      "grad_norm": 0.7271888256072998,
      "learning_rate": 0.00019700983649779334,
      "loss": 0.625,
      "step": 3350
    },
    {
      "epoch": 0.21212121212121213,
      "grad_norm": 0.3953024744987488,
      "learning_rate": 0.0001969849720277313,
      "loss": 0.8956,
      "step": 3360
    },
    {
      "epoch": 0.21275252525252525,
      "grad_norm": 0.38531607389450073,
      "learning_rate": 0.0001969600061881708,
      "loss": 0.7324,
      "step": 3370
    },
    {
      "epoch": 0.21338383838383837,
      "grad_norm": 0.44226130843162537,
      "learning_rate": 0.00019693493900520644,
      "loss": 0.6489,
      "step": 3380
    },
    {
      "epoch": 0.21401515151515152,
      "grad_norm": 0.4701426923274994,
      "learning_rate": 0.0001969097705050386,
      "loss": 0.6295,
      "step": 3390
    },
    {
      "epoch": 0.21464646464646464,
      "grad_norm": 0.7624122500419617,
      "learning_rate": 0.00019688450071397357,
      "loss": 0.5849,
      "step": 3400
    },
    {
      "epoch": 0.2152777777777778,
      "grad_norm": 0.3788447380065918,
      "learning_rate": 0.00019685912965842359,
      "loss": 0.8432,
      "step": 3410
    },
    {
      "epoch": 0.2159090909090909,
      "grad_norm": 0.39805832505226135,
      "learning_rate": 0.00019683365736490672,
      "loss": 0.7475,
      "step": 3420
    },
    {
      "epoch": 0.21654040404040403,
      "grad_norm": 0.4438975155353546,
      "learning_rate": 0.00019680808386004673,
      "loss": 0.6443,
      "step": 3430
    },
    {
      "epoch": 0.21717171717171718,
      "grad_norm": 0.4832707345485687,
      "learning_rate": 0.00019678240917057335,
      "loss": 0.5986,
      "step": 3440
    },
    {
      "epoch": 0.2178030303030303,
      "grad_norm": 0.6862643957138062,
      "learning_rate": 0.0001967566333233219,
      "loss": 0.5951,
      "step": 3450
    },
    {
      "epoch": 0.21843434343434343,
      "grad_norm": 0.3841138482093811,
      "learning_rate": 0.0001967307563452336,
      "loss": 0.8668,
      "step": 3460
    },
    {
      "epoch": 0.21906565656565657,
      "grad_norm": 0.3831775486469269,
      "learning_rate": 0.00019670477826335517,
      "loss": 0.7394,
      "step": 3470
    },
    {
      "epoch": 0.2196969696969697,
      "grad_norm": 0.4340337812900543,
      "learning_rate": 0.00019667869910483922,
      "loss": 0.6224,
      "step": 3480
    },
    {
      "epoch": 0.22032828282828282,
      "grad_norm": 0.4937151074409485,
      "learning_rate": 0.00019665251889694387,
      "loss": 0.5641,
      "step": 3490
    },
    {
      "epoch": 0.22095959595959597,
      "grad_norm": 0.7290979027748108,
      "learning_rate": 0.00019662623766703285,
      "loss": 0.5982,
      "step": 3500
    },
    {
      "epoch": 0.2215909090909091,
      "grad_norm": 0.38911178708076477,
      "learning_rate": 0.00019659985544257557,
      "loss": 0.8476,
      "step": 3510
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.444370299577713,
      "learning_rate": 0.00019657337225114692,
      "loss": 0.7419,
      "step": 3520
    },
    {
      "epoch": 0.22285353535353536,
      "grad_norm": 0.45044952630996704,
      "learning_rate": 0.0001965467881204274,
      "loss": 0.6082,
      "step": 3530
    },
    {
      "epoch": 0.22348484848484848,
      "grad_norm": 0.4534218907356262,
      "learning_rate": 0.00019652010307820287,
      "loss": 0.6181,
      "step": 3540
    },
    {
      "epoch": 0.22411616161616163,
      "grad_norm": 0.7369053959846497,
      "learning_rate": 0.00019649331715236484,
      "loss": 0.635,
      "step": 3550
    },
    {
      "epoch": 0.22474747474747475,
      "grad_norm": 0.3644542098045349,
      "learning_rate": 0.00019646643037091017,
      "loss": 0.9129,
      "step": 3560
    },
    {
      "epoch": 0.22537878787878787,
      "grad_norm": 0.43761345744132996,
      "learning_rate": 0.00019643944276194112,
      "loss": 0.7615,
      "step": 3570
    },
    {
      "epoch": 0.22601010101010102,
      "grad_norm": 0.435609370470047,
      "learning_rate": 0.0001964123543536654,
      "loss": 0.6637,
      "step": 3580
    },
    {
      "epoch": 0.22664141414141414,
      "grad_norm": 0.5238919258117676,
      "learning_rate": 0.00019638516517439598,
      "loss": 0.5851,
      "step": 3590
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 0.6796820759773254,
      "learning_rate": 0.0001963578752525513,
      "loss": 0.5819,
      "step": 3600
    },
    {
      "epoch": 0.22790404040404041,
      "grad_norm": 0.37846624851226807,
      "learning_rate": 0.00019633048461665492,
      "loss": 0.8626,
      "step": 3610
    },
    {
      "epoch": 0.22853535353535354,
      "grad_norm": 0.4059438705444336,
      "learning_rate": 0.00019630299329533583,
      "loss": 0.737,
      "step": 3620
    },
    {
      "epoch": 0.22916666666666666,
      "grad_norm": 0.41756874322891235,
      "learning_rate": 0.00019627540131732815,
      "loss": 0.6273,
      "step": 3630
    },
    {
      "epoch": 0.2297979797979798,
      "grad_norm": 0.46006980538368225,
      "learning_rate": 0.0001962477087114713,
      "loss": 0.6025,
      "step": 3640
    },
    {
      "epoch": 0.23042929292929293,
      "grad_norm": 0.7921679615974426,
      "learning_rate": 0.00019621991550670975,
      "loss": 0.6214,
      "step": 3650
    },
    {
      "epoch": 0.23106060606060605,
      "grad_norm": 0.36813727021217346,
      "learning_rate": 0.0001961920217320932,
      "loss": 0.8452,
      "step": 3660
    },
    {
      "epoch": 0.2316919191919192,
      "grad_norm": 0.4480901062488556,
      "learning_rate": 0.0001961640274167765,
      "loss": 0.7201,
      "step": 3670
    },
    {
      "epoch": 0.23232323232323232,
      "grad_norm": 0.40791231393814087,
      "learning_rate": 0.0001961359325900195,
      "loss": 0.6318,
      "step": 3680
    },
    {
      "epoch": 0.23295454545454544,
      "grad_norm": 0.5197663307189941,
      "learning_rate": 0.0001961077372811872,
      "loss": 0.5636,
      "step": 3690
    },
    {
      "epoch": 0.2335858585858586,
      "grad_norm": 0.7341287136077881,
      "learning_rate": 0.0001960794415197495,
      "loss": 0.6204,
      "step": 3700
    },
    {
      "epoch": 0.2342171717171717,
      "grad_norm": 0.3518388271331787,
      "learning_rate": 0.00019605104533528138,
      "loss": 0.8971,
      "step": 3710
    },
    {
      "epoch": 0.23484848484848486,
      "grad_norm": 0.4098936915397644,
      "learning_rate": 0.00019602254875746283,
      "loss": 0.7214,
      "step": 3720
    },
    {
      "epoch": 0.23547979797979798,
      "grad_norm": 0.4942808151245117,
      "learning_rate": 0.00019599395181607864,
      "loss": 0.6626,
      "step": 3730
    },
    {
      "epoch": 0.2361111111111111,
      "grad_norm": 0.45791202783584595,
      "learning_rate": 0.00019596525454101856,
      "loss": 0.5712,
      "step": 3740
    },
    {
      "epoch": 0.23674242424242425,
      "grad_norm": 0.7537583112716675,
      "learning_rate": 0.0001959364569622773,
      "loss": 0.6012,
      "step": 3750
    },
    {
      "epoch": 0.23737373737373738,
      "grad_norm": 0.3793725073337555,
      "learning_rate": 0.00019590755910995426,
      "loss": 0.8913,
      "step": 3760
    },
    {
      "epoch": 0.2380050505050505,
      "grad_norm": 0.4032926857471466,
      "learning_rate": 0.00019587856101425377,
      "loss": 0.747,
      "step": 3770
    },
    {
      "epoch": 0.23863636363636365,
      "grad_norm": 0.43147554993629456,
      "learning_rate": 0.00019584946270548482,
      "loss": 0.6419,
      "step": 3780
    },
    {
      "epoch": 0.23926767676767677,
      "grad_norm": 0.47169506549835205,
      "learning_rate": 0.00019582026421406125,
      "loss": 0.591,
      "step": 3790
    },
    {
      "epoch": 0.2398989898989899,
      "grad_norm": 0.6961174011230469,
      "learning_rate": 0.00019579096557050156,
      "loss": 0.5917,
      "step": 3800
    },
    {
      "epoch": 0.24053030303030304,
      "grad_norm": 0.3776510953903198,
      "learning_rate": 0.0001957615668054289,
      "loss": 0.8407,
      "step": 3810
    },
    {
      "epoch": 0.24116161616161616,
      "grad_norm": 0.3869006931781769,
      "learning_rate": 0.00019573206794957116,
      "loss": 0.737,
      "step": 3820
    },
    {
      "epoch": 0.24179292929292928,
      "grad_norm": 0.4652236998081207,
      "learning_rate": 0.00019570246903376071,
      "loss": 0.6206,
      "step": 3830
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 0.39895081520080566,
      "learning_rate": 0.00019567277008893466,
      "loss": 0.5666,
      "step": 3840
    },
    {
      "epoch": 0.24305555555555555,
      "grad_norm": 0.7104319930076599,
      "learning_rate": 0.0001956429711461346,
      "loss": 0.6183,
      "step": 3850
    },
    {
      "epoch": 0.24368686868686867,
      "grad_norm": 0.4207484722137451,
      "learning_rate": 0.00019561307223650654,
      "loss": 0.8952,
      "step": 3860
    },
    {
      "epoch": 0.24431818181818182,
      "grad_norm": 0.40016207098960876,
      "learning_rate": 0.00019558307339130116,
      "loss": 0.6901,
      "step": 3870
    },
    {
      "epoch": 0.24494949494949494,
      "grad_norm": 0.436536967754364,
      "learning_rate": 0.00019555297464187343,
      "loss": 0.6446,
      "step": 3880
    },
    {
      "epoch": 0.2455808080808081,
      "grad_norm": 0.4656459391117096,
      "learning_rate": 0.0001955227760196829,
      "loss": 0.6218,
      "step": 3890
    },
    {
      "epoch": 0.24621212121212122,
      "grad_norm": 0.7757580280303955,
      "learning_rate": 0.00019549247755629333,
      "loss": 0.6018,
      "step": 3900
    },
    {
      "epoch": 0.24684343434343434,
      "grad_norm": 0.39659473299980164,
      "learning_rate": 0.00019546207928337298,
      "loss": 0.8791,
      "step": 3910
    },
    {
      "epoch": 0.2474747474747475,
      "grad_norm": 0.4240072965621948,
      "learning_rate": 0.00019543158123269439,
      "loss": 0.7247,
      "step": 3920
    },
    {
      "epoch": 0.2481060606060606,
      "grad_norm": 0.43022316694259644,
      "learning_rate": 0.00019540098343613435,
      "loss": 0.6782,
      "step": 3930
    },
    {
      "epoch": 0.24873737373737373,
      "grad_norm": 0.5282467603683472,
      "learning_rate": 0.0001953702859256739,
      "loss": 0.5978,
      "step": 3940
    },
    {
      "epoch": 0.24936868686868688,
      "grad_norm": 0.7553126811981201,
      "learning_rate": 0.00019533948873339836,
      "loss": 0.5592,
      "step": 3950
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.3874015510082245,
      "learning_rate": 0.00019530859189149723,
      "loss": 0.8536,
      "step": 3960
    },
    {
      "epoch": 0.25063131313131315,
      "grad_norm": 0.3864027261734009,
      "learning_rate": 0.00019527759543226414,
      "loss": 0.737,
      "step": 3970
    },
    {
      "epoch": 0.25126262626262624,
      "grad_norm": 0.42396479845046997,
      "learning_rate": 0.00019524649938809681,
      "loss": 0.6537,
      "step": 3980
    },
    {
      "epoch": 0.2518939393939394,
      "grad_norm": 0.5246989130973816,
      "learning_rate": 0.00019521530379149716,
      "loss": 0.5857,
      "step": 3990
    },
    {
      "epoch": 0.25252525252525254,
      "grad_norm": 0.7713660597801208,
      "learning_rate": 0.00019518400867507102,
      "loss": 0.6169,
      "step": 4000
    },
    {
      "epoch": 0.25252525252525254,
      "eval_loss": 0.6807142496109009,
      "eval_runtime": 43.9466,
      "eval_samples_per_second": 58.253,
      "eval_steps_per_second": 7.282,
      "step": 4000
    },
    {
      "epoch": 0.25315656565656564,
      "grad_norm": 0.3930577039718628,
      "learning_rate": 0.0001951526140715283,
      "loss": 0.8627,
      "step": 4010
    },
    {
      "epoch": 0.2537878787878788,
      "grad_norm": 0.39662089943885803,
      "learning_rate": 0.00019512112001368297,
      "loss": 0.7159,
      "step": 4020
    },
    {
      "epoch": 0.25441919191919193,
      "grad_norm": 0.44987502694129944,
      "learning_rate": 0.0001950895265344528,
      "loss": 0.592,
      "step": 4030
    },
    {
      "epoch": 0.255050505050505,
      "grad_norm": 0.4558556377887726,
      "learning_rate": 0.00019505783366685958,
      "loss": 0.5848,
      "step": 4040
    },
    {
      "epoch": 0.2556818181818182,
      "grad_norm": 0.7433941960334778,
      "learning_rate": 0.00019502604144402903,
      "loss": 0.6,
      "step": 4050
    },
    {
      "epoch": 0.2563131313131313,
      "grad_norm": 0.3793952167034149,
      "learning_rate": 0.00019499414989919054,
      "loss": 0.9025,
      "step": 4060
    },
    {
      "epoch": 0.2569444444444444,
      "grad_norm": 0.43610185384750366,
      "learning_rate": 0.00019496215906567748,
      "loss": 0.759,
      "step": 4070
    },
    {
      "epoch": 0.25757575757575757,
      "grad_norm": 0.45441287755966187,
      "learning_rate": 0.0001949300689769269,
      "loss": 0.6508,
      "step": 4080
    },
    {
      "epoch": 0.2582070707070707,
      "grad_norm": 0.5239176750183105,
      "learning_rate": 0.0001948978796664797,
      "loss": 0.5707,
      "step": 4090
    },
    {
      "epoch": 0.2588383838383838,
      "grad_norm": 0.8080015778541565,
      "learning_rate": 0.00019486559116798028,
      "loss": 0.6184,
      "step": 4100
    },
    {
      "epoch": 0.25946969696969696,
      "grad_norm": 0.37541329860687256,
      "learning_rate": 0.00019483320351517698,
      "loss": 0.8895,
      "step": 4110
    },
    {
      "epoch": 0.2601010101010101,
      "grad_norm": 0.42152413725852966,
      "learning_rate": 0.00019480071674192158,
      "loss": 0.7088,
      "step": 4120
    },
    {
      "epoch": 0.26073232323232326,
      "grad_norm": 0.48051321506500244,
      "learning_rate": 0.00019476813088216955,
      "loss": 0.6572,
      "step": 4130
    },
    {
      "epoch": 0.26136363636363635,
      "grad_norm": 0.45465436577796936,
      "learning_rate": 0.00019473544596997986,
      "loss": 0.5591,
      "step": 4140
    },
    {
      "epoch": 0.2619949494949495,
      "grad_norm": 0.7121936082839966,
      "learning_rate": 0.0001947026620395151,
      "loss": 0.5859,
      "step": 4150
    },
    {
      "epoch": 0.26262626262626265,
      "grad_norm": 0.38401609659194946,
      "learning_rate": 0.00019466977912504127,
      "loss": 0.8883,
      "step": 4160
    },
    {
      "epoch": 0.26325757575757575,
      "grad_norm": 0.4020387828350067,
      "learning_rate": 0.00019463679726092791,
      "loss": 0.7277,
      "step": 4170
    },
    {
      "epoch": 0.2638888888888889,
      "grad_norm": 0.44602224230766296,
      "learning_rate": 0.0001946037164816479,
      "loss": 0.6763,
      "step": 4180
    },
    {
      "epoch": 0.26452020202020204,
      "grad_norm": 0.46728405356407166,
      "learning_rate": 0.00019457053682177754,
      "loss": 0.574,
      "step": 4190
    },
    {
      "epoch": 0.26515151515151514,
      "grad_norm": 0.7907575964927673,
      "learning_rate": 0.00019453725831599652,
      "loss": 0.6023,
      "step": 4200
    },
    {
      "epoch": 0.2657828282828283,
      "grad_norm": 0.40261733531951904,
      "learning_rate": 0.0001945038809990878,
      "loss": 0.9092,
      "step": 4210
    },
    {
      "epoch": 0.26641414141414144,
      "grad_norm": 0.4352867305278778,
      "learning_rate": 0.0001944704049059376,
      "loss": 0.706,
      "step": 4220
    },
    {
      "epoch": 0.26704545454545453,
      "grad_norm": 0.45136362314224243,
      "learning_rate": 0.00019443683007153544,
      "loss": 0.6361,
      "step": 4230
    },
    {
      "epoch": 0.2676767676767677,
      "grad_norm": 0.5844045877456665,
      "learning_rate": 0.00019440315653097398,
      "loss": 0.5707,
      "step": 4240
    },
    {
      "epoch": 0.26830808080808083,
      "grad_norm": 0.7480499744415283,
      "learning_rate": 0.00019436938431944916,
      "loss": 0.5979,
      "step": 4250
    },
    {
      "epoch": 0.2689393939393939,
      "grad_norm": 0.41275501251220703,
      "learning_rate": 0.0001943355134722599,
      "loss": 0.8755,
      "step": 4260
    },
    {
      "epoch": 0.26957070707070707,
      "grad_norm": 0.44459787011146545,
      "learning_rate": 0.00019430154402480832,
      "loss": 0.7438,
      "step": 4270
    },
    {
      "epoch": 0.2702020202020202,
      "grad_norm": 0.44179412722587585,
      "learning_rate": 0.0001942674760125996,
      "loss": 0.6349,
      "step": 4280
    },
    {
      "epoch": 0.2708333333333333,
      "grad_norm": 0.48680731654167175,
      "learning_rate": 0.00019423330947124183,
      "loss": 0.5725,
      "step": 4290
    },
    {
      "epoch": 0.27146464646464646,
      "grad_norm": 0.888656735420227,
      "learning_rate": 0.00019419904443644624,
      "loss": 0.6132,
      "step": 4300
    },
    {
      "epoch": 0.2720959595959596,
      "grad_norm": 0.35811442136764526,
      "learning_rate": 0.00019416468094402687,
      "loss": 0.8764,
      "step": 4310
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.43544653058052063,
      "learning_rate": 0.00019413021902990078,
      "loss": 0.7372,
      "step": 4320
    },
    {
      "epoch": 0.27335858585858586,
      "grad_norm": 0.44319087266921997,
      "learning_rate": 0.00019409565873008782,
      "loss": 0.6281,
      "step": 4330
    },
    {
      "epoch": 0.273989898989899,
      "grad_norm": 0.4961743652820587,
      "learning_rate": 0.0001940610000807107,
      "loss": 0.5726,
      "step": 4340
    },
    {
      "epoch": 0.2746212121212121,
      "grad_norm": 0.7466602921485901,
      "learning_rate": 0.00019402624311799495,
      "loss": 0.6092,
      "step": 4350
    },
    {
      "epoch": 0.27525252525252525,
      "grad_norm": 0.3954801857471466,
      "learning_rate": 0.00019399138787826883,
      "loss": 0.8216,
      "step": 4360
    },
    {
      "epoch": 0.2758838383838384,
      "grad_norm": 0.42018818855285645,
      "learning_rate": 0.0001939564343979633,
      "loss": 0.7174,
      "step": 4370
    },
    {
      "epoch": 0.2765151515151515,
      "grad_norm": 0.4260813295841217,
      "learning_rate": 0.00019392138271361205,
      "loss": 0.6025,
      "step": 4380
    },
    {
      "epoch": 0.27714646464646464,
      "grad_norm": 0.4636895954608917,
      "learning_rate": 0.00019388623286185138,
      "loss": 0.5777,
      "step": 4390
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.690486490726471,
      "learning_rate": 0.00019385098487942023,
      "loss": 0.6258,
      "step": 4400
    },
    {
      "epoch": 0.2784090909090909,
      "grad_norm": 0.3858344554901123,
      "learning_rate": 0.00019381563880316004,
      "loss": 0.8364,
      "step": 4410
    },
    {
      "epoch": 0.27904040404040403,
      "grad_norm": 0.4150519073009491,
      "learning_rate": 0.0001937801946700149,
      "loss": 0.6973,
      "step": 4420
    },
    {
      "epoch": 0.2796717171717172,
      "grad_norm": 0.4339175820350647,
      "learning_rate": 0.00019374465251703122,
      "loss": 0.6661,
      "step": 4430
    },
    {
      "epoch": 0.2803030303030303,
      "grad_norm": 0.44358494877815247,
      "learning_rate": 0.00019370901238135804,
      "loss": 0.6035,
      "step": 4440
    },
    {
      "epoch": 0.2809343434343434,
      "grad_norm": 0.9174742102622986,
      "learning_rate": 0.00019367327430024663,
      "loss": 0.6103,
      "step": 4450
    },
    {
      "epoch": 0.2815656565656566,
      "grad_norm": 0.4008546769618988,
      "learning_rate": 0.00019363743831105081,
      "loss": 0.88,
      "step": 4460
    },
    {
      "epoch": 0.2821969696969697,
      "grad_norm": 0.4123026430606842,
      "learning_rate": 0.00019360150445122665,
      "loss": 0.6848,
      "step": 4470
    },
    {
      "epoch": 0.2828282828282828,
      "grad_norm": 0.45181113481521606,
      "learning_rate": 0.0001935654727583325,
      "loss": 0.6363,
      "step": 4480
    },
    {
      "epoch": 0.28345959595959597,
      "grad_norm": 0.5091017484664917,
      "learning_rate": 0.00019352934327002892,
      "loss": 0.5917,
      "step": 4490
    },
    {
      "epoch": 0.2840909090909091,
      "grad_norm": 0.9049355983734131,
      "learning_rate": 0.00019349311602407884,
      "loss": 0.6138,
      "step": 4500
    },
    {
      "epoch": 0.2847222222222222,
      "grad_norm": 0.42142966389656067,
      "learning_rate": 0.00019345679105834727,
      "loss": 0.9018,
      "step": 4510
    },
    {
      "epoch": 0.28535353535353536,
      "grad_norm": 0.46364927291870117,
      "learning_rate": 0.00019342036841080132,
      "loss": 0.7397,
      "step": 4520
    },
    {
      "epoch": 0.2859848484848485,
      "grad_norm": 0.4589638113975525,
      "learning_rate": 0.00019338384811951027,
      "loss": 0.6557,
      "step": 4530
    },
    {
      "epoch": 0.2866161616161616,
      "grad_norm": 0.47178831696510315,
      "learning_rate": 0.00019334723022264544,
      "loss": 0.5508,
      "step": 4540
    },
    {
      "epoch": 0.28724747474747475,
      "grad_norm": 0.7336918711662292,
      "learning_rate": 0.00019331051475848018,
      "loss": 0.6135,
      "step": 4550
    },
    {
      "epoch": 0.2878787878787879,
      "grad_norm": 0.39437609910964966,
      "learning_rate": 0.0001932737017653897,
      "loss": 0.8032,
      "step": 4560
    },
    {
      "epoch": 0.288510101010101,
      "grad_norm": 0.4321289360523224,
      "learning_rate": 0.00019323679128185135,
      "loss": 0.7498,
      "step": 4570
    },
    {
      "epoch": 0.28914141414141414,
      "grad_norm": 0.43447819352149963,
      "learning_rate": 0.00019319978334644426,
      "loss": 0.6328,
      "step": 4580
    },
    {
      "epoch": 0.2897727272727273,
      "grad_norm": 0.4836736023426056,
      "learning_rate": 0.00019316267799784938,
      "loss": 0.5851,
      "step": 4590
    },
    {
      "epoch": 0.2904040404040404,
      "grad_norm": 0.8370457291603088,
      "learning_rate": 0.00019312547527484958,
      "loss": 0.587,
      "step": 4600
    },
    {
      "epoch": 0.29103535353535354,
      "grad_norm": 0.37738505005836487,
      "learning_rate": 0.00019308817521632943,
      "loss": 0.8471,
      "step": 4610
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.38932323455810547,
      "learning_rate": 0.00019305077786127526,
      "loss": 0.7431,
      "step": 4620
    },
    {
      "epoch": 0.2922979797979798,
      "grad_norm": 0.4441913068294525,
      "learning_rate": 0.00019301328324877512,
      "loss": 0.6276,
      "step": 4630
    },
    {
      "epoch": 0.29292929292929293,
      "grad_norm": 0.4622509777545929,
      "learning_rate": 0.00019297569141801867,
      "loss": 0.5733,
      "step": 4640
    },
    {
      "epoch": 0.2935606060606061,
      "grad_norm": 0.7473905682563782,
      "learning_rate": 0.00019293800240829717,
      "loss": 0.5893,
      "step": 4650
    },
    {
      "epoch": 0.29419191919191917,
      "grad_norm": 0.37569475173950195,
      "learning_rate": 0.00019290021625900354,
      "loss": 0.8104,
      "step": 4660
    },
    {
      "epoch": 0.2948232323232323,
      "grad_norm": 0.4313589930534363,
      "learning_rate": 0.00019286233300963218,
      "loss": 0.7208,
      "step": 4670
    },
    {
      "epoch": 0.29545454545454547,
      "grad_norm": 0.45192277431488037,
      "learning_rate": 0.00019282435269977894,
      "loss": 0.6308,
      "step": 4680
    },
    {
      "epoch": 0.29608585858585856,
      "grad_norm": 0.4912082850933075,
      "learning_rate": 0.00019278627536914117,
      "loss": 0.5841,
      "step": 4690
    },
    {
      "epoch": 0.2967171717171717,
      "grad_norm": 0.6841267943382263,
      "learning_rate": 0.00019274810105751762,
      "loss": 0.5995,
      "step": 4700
    },
    {
      "epoch": 0.29734848484848486,
      "grad_norm": 0.4024570882320404,
      "learning_rate": 0.0001927098298048084,
      "loss": 0.8408,
      "step": 4710
    },
    {
      "epoch": 0.29797979797979796,
      "grad_norm": 0.4156337380409241,
      "learning_rate": 0.00019267146165101491,
      "loss": 0.7633,
      "step": 4720
    },
    {
      "epoch": 0.2986111111111111,
      "grad_norm": 0.46911922097206116,
      "learning_rate": 0.0001926329966362399,
      "loss": 0.6643,
      "step": 4730
    },
    {
      "epoch": 0.29924242424242425,
      "grad_norm": 0.454987108707428,
      "learning_rate": 0.0001925944348006873,
      "loss": 0.5604,
      "step": 4740
    },
    {
      "epoch": 0.29987373737373735,
      "grad_norm": 0.7683812379837036,
      "learning_rate": 0.00019255577618466227,
      "loss": 0.594,
      "step": 4750
    },
    {
      "epoch": 0.3005050505050505,
      "grad_norm": 0.38043931126594543,
      "learning_rate": 0.0001925170208285711,
      "loss": 0.8306,
      "step": 4760
    },
    {
      "epoch": 0.30113636363636365,
      "grad_norm": 0.45533668994903564,
      "learning_rate": 0.00019247816877292125,
      "loss": 0.6959,
      "step": 4770
    },
    {
      "epoch": 0.30176767676767674,
      "grad_norm": 0.4582703411579132,
      "learning_rate": 0.0001924392200583212,
      "loss": 0.6102,
      "step": 4780
    },
    {
      "epoch": 0.3023989898989899,
      "grad_norm": 0.5387428402900696,
      "learning_rate": 0.00019240017472548044,
      "loss": 0.5478,
      "step": 4790
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.7827499508857727,
      "learning_rate": 0.0001923610328152095,
      "loss": 0.5905,
      "step": 4800
    },
    {
      "epoch": 0.3036616161616162,
      "grad_norm": 0.3864309787750244,
      "learning_rate": 0.00019232179436841983,
      "loss": 0.8638,
      "step": 4810
    },
    {
      "epoch": 0.3042929292929293,
      "grad_norm": 0.4099048972129822,
      "learning_rate": 0.00019228245942612374,
      "loss": 0.7432,
      "step": 4820
    },
    {
      "epoch": 0.30492424242424243,
      "grad_norm": 0.44393762946128845,
      "learning_rate": 0.0001922430280294345,
      "loss": 0.6205,
      "step": 4830
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 0.4491736590862274,
      "learning_rate": 0.0001922035002195661,
      "loss": 0.5745,
      "step": 4840
    },
    {
      "epoch": 0.3061868686868687,
      "grad_norm": 0.7174714803695679,
      "learning_rate": 0.00019216387603783334,
      "loss": 0.6168,
      "step": 4850
    },
    {
      "epoch": 0.3068181818181818,
      "grad_norm": 0.40431201457977295,
      "learning_rate": 0.00019212415552565174,
      "loss": 0.8619,
      "step": 4860
    },
    {
      "epoch": 0.307449494949495,
      "grad_norm": 0.40708622336387634,
      "learning_rate": 0.00019208433872453754,
      "loss": 0.7037,
      "step": 4870
    },
    {
      "epoch": 0.30808080808080807,
      "grad_norm": 0.41407790780067444,
      "learning_rate": 0.00019204442567610756,
      "loss": 0.6476,
      "step": 4880
    },
    {
      "epoch": 0.3087121212121212,
      "grad_norm": 0.4371597170829773,
      "learning_rate": 0.00019200441642207923,
      "loss": 0.5536,
      "step": 4890
    },
    {
      "epoch": 0.30934343434343436,
      "grad_norm": 0.8555623292922974,
      "learning_rate": 0.0001919643110042706,
      "loss": 0.6092,
      "step": 4900
    },
    {
      "epoch": 0.30997474747474746,
      "grad_norm": 0.35888251662254333,
      "learning_rate": 0.00019192410946460015,
      "loss": 0.8499,
      "step": 4910
    },
    {
      "epoch": 0.3106060606060606,
      "grad_norm": 0.402083158493042,
      "learning_rate": 0.00019188381184508688,
      "loss": 0.7089,
      "step": 4920
    },
    {
      "epoch": 0.31123737373737376,
      "grad_norm": 0.45213285088539124,
      "learning_rate": 0.0001918434181878502,
      "loss": 0.6275,
      "step": 4930
    },
    {
      "epoch": 0.31186868686868685,
      "grad_norm": 0.5398728251457214,
      "learning_rate": 0.00019180292853510992,
      "loss": 0.5741,
      "step": 4940
    },
    {
      "epoch": 0.3125,
      "grad_norm": 0.6724386811256409,
      "learning_rate": 0.00019176234292918608,
      "loss": 0.5926,
      "step": 4950
    },
    {
      "epoch": 0.31313131313131315,
      "grad_norm": 0.37484773993492126,
      "learning_rate": 0.00019172166141249915,
      "loss": 0.8584,
      "step": 4960
    },
    {
      "epoch": 0.31376262626262624,
      "grad_norm": 0.41757529973983765,
      "learning_rate": 0.00019168088402756985,
      "loss": 0.7207,
      "step": 4970
    },
    {
      "epoch": 0.3143939393939394,
      "grad_norm": 0.41859346628189087,
      "learning_rate": 0.0001916400108170189,
      "loss": 0.629,
      "step": 4980
    },
    {
      "epoch": 0.31502525252525254,
      "grad_norm": 0.5143744945526123,
      "learning_rate": 0.00019159904182356746,
      "loss": 0.5589,
      "step": 4990
    },
    {
      "epoch": 0.31565656565656564,
      "grad_norm": 0.7089598178863525,
      "learning_rate": 0.00019155797709003656,
      "loss": 0.5634,
      "step": 5000
    },
    {
      "epoch": 0.31565656565656564,
      "eval_loss": 0.6830705404281616,
      "eval_runtime": 43.7888,
      "eval_samples_per_second": 58.462,
      "eval_steps_per_second": 7.308,
      "step": 5000
    },
    {
      "epoch": 0.3162878787878788,
      "grad_norm": 0.3812824785709381,
      "learning_rate": 0.00019151681665934746,
      "loss": 0.8731,
      "step": 5010
    },
    {
      "epoch": 0.31691919191919193,
      "grad_norm": 0.4210079312324524,
      "learning_rate": 0.00019147556057452135,
      "loss": 0.7051,
      "step": 5020
    },
    {
      "epoch": 0.317550505050505,
      "grad_norm": 0.44028860330581665,
      "learning_rate": 0.00019143420887867945,
      "loss": 0.6301,
      "step": 5030
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 0.552679181098938,
      "learning_rate": 0.00019139276161504287,
      "loss": 0.5571,
      "step": 5040
    },
    {
      "epoch": 0.3188131313131313,
      "grad_norm": 0.6801038980484009,
      "learning_rate": 0.00019135121882693268,
      "loss": 0.6025,
      "step": 5050
    },
    {
      "epoch": 0.3194444444444444,
      "grad_norm": 0.42027023434638977,
      "learning_rate": 0.00019130958055776969,
      "loss": 0.8372,
      "step": 5060
    },
    {
      "epoch": 0.32007575757575757,
      "grad_norm": 0.41593456268310547,
      "learning_rate": 0.00019126784685107463,
      "loss": 0.7131,
      "step": 5070
    },
    {
      "epoch": 0.3207070707070707,
      "grad_norm": 0.4497692883014679,
      "learning_rate": 0.00019122601775046789,
      "loss": 0.6362,
      "step": 5080
    },
    {
      "epoch": 0.3213383838383838,
      "grad_norm": 0.4657299518585205,
      "learning_rate": 0.00019118409329966956,
      "loss": 0.5786,
      "step": 5090
    },
    {
      "epoch": 0.32196969696969696,
      "grad_norm": 0.796588122844696,
      "learning_rate": 0.00019114207354249948,
      "loss": 0.6302,
      "step": 5100
    },
    {
      "epoch": 0.3226010101010101,
      "grad_norm": 0.38581547141075134,
      "learning_rate": 0.00019109995852287698,
      "loss": 0.8719,
      "step": 5110
    },
    {
      "epoch": 0.32323232323232326,
      "grad_norm": 0.45165178179740906,
      "learning_rate": 0.0001910577482848211,
      "loss": 0.7161,
      "step": 5120
    },
    {
      "epoch": 0.32386363636363635,
      "grad_norm": 0.4394280016422272,
      "learning_rate": 0.0001910154428724503,
      "loss": 0.6486,
      "step": 5130
    },
    {
      "epoch": 0.3244949494949495,
      "grad_norm": 0.4371623694896698,
      "learning_rate": 0.00019097304232998255,
      "loss": 0.5289,
      "step": 5140
    },
    {
      "epoch": 0.32512626262626265,
      "grad_norm": 0.7762016654014587,
      "learning_rate": 0.00019093054670173522,
      "loss": 0.5906,
      "step": 5150
    },
    {
      "epoch": 0.32575757575757575,
      "grad_norm": 0.4026823341846466,
      "learning_rate": 0.00019088795603212517,
      "loss": 0.8492,
      "step": 5160
    },
    {
      "epoch": 0.3263888888888889,
      "grad_norm": 1.175655484199524,
      "learning_rate": 0.00019084527036566847,
      "loss": 0.7248,
      "step": 5170
    },
    {
      "epoch": 0.32702020202020204,
      "grad_norm": 0.4619818925857544,
      "learning_rate": 0.0001908024897469805,
      "loss": 0.6224,
      "step": 5180
    },
    {
      "epoch": 0.32765151515151514,
      "grad_norm": 0.4556049108505249,
      "learning_rate": 0.00019075961422077597,
      "loss": 0.5604,
      "step": 5190
    },
    {
      "epoch": 0.3282828282828283,
      "grad_norm": 0.7196685671806335,
      "learning_rate": 0.00019071664383186874,
      "loss": 0.6181,
      "step": 5200
    },
    {
      "epoch": 0.32891414141414144,
      "grad_norm": 0.3926648199558258,
      "learning_rate": 0.00019067357862517177,
      "loss": 0.8359,
      "step": 5210
    },
    {
      "epoch": 0.32954545454545453,
      "grad_norm": 0.42505741119384766,
      "learning_rate": 0.00019063041864569722,
      "loss": 0.7478,
      "step": 5220
    },
    {
      "epoch": 0.3301767676767677,
      "grad_norm": 0.4478210508823395,
      "learning_rate": 0.00019058716393855624,
      "loss": 0.648,
      "step": 5230
    },
    {
      "epoch": 0.33080808080808083,
      "grad_norm": 0.4903140962123871,
      "learning_rate": 0.000190543814548959,
      "loss": 0.5352,
      "step": 5240
    },
    {
      "epoch": 0.3314393939393939,
      "grad_norm": 0.7881852984428406,
      "learning_rate": 0.00019050037052221463,
      "loss": 0.6101,
      "step": 5250
    },
    {
      "epoch": 0.33207070707070707,
      "grad_norm": 0.39541104435920715,
      "learning_rate": 0.00019045683190373124,
      "loss": 0.8343,
      "step": 5260
    },
    {
      "epoch": 0.3327020202020202,
      "grad_norm": 0.40582701563835144,
      "learning_rate": 0.00019041319873901573,
      "loss": 0.7249,
      "step": 5270
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.427056223154068,
      "learning_rate": 0.00019036947107367382,
      "loss": 0.6391,
      "step": 5280
    },
    {
      "epoch": 0.33396464646464646,
      "grad_norm": 0.4658272862434387,
      "learning_rate": 0.00019032564895341006,
      "loss": 0.5451,
      "step": 5290
    },
    {
      "epoch": 0.3345959595959596,
      "grad_norm": 0.8125989437103271,
      "learning_rate": 0.00019028173242402767,
      "loss": 0.6297,
      "step": 5300
    },
    {
      "epoch": 0.3352272727272727,
      "grad_norm": 0.4059601128101349,
      "learning_rate": 0.0001902377215314286,
      "loss": 0.8926,
      "step": 5310
    },
    {
      "epoch": 0.33585858585858586,
      "grad_norm": 0.37660691142082214,
      "learning_rate": 0.00019019361632161336,
      "loss": 0.7033,
      "step": 5320
    },
    {
      "epoch": 0.336489898989899,
      "grad_norm": 0.44477400183677673,
      "learning_rate": 0.00019014941684068114,
      "loss": 0.6174,
      "step": 5330
    },
    {
      "epoch": 0.3371212121212121,
      "grad_norm": 0.4407219886779785,
      "learning_rate": 0.00019010512313482955,
      "loss": 0.5759,
      "step": 5340
    },
    {
      "epoch": 0.33775252525252525,
      "grad_norm": 0.6663500666618347,
      "learning_rate": 0.00019006073525035477,
      "loss": 0.5577,
      "step": 5350
    },
    {
      "epoch": 0.3383838383838384,
      "grad_norm": 0.40525829792022705,
      "learning_rate": 0.0001900162532336514,
      "loss": 0.8839,
      "step": 5360
    },
    {
      "epoch": 0.3390151515151515,
      "grad_norm": 0.4324905574321747,
      "learning_rate": 0.00018997167713121236,
      "loss": 0.7302,
      "step": 5370
    },
    {
      "epoch": 0.33964646464646464,
      "grad_norm": 0.44578155875205994,
      "learning_rate": 0.000189927006989629,
      "loss": 0.6438,
      "step": 5380
    },
    {
      "epoch": 0.3402777777777778,
      "grad_norm": 0.4674786925315857,
      "learning_rate": 0.0001898822428555909,
      "loss": 0.5705,
      "step": 5390
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 0.8017870187759399,
      "learning_rate": 0.00018983738477588595,
      "loss": 0.6008,
      "step": 5400
    },
    {
      "epoch": 0.34154040404040403,
      "grad_norm": 0.37178176641464233,
      "learning_rate": 0.00018979243279740015,
      "loss": 0.855,
      "step": 5410
    },
    {
      "epoch": 0.3421717171717172,
      "grad_norm": 0.47885406017303467,
      "learning_rate": 0.00018974738696711768,
      "loss": 0.723,
      "step": 5420
    },
    {
      "epoch": 0.3428030303030303,
      "grad_norm": 0.42722606658935547,
      "learning_rate": 0.00018970224733212083,
      "loss": 0.6157,
      "step": 5430
    },
    {
      "epoch": 0.3434343434343434,
      "grad_norm": 0.4981566369533539,
      "learning_rate": 0.0001896570139395899,
      "loss": 0.5569,
      "step": 5440
    },
    {
      "epoch": 0.3440656565656566,
      "grad_norm": 0.7352506518363953,
      "learning_rate": 0.00018961168683680326,
      "loss": 0.6045,
      "step": 5450
    },
    {
      "epoch": 0.3446969696969697,
      "grad_norm": 0.38711386919021606,
      "learning_rate": 0.0001895662660711371,
      "loss": 0.8781,
      "step": 5460
    },
    {
      "epoch": 0.3453282828282828,
      "grad_norm": 0.43719127774238586,
      "learning_rate": 0.00018952075169006568,
      "loss": 0.7116,
      "step": 5470
    },
    {
      "epoch": 0.34595959595959597,
      "grad_norm": 0.46694493293762207,
      "learning_rate": 0.00018947514374116089,
      "loss": 0.6449,
      "step": 5480
    },
    {
      "epoch": 0.3465909090909091,
      "grad_norm": 0.5041850209236145,
      "learning_rate": 0.00018942944227209264,
      "loss": 0.5659,
      "step": 5490
    },
    {
      "epoch": 0.3472222222222222,
      "grad_norm": 0.7523750066757202,
      "learning_rate": 0.0001893836473306284,
      "loss": 0.6225,
      "step": 5500
    },
    {
      "epoch": 0.34785353535353536,
      "grad_norm": 0.3766830265522003,
      "learning_rate": 0.00018933775896463347,
      "loss": 0.858,
      "step": 5510
    },
    {
      "epoch": 0.3484848484848485,
      "grad_norm": 0.40614113211631775,
      "learning_rate": 0.00018929177722207076,
      "loss": 0.7301,
      "step": 5520
    },
    {
      "epoch": 0.3491161616161616,
      "grad_norm": 0.4365730285644531,
      "learning_rate": 0.00018924570215100075,
      "loss": 0.6327,
      "step": 5530
    },
    {
      "epoch": 0.34974747474747475,
      "grad_norm": 0.4846203923225403,
      "learning_rate": 0.0001891995337995815,
      "loss": 0.5993,
      "step": 5540
    },
    {
      "epoch": 0.3503787878787879,
      "grad_norm": 0.6876258850097656,
      "learning_rate": 0.00018915327221606854,
      "loss": 0.5587,
      "step": 5550
    },
    {
      "epoch": 0.351010101010101,
      "grad_norm": 0.39217251539230347,
      "learning_rate": 0.0001891069174488149,
      "loss": 0.8437,
      "step": 5560
    },
    {
      "epoch": 0.35164141414141414,
      "grad_norm": 0.4686998724937439,
      "learning_rate": 0.0001890604695462709,
      "loss": 0.7116,
      "step": 5570
    },
    {
      "epoch": 0.3522727272727273,
      "grad_norm": 0.44547033309936523,
      "learning_rate": 0.0001890139285569843,
      "loss": 0.6618,
      "step": 5580
    },
    {
      "epoch": 0.3529040404040404,
      "grad_norm": 0.49424225091934204,
      "learning_rate": 0.00018896729452960015,
      "loss": 0.5988,
      "step": 5590
    },
    {
      "epoch": 0.35353535353535354,
      "grad_norm": 0.7398842573165894,
      "learning_rate": 0.00018892056751286073,
      "loss": 0.5831,
      "step": 5600
    },
    {
      "epoch": 0.3541666666666667,
      "grad_norm": 0.37783530354499817,
      "learning_rate": 0.0001888737475556055,
      "loss": 0.9026,
      "step": 5610
    },
    {
      "epoch": 0.3547979797979798,
      "grad_norm": 0.45520129799842834,
      "learning_rate": 0.00018882683470677103,
      "loss": 0.7228,
      "step": 5620
    },
    {
      "epoch": 0.35542929292929293,
      "grad_norm": 0.39093631505966187,
      "learning_rate": 0.00018877982901539103,
      "loss": 0.6085,
      "step": 5630
    },
    {
      "epoch": 0.3560606060606061,
      "grad_norm": 0.5074058175086975,
      "learning_rate": 0.00018873273053059627,
      "loss": 0.5906,
      "step": 5640
    },
    {
      "epoch": 0.35669191919191917,
      "grad_norm": 0.7036303281784058,
      "learning_rate": 0.00018868553930161447,
      "loss": 0.6064,
      "step": 5650
    },
    {
      "epoch": 0.3573232323232323,
      "grad_norm": 0.3932410180568695,
      "learning_rate": 0.00018863825537777026,
      "loss": 0.8479,
      "step": 5660
    },
    {
      "epoch": 0.35795454545454547,
      "grad_norm": 0.4250602424144745,
      "learning_rate": 0.00018859087880848525,
      "loss": 0.7165,
      "step": 5670
    },
    {
      "epoch": 0.35858585858585856,
      "grad_norm": 0.41753333806991577,
      "learning_rate": 0.0001885434096432778,
      "loss": 0.6354,
      "step": 5680
    },
    {
      "epoch": 0.3592171717171717,
      "grad_norm": 0.4474669396877289,
      "learning_rate": 0.00018849584793176303,
      "loss": 0.5828,
      "step": 5690
    },
    {
      "epoch": 0.35984848484848486,
      "grad_norm": 0.6194934248924255,
      "learning_rate": 0.00018844819372365286,
      "loss": 0.6046,
      "step": 5700
    },
    {
      "epoch": 0.36047979797979796,
      "grad_norm": 0.4065728485584259,
      "learning_rate": 0.0001884004470687559,
      "loss": 0.8965,
      "step": 5710
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 0.41703757643699646,
      "learning_rate": 0.00018835260801697734,
      "loss": 0.6916,
      "step": 5720
    },
    {
      "epoch": 0.36174242424242425,
      "grad_norm": 0.47299548983573914,
      "learning_rate": 0.00018830467661831891,
      "loss": 0.6129,
      "step": 5730
    },
    {
      "epoch": 0.36237373737373735,
      "grad_norm": 0.44046077132225037,
      "learning_rate": 0.00018825665292287894,
      "loss": 0.5873,
      "step": 5740
    },
    {
      "epoch": 0.3630050505050505,
      "grad_norm": 0.6833116412162781,
      "learning_rate": 0.0001882085369808522,
      "loss": 0.5717,
      "step": 5750
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.3916407525539398,
      "learning_rate": 0.00018816032884252988,
      "loss": 0.9192,
      "step": 5760
    },
    {
      "epoch": 0.36426767676767674,
      "grad_norm": 0.42476969957351685,
      "learning_rate": 0.0001881120285582995,
      "loss": 0.749,
      "step": 5770
    },
    {
      "epoch": 0.3648989898989899,
      "grad_norm": 0.4132503569126129,
      "learning_rate": 0.00018806363617864493,
      "loss": 0.6186,
      "step": 5780
    },
    {
      "epoch": 0.36553030303030304,
      "grad_norm": 0.4559709131717682,
      "learning_rate": 0.00018801515175414629,
      "loss": 0.5692,
      "step": 5790
    },
    {
      "epoch": 0.3661616161616162,
      "grad_norm": 0.6527326107025146,
      "learning_rate": 0.00018796657533547988,
      "loss": 0.5919,
      "step": 5800
    },
    {
      "epoch": 0.3667929292929293,
      "grad_norm": 0.3991287350654602,
      "learning_rate": 0.0001879179069734182,
      "loss": 0.8336,
      "step": 5810
    },
    {
      "epoch": 0.36742424242424243,
      "grad_norm": 0.39567938446998596,
      "learning_rate": 0.00018786914671882983,
      "loss": 0.7037,
      "step": 5820
    },
    {
      "epoch": 0.3680555555555556,
      "grad_norm": 0.4182731807231903,
      "learning_rate": 0.0001878202946226794,
      "loss": 0.654,
      "step": 5830
    },
    {
      "epoch": 0.3686868686868687,
      "grad_norm": 0.44135165214538574,
      "learning_rate": 0.00018777135073602748,
      "loss": 0.5861,
      "step": 5840
    },
    {
      "epoch": 0.3693181818181818,
      "grad_norm": 0.7186405062675476,
      "learning_rate": 0.00018772231511003068,
      "loss": 0.5913,
      "step": 5850
    },
    {
      "epoch": 0.369949494949495,
      "grad_norm": 0.4040564298629761,
      "learning_rate": 0.0001876731877959414,
      "loss": 0.829,
      "step": 5860
    },
    {
      "epoch": 0.37058080808080807,
      "grad_norm": 0.40387868881225586,
      "learning_rate": 0.00018762396884510797,
      "loss": 0.7156,
      "step": 5870
    },
    {
      "epoch": 0.3712121212121212,
      "grad_norm": 0.43394115567207336,
      "learning_rate": 0.0001875746583089744,
      "loss": 0.6464,
      "step": 5880
    },
    {
      "epoch": 0.37184343434343436,
      "grad_norm": 0.4811636507511139,
      "learning_rate": 0.0001875252562390805,
      "loss": 0.5778,
      "step": 5890
    },
    {
      "epoch": 0.37247474747474746,
      "grad_norm": 0.765749454498291,
      "learning_rate": 0.00018747576268706172,
      "loss": 0.5794,
      "step": 5900
    },
    {
      "epoch": 0.3731060606060606,
      "grad_norm": 0.40137478709220886,
      "learning_rate": 0.0001874261777046491,
      "loss": 0.839,
      "step": 5910
    },
    {
      "epoch": 0.37373737373737376,
      "grad_norm": 0.4050792455673218,
      "learning_rate": 0.00018737650134366927,
      "loss": 0.7214,
      "step": 5920
    },
    {
      "epoch": 0.37436868686868685,
      "grad_norm": 0.4549245238304138,
      "learning_rate": 0.00018732673365604447,
      "loss": 0.6339,
      "step": 5930
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.44701096415519714,
      "learning_rate": 0.0001872768746937922,
      "loss": 0.5527,
      "step": 5940
    },
    {
      "epoch": 0.37563131313131315,
      "grad_norm": 0.7051013708114624,
      "learning_rate": 0.00018722692450902551,
      "loss": 0.5792,
      "step": 5950
    },
    {
      "epoch": 0.37626262626262624,
      "grad_norm": 0.4042966067790985,
      "learning_rate": 0.0001871768831539527,
      "loss": 0.8565,
      "step": 5960
    },
    {
      "epoch": 0.3768939393939394,
      "grad_norm": 0.43944114446640015,
      "learning_rate": 0.00018712675068087746,
      "loss": 0.7397,
      "step": 5970
    },
    {
      "epoch": 0.37752525252525254,
      "grad_norm": 0.47394564747810364,
      "learning_rate": 0.00018707652714219868,
      "loss": 0.6384,
      "step": 5980
    },
    {
      "epoch": 0.37815656565656564,
      "grad_norm": 0.4379049837589264,
      "learning_rate": 0.00018702621259041036,
      "loss": 0.5524,
      "step": 5990
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 0.7316553592681885,
      "learning_rate": 0.00018697580707810173,
      "loss": 0.6067,
      "step": 6000
    },
    {
      "epoch": 0.3787878787878788,
      "eval_loss": 0.6705648303031921,
      "eval_runtime": 48.0114,
      "eval_samples_per_second": 53.321,
      "eval_steps_per_second": 6.665,
      "step": 6000
    },
    {
      "epoch": 0.37941919191919193,
      "grad_norm": 0.3968477249145508,
      "learning_rate": 0.00018692531065795702,
      "loss": 0.8887,
      "step": 6010
    },
    {
      "epoch": 0.380050505050505,
      "grad_norm": 0.41562703251838684,
      "learning_rate": 0.00018687472338275557,
      "loss": 0.717,
      "step": 6020
    },
    {
      "epoch": 0.3806818181818182,
      "grad_norm": 0.4618544280529022,
      "learning_rate": 0.00018682404530537155,
      "loss": 0.6541,
      "step": 6030
    },
    {
      "epoch": 0.3813131313131313,
      "grad_norm": 0.45540526509284973,
      "learning_rate": 0.00018677327647877412,
      "loss": 0.5161,
      "step": 6040
    },
    {
      "epoch": 0.3819444444444444,
      "grad_norm": 0.7183984518051147,
      "learning_rate": 0.00018672241695602733,
      "loss": 0.5829,
      "step": 6050
    },
    {
      "epoch": 0.38257575757575757,
      "grad_norm": 0.409047931432724,
      "learning_rate": 0.0001866714667902899,
      "loss": 0.8491,
      "step": 6060
    },
    {
      "epoch": 0.3832070707070707,
      "grad_norm": 0.45113512873649597,
      "learning_rate": 0.00018662042603481542,
      "loss": 0.7221,
      "step": 6070
    },
    {
      "epoch": 0.3838383838383838,
      "grad_norm": 0.45260241627693176,
      "learning_rate": 0.00018656929474295209,
      "loss": 0.6593,
      "step": 6080
    },
    {
      "epoch": 0.38446969696969696,
      "grad_norm": 0.4620872437953949,
      "learning_rate": 0.00018651807296814278,
      "loss": 0.5735,
      "step": 6090
    },
    {
      "epoch": 0.3851010101010101,
      "grad_norm": 0.6782142519950867,
      "learning_rate": 0.0001864667607639249,
      "loss": 0.5682,
      "step": 6100
    },
    {
      "epoch": 0.38573232323232326,
      "grad_norm": 0.4242574870586395,
      "learning_rate": 0.0001864153581839304,
      "loss": 0.8409,
      "step": 6110
    },
    {
      "epoch": 0.38636363636363635,
      "grad_norm": 0.425813227891922,
      "learning_rate": 0.00018636386528188568,
      "loss": 0.6977,
      "step": 6120
    },
    {
      "epoch": 0.3869949494949495,
      "grad_norm": 0.48791342973709106,
      "learning_rate": 0.00018631228211161152,
      "loss": 0.603,
      "step": 6130
    },
    {
      "epoch": 0.38762626262626265,
      "grad_norm": 0.4535306990146637,
      "learning_rate": 0.00018626060872702313,
      "loss": 0.5583,
      "step": 6140
    },
    {
      "epoch": 0.38825757575757575,
      "grad_norm": 0.8153489232063293,
      "learning_rate": 0.00018620884518212995,
      "loss": 0.5973,
      "step": 6150
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.37033480405807495,
      "learning_rate": 0.00018615699153103562,
      "loss": 0.8787,
      "step": 6160
    },
    {
      "epoch": 0.38952020202020204,
      "grad_norm": 0.4426262378692627,
      "learning_rate": 0.00018610504782793808,
      "loss": 0.7065,
      "step": 6170
    },
    {
      "epoch": 0.39015151515151514,
      "grad_norm": 0.44312596321105957,
      "learning_rate": 0.00018605301412712922,
      "loss": 0.6436,
      "step": 6180
    },
    {
      "epoch": 0.3907828282828283,
      "grad_norm": 0.4638972580432892,
      "learning_rate": 0.0001860008904829952,
      "loss": 0.5628,
      "step": 6190
    },
    {
      "epoch": 0.39141414141414144,
      "grad_norm": 0.6927832961082458,
      "learning_rate": 0.00018594867695001605,
      "loss": 0.586,
      "step": 6200
    },
    {
      "epoch": 0.39204545454545453,
      "grad_norm": 0.40802228450775146,
      "learning_rate": 0.00018589637358276578,
      "loss": 0.8855,
      "step": 6210
    },
    {
      "epoch": 0.3926767676767677,
      "grad_norm": 0.46118679642677307,
      "learning_rate": 0.0001858439804359123,
      "loss": 0.7578,
      "step": 6220
    },
    {
      "epoch": 0.39330808080808083,
      "grad_norm": 0.4086286425590515,
      "learning_rate": 0.00018579149756421735,
      "loss": 0.6185,
      "step": 6230
    },
    {
      "epoch": 0.3939393939393939,
      "grad_norm": 0.4956761598587036,
      "learning_rate": 0.0001857389250225365,
      "loss": 0.5747,
      "step": 6240
    },
    {
      "epoch": 0.39457070707070707,
      "grad_norm": 0.7176917791366577,
      "learning_rate": 0.00018568626286581897,
      "loss": 0.5684,
      "step": 6250
    },
    {
      "epoch": 0.3952020202020202,
      "grad_norm": 0.3962595760822296,
      "learning_rate": 0.0001856335111491077,
      "loss": 0.8524,
      "step": 6260
    },
    {
      "epoch": 0.3958333333333333,
      "grad_norm": 0.42542126774787903,
      "learning_rate": 0.00018558066992753925,
      "loss": 0.7426,
      "step": 6270
    },
    {
      "epoch": 0.39646464646464646,
      "grad_norm": 0.4379388391971588,
      "learning_rate": 0.00018552773925634367,
      "loss": 0.6209,
      "step": 6280
    },
    {
      "epoch": 0.3970959595959596,
      "grad_norm": 0.4900154769420624,
      "learning_rate": 0.00018547471919084453,
      "loss": 0.5745,
      "step": 6290
    },
    {
      "epoch": 0.3977272727272727,
      "grad_norm": 0.7434626221656799,
      "learning_rate": 0.00018542160978645886,
      "loss": 0.5949,
      "step": 6300
    },
    {
      "epoch": 0.39835858585858586,
      "grad_norm": 0.3943726122379303,
      "learning_rate": 0.00018536841109869704,
      "loss": 0.8723,
      "step": 6310
    },
    {
      "epoch": 0.398989898989899,
      "grad_norm": 0.42939987778663635,
      "learning_rate": 0.00018531512318316283,
      "loss": 0.6936,
      "step": 6320
    },
    {
      "epoch": 0.3996212121212121,
      "grad_norm": 0.4747758209705353,
      "learning_rate": 0.0001852617460955531,
      "loss": 0.6306,
      "step": 6330
    },
    {
      "epoch": 0.40025252525252525,
      "grad_norm": 0.4980335831642151,
      "learning_rate": 0.00018520827989165813,
      "loss": 0.572,
      "step": 6340
    },
    {
      "epoch": 0.4008838383838384,
      "grad_norm": 0.6998749375343323,
      "learning_rate": 0.0001851547246273612,
      "loss": 0.5883,
      "step": 6350
    },
    {
      "epoch": 0.4015151515151515,
      "grad_norm": 0.37207508087158203,
      "learning_rate": 0.00018510108035863868,
      "loss": 0.8875,
      "step": 6360
    },
    {
      "epoch": 0.40214646464646464,
      "grad_norm": 0.42119747400283813,
      "learning_rate": 0.00018504734714156008,
      "loss": 0.7352,
      "step": 6370
    },
    {
      "epoch": 0.4027777777777778,
      "grad_norm": 0.4569248557090759,
      "learning_rate": 0.00018499352503228774,
      "loss": 0.6777,
      "step": 6380
    },
    {
      "epoch": 0.4034090909090909,
      "grad_norm": 0.5147514343261719,
      "learning_rate": 0.000184939614087077,
      "loss": 0.5969,
      "step": 6390
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 0.6656413674354553,
      "learning_rate": 0.00018488561436227603,
      "loss": 0.5659,
      "step": 6400
    },
    {
      "epoch": 0.4046717171717172,
      "grad_norm": 0.3977372944355011,
      "learning_rate": 0.0001848315259143258,
      "loss": 0.8383,
      "step": 6410
    },
    {
      "epoch": 0.4053030303030303,
      "grad_norm": 0.39718106389045715,
      "learning_rate": 0.00018477734879976,
      "loss": 0.7278,
      "step": 6420
    },
    {
      "epoch": 0.4059343434343434,
      "grad_norm": 0.42985475063323975,
      "learning_rate": 0.00018472308307520497,
      "loss": 0.6128,
      "step": 6430
    },
    {
      "epoch": 0.4065656565656566,
      "grad_norm": 0.447091668844223,
      "learning_rate": 0.0001846687287973797,
      "loss": 0.5247,
      "step": 6440
    },
    {
      "epoch": 0.4071969696969697,
      "grad_norm": 0.7055084109306335,
      "learning_rate": 0.0001846142860230958,
      "loss": 0.5831,
      "step": 6450
    },
    {
      "epoch": 0.4078282828282828,
      "grad_norm": 0.39369502663612366,
      "learning_rate": 0.00018455975480925722,
      "loss": 0.8188,
      "step": 6460
    },
    {
      "epoch": 0.40845959595959597,
      "grad_norm": 0.4419882893562317,
      "learning_rate": 0.0001845051352128605,
      "loss": 0.685,
      "step": 6470
    },
    {
      "epoch": 0.4090909090909091,
      "grad_norm": 0.4735686480998993,
      "learning_rate": 0.00018445042729099445,
      "loss": 0.6306,
      "step": 6480
    },
    {
      "epoch": 0.4097222222222222,
      "grad_norm": 0.4628712832927704,
      "learning_rate": 0.00018439563110084033,
      "loss": 0.5696,
      "step": 6490
    },
    {
      "epoch": 0.41035353535353536,
      "grad_norm": 0.7321606874465942,
      "learning_rate": 0.00018434074669967148,
      "loss": 0.5625,
      "step": 6500
    },
    {
      "epoch": 0.4109848484848485,
      "grad_norm": 0.4049592912197113,
      "learning_rate": 0.00018428577414485357,
      "loss": 0.8414,
      "step": 6510
    },
    {
      "epoch": 0.4116161616161616,
      "grad_norm": 0.4298216998577118,
      "learning_rate": 0.00018423071349384435,
      "loss": 0.7226,
      "step": 6520
    },
    {
      "epoch": 0.41224747474747475,
      "grad_norm": 0.4269390404224396,
      "learning_rate": 0.00018417556480419372,
      "loss": 0.6498,
      "step": 6530
    },
    {
      "epoch": 0.4128787878787879,
      "grad_norm": 0.4845447540283203,
      "learning_rate": 0.00018412032813354347,
      "loss": 0.5496,
      "step": 6540
    },
    {
      "epoch": 0.413510101010101,
      "grad_norm": 0.8229667544364929,
      "learning_rate": 0.0001840650035396275,
      "loss": 0.5938,
      "step": 6550
    },
    {
      "epoch": 0.41414141414141414,
      "grad_norm": 0.4014977216720581,
      "learning_rate": 0.0001840095910802715,
      "loss": 0.8507,
      "step": 6560
    },
    {
      "epoch": 0.4147727272727273,
      "grad_norm": 0.4360630512237549,
      "learning_rate": 0.00018395409081339305,
      "loss": 0.744,
      "step": 6570
    },
    {
      "epoch": 0.4154040404040404,
      "grad_norm": 0.41039130091667175,
      "learning_rate": 0.00018389850279700148,
      "loss": 0.6259,
      "step": 6580
    },
    {
      "epoch": 0.41603535353535354,
      "grad_norm": 0.4892594516277313,
      "learning_rate": 0.00018384282708919784,
      "loss": 0.6023,
      "step": 6590
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.6723920106887817,
      "learning_rate": 0.00018378706374817485,
      "loss": 0.5961,
      "step": 6600
    },
    {
      "epoch": 0.4172979797979798,
      "grad_norm": 0.39217162132263184,
      "learning_rate": 0.00018373121283221682,
      "loss": 0.8704,
      "step": 6610
    },
    {
      "epoch": 0.41792929292929293,
      "grad_norm": 0.40469881892204285,
      "learning_rate": 0.00018367527439969958,
      "loss": 0.7353,
      "step": 6620
    },
    {
      "epoch": 0.4185606060606061,
      "grad_norm": 0.45490220189094543,
      "learning_rate": 0.00018361924850909044,
      "loss": 0.6567,
      "step": 6630
    },
    {
      "epoch": 0.41919191919191917,
      "grad_norm": 0.5062025189399719,
      "learning_rate": 0.00018356313521894816,
      "loss": 0.6143,
      "step": 6640
    },
    {
      "epoch": 0.4198232323232323,
      "grad_norm": 0.6896225214004517,
      "learning_rate": 0.00018350693458792279,
      "loss": 0.5787,
      "step": 6650
    },
    {
      "epoch": 0.42045454545454547,
      "grad_norm": 0.42451024055480957,
      "learning_rate": 0.0001834506466747557,
      "loss": 0.8623,
      "step": 6660
    },
    {
      "epoch": 0.42108585858585856,
      "grad_norm": 0.41179347038269043,
      "learning_rate": 0.0001833942715382795,
      "loss": 0.7191,
      "step": 6670
    },
    {
      "epoch": 0.4217171717171717,
      "grad_norm": 0.45525091886520386,
      "learning_rate": 0.00018333780923741788,
      "loss": 0.6443,
      "step": 6680
    },
    {
      "epoch": 0.42234848484848486,
      "grad_norm": 0.4571419954299927,
      "learning_rate": 0.0001832812598311858,
      "loss": 0.5806,
      "step": 6690
    },
    {
      "epoch": 0.42297979797979796,
      "grad_norm": 0.878010094165802,
      "learning_rate": 0.00018322462337868914,
      "loss": 0.6153,
      "step": 6700
    },
    {
      "epoch": 0.4236111111111111,
      "grad_norm": 0.40077802538871765,
      "learning_rate": 0.00018316789993912477,
      "loss": 0.8345,
      "step": 6710
    },
    {
      "epoch": 0.42424242424242425,
      "grad_norm": 0.43605807423591614,
      "learning_rate": 0.0001831110895717805,
      "loss": 0.7063,
      "step": 6720
    },
    {
      "epoch": 0.42487373737373735,
      "grad_norm": 0.4311387240886688,
      "learning_rate": 0.00018305419233603508,
      "loss": 0.6524,
      "step": 6730
    },
    {
      "epoch": 0.4255050505050505,
      "grad_norm": 0.482345849275589,
      "learning_rate": 0.00018299720829135786,
      "loss": 0.5539,
      "step": 6740
    },
    {
      "epoch": 0.42613636363636365,
      "grad_norm": 0.703144371509552,
      "learning_rate": 0.00018294013749730904,
      "loss": 0.5472,
      "step": 6750
    },
    {
      "epoch": 0.42676767676767674,
      "grad_norm": 0.43903061747550964,
      "learning_rate": 0.00018288298001353957,
      "loss": 0.8531,
      "step": 6760
    },
    {
      "epoch": 0.4273989898989899,
      "grad_norm": 0.41059720516204834,
      "learning_rate": 0.00018282573589979085,
      "loss": 0.6926,
      "step": 6770
    },
    {
      "epoch": 0.42803030303030304,
      "grad_norm": 0.4338013529777527,
      "learning_rate": 0.00018276840521589497,
      "loss": 0.6499,
      "step": 6780
    },
    {
      "epoch": 0.4286616161616162,
      "grad_norm": 0.4570932686328888,
      "learning_rate": 0.0001827109880217744,
      "loss": 0.5519,
      "step": 6790
    },
    {
      "epoch": 0.4292929292929293,
      "grad_norm": 0.7090271711349487,
      "learning_rate": 0.0001826534843774421,
      "loss": 0.6058,
      "step": 6800
    },
    {
      "epoch": 0.42992424242424243,
      "grad_norm": 0.39884841442108154,
      "learning_rate": 0.0001825958943430013,
      "loss": 0.8929,
      "step": 6810
    },
    {
      "epoch": 0.4305555555555556,
      "grad_norm": 0.4060809910297394,
      "learning_rate": 0.00018253821797864562,
      "loss": 0.7405,
      "step": 6820
    },
    {
      "epoch": 0.4311868686868687,
      "grad_norm": 0.48472145199775696,
      "learning_rate": 0.00018248045534465884,
      "loss": 0.645,
      "step": 6830
    },
    {
      "epoch": 0.4318181818181818,
      "grad_norm": 0.4698902666568756,
      "learning_rate": 0.00018242260650141502,
      "loss": 0.5535,
      "step": 6840
    },
    {
      "epoch": 0.432449494949495,
      "grad_norm": 0.7497288584709167,
      "learning_rate": 0.0001823646715093782,
      "loss": 0.5885,
      "step": 6850
    },
    {
      "epoch": 0.43308080808080807,
      "grad_norm": 0.40101051330566406,
      "learning_rate": 0.00018230665042910248,
      "loss": 0.8453,
      "step": 6860
    },
    {
      "epoch": 0.4337121212121212,
      "grad_norm": 0.40551695227622986,
      "learning_rate": 0.00018224854332123206,
      "loss": 0.6993,
      "step": 6870
    },
    {
      "epoch": 0.43434343434343436,
      "grad_norm": 0.39929330348968506,
      "learning_rate": 0.0001821903502465009,
      "loss": 0.6199,
      "step": 6880
    },
    {
      "epoch": 0.43497474747474746,
      "grad_norm": 0.4746890068054199,
      "learning_rate": 0.00018213207126573292,
      "loss": 0.5792,
      "step": 6890
    },
    {
      "epoch": 0.4356060606060606,
      "grad_norm": 0.7999115586280823,
      "learning_rate": 0.00018207370643984178,
      "loss": 0.592,
      "step": 6900
    },
    {
      "epoch": 0.43623737373737376,
      "grad_norm": 0.3994639217853546,
      "learning_rate": 0.0001820152558298309,
      "loss": 0.8475,
      "step": 6910
    },
    {
      "epoch": 0.43686868686868685,
      "grad_norm": 0.4044354557991028,
      "learning_rate": 0.00018195671949679333,
      "loss": 0.7353,
      "step": 6920
    },
    {
      "epoch": 0.4375,
      "grad_norm": 0.47582197189331055,
      "learning_rate": 0.0001818980975019117,
      "loss": 0.6289,
      "step": 6930
    },
    {
      "epoch": 0.43813131313131315,
      "grad_norm": 0.47519588470458984,
      "learning_rate": 0.00018183938990645827,
      "loss": 0.5369,
      "step": 6940
    },
    {
      "epoch": 0.43876262626262624,
      "grad_norm": 0.6955175399780273,
      "learning_rate": 0.00018178059677179467,
      "loss": 0.5696,
      "step": 6950
    },
    {
      "epoch": 0.4393939393939394,
      "grad_norm": 0.40415894985198975,
      "learning_rate": 0.00018172171815937195,
      "loss": 0.8549,
      "step": 6960
    },
    {
      "epoch": 0.44002525252525254,
      "grad_norm": 0.4124080538749695,
      "learning_rate": 0.00018166275413073062,
      "loss": 0.6945,
      "step": 6970
    },
    {
      "epoch": 0.44065656565656564,
      "grad_norm": 0.4698578417301178,
      "learning_rate": 0.00018160370474750023,
      "loss": 0.6374,
      "step": 6980
    },
    {
      "epoch": 0.4412878787878788,
      "grad_norm": 0.44678831100463867,
      "learning_rate": 0.0001815445700713998,
      "loss": 0.5632,
      "step": 6990
    },
    {
      "epoch": 0.44191919191919193,
      "grad_norm": 0.8519172072410583,
      "learning_rate": 0.00018148535016423734,
      "loss": 0.5723,
      "step": 7000
    },
    {
      "epoch": 0.44191919191919193,
      "eval_loss": 0.6673529744148254,
      "eval_runtime": 48.2813,
      "eval_samples_per_second": 53.023,
      "eval_steps_per_second": 6.628,
      "step": 7000
    },
    {
      "epoch": 0.442550505050505,
      "grad_norm": 0.40555816888809204,
      "learning_rate": 0.00018142604508791,
      "loss": 0.8838,
      "step": 7010
    },
    {
      "epoch": 0.4431818181818182,
      "grad_norm": 0.4215138554573059,
      "learning_rate": 0.00018136665490440393,
      "loss": 0.7306,
      "step": 7020
    },
    {
      "epoch": 0.4438131313131313,
      "grad_norm": 0.46851733326911926,
      "learning_rate": 0.00018130717967579423,
      "loss": 0.6633,
      "step": 7030
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.4794832170009613,
      "learning_rate": 0.00018124761946424492,
      "loss": 0.5726,
      "step": 7040
    },
    {
      "epoch": 0.44507575757575757,
      "grad_norm": 0.7100147604942322,
      "learning_rate": 0.00018118797433200882,
      "loss": 0.5668,
      "step": 7050
    },
    {
      "epoch": 0.4457070707070707,
      "grad_norm": 0.41511744260787964,
      "learning_rate": 0.00018112824434142753,
      "loss": 0.8598,
      "step": 7060
    },
    {
      "epoch": 0.4463383838383838,
      "grad_norm": 0.4453066885471344,
      "learning_rate": 0.00018106842955493133,
      "loss": 0.6846,
      "step": 7070
    },
    {
      "epoch": 0.44696969696969696,
      "grad_norm": 0.4827601909637451,
      "learning_rate": 0.00018100853003503916,
      "loss": 0.6295,
      "step": 7080
    },
    {
      "epoch": 0.4476010101010101,
      "grad_norm": 0.5207152366638184,
      "learning_rate": 0.00018094854584435843,
      "loss": 0.5669,
      "step": 7090
    },
    {
      "epoch": 0.44823232323232326,
      "grad_norm": 0.7845364212989807,
      "learning_rate": 0.00018088847704558517,
      "loss": 0.5996,
      "step": 7100
    },
    {
      "epoch": 0.44886363636363635,
      "grad_norm": 0.3825257122516632,
      "learning_rate": 0.00018082832370150374,
      "loss": 0.8729,
      "step": 7110
    },
    {
      "epoch": 0.4494949494949495,
      "grad_norm": 0.4269862473011017,
      "learning_rate": 0.00018076808587498696,
      "loss": 0.7228,
      "step": 7120
    },
    {
      "epoch": 0.45012626262626265,
      "grad_norm": 0.4344416856765747,
      "learning_rate": 0.00018070776362899587,
      "loss": 0.6341,
      "step": 7130
    },
    {
      "epoch": 0.45075757575757575,
      "grad_norm": 0.48202356696128845,
      "learning_rate": 0.0001806473570265798,
      "loss": 0.5551,
      "step": 7140
    },
    {
      "epoch": 0.4513888888888889,
      "grad_norm": 0.732477605342865,
      "learning_rate": 0.00018058686613087624,
      "loss": 0.543,
      "step": 7150
    },
    {
      "epoch": 0.45202020202020204,
      "grad_norm": 0.3965708315372467,
      "learning_rate": 0.00018052629100511077,
      "loss": 0.8467,
      "step": 7160
    },
    {
      "epoch": 0.45265151515151514,
      "grad_norm": 0.4409165680408478,
      "learning_rate": 0.00018046563171259701,
      "loss": 0.6883,
      "step": 7170
    },
    {
      "epoch": 0.4532828282828283,
      "grad_norm": 0.43090301752090454,
      "learning_rate": 0.00018040488831673655,
      "loss": 0.6241,
      "step": 7180
    },
    {
      "epoch": 0.45391414141414144,
      "grad_norm": 0.5037727355957031,
      "learning_rate": 0.00018034406088101893,
      "loss": 0.5634,
      "step": 7190
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.6650064587593079,
      "learning_rate": 0.00018028314946902144,
      "loss": 0.6091,
      "step": 7200
    },
    {
      "epoch": 0.4551767676767677,
      "grad_norm": 0.3908046782016754,
      "learning_rate": 0.00018022215414440924,
      "loss": 0.9623,
      "step": 7210
    },
    {
      "epoch": 0.45580808080808083,
      "grad_norm": 0.41771337389945984,
      "learning_rate": 0.00018016107497093514,
      "loss": 0.7119,
      "step": 7220
    },
    {
      "epoch": 0.4564393939393939,
      "grad_norm": 0.4310476779937744,
      "learning_rate": 0.00018009991201243955,
      "loss": 0.6122,
      "step": 7230
    },
    {
      "epoch": 0.45707070707070707,
      "grad_norm": 0.5291949510574341,
      "learning_rate": 0.00018003866533285054,
      "loss": 0.5609,
      "step": 7240
    },
    {
      "epoch": 0.4577020202020202,
      "grad_norm": 0.7327936887741089,
      "learning_rate": 0.00017997733499618365,
      "loss": 0.5842,
      "step": 7250
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.4167431890964508,
      "learning_rate": 0.00017991592106654186,
      "loss": 0.8178,
      "step": 7260
    },
    {
      "epoch": 0.45896464646464646,
      "grad_norm": 0.44018709659576416,
      "learning_rate": 0.00017985442360811553,
      "loss": 0.6972,
      "step": 7270
    },
    {
      "epoch": 0.4595959595959596,
      "grad_norm": 0.4272181987762451,
      "learning_rate": 0.00017979284268518228,
      "loss": 0.6256,
      "step": 7280
    },
    {
      "epoch": 0.4602272727272727,
      "grad_norm": 0.47338584065437317,
      "learning_rate": 0.00017973117836210702,
      "loss": 0.5708,
      "step": 7290
    },
    {
      "epoch": 0.46085858585858586,
      "grad_norm": 0.7453432083129883,
      "learning_rate": 0.00017966943070334184,
      "loss": 0.5963,
      "step": 7300
    },
    {
      "epoch": 0.461489898989899,
      "grad_norm": 0.4029906094074249,
      "learning_rate": 0.00017960759977342586,
      "loss": 0.8439,
      "step": 7310
    },
    {
      "epoch": 0.4621212121212121,
      "grad_norm": 0.4686511158943176,
      "learning_rate": 0.0001795456856369853,
      "loss": 0.7012,
      "step": 7320
    },
    {
      "epoch": 0.46275252525252525,
      "grad_norm": 0.5853444337844849,
      "learning_rate": 0.00017948368835873332,
      "loss": 0.6224,
      "step": 7330
    },
    {
      "epoch": 0.4633838383838384,
      "grad_norm": 0.49485859274864197,
      "learning_rate": 0.00017942160800347,
      "loss": 0.5585,
      "step": 7340
    },
    {
      "epoch": 0.4640151515151515,
      "grad_norm": 0.7539072632789612,
      "learning_rate": 0.00017935944463608227,
      "loss": 0.5921,
      "step": 7350
    },
    {
      "epoch": 0.46464646464646464,
      "grad_norm": 0.40767204761505127,
      "learning_rate": 0.00017929719832154376,
      "loss": 0.8888,
      "step": 7360
    },
    {
      "epoch": 0.4652777777777778,
      "grad_norm": 0.4092262089252472,
      "learning_rate": 0.00017923486912491482,
      "loss": 0.7058,
      "step": 7370
    },
    {
      "epoch": 0.4659090909090909,
      "grad_norm": 0.4639788269996643,
      "learning_rate": 0.0001791724571113425,
      "loss": 0.6453,
      "step": 7380
    },
    {
      "epoch": 0.46654040404040403,
      "grad_norm": 0.4480709433555603,
      "learning_rate": 0.0001791099623460603,
      "loss": 0.5668,
      "step": 7390
    },
    {
      "epoch": 0.4671717171717172,
      "grad_norm": 0.7305364012718201,
      "learning_rate": 0.00017904738489438836,
      "loss": 0.5917,
      "step": 7400
    },
    {
      "epoch": 0.4678030303030303,
      "grad_norm": 0.3693515956401825,
      "learning_rate": 0.00017898472482173302,
      "loss": 0.8628,
      "step": 7410
    },
    {
      "epoch": 0.4684343434343434,
      "grad_norm": 0.4362490773200989,
      "learning_rate": 0.0001789219821935872,
      "loss": 0.7138,
      "step": 7420
    },
    {
      "epoch": 0.4690656565656566,
      "grad_norm": 0.4394151568412781,
      "learning_rate": 0.00017885915707552998,
      "loss": 0.6391,
      "step": 7430
    },
    {
      "epoch": 0.4696969696969697,
      "grad_norm": 0.4939110577106476,
      "learning_rate": 0.0001787962495332267,
      "loss": 0.5599,
      "step": 7440
    },
    {
      "epoch": 0.4703282828282828,
      "grad_norm": 0.6626918911933899,
      "learning_rate": 0.00017873325963242888,
      "loss": 0.5729,
      "step": 7450
    },
    {
      "epoch": 0.47095959595959597,
      "grad_norm": 0.37481653690338135,
      "learning_rate": 0.00017867018743897406,
      "loss": 0.8057,
      "step": 7460
    },
    {
      "epoch": 0.4715909090909091,
      "grad_norm": 0.41604503989219666,
      "learning_rate": 0.0001786070330187858,
      "loss": 0.7318,
      "step": 7470
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 0.4455414116382599,
      "learning_rate": 0.00017854379643787363,
      "loss": 0.6404,
      "step": 7480
    },
    {
      "epoch": 0.47285353535353536,
      "grad_norm": 0.499744176864624,
      "learning_rate": 0.000178480477762333,
      "loss": 0.5729,
      "step": 7490
    },
    {
      "epoch": 0.4734848484848485,
      "grad_norm": 0.7276878952980042,
      "learning_rate": 0.00017841707705834505,
      "loss": 0.5506,
      "step": 7500
    },
    {
      "epoch": 0.4741161616161616,
      "grad_norm": 0.4075162410736084,
      "learning_rate": 0.00017835359439217677,
      "loss": 0.8468,
      "step": 7510
    },
    {
      "epoch": 0.47474747474747475,
      "grad_norm": 0.4850195646286011,
      "learning_rate": 0.00017829002983018075,
      "loss": 0.7046,
      "step": 7520
    },
    {
      "epoch": 0.4753787878787879,
      "grad_norm": 0.47122296690940857,
      "learning_rate": 0.0001782263834387952,
      "loss": 0.6316,
      "step": 7530
    },
    {
      "epoch": 0.476010101010101,
      "grad_norm": 0.5207087993621826,
      "learning_rate": 0.00017816265528454382,
      "loss": 0.566,
      "step": 7540
    },
    {
      "epoch": 0.47664141414141414,
      "grad_norm": 0.7464081645011902,
      "learning_rate": 0.0001780988454340359,
      "loss": 0.6024,
      "step": 7550
    },
    {
      "epoch": 0.4772727272727273,
      "grad_norm": 0.3892696797847748,
      "learning_rate": 0.00017803495395396593,
      "loss": 0.8202,
      "step": 7560
    },
    {
      "epoch": 0.4779040404040404,
      "grad_norm": 0.44050052762031555,
      "learning_rate": 0.0001779709809111139,
      "loss": 0.6752,
      "step": 7570
    },
    {
      "epoch": 0.47853535353535354,
      "grad_norm": 0.44174250960350037,
      "learning_rate": 0.00017790692637234488,
      "loss": 0.5805,
      "step": 7580
    },
    {
      "epoch": 0.4791666666666667,
      "grad_norm": 0.49928709864616394,
      "learning_rate": 0.00017784279040460924,
      "loss": 0.5642,
      "step": 7590
    },
    {
      "epoch": 0.4797979797979798,
      "grad_norm": 0.877342164516449,
      "learning_rate": 0.00017777857307494247,
      "loss": 0.6199,
      "step": 7600
    },
    {
      "epoch": 0.48042929292929293,
      "grad_norm": 0.3973231315612793,
      "learning_rate": 0.000177714274450465,
      "loss": 0.8459,
      "step": 7610
    },
    {
      "epoch": 0.4810606060606061,
      "grad_norm": 0.4296554625034332,
      "learning_rate": 0.00017764989459838232,
      "loss": 0.7289,
      "step": 7620
    },
    {
      "epoch": 0.48169191919191917,
      "grad_norm": 0.4503133296966553,
      "learning_rate": 0.00017758543358598476,
      "loss": 0.5974,
      "step": 7630
    },
    {
      "epoch": 0.4823232323232323,
      "grad_norm": 0.5078423619270325,
      "learning_rate": 0.00017752089148064752,
      "loss": 0.5517,
      "step": 7640
    },
    {
      "epoch": 0.48295454545454547,
      "grad_norm": 0.7479671239852905,
      "learning_rate": 0.00017745626834983055,
      "loss": 0.6226,
      "step": 7650
    },
    {
      "epoch": 0.48358585858585856,
      "grad_norm": 0.40103450417518616,
      "learning_rate": 0.00017739156426107845,
      "loss": 0.8108,
      "step": 7660
    },
    {
      "epoch": 0.4842171717171717,
      "grad_norm": 0.43510350584983826,
      "learning_rate": 0.00017732677928202053,
      "loss": 0.721,
      "step": 7670
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 0.4472430348396301,
      "learning_rate": 0.00017726191348037054,
      "loss": 0.6138,
      "step": 7680
    },
    {
      "epoch": 0.48547979797979796,
      "grad_norm": 0.47048601508140564,
      "learning_rate": 0.00017719696692392677,
      "loss": 0.5537,
      "step": 7690
    },
    {
      "epoch": 0.4861111111111111,
      "grad_norm": 0.7483005523681641,
      "learning_rate": 0.0001771319396805719,
      "loss": 0.6265,
      "step": 7700
    },
    {
      "epoch": 0.48674242424242425,
      "grad_norm": 0.40934330224990845,
      "learning_rate": 0.00017706683181827295,
      "loss": 0.8419,
      "step": 7710
    },
    {
      "epoch": 0.48737373737373735,
      "grad_norm": 0.4250480830669403,
      "learning_rate": 0.00017700164340508117,
      "loss": 0.7138,
      "step": 7720
    },
    {
      "epoch": 0.4880050505050505,
      "grad_norm": 0.43944406509399414,
      "learning_rate": 0.0001769363745091321,
      "loss": 0.6422,
      "step": 7730
    },
    {
      "epoch": 0.48863636363636365,
      "grad_norm": 0.4884706139564514,
      "learning_rate": 0.00017687102519864525,
      "loss": 0.5816,
      "step": 7740
    },
    {
      "epoch": 0.48926767676767674,
      "grad_norm": 0.7424509525299072,
      "learning_rate": 0.0001768055955419243,
      "loss": 0.6111,
      "step": 7750
    },
    {
      "epoch": 0.4898989898989899,
      "grad_norm": 0.37708768248558044,
      "learning_rate": 0.0001767400856073569,
      "loss": 0.865,
      "step": 7760
    },
    {
      "epoch": 0.49053030303030304,
      "grad_norm": 0.478595495223999,
      "learning_rate": 0.00017667449546341453,
      "loss": 0.7542,
      "step": 7770
    },
    {
      "epoch": 0.4911616161616162,
      "grad_norm": 0.4069937765598297,
      "learning_rate": 0.00017660882517865254,
      "loss": 0.6246,
      "step": 7780
    },
    {
      "epoch": 0.4917929292929293,
      "grad_norm": 0.49260807037353516,
      "learning_rate": 0.00017654307482171014,
      "loss": 0.5716,
      "step": 7790
    },
    {
      "epoch": 0.49242424242424243,
      "grad_norm": 0.7252025604248047,
      "learning_rate": 0.00017647724446131005,
      "loss": 0.6012,
      "step": 7800
    },
    {
      "epoch": 0.4930555555555556,
      "grad_norm": 0.401273250579834,
      "learning_rate": 0.00017641133416625878,
      "loss": 0.8264,
      "step": 7810
    },
    {
      "epoch": 0.4936868686868687,
      "grad_norm": 0.38548046350479126,
      "learning_rate": 0.00017634534400544631,
      "loss": 0.7069,
      "step": 7820
    },
    {
      "epoch": 0.4943181818181818,
      "grad_norm": 0.437144011259079,
      "learning_rate": 0.00017627927404784607,
      "loss": 0.6243,
      "step": 7830
    },
    {
      "epoch": 0.494949494949495,
      "grad_norm": 0.4569688141345978,
      "learning_rate": 0.00017621312436251496,
      "loss": 0.5443,
      "step": 7840
    },
    {
      "epoch": 0.49558080808080807,
      "grad_norm": 0.6961487531661987,
      "learning_rate": 0.00017614689501859316,
      "loss": 0.5961,
      "step": 7850
    },
    {
      "epoch": 0.4962121212121212,
      "grad_norm": 0.40618544816970825,
      "learning_rate": 0.00017608058608530413,
      "loss": 0.8609,
      "step": 7860
    },
    {
      "epoch": 0.49684343434343436,
      "grad_norm": 0.41913264989852905,
      "learning_rate": 0.00017601419763195453,
      "loss": 0.6973,
      "step": 7870
    },
    {
      "epoch": 0.49747474747474746,
      "grad_norm": 0.4505308270454407,
      "learning_rate": 0.0001759477297279341,
      "loss": 0.6129,
      "step": 7880
    },
    {
      "epoch": 0.4981060606060606,
      "grad_norm": 0.4411943852901459,
      "learning_rate": 0.00017588118244271568,
      "loss": 0.5566,
      "step": 7890
    },
    {
      "epoch": 0.49873737373737376,
      "grad_norm": 0.7304840683937073,
      "learning_rate": 0.00017581455584585507,
      "loss": 0.5772,
      "step": 7900
    },
    {
      "epoch": 0.49936868686868685,
      "grad_norm": 0.3914807438850403,
      "learning_rate": 0.00017574785000699084,
      "loss": 0.8545,
      "step": 7910
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.4065745770931244,
      "learning_rate": 0.0001756810649958446,
      "loss": 0.648,
      "step": 7920
    },
    {
      "epoch": 0.5006313131313131,
      "grad_norm": 0.4493197202682495,
      "learning_rate": 0.00017561420088222054,
      "loss": 0.608,
      "step": 7930
    },
    {
      "epoch": 0.5012626262626263,
      "grad_norm": 0.4870010316371918,
      "learning_rate": 0.0001755472577360056,
      "loss": 0.5585,
      "step": 7940
    },
    {
      "epoch": 0.5018939393939394,
      "grad_norm": 0.7219800353050232,
      "learning_rate": 0.0001754802356271693,
      "loss": 0.595,
      "step": 7950
    },
    {
      "epoch": 0.5025252525252525,
      "grad_norm": 0.4148313105106354,
      "learning_rate": 0.00017541313462576368,
      "loss": 0.835,
      "step": 7960
    },
    {
      "epoch": 0.5031565656565656,
      "grad_norm": 0.4173157513141632,
      "learning_rate": 0.0001753459548019233,
      "loss": 0.6771,
      "step": 7970
    },
    {
      "epoch": 0.5037878787878788,
      "grad_norm": 0.4381641745567322,
      "learning_rate": 0.0001752786962258651,
      "loss": 0.6282,
      "step": 7980
    },
    {
      "epoch": 0.5044191919191919,
      "grad_norm": 0.4874686896800995,
      "learning_rate": 0.00017521135896788828,
      "loss": 0.5838,
      "step": 7990
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 0.7189522981643677,
      "learning_rate": 0.00017514394309837424,
      "loss": 0.5749,
      "step": 8000
    },
    {
      "epoch": 0.5050505050505051,
      "eval_loss": 0.6593838334083557,
      "eval_runtime": 48.7681,
      "eval_samples_per_second": 52.493,
      "eval_steps_per_second": 6.562,
      "step": 8000
    },
    {
      "epoch": 0.5056818181818182,
      "grad_norm": 0.40362119674682617,
      "learning_rate": 0.0001750764486877867,
      "loss": 0.8241,
      "step": 8010
    },
    {
      "epoch": 0.5063131313131313,
      "grad_norm": 0.40527528524398804,
      "learning_rate": 0.0001750088758066713,
      "loss": 0.7062,
      "step": 8020
    },
    {
      "epoch": 0.5069444444444444,
      "grad_norm": 0.4524995684623718,
      "learning_rate": 0.00017494122452565582,
      "loss": 0.6433,
      "step": 8030
    },
    {
      "epoch": 0.5075757575757576,
      "grad_norm": 0.4595588445663452,
      "learning_rate": 0.00017487349491544996,
      "loss": 0.5457,
      "step": 8040
    },
    {
      "epoch": 0.5082070707070707,
      "grad_norm": 0.8193140625953674,
      "learning_rate": 0.00017480568704684521,
      "loss": 0.5708,
      "step": 8050
    },
    {
      "epoch": 0.5088383838383839,
      "grad_norm": 0.39303097128868103,
      "learning_rate": 0.00017473780099071498,
      "loss": 0.8007,
      "step": 8060
    },
    {
      "epoch": 0.509469696969697,
      "grad_norm": 0.5261770486831665,
      "learning_rate": 0.0001746698368180143,
      "loss": 0.7026,
      "step": 8070
    },
    {
      "epoch": 0.51010101010101,
      "grad_norm": 0.4241460859775543,
      "learning_rate": 0.0001746017945997799,
      "loss": 0.6212,
      "step": 8080
    },
    {
      "epoch": 0.5107323232323232,
      "grad_norm": 0.572301983833313,
      "learning_rate": 0.00017453367440713007,
      "loss": 0.5531,
      "step": 8090
    },
    {
      "epoch": 0.5113636363636364,
      "grad_norm": 0.699672520160675,
      "learning_rate": 0.00017446547631126463,
      "loss": 0.6111,
      "step": 8100
    },
    {
      "epoch": 0.5119949494949495,
      "grad_norm": 0.39165499806404114,
      "learning_rate": 0.00017439720038346472,
      "loss": 0.8404,
      "step": 8110
    },
    {
      "epoch": 0.5126262626262627,
      "grad_norm": 0.42424654960632324,
      "learning_rate": 0.00017432884669509299,
      "loss": 0.7436,
      "step": 8120
    },
    {
      "epoch": 0.5132575757575758,
      "grad_norm": 0.40817728638648987,
      "learning_rate": 0.0001742604153175932,
      "loss": 0.643,
      "step": 8130
    },
    {
      "epoch": 0.5138888888888888,
      "grad_norm": 0.5217949748039246,
      "learning_rate": 0.00017419190632249053,
      "loss": 0.559,
      "step": 8140
    },
    {
      "epoch": 0.514520202020202,
      "grad_norm": 0.762198805809021,
      "learning_rate": 0.000174123319781391,
      "loss": 0.6006,
      "step": 8150
    },
    {
      "epoch": 0.5151515151515151,
      "grad_norm": 0.38980019092559814,
      "learning_rate": 0.0001740546557659819,
      "loss": 0.8727,
      "step": 8160
    },
    {
      "epoch": 0.5157828282828283,
      "grad_norm": 0.4095879793167114,
      "learning_rate": 0.00017398591434803143,
      "loss": 0.7248,
      "step": 8170
    },
    {
      "epoch": 0.5164141414141414,
      "grad_norm": 0.4323713779449463,
      "learning_rate": 0.0001739170955993887,
      "loss": 0.6227,
      "step": 8180
    },
    {
      "epoch": 0.5170454545454546,
      "grad_norm": 0.5170940160751343,
      "learning_rate": 0.0001738481995919836,
      "loss": 0.545,
      "step": 8190
    },
    {
      "epoch": 0.5176767676767676,
      "grad_norm": 0.7553445100784302,
      "learning_rate": 0.00017377922639782685,
      "loss": 0.5804,
      "step": 8200
    },
    {
      "epoch": 0.5183080808080808,
      "grad_norm": 0.3897041380405426,
      "learning_rate": 0.00017371017608900982,
      "loss": 0.7905,
      "step": 8210
    },
    {
      "epoch": 0.5189393939393939,
      "grad_norm": 0.4027312695980072,
      "learning_rate": 0.0001736410487377044,
      "loss": 0.7124,
      "step": 8220
    },
    {
      "epoch": 0.5195707070707071,
      "grad_norm": 0.45330488681793213,
      "learning_rate": 0.0001735718444161631,
      "loss": 0.6287,
      "step": 8230
    },
    {
      "epoch": 0.5202020202020202,
      "grad_norm": 0.5047810673713684,
      "learning_rate": 0.00017350256319671888,
      "loss": 0.5511,
      "step": 8240
    },
    {
      "epoch": 0.5208333333333334,
      "grad_norm": 0.8267665505409241,
      "learning_rate": 0.000173433205151785,
      "loss": 0.5785,
      "step": 8250
    },
    {
      "epoch": 0.5214646464646465,
      "grad_norm": 0.4081098139286041,
      "learning_rate": 0.0001733637703538551,
      "loss": 0.8635,
      "step": 8260
    },
    {
      "epoch": 0.5220959595959596,
      "grad_norm": 0.4186636209487915,
      "learning_rate": 0.000173294258875503,
      "loss": 0.7171,
      "step": 8270
    },
    {
      "epoch": 0.5227272727272727,
      "grad_norm": 0.45844393968582153,
      "learning_rate": 0.0001732246707893827,
      "loss": 0.6052,
      "step": 8280
    },
    {
      "epoch": 0.5233585858585859,
      "grad_norm": 0.49019795656204224,
      "learning_rate": 0.0001731550061682282,
      "loss": 0.5447,
      "step": 8290
    },
    {
      "epoch": 0.523989898989899,
      "grad_norm": 0.7179405093193054,
      "learning_rate": 0.00017308526508485352,
      "loss": 0.5493,
      "step": 8300
    },
    {
      "epoch": 0.5246212121212122,
      "grad_norm": 0.3960726261138916,
      "learning_rate": 0.0001730154476121527,
      "loss": 0.8244,
      "step": 8310
    },
    {
      "epoch": 0.5252525252525253,
      "grad_norm": 0.42610958218574524,
      "learning_rate": 0.00017294555382309947,
      "loss": 0.7059,
      "step": 8320
    },
    {
      "epoch": 0.5258838383838383,
      "grad_norm": 0.4537331163883209,
      "learning_rate": 0.00017287558379074747,
      "loss": 0.6328,
      "step": 8330
    },
    {
      "epoch": 0.5265151515151515,
      "grad_norm": 0.4490770995616913,
      "learning_rate": 0.0001728055375882299,
      "loss": 0.5711,
      "step": 8340
    },
    {
      "epoch": 0.5271464646464646,
      "grad_norm": 0.8724656105041504,
      "learning_rate": 0.00017273541528875966,
      "loss": 0.5561,
      "step": 8350
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 0.4099646210670471,
      "learning_rate": 0.00017266521696562913,
      "loss": 0.8467,
      "step": 8360
    },
    {
      "epoch": 0.5284090909090909,
      "grad_norm": 0.43507230281829834,
      "learning_rate": 0.0001725949426922102,
      "loss": 0.6966,
      "step": 8370
    },
    {
      "epoch": 0.5290404040404041,
      "grad_norm": 0.44010135531425476,
      "learning_rate": 0.00017252459254195413,
      "loss": 0.6428,
      "step": 8380
    },
    {
      "epoch": 0.5296717171717171,
      "grad_norm": 0.4776274263858795,
      "learning_rate": 0.00017245416658839152,
      "loss": 0.5646,
      "step": 8390
    },
    {
      "epoch": 0.5303030303030303,
      "grad_norm": 0.7282297611236572,
      "learning_rate": 0.00017238366490513206,
      "loss": 0.6113,
      "step": 8400
    },
    {
      "epoch": 0.5309343434343434,
      "grad_norm": 0.40660786628723145,
      "learning_rate": 0.00017231308756586477,
      "loss": 0.8328,
      "step": 8410
    },
    {
      "epoch": 0.5315656565656566,
      "grad_norm": 0.4930444359779358,
      "learning_rate": 0.00017224243464435766,
      "loss": 0.7137,
      "step": 8420
    },
    {
      "epoch": 0.5321969696969697,
      "grad_norm": 0.419710636138916,
      "learning_rate": 0.00017217170621445775,
      "loss": 0.6182,
      "step": 8430
    },
    {
      "epoch": 0.5328282828282829,
      "grad_norm": 0.43134984374046326,
      "learning_rate": 0.00017210090235009098,
      "loss": 0.5541,
      "step": 8440
    },
    {
      "epoch": 0.5334595959595959,
      "grad_norm": 0.7428593635559082,
      "learning_rate": 0.00017203002312526214,
      "loss": 0.5675,
      "step": 8450
    },
    {
      "epoch": 0.5340909090909091,
      "grad_norm": 0.3676441013813019,
      "learning_rate": 0.00017195906861405477,
      "loss": 0.82,
      "step": 8460
    },
    {
      "epoch": 0.5347222222222222,
      "grad_norm": 0.4131109118461609,
      "learning_rate": 0.00017188803889063112,
      "loss": 0.6912,
      "step": 8470
    },
    {
      "epoch": 0.5353535353535354,
      "grad_norm": 0.42191988229751587,
      "learning_rate": 0.00017181693402923206,
      "loss": 0.6251,
      "step": 8480
    },
    {
      "epoch": 0.5359848484848485,
      "grad_norm": 0.5003572106361389,
      "learning_rate": 0.00017174575410417697,
      "loss": 0.5622,
      "step": 8490
    },
    {
      "epoch": 0.5366161616161617,
      "grad_norm": 0.6801080107688904,
      "learning_rate": 0.0001716744991898637,
      "loss": 0.5706,
      "step": 8500
    },
    {
      "epoch": 0.5372474747474747,
      "grad_norm": 0.4159877300262451,
      "learning_rate": 0.00017160316936076848,
      "loss": 0.857,
      "step": 8510
    },
    {
      "epoch": 0.5378787878787878,
      "grad_norm": 0.4131840169429779,
      "learning_rate": 0.00017153176469144585,
      "loss": 0.6686,
      "step": 8520
    },
    {
      "epoch": 0.538510101010101,
      "grad_norm": 0.4587972164154053,
      "learning_rate": 0.00017146028525652856,
      "loss": 0.618,
      "step": 8530
    },
    {
      "epoch": 0.5391414141414141,
      "grad_norm": 0.49946752190589905,
      "learning_rate": 0.0001713887311307275,
      "loss": 0.5659,
      "step": 8540
    },
    {
      "epoch": 0.5397727272727273,
      "grad_norm": 0.7989489436149597,
      "learning_rate": 0.00017131710238883164,
      "loss": 0.601,
      "step": 8550
    },
    {
      "epoch": 0.5404040404040404,
      "grad_norm": 0.4137639105319977,
      "learning_rate": 0.00017124539910570792,
      "loss": 0.8267,
      "step": 8560
    },
    {
      "epoch": 0.5410353535353535,
      "grad_norm": 0.40802446007728577,
      "learning_rate": 0.0001711736213563012,
      "loss": 0.6989,
      "step": 8570
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.42450907826423645,
      "learning_rate": 0.00017110176921563425,
      "loss": 0.613,
      "step": 8580
    },
    {
      "epoch": 0.5422979797979798,
      "grad_norm": 0.43025633692741394,
      "learning_rate": 0.00017102984275880746,
      "loss": 0.5426,
      "step": 8590
    },
    {
      "epoch": 0.5429292929292929,
      "grad_norm": 0.7252299785614014,
      "learning_rate": 0.00017095784206099896,
      "loss": 0.5453,
      "step": 8600
    },
    {
      "epoch": 0.5435606060606061,
      "grad_norm": 0.3931393623352051,
      "learning_rate": 0.00017088576719746453,
      "loss": 0.7943,
      "step": 8610
    },
    {
      "epoch": 0.5441919191919192,
      "grad_norm": 0.41672444343566895,
      "learning_rate": 0.00017081361824353736,
      "loss": 0.6863,
      "step": 8620
    },
    {
      "epoch": 0.5448232323232324,
      "grad_norm": 0.4729630649089813,
      "learning_rate": 0.00017074139527462818,
      "loss": 0.6312,
      "step": 8630
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.46601128578186035,
      "learning_rate": 0.000170669098366225,
      "loss": 0.5579,
      "step": 8640
    },
    {
      "epoch": 0.5460858585858586,
      "grad_norm": 0.662912130355835,
      "learning_rate": 0.00017059672759389317,
      "loss": 0.5915,
      "step": 8650
    },
    {
      "epoch": 0.5467171717171717,
      "grad_norm": 0.4025403559207916,
      "learning_rate": 0.0001705242830332752,
      "loss": 0.8172,
      "step": 8660
    },
    {
      "epoch": 0.5473484848484849,
      "grad_norm": 0.44232437014579773,
      "learning_rate": 0.00017045176476009074,
      "loss": 0.6968,
      "step": 8670
    },
    {
      "epoch": 0.547979797979798,
      "grad_norm": 0.4739529490470886,
      "learning_rate": 0.00017037917285013654,
      "loss": 0.627,
      "step": 8680
    },
    {
      "epoch": 0.5486111111111112,
      "grad_norm": 0.47864460945129395,
      "learning_rate": 0.00017030650737928627,
      "loss": 0.542,
      "step": 8690
    },
    {
      "epoch": 0.5492424242424242,
      "grad_norm": 0.8715550303459167,
      "learning_rate": 0.00017023376842349041,
      "loss": 0.605,
      "step": 8700
    },
    {
      "epoch": 0.5498737373737373,
      "grad_norm": 0.3950721025466919,
      "learning_rate": 0.00017016095605877637,
      "loss": 0.8682,
      "step": 8710
    },
    {
      "epoch": 0.5505050505050505,
      "grad_norm": 0.4288584291934967,
      "learning_rate": 0.00017008807036124828,
      "loss": 0.7111,
      "step": 8720
    },
    {
      "epoch": 0.5511363636363636,
      "grad_norm": 0.44164425134658813,
      "learning_rate": 0.0001700151114070868,
      "loss": 0.6264,
      "step": 8730
    },
    {
      "epoch": 0.5517676767676768,
      "grad_norm": 0.5102583169937134,
      "learning_rate": 0.00016994207927254924,
      "loss": 0.5521,
      "step": 8740
    },
    {
      "epoch": 0.55239898989899,
      "grad_norm": 0.7638320922851562,
      "learning_rate": 0.00016986897403396944,
      "loss": 0.6054,
      "step": 8750
    },
    {
      "epoch": 0.553030303030303,
      "grad_norm": 0.3824065029621124,
      "learning_rate": 0.00016979579576775758,
      "loss": 0.8212,
      "step": 8760
    },
    {
      "epoch": 0.5536616161616161,
      "grad_norm": 0.4153311848640442,
      "learning_rate": 0.00016972254455040021,
      "loss": 0.6977,
      "step": 8770
    },
    {
      "epoch": 0.5542929292929293,
      "grad_norm": 0.4622271656990051,
      "learning_rate": 0.0001696492204584601,
      "loss": 0.6432,
      "step": 8780
    },
    {
      "epoch": 0.5549242424242424,
      "grad_norm": 0.46364542841911316,
      "learning_rate": 0.00016957582356857617,
      "loss": 0.5359,
      "step": 8790
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.7733799815177917,
      "learning_rate": 0.00016950235395746347,
      "loss": 0.5804,
      "step": 8800
    },
    {
      "epoch": 0.5561868686868687,
      "grad_norm": 0.4072363078594208,
      "learning_rate": 0.0001694288117019131,
      "loss": 0.8071,
      "step": 8810
    },
    {
      "epoch": 0.5568181818181818,
      "grad_norm": 0.4458349645137787,
      "learning_rate": 0.000169355196878792,
      "loss": 0.7386,
      "step": 8820
    },
    {
      "epoch": 0.5574494949494949,
      "grad_norm": 0.4340202808380127,
      "learning_rate": 0.00016928150956504293,
      "loss": 0.626,
      "step": 8830
    },
    {
      "epoch": 0.5580808080808081,
      "grad_norm": 0.5389103293418884,
      "learning_rate": 0.0001692077498376846,
      "loss": 0.5511,
      "step": 8840
    },
    {
      "epoch": 0.5587121212121212,
      "grad_norm": 0.7130245566368103,
      "learning_rate": 0.00016913391777381124,
      "loss": 0.5601,
      "step": 8850
    },
    {
      "epoch": 0.5593434343434344,
      "grad_norm": 0.3813481330871582,
      "learning_rate": 0.00016906001345059273,
      "loss": 0.8142,
      "step": 8860
    },
    {
      "epoch": 0.5599747474747475,
      "grad_norm": 0.402855783700943,
      "learning_rate": 0.00016898603694527443,
      "loss": 0.6998,
      "step": 8870
    },
    {
      "epoch": 0.5606060606060606,
      "grad_norm": 0.42272812128067017,
      "learning_rate": 0.00016891198833517729,
      "loss": 0.6375,
      "step": 8880
    },
    {
      "epoch": 0.5612373737373737,
      "grad_norm": 0.45923393964767456,
      "learning_rate": 0.00016883786769769752,
      "loss": 0.5539,
      "step": 8890
    },
    {
      "epoch": 0.5618686868686869,
      "grad_norm": 0.6411272287368774,
      "learning_rate": 0.00016876367511030655,
      "loss": 0.6131,
      "step": 8900
    },
    {
      "epoch": 0.5625,
      "grad_norm": 0.418174684047699,
      "learning_rate": 0.00016868941065055116,
      "loss": 0.834,
      "step": 8910
    },
    {
      "epoch": 0.5631313131313131,
      "grad_norm": 0.45342710614204407,
      "learning_rate": 0.00016861507439605317,
      "loss": 0.6697,
      "step": 8920
    },
    {
      "epoch": 0.5637626262626263,
      "grad_norm": 0.47424378991127014,
      "learning_rate": 0.00016854066642450942,
      "loss": 0.6038,
      "step": 8930
    },
    {
      "epoch": 0.5643939393939394,
      "grad_norm": 0.4774588942527771,
      "learning_rate": 0.00016846618681369178,
      "loss": 0.5556,
      "step": 8940
    },
    {
      "epoch": 0.5650252525252525,
      "grad_norm": 0.7819096446037292,
      "learning_rate": 0.00016839163564144694,
      "loss": 0.6157,
      "step": 8950
    },
    {
      "epoch": 0.5656565656565656,
      "grad_norm": 0.4091052711009979,
      "learning_rate": 0.0001683170129856964,
      "loss": 0.7817,
      "step": 8960
    },
    {
      "epoch": 0.5662878787878788,
      "grad_norm": 0.42634662985801697,
      "learning_rate": 0.00016824231892443635,
      "loss": 0.6909,
      "step": 8970
    },
    {
      "epoch": 0.5669191919191919,
      "grad_norm": 0.4536641836166382,
      "learning_rate": 0.0001681675535357377,
      "loss": 0.6285,
      "step": 8980
    },
    {
      "epoch": 0.5675505050505051,
      "grad_norm": 0.43508604168891907,
      "learning_rate": 0.00016809271689774584,
      "loss": 0.5481,
      "step": 8990
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 0.736126720905304,
      "learning_rate": 0.00016801780908868062,
      "loss": 0.5695,
      "step": 9000
    },
    {
      "epoch": 0.5681818181818182,
      "eval_loss": 0.6593811511993408,
      "eval_runtime": 48.2626,
      "eval_samples_per_second": 53.043,
      "eval_steps_per_second": 6.63,
      "step": 9000
    },
    {
      "epoch": 0.5688131313131313,
      "grad_norm": 0.40754011273384094,
      "learning_rate": 0.00016794283018683632,
      "loss": 0.8828,
      "step": 9010
    },
    {
      "epoch": 0.5694444444444444,
      "grad_norm": 0.418109267950058,
      "learning_rate": 0.00016786778027058153,
      "loss": 0.6702,
      "step": 9020
    },
    {
      "epoch": 0.5700757575757576,
      "grad_norm": 0.4586217999458313,
      "learning_rate": 0.00016779265941835897,
      "loss": 0.6155,
      "step": 9030
    },
    {
      "epoch": 0.5707070707070707,
      "grad_norm": 0.4530629813671112,
      "learning_rate": 0.00016771746770868567,
      "loss": 0.554,
      "step": 9040
    },
    {
      "epoch": 0.5713383838383839,
      "grad_norm": 0.8037557005882263,
      "learning_rate": 0.00016764220522015263,
      "loss": 0.5491,
      "step": 9050
    },
    {
      "epoch": 0.571969696969697,
      "grad_norm": 0.39894017577171326,
      "learning_rate": 0.0001675668720314248,
      "loss": 0.857,
      "step": 9060
    },
    {
      "epoch": 0.57260101010101,
      "grad_norm": 0.411564439535141,
      "learning_rate": 0.00016749146822124097,
      "loss": 0.7131,
      "step": 9070
    },
    {
      "epoch": 0.5732323232323232,
      "grad_norm": 0.48400771617889404,
      "learning_rate": 0.00016741599386841397,
      "loss": 0.6269,
      "step": 9080
    },
    {
      "epoch": 0.5738636363636364,
      "grad_norm": 0.5027517676353455,
      "learning_rate": 0.00016734044905183012,
      "loss": 0.5362,
      "step": 9090
    },
    {
      "epoch": 0.5744949494949495,
      "grad_norm": 0.6592738628387451,
      "learning_rate": 0.00016726483385044958,
      "loss": 0.5662,
      "step": 9100
    },
    {
      "epoch": 0.5751262626262627,
      "grad_norm": 0.4060373604297638,
      "learning_rate": 0.0001671891483433059,
      "loss": 0.8301,
      "step": 9110
    },
    {
      "epoch": 0.5757575757575758,
      "grad_norm": 0.3880513906478882,
      "learning_rate": 0.00016711339260950622,
      "loss": 0.6702,
      "step": 9120
    },
    {
      "epoch": 0.5763888888888888,
      "grad_norm": 0.44029301404953003,
      "learning_rate": 0.0001670375667282311,
      "loss": 0.5996,
      "step": 9130
    },
    {
      "epoch": 0.577020202020202,
      "grad_norm": 0.49652960896492004,
      "learning_rate": 0.00016696167077873435,
      "loss": 0.5398,
      "step": 9140
    },
    {
      "epoch": 0.5776515151515151,
      "grad_norm": 0.8214009404182434,
      "learning_rate": 0.00016688570484034307,
      "loss": 0.5692,
      "step": 9150
    },
    {
      "epoch": 0.5782828282828283,
      "grad_norm": 0.4050739109516144,
      "learning_rate": 0.00016680966899245748,
      "loss": 0.8201,
      "step": 9160
    },
    {
      "epoch": 0.5789141414141414,
      "grad_norm": 0.4729868471622467,
      "learning_rate": 0.00016673356331455084,
      "loss": 0.6898,
      "step": 9170
    },
    {
      "epoch": 0.5795454545454546,
      "grad_norm": 0.44679656624794006,
      "learning_rate": 0.00016665738788616946,
      "loss": 0.6077,
      "step": 9180
    },
    {
      "epoch": 0.5801767676767676,
      "grad_norm": 0.519686758518219,
      "learning_rate": 0.0001665811427869326,
      "loss": 0.5239,
      "step": 9190
    },
    {
      "epoch": 0.5808080808080808,
      "grad_norm": 0.8407021164894104,
      "learning_rate": 0.00016650482809653217,
      "loss": 0.5679,
      "step": 9200
    },
    {
      "epoch": 0.5814393939393939,
      "grad_norm": 0.4038034975528717,
      "learning_rate": 0.000166428443894733,
      "loss": 0.8112,
      "step": 9210
    },
    {
      "epoch": 0.5820707070707071,
      "grad_norm": 0.40916162729263306,
      "learning_rate": 0.00016635199026137243,
      "loss": 0.7022,
      "step": 9220
    },
    {
      "epoch": 0.5827020202020202,
      "grad_norm": 0.4553644359111786,
      "learning_rate": 0.00016627546727636044,
      "loss": 0.6011,
      "step": 9230
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.4171730875968933,
      "learning_rate": 0.00016619887501967954,
      "loss": 0.5571,
      "step": 9240
    },
    {
      "epoch": 0.5839646464646465,
      "grad_norm": 0.6887004971504211,
      "learning_rate": 0.00016612221357138453,
      "loss": 0.5461,
      "step": 9250
    },
    {
      "epoch": 0.5845959595959596,
      "grad_norm": 0.39235255122184753,
      "learning_rate": 0.00016604548301160264,
      "loss": 0.8251,
      "step": 9260
    },
    {
      "epoch": 0.5852272727272727,
      "grad_norm": 0.4149330258369446,
      "learning_rate": 0.00016596868342053325,
      "loss": 0.7012,
      "step": 9270
    },
    {
      "epoch": 0.5858585858585859,
      "grad_norm": 0.44631659984588623,
      "learning_rate": 0.000165891814878448,
      "loss": 0.6192,
      "step": 9280
    },
    {
      "epoch": 0.586489898989899,
      "grad_norm": 0.48827654123306274,
      "learning_rate": 0.00016581487746569043,
      "loss": 0.5614,
      "step": 9290
    },
    {
      "epoch": 0.5871212121212122,
      "grad_norm": 0.700650155544281,
      "learning_rate": 0.0001657378712626762,
      "loss": 0.5932,
      "step": 9300
    },
    {
      "epoch": 0.5877525252525253,
      "grad_norm": 0.401726096868515,
      "learning_rate": 0.00016566079634989289,
      "loss": 0.8649,
      "step": 9310
    },
    {
      "epoch": 0.5883838383838383,
      "grad_norm": 0.4175480008125305,
      "learning_rate": 0.00016558365280789977,
      "loss": 0.6959,
      "step": 9320
    },
    {
      "epoch": 0.5890151515151515,
      "grad_norm": 0.45647260546684265,
      "learning_rate": 0.00016550644071732794,
      "loss": 0.6476,
      "step": 9330
    },
    {
      "epoch": 0.5896464646464646,
      "grad_norm": 0.5066510438919067,
      "learning_rate": 0.0001654291601588801,
      "loss": 0.5569,
      "step": 9340
    },
    {
      "epoch": 0.5902777777777778,
      "grad_norm": 0.7696815133094788,
      "learning_rate": 0.00016535181121333058,
      "loss": 0.5497,
      "step": 9350
    },
    {
      "epoch": 0.5909090909090909,
      "grad_norm": 0.3719872236251831,
      "learning_rate": 0.00016527439396152508,
      "loss": 0.8772,
      "step": 9360
    },
    {
      "epoch": 0.5915404040404041,
      "grad_norm": 0.41837212443351746,
      "learning_rate": 0.0001651969084843808,
      "loss": 0.689,
      "step": 9370
    },
    {
      "epoch": 0.5921717171717171,
      "grad_norm": 0.4964379072189331,
      "learning_rate": 0.00016511935486288618,
      "loss": 0.6317,
      "step": 9380
    },
    {
      "epoch": 0.5928030303030303,
      "grad_norm": 0.4800480604171753,
      "learning_rate": 0.0001650417331781009,
      "loss": 0.5686,
      "step": 9390
    },
    {
      "epoch": 0.5934343434343434,
      "grad_norm": 0.6853623986244202,
      "learning_rate": 0.0001649640435111558,
      "loss": 0.5951,
      "step": 9400
    },
    {
      "epoch": 0.5940656565656566,
      "grad_norm": 0.4005526006221771,
      "learning_rate": 0.00016488628594325277,
      "loss": 0.8825,
      "step": 9410
    },
    {
      "epoch": 0.5946969696969697,
      "grad_norm": 0.4181072413921356,
      "learning_rate": 0.0001648084605556647,
      "loss": 0.7092,
      "step": 9420
    },
    {
      "epoch": 0.5953282828282829,
      "grad_norm": 0.45258957147598267,
      "learning_rate": 0.00016473056742973526,
      "loss": 0.6359,
      "step": 9430
    },
    {
      "epoch": 0.5959595959595959,
      "grad_norm": 0.5074449181556702,
      "learning_rate": 0.00016465260664687902,
      "loss": 0.5982,
      "step": 9440
    },
    {
      "epoch": 0.5965909090909091,
      "grad_norm": 0.6714109182357788,
      "learning_rate": 0.0001645745782885813,
      "loss": 0.5945,
      "step": 9450
    },
    {
      "epoch": 0.5972222222222222,
      "grad_norm": 0.3858293294906616,
      "learning_rate": 0.00016449648243639788,
      "loss": 0.9039,
      "step": 9460
    },
    {
      "epoch": 0.5978535353535354,
      "grad_norm": 0.4088331162929535,
      "learning_rate": 0.0001644183191719553,
      "loss": 0.6983,
      "step": 9470
    },
    {
      "epoch": 0.5984848484848485,
      "grad_norm": 0.5117666721343994,
      "learning_rate": 0.00016434008857695037,
      "loss": 0.617,
      "step": 9480
    },
    {
      "epoch": 0.5991161616161617,
      "grad_norm": 1.4203832149505615,
      "learning_rate": 0.0001642617907331504,
      "loss": 0.5723,
      "step": 9490
    },
    {
      "epoch": 0.5997474747474747,
      "grad_norm": 0.748537540435791,
      "learning_rate": 0.00016418342572239292,
      "loss": 0.5823,
      "step": 9500
    },
    {
      "epoch": 0.6003787878787878,
      "grad_norm": 0.3906300365924835,
      "learning_rate": 0.0001641049936265857,
      "loss": 0.8102,
      "step": 9510
    },
    {
      "epoch": 0.601010101010101,
      "grad_norm": 0.3894568681716919,
      "learning_rate": 0.00016402649452770666,
      "loss": 0.7095,
      "step": 9520
    },
    {
      "epoch": 0.6016414141414141,
      "grad_norm": 0.5114539861679077,
      "learning_rate": 0.00016394792850780364,
      "loss": 0.6168,
      "step": 9530
    },
    {
      "epoch": 0.6022727272727273,
      "grad_norm": 0.44795045256614685,
      "learning_rate": 0.00016386929564899457,
      "loss": 0.5497,
      "step": 9540
    },
    {
      "epoch": 0.6029040404040404,
      "grad_norm": 0.6674001216888428,
      "learning_rate": 0.0001637905960334671,
      "loss": 0.5791,
      "step": 9550
    },
    {
      "epoch": 0.6035353535353535,
      "grad_norm": 0.4138997793197632,
      "learning_rate": 0.00016371182974347876,
      "loss": 0.8902,
      "step": 9560
    },
    {
      "epoch": 0.6041666666666666,
      "grad_norm": 0.42814698815345764,
      "learning_rate": 0.0001636329968613567,
      "loss": 0.7284,
      "step": 9570
    },
    {
      "epoch": 0.6047979797979798,
      "grad_norm": 0.45881250500679016,
      "learning_rate": 0.00016355409746949778,
      "loss": 0.6397,
      "step": 9580
    },
    {
      "epoch": 0.6054292929292929,
      "grad_norm": 0.4848956763744354,
      "learning_rate": 0.0001634751316503682,
      "loss": 0.5542,
      "step": 9590
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 0.7009353041648865,
      "learning_rate": 0.00016339609948650376,
      "loss": 0.5685,
      "step": 9600
    },
    {
      "epoch": 0.6066919191919192,
      "grad_norm": 0.40330901741981506,
      "learning_rate": 0.0001633170010605095,
      "loss": 0.8716,
      "step": 9610
    },
    {
      "epoch": 0.6073232323232324,
      "grad_norm": 0.41515830159187317,
      "learning_rate": 0.00016323783645505975,
      "loss": 0.7044,
      "step": 9620
    },
    {
      "epoch": 0.6079545454545454,
      "grad_norm": 0.4399987459182739,
      "learning_rate": 0.000163158605752898,
      "loss": 0.6127,
      "step": 9630
    },
    {
      "epoch": 0.6085858585858586,
      "grad_norm": 0.49775514006614685,
      "learning_rate": 0.0001630793090368369,
      "loss": 0.591,
      "step": 9640
    },
    {
      "epoch": 0.6092171717171717,
      "grad_norm": 0.7093541622161865,
      "learning_rate": 0.00016299994638975797,
      "loss": 0.5954,
      "step": 9650
    },
    {
      "epoch": 0.6098484848484849,
      "grad_norm": 0.377464234828949,
      "learning_rate": 0.0001629205178946118,
      "loss": 0.8391,
      "step": 9660
    },
    {
      "epoch": 0.610479797979798,
      "grad_norm": 0.4342964291572571,
      "learning_rate": 0.00016284102363441758,
      "loss": 0.6909,
      "step": 9670
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.4547266662120819,
      "learning_rate": 0.00016276146369226347,
      "loss": 0.6177,
      "step": 9680
    },
    {
      "epoch": 0.6117424242424242,
      "grad_norm": 0.4499117136001587,
      "learning_rate": 0.00016268183815130614,
      "loss": 0.5563,
      "step": 9690
    },
    {
      "epoch": 0.6123737373737373,
      "grad_norm": 0.7207030653953552,
      "learning_rate": 0.00016260214709477088,
      "loss": 0.5483,
      "step": 9700
    },
    {
      "epoch": 0.6130050505050505,
      "grad_norm": 0.3764795958995819,
      "learning_rate": 0.00016252239060595146,
      "loss": 0.8413,
      "step": 9710
    },
    {
      "epoch": 0.6136363636363636,
      "grad_norm": 0.4311712384223938,
      "learning_rate": 0.00016244256876821,
      "loss": 0.7314,
      "step": 9720
    },
    {
      "epoch": 0.6142676767676768,
      "grad_norm": 0.44231313467025757,
      "learning_rate": 0.00016236268166497695,
      "loss": 0.6315,
      "step": 9730
    },
    {
      "epoch": 0.61489898989899,
      "grad_norm": 0.45901212096214294,
      "learning_rate": 0.000162282729379751,
      "loss": 0.5372,
      "step": 9740
    },
    {
      "epoch": 0.615530303030303,
      "grad_norm": 0.7797766327857971,
      "learning_rate": 0.0001622027119960989,
      "loss": 0.5506,
      "step": 9750
    },
    {
      "epoch": 0.6161616161616161,
      "grad_norm": 0.42284026741981506,
      "learning_rate": 0.0001621226295976555,
      "loss": 0.8449,
      "step": 9760
    },
    {
      "epoch": 0.6167929292929293,
      "grad_norm": 0.4076808989048004,
      "learning_rate": 0.00016204248226812365,
      "loss": 0.7383,
      "step": 9770
    },
    {
      "epoch": 0.6174242424242424,
      "grad_norm": 0.4630199670791626,
      "learning_rate": 0.0001619622700912739,
      "loss": 0.6429,
      "step": 9780
    },
    {
      "epoch": 0.6180555555555556,
      "grad_norm": 0.4392853081226349,
      "learning_rate": 0.00016188199315094473,
      "loss": 0.5363,
      "step": 9790
    },
    {
      "epoch": 0.6186868686868687,
      "grad_norm": 0.7172645926475525,
      "learning_rate": 0.0001618016515310423,
      "loss": 0.5798,
      "step": 9800
    },
    {
      "epoch": 0.6193181818181818,
      "grad_norm": 0.4024762511253357,
      "learning_rate": 0.0001617212453155403,
      "loss": 0.8118,
      "step": 9810
    },
    {
      "epoch": 0.6199494949494949,
      "grad_norm": 0.4318335950374603,
      "learning_rate": 0.00016164077458847995,
      "loss": 0.7119,
      "step": 9820
    },
    {
      "epoch": 0.6205808080808081,
      "grad_norm": 0.4452487826347351,
      "learning_rate": 0.00016156023943396998,
      "loss": 0.6239,
      "step": 9830
    },
    {
      "epoch": 0.6212121212121212,
      "grad_norm": 0.41459718346595764,
      "learning_rate": 0.0001614796399361864,
      "loss": 0.5426,
      "step": 9840
    },
    {
      "epoch": 0.6218434343434344,
      "grad_norm": 0.7804189324378967,
      "learning_rate": 0.00016139897617937238,
      "loss": 0.5894,
      "step": 9850
    },
    {
      "epoch": 0.6224747474747475,
      "grad_norm": 0.4095876216888428,
      "learning_rate": 0.00016131824824783847,
      "loss": 0.8689,
      "step": 9860
    },
    {
      "epoch": 0.6231060606060606,
      "grad_norm": 0.43314075469970703,
      "learning_rate": 0.00016123745622596212,
      "loss": 0.7274,
      "step": 9870
    },
    {
      "epoch": 0.6237373737373737,
      "grad_norm": 0.4404180347919464,
      "learning_rate": 0.0001611566001981878,
      "loss": 0.6286,
      "step": 9880
    },
    {
      "epoch": 0.6243686868686869,
      "grad_norm": 0.4505847096443176,
      "learning_rate": 0.00016107568024902697,
      "loss": 0.562,
      "step": 9890
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.7044526934623718,
      "learning_rate": 0.00016099469646305777,
      "loss": 0.6021,
      "step": 9900
    },
    {
      "epoch": 0.6256313131313131,
      "grad_norm": 0.4008995592594147,
      "learning_rate": 0.00016091364892492516,
      "loss": 0.871,
      "step": 9910
    },
    {
      "epoch": 0.6262626262626263,
      "grad_norm": 0.42819517850875854,
      "learning_rate": 0.00016083253771934065,
      "loss": 0.7022,
      "step": 9920
    },
    {
      "epoch": 0.6268939393939394,
      "grad_norm": 0.48512062430381775,
      "learning_rate": 0.00016075136293108245,
      "loss": 0.6254,
      "step": 9930
    },
    {
      "epoch": 0.6275252525252525,
      "grad_norm": 0.4631679058074951,
      "learning_rate": 0.000160670124644995,
      "loss": 0.5529,
      "step": 9940
    },
    {
      "epoch": 0.6281565656565656,
      "grad_norm": 0.7802549004554749,
      "learning_rate": 0.00016058882294598932,
      "loss": 0.5648,
      "step": 9950
    },
    {
      "epoch": 0.6287878787878788,
      "grad_norm": 0.39236515760421753,
      "learning_rate": 0.00016050745791904256,
      "loss": 0.875,
      "step": 9960
    },
    {
      "epoch": 0.6294191919191919,
      "grad_norm": 0.42303916811943054,
      "learning_rate": 0.00016042602964919816,
      "loss": 0.6808,
      "step": 9970
    },
    {
      "epoch": 0.6300505050505051,
      "grad_norm": 0.45704135298728943,
      "learning_rate": 0.0001603445382215656,
      "loss": 0.6498,
      "step": 9980
    },
    {
      "epoch": 0.6306818181818182,
      "grad_norm": 0.549001157283783,
      "learning_rate": 0.00016026298372132046,
      "loss": 0.5937,
      "step": 9990
    },
    {
      "epoch": 0.6313131313131313,
      "grad_norm": 0.7374277710914612,
      "learning_rate": 0.00016018136623370408,
      "loss": 0.5853,
      "step": 10000
    },
    {
      "epoch": 0.6313131313131313,
      "eval_loss": 0.6491060853004456,
      "eval_runtime": 48.5718,
      "eval_samples_per_second": 52.705,
      "eval_steps_per_second": 6.588,
      "step": 10000
    },
    {
      "epoch": 0.6319444444444444,
      "grad_norm": 0.4134005308151245,
      "learning_rate": 0.00016009968584402383,
      "loss": 0.8366,
      "step": 10010
    },
    {
      "epoch": 0.6325757575757576,
      "grad_norm": 0.41340354084968567,
      "learning_rate": 0.00016001794263765265,
      "loss": 0.7029,
      "step": 10020
    },
    {
      "epoch": 0.6332070707070707,
      "grad_norm": 0.46128830313682556,
      "learning_rate": 0.0001599361367000293,
      "loss": 0.6117,
      "step": 10030
    },
    {
      "epoch": 0.6338383838383839,
      "grad_norm": 0.4525044858455658,
      "learning_rate": 0.000159854268116658,
      "loss": 0.5816,
      "step": 10040
    },
    {
      "epoch": 0.634469696969697,
      "grad_norm": 0.6742972731590271,
      "learning_rate": 0.00015977233697310838,
      "loss": 0.5805,
      "step": 10050
    },
    {
      "epoch": 0.63510101010101,
      "grad_norm": 0.4027636647224426,
      "learning_rate": 0.00015969034335501568,
      "loss": 0.8887,
      "step": 10060
    },
    {
      "epoch": 0.6357323232323232,
      "grad_norm": 0.40957918763160706,
      "learning_rate": 0.00015960828734808027,
      "loss": 0.7007,
      "step": 10070
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.4484250843524933,
      "learning_rate": 0.00015952616903806775,
      "loss": 0.6153,
      "step": 10080
    },
    {
      "epoch": 0.6369949494949495,
      "grad_norm": 0.4638078212738037,
      "learning_rate": 0.00015944398851080885,
      "loss": 0.5678,
      "step": 10090
    },
    {
      "epoch": 0.6376262626262627,
      "grad_norm": 0.7539432644844055,
      "learning_rate": 0.00015936174585219937,
      "loss": 0.571,
      "step": 10100
    },
    {
      "epoch": 0.6382575757575758,
      "grad_norm": 0.4094163179397583,
      "learning_rate": 0.00015927944114820005,
      "loss": 0.8053,
      "step": 10110
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 0.4153628945350647,
      "learning_rate": 0.00015919707448483638,
      "loss": 0.7251,
      "step": 10120
    },
    {
      "epoch": 0.639520202020202,
      "grad_norm": 0.444825679063797,
      "learning_rate": 0.0001591146459481987,
      "loss": 0.6064,
      "step": 10130
    },
    {
      "epoch": 0.6401515151515151,
      "grad_norm": 0.5130223035812378,
      "learning_rate": 0.00015903215562444202,
      "loss": 0.5782,
      "step": 10140
    },
    {
      "epoch": 0.6407828282828283,
      "grad_norm": 0.8112844824790955,
      "learning_rate": 0.00015894960359978593,
      "loss": 0.5684,
      "step": 10150
    },
    {
      "epoch": 0.6414141414141414,
      "grad_norm": 0.3906404972076416,
      "learning_rate": 0.00015886698996051446,
      "loss": 0.8059,
      "step": 10160
    },
    {
      "epoch": 0.6420454545454546,
      "grad_norm": 0.4094386696815491,
      "learning_rate": 0.00015878431479297603,
      "loss": 0.7113,
      "step": 10170
    },
    {
      "epoch": 0.6426767676767676,
      "grad_norm": 0.4461304843425751,
      "learning_rate": 0.0001587015781835835,
      "loss": 0.6163,
      "step": 10180
    },
    {
      "epoch": 0.6433080808080808,
      "grad_norm": 0.5449422597885132,
      "learning_rate": 0.00015861878021881384,
      "loss": 0.5889,
      "step": 10190
    },
    {
      "epoch": 0.6439393939393939,
      "grad_norm": 0.7111195921897888,
      "learning_rate": 0.0001585359209852081,
      "loss": 0.5663,
      "step": 10200
    },
    {
      "epoch": 0.6445707070707071,
      "grad_norm": 0.38970449566841125,
      "learning_rate": 0.00015845300056937153,
      "loss": 0.9297,
      "step": 10210
    },
    {
      "epoch": 0.6452020202020202,
      "grad_norm": 0.3895106911659241,
      "learning_rate": 0.0001583700190579732,
      "loss": 0.6933,
      "step": 10220
    },
    {
      "epoch": 0.6458333333333334,
      "grad_norm": 0.44756925106048584,
      "learning_rate": 0.00015828697653774606,
      "loss": 0.5975,
      "step": 10230
    },
    {
      "epoch": 0.6464646464646465,
      "grad_norm": 0.4932132661342621,
      "learning_rate": 0.00015820387309548685,
      "loss": 0.5333,
      "step": 10240
    },
    {
      "epoch": 0.6470959595959596,
      "grad_norm": 0.6717033982276917,
      "learning_rate": 0.00015812070881805602,
      "loss": 0.5793,
      "step": 10250
    },
    {
      "epoch": 0.6477272727272727,
      "grad_norm": 0.41299739480018616,
      "learning_rate": 0.00015803748379237747,
      "loss": 0.8505,
      "step": 10260
    },
    {
      "epoch": 0.6483585858585859,
      "grad_norm": 0.43026790022850037,
      "learning_rate": 0.00015795419810543882,
      "loss": 0.6965,
      "step": 10270
    },
    {
      "epoch": 0.648989898989899,
      "grad_norm": 0.4509985148906708,
      "learning_rate": 0.00015787085184429086,
      "loss": 0.6101,
      "step": 10280
    },
    {
      "epoch": 0.6496212121212122,
      "grad_norm": 0.42193737626075745,
      "learning_rate": 0.0001577874450960478,
      "loss": 0.5496,
      "step": 10290
    },
    {
      "epoch": 0.6502525252525253,
      "grad_norm": 0.6997890472412109,
      "learning_rate": 0.00015770397794788706,
      "loss": 0.5654,
      "step": 10300
    },
    {
      "epoch": 0.6508838383838383,
      "grad_norm": 0.4125702381134033,
      "learning_rate": 0.00015762045048704927,
      "loss": 0.8605,
      "step": 10310
    },
    {
      "epoch": 0.6515151515151515,
      "grad_norm": 0.41746801137924194,
      "learning_rate": 0.00015753686280083797,
      "loss": 0.6797,
      "step": 10320
    },
    {
      "epoch": 0.6521464646464646,
      "grad_norm": 0.4512667953968048,
      "learning_rate": 0.00015745321497661973,
      "loss": 0.6213,
      "step": 10330
    },
    {
      "epoch": 0.6527777777777778,
      "grad_norm": 0.40959131717681885,
      "learning_rate": 0.00015736950710182392,
      "loss": 0.5567,
      "step": 10340
    },
    {
      "epoch": 0.6534090909090909,
      "grad_norm": 0.8048436641693115,
      "learning_rate": 0.00015728573926394271,
      "loss": 0.6027,
      "step": 10350
    },
    {
      "epoch": 0.6540404040404041,
      "grad_norm": 0.4183523654937744,
      "learning_rate": 0.00015720191155053098,
      "loss": 0.8296,
      "step": 10360
    },
    {
      "epoch": 0.6546717171717171,
      "grad_norm": 0.40642276406288147,
      "learning_rate": 0.0001571180240492061,
      "loss": 0.6738,
      "step": 10370
    },
    {
      "epoch": 0.6553030303030303,
      "grad_norm": 0.4204613268375397,
      "learning_rate": 0.00015703407684764802,
      "loss": 0.6118,
      "step": 10380
    },
    {
      "epoch": 0.6559343434343434,
      "grad_norm": 0.4878062307834625,
      "learning_rate": 0.00015695007003359908,
      "loss": 0.5069,
      "step": 10390
    },
    {
      "epoch": 0.6565656565656566,
      "grad_norm": 0.7318800687789917,
      "learning_rate": 0.0001568660036948638,
      "loss": 0.577,
      "step": 10400
    },
    {
      "epoch": 0.6571969696969697,
      "grad_norm": 0.3963751196861267,
      "learning_rate": 0.0001567818779193091,
      "loss": 0.8429,
      "step": 10410
    },
    {
      "epoch": 0.6578282828282829,
      "grad_norm": 0.4163340628147125,
      "learning_rate": 0.0001566976927948639,
      "loss": 0.7041,
      "step": 10420
    },
    {
      "epoch": 0.6584595959595959,
      "grad_norm": 0.43579572439193726,
      "learning_rate": 0.0001566134484095192,
      "loss": 0.6076,
      "step": 10430
    },
    {
      "epoch": 0.6590909090909091,
      "grad_norm": 0.449829638004303,
      "learning_rate": 0.00015652914485132796,
      "loss": 0.5491,
      "step": 10440
    },
    {
      "epoch": 0.6597222222222222,
      "grad_norm": 0.7012022137641907,
      "learning_rate": 0.00015644478220840492,
      "loss": 0.5856,
      "step": 10450
    },
    {
      "epoch": 0.6603535353535354,
      "grad_norm": 0.3992641568183899,
      "learning_rate": 0.00015636036056892663,
      "loss": 0.8582,
      "step": 10460
    },
    {
      "epoch": 0.6609848484848485,
      "grad_norm": 0.4417213201522827,
      "learning_rate": 0.00015627588002113134,
      "loss": 0.6796,
      "step": 10470
    },
    {
      "epoch": 0.6616161616161617,
      "grad_norm": 0.4921712875366211,
      "learning_rate": 0.00015619134065331873,
      "loss": 0.6323,
      "step": 10480
    },
    {
      "epoch": 0.6622474747474747,
      "grad_norm": 0.4822782278060913,
      "learning_rate": 0.0001561067425538501,
      "loss": 0.5424,
      "step": 10490
    },
    {
      "epoch": 0.6628787878787878,
      "grad_norm": 0.7411313056945801,
      "learning_rate": 0.00015602208581114808,
      "loss": 0.5416,
      "step": 10500
    },
    {
      "epoch": 0.663510101010101,
      "grad_norm": 0.37582096457481384,
      "learning_rate": 0.00015593737051369655,
      "loss": 0.7964,
      "step": 10510
    },
    {
      "epoch": 0.6641414141414141,
      "grad_norm": 0.425912082195282,
      "learning_rate": 0.00015585259675004076,
      "loss": 0.7283,
      "step": 10520
    },
    {
      "epoch": 0.6647727272727273,
      "grad_norm": 0.44259706139564514,
      "learning_rate": 0.00015576776460878686,
      "loss": 0.6295,
      "step": 10530
    },
    {
      "epoch": 0.6654040404040404,
      "grad_norm": 0.5002459287643433,
      "learning_rate": 0.0001556828741786021,
      "loss": 0.5771,
      "step": 10540
    },
    {
      "epoch": 0.6660353535353535,
      "grad_norm": 0.7993687987327576,
      "learning_rate": 0.00015559792554821472,
      "loss": 0.5881,
      "step": 10550
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.40294739603996277,
      "learning_rate": 0.00015551291880641372,
      "loss": 0.8131,
      "step": 10560
    },
    {
      "epoch": 0.6672979797979798,
      "grad_norm": 0.39747753739356995,
      "learning_rate": 0.00015542785404204883,
      "loss": 0.6885,
      "step": 10570
    },
    {
      "epoch": 0.6679292929292929,
      "grad_norm": 0.44849663972854614,
      "learning_rate": 0.00015534273134403047,
      "loss": 0.6006,
      "step": 10580
    },
    {
      "epoch": 0.6685606060606061,
      "grad_norm": 0.4960520267486572,
      "learning_rate": 0.00015525755080132955,
      "loss": 0.5394,
      "step": 10590
    },
    {
      "epoch": 0.6691919191919192,
      "grad_norm": 0.7240284085273743,
      "learning_rate": 0.0001551723125029775,
      "loss": 0.5415,
      "step": 10600
    },
    {
      "epoch": 0.6698232323232324,
      "grad_norm": 0.42753252387046814,
      "learning_rate": 0.00015508701653806615,
      "loss": 0.8985,
      "step": 10610
    },
    {
      "epoch": 0.6704545454545454,
      "grad_norm": 0.4255945086479187,
      "learning_rate": 0.0001550016629957475,
      "loss": 0.7391,
      "step": 10620
    },
    {
      "epoch": 0.6710858585858586,
      "grad_norm": 0.4581747055053711,
      "learning_rate": 0.0001549162519652338,
      "loss": 0.6371,
      "step": 10630
    },
    {
      "epoch": 0.6717171717171717,
      "grad_norm": 0.47391682863235474,
      "learning_rate": 0.00015483078353579734,
      "loss": 0.5613,
      "step": 10640
    },
    {
      "epoch": 0.6723484848484849,
      "grad_norm": 0.7088243961334229,
      "learning_rate": 0.00015474525779677047,
      "loss": 0.5917,
      "step": 10650
    },
    {
      "epoch": 0.672979797979798,
      "grad_norm": 0.3993774950504303,
      "learning_rate": 0.00015465967483754538,
      "loss": 0.8392,
      "step": 10660
    },
    {
      "epoch": 0.6736111111111112,
      "grad_norm": 0.44586288928985596,
      "learning_rate": 0.00015457403474757405,
      "loss": 0.7113,
      "step": 10670
    },
    {
      "epoch": 0.6742424242424242,
      "grad_norm": 0.4333907961845398,
      "learning_rate": 0.0001544883376163683,
      "loss": 0.6121,
      "step": 10680
    },
    {
      "epoch": 0.6748737373737373,
      "grad_norm": 0.5133373141288757,
      "learning_rate": 0.00015440258353349945,
      "loss": 0.5476,
      "step": 10690
    },
    {
      "epoch": 0.6755050505050505,
      "grad_norm": 0.7431175708770752,
      "learning_rate": 0.00015431677258859837,
      "loss": 0.5921,
      "step": 10700
    },
    {
      "epoch": 0.6761363636363636,
      "grad_norm": 0.38381972908973694,
      "learning_rate": 0.00015423090487135535,
      "loss": 0.8419,
      "step": 10710
    },
    {
      "epoch": 0.6767676767676768,
      "grad_norm": 0.3975052237510681,
      "learning_rate": 0.00015414498047152007,
      "loss": 0.6904,
      "step": 10720
    },
    {
      "epoch": 0.67739898989899,
      "grad_norm": 0.47594496607780457,
      "learning_rate": 0.00015405899947890146,
      "loss": 0.6211,
      "step": 10730
    },
    {
      "epoch": 0.678030303030303,
      "grad_norm": 0.4444319009780884,
      "learning_rate": 0.0001539729619833675,
      "loss": 0.5258,
      "step": 10740
    },
    {
      "epoch": 0.6786616161616161,
      "grad_norm": 0.8110677599906921,
      "learning_rate": 0.00015388686807484532,
      "loss": 0.5782,
      "step": 10750
    },
    {
      "epoch": 0.6792929292929293,
      "grad_norm": 0.41576510667800903,
      "learning_rate": 0.000153800717843321,
      "loss": 0.846,
      "step": 10760
    },
    {
      "epoch": 0.6799242424242424,
      "grad_norm": 0.4272156059741974,
      "learning_rate": 0.00015371451137883945,
      "loss": 0.7194,
      "step": 10770
    },
    {
      "epoch": 0.6805555555555556,
      "grad_norm": 0.41754400730133057,
      "learning_rate": 0.00015362824877150443,
      "loss": 0.5876,
      "step": 10780
    },
    {
      "epoch": 0.6811868686868687,
      "grad_norm": 0.45785632729530334,
      "learning_rate": 0.0001535419301114783,
      "loss": 0.5353,
      "step": 10790
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 0.6939946413040161,
      "learning_rate": 0.00015345555548898206,
      "loss": 0.6168,
      "step": 10800
    },
    {
      "epoch": 0.6824494949494949,
      "grad_norm": 0.4056255519390106,
      "learning_rate": 0.00015336912499429515,
      "loss": 0.8028,
      "step": 10810
    },
    {
      "epoch": 0.6830808080808081,
      "grad_norm": 0.42950379848480225,
      "learning_rate": 0.00015328263871775544,
      "loss": 0.6876,
      "step": 10820
    },
    {
      "epoch": 0.6837121212121212,
      "grad_norm": 0.44323211908340454,
      "learning_rate": 0.0001531960967497592,
      "loss": 0.6174,
      "step": 10830
    },
    {
      "epoch": 0.6843434343434344,
      "grad_norm": 0.4476737380027771,
      "learning_rate": 0.00015310949918076068,
      "loss": 0.558,
      "step": 10840
    },
    {
      "epoch": 0.6849747474747475,
      "grad_norm": 0.8406587839126587,
      "learning_rate": 0.00015302284610127246,
      "loss": 0.5578,
      "step": 10850
    },
    {
      "epoch": 0.6856060606060606,
      "grad_norm": 0.3932664096355438,
      "learning_rate": 0.00015293613760186502,
      "loss": 0.8351,
      "step": 10860
    },
    {
      "epoch": 0.6862373737373737,
      "grad_norm": 0.4036397933959961,
      "learning_rate": 0.00015284937377316684,
      "loss": 0.7026,
      "step": 10870
    },
    {
      "epoch": 0.6868686868686869,
      "grad_norm": 0.4404904544353485,
      "learning_rate": 0.00015276255470586417,
      "loss": 0.5911,
      "step": 10880
    },
    {
      "epoch": 0.6875,
      "grad_norm": 0.43705394864082336,
      "learning_rate": 0.00015267568049070102,
      "loss": 0.5667,
      "step": 10890
    },
    {
      "epoch": 0.6881313131313131,
      "grad_norm": 0.7127418518066406,
      "learning_rate": 0.00015258875121847902,
      "loss": 0.546,
      "step": 10900
    },
    {
      "epoch": 0.6887626262626263,
      "grad_norm": 0.3927711546421051,
      "learning_rate": 0.00015250176698005744,
      "loss": 0.8294,
      "step": 10910
    },
    {
      "epoch": 0.6893939393939394,
      "grad_norm": 0.4161592125892639,
      "learning_rate": 0.00015241472786635288,
      "loss": 0.7196,
      "step": 10920
    },
    {
      "epoch": 0.6900252525252525,
      "grad_norm": 0.45967453718185425,
      "learning_rate": 0.0001523276339683393,
      "loss": 0.6355,
      "step": 10930
    },
    {
      "epoch": 0.6906565656565656,
      "grad_norm": 0.42279937863349915,
      "learning_rate": 0.0001522404853770481,
      "loss": 0.5373,
      "step": 10940
    },
    {
      "epoch": 0.6912878787878788,
      "grad_norm": 0.7267460227012634,
      "learning_rate": 0.00015215328218356756,
      "loss": 0.5707,
      "step": 10950
    },
    {
      "epoch": 0.6919191919191919,
      "grad_norm": 0.4057493507862091,
      "learning_rate": 0.00015206602447904327,
      "loss": 0.8605,
      "step": 10960
    },
    {
      "epoch": 0.6925505050505051,
      "grad_norm": 0.4684038460254669,
      "learning_rate": 0.00015197871235467765,
      "loss": 0.6985,
      "step": 10970
    },
    {
      "epoch": 0.6931818181818182,
      "grad_norm": 0.43274351954460144,
      "learning_rate": 0.00015189134590173014,
      "loss": 0.6551,
      "step": 10980
    },
    {
      "epoch": 0.6938131313131313,
      "grad_norm": 0.3959287703037262,
      "learning_rate": 0.00015180392521151677,
      "loss": 0.5457,
      "step": 10990
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 0.6273152828216553,
      "learning_rate": 0.0001517164503754105,
      "loss": 0.5929,
      "step": 11000
    },
    {
      "epoch": 0.6944444444444444,
      "eval_loss": 0.648476243019104,
      "eval_runtime": 48.1366,
      "eval_samples_per_second": 53.182,
      "eval_steps_per_second": 6.648,
      "step": 11000
    }
  ],
  "logging_steps": 10,
  "max_steps": 31680,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.0666135774429184e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}

{
  "best_global_step": 1000,
  "best_metric": 0.7599223852157593,
  "best_model_checkpoint": "math_tutor_model/math_sft_adapter/v1/checkpoint-1000",
  "epoch": 0.06313131313131314,
  "eval_steps": 1000,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "eval_loss": 1.7798073291778564,
      "eval_runtime": 43.9046,
      "eval_samples_per_second": 58.308,
      "eval_steps_per_second": 7.289,
      "step": 0
    },
    {
      "epoch": 6.313131313131313e-05,
      "grad_norm": 0.5149620175361633,
      "learning_rate": 0.0,
      "loss": 1.0152,
      "step": 1
    },
    {
      "epoch": 0.0006313131313131314,
      "grad_norm": 1.0590860843658447,
      "learning_rate": 1.8927444794952682e-06,
      "loss": 1.4023,
      "step": 10
    },
    {
      "epoch": 0.0012626262626262627,
      "grad_norm": 1.3984099626541138,
      "learning_rate": 3.995793901156677e-06,
      "loss": 1.5836,
      "step": 20
    },
    {
      "epoch": 0.001893939393939394,
      "grad_norm": 1.4594887495040894,
      "learning_rate": 6.098843322818087e-06,
      "loss": 1.6661,
      "step": 30
    },
    {
      "epoch": 0.0025252525252525255,
      "grad_norm": 1.486985206604004,
      "learning_rate": 8.201892744479495e-06,
      "loss": 1.682,
      "step": 40
    },
    {
      "epoch": 0.0031565656565656565,
      "grad_norm": 2.7685816287994385,
      "learning_rate": 1.0304942166140905e-05,
      "loss": 1.7673,
      "step": 50
    },
    {
      "epoch": 0.003787878787878788,
      "grad_norm": 0.7351201772689819,
      "learning_rate": 1.2407991587802314e-05,
      "loss": 1.2202,
      "step": 60
    },
    {
      "epoch": 0.004419191919191919,
      "grad_norm": 1.0852664709091187,
      "learning_rate": 1.4511041009463724e-05,
      "loss": 1.1873,
      "step": 70
    },
    {
      "epoch": 0.005050505050505051,
      "grad_norm": 1.0271210670471191,
      "learning_rate": 1.661409043112513e-05,
      "loss": 1.008,
      "step": 80
    },
    {
      "epoch": 0.005681818181818182,
      "grad_norm": 0.8229262828826904,
      "learning_rate": 1.871713985278654e-05,
      "loss": 0.8739,
      "step": 90
    },
    {
      "epoch": 0.006313131313131313,
      "grad_norm": 1.5263402462005615,
      "learning_rate": 2.0820189274447953e-05,
      "loss": 0.9234,
      "step": 100
    },
    {
      "epoch": 0.006944444444444444,
      "grad_norm": 0.5606915354728699,
      "learning_rate": 2.292323869610936e-05,
      "loss": 0.9936,
      "step": 110
    },
    {
      "epoch": 0.007575757575757576,
      "grad_norm": 0.6033541560173035,
      "learning_rate": 2.5026288117770768e-05,
      "loss": 0.905,
      "step": 120
    },
    {
      "epoch": 0.008207070707070708,
      "grad_norm": 0.6884019374847412,
      "learning_rate": 2.7129337539432176e-05,
      "loss": 0.8487,
      "step": 130
    },
    {
      "epoch": 0.008838383838383838,
      "grad_norm": 0.7692236304283142,
      "learning_rate": 2.9232386961093587e-05,
      "loss": 0.7446,
      "step": 140
    },
    {
      "epoch": 0.00946969696969697,
      "grad_norm": 1.3975268602371216,
      "learning_rate": 3.1335436382754995e-05,
      "loss": 0.8583,
      "step": 150
    },
    {
      "epoch": 0.010101010101010102,
      "grad_norm": 0.5787802934646606,
      "learning_rate": 3.34384858044164e-05,
      "loss": 0.9908,
      "step": 160
    },
    {
      "epoch": 0.010732323232323232,
      "grad_norm": 0.5872055292129517,
      "learning_rate": 3.554153522607782e-05,
      "loss": 0.8556,
      "step": 170
    },
    {
      "epoch": 0.011363636363636364,
      "grad_norm": 0.6680330634117126,
      "learning_rate": 3.7644584647739225e-05,
      "loss": 0.8215,
      "step": 180
    },
    {
      "epoch": 0.011994949494949494,
      "grad_norm": 0.7466880679130554,
      "learning_rate": 3.974763406940063e-05,
      "loss": 0.7457,
      "step": 190
    },
    {
      "epoch": 0.012626262626262626,
      "grad_norm": 1.5690171718597412,
      "learning_rate": 4.185068349106204e-05,
      "loss": 0.7785,
      "step": 200
    },
    {
      "epoch": 0.013257575757575758,
      "grad_norm": 0.5490148067474365,
      "learning_rate": 4.395373291272345e-05,
      "loss": 0.9685,
      "step": 210
    },
    {
      "epoch": 0.013888888888888888,
      "grad_norm": 0.5898391008377075,
      "learning_rate": 4.6056782334384864e-05,
      "loss": 0.8678,
      "step": 220
    },
    {
      "epoch": 0.01452020202020202,
      "grad_norm": 0.696190595626831,
      "learning_rate": 4.815983175604627e-05,
      "loss": 0.7886,
      "step": 230
    },
    {
      "epoch": 0.015151515151515152,
      "grad_norm": 0.8659929633140564,
      "learning_rate": 5.026288117770768e-05,
      "loss": 0.6562,
      "step": 240
    },
    {
      "epoch": 0.015782828282828284,
      "grad_norm": 1.6167373657226562,
      "learning_rate": 5.236593059936909e-05,
      "loss": 0.7363,
      "step": 250
    },
    {
      "epoch": 0.016414141414141416,
      "grad_norm": 0.5406965613365173,
      "learning_rate": 5.44689800210305e-05,
      "loss": 0.9399,
      "step": 260
    },
    {
      "epoch": 0.017045454545454544,
      "grad_norm": 0.5902543067932129,
      "learning_rate": 5.657202944269191e-05,
      "loss": 0.8453,
      "step": 270
    },
    {
      "epoch": 0.017676767676767676,
      "grad_norm": 0.6322184801101685,
      "learning_rate": 5.867507886435332e-05,
      "loss": 0.7432,
      "step": 280
    },
    {
      "epoch": 0.018308080808080808,
      "grad_norm": 0.7221492528915405,
      "learning_rate": 6.0778128286014725e-05,
      "loss": 0.6931,
      "step": 290
    },
    {
      "epoch": 0.01893939393939394,
      "grad_norm": 1.1090178489685059,
      "learning_rate": 6.288117770767613e-05,
      "loss": 0.7144,
      "step": 300
    },
    {
      "epoch": 0.019570707070707072,
      "grad_norm": 0.5438726544380188,
      "learning_rate": 6.498422712933754e-05,
      "loss": 0.9249,
      "step": 310
    },
    {
      "epoch": 0.020202020202020204,
      "grad_norm": 0.5932243466377258,
      "learning_rate": 6.708727655099896e-05,
      "loss": 0.7985,
      "step": 320
    },
    {
      "epoch": 0.020833333333333332,
      "grad_norm": 0.6112260222434998,
      "learning_rate": 6.919032597266037e-05,
      "loss": 0.7474,
      "step": 330
    },
    {
      "epoch": 0.021464646464646464,
      "grad_norm": 0.6589015126228333,
      "learning_rate": 7.129337539432177e-05,
      "loss": 0.6726,
      "step": 340
    },
    {
      "epoch": 0.022095959595959596,
      "grad_norm": 1.19911789894104,
      "learning_rate": 7.339642481598317e-05,
      "loss": 0.7288,
      "step": 350
    },
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 0.5226461887359619,
      "learning_rate": 7.549947423764459e-05,
      "loss": 0.9047,
      "step": 360
    },
    {
      "epoch": 0.02335858585858586,
      "grad_norm": 0.5557655692100525,
      "learning_rate": 7.760252365930599e-05,
      "loss": 0.7987,
      "step": 370
    },
    {
      "epoch": 0.023989898989898988,
      "grad_norm": 0.5850486159324646,
      "learning_rate": 7.970557308096742e-05,
      "loss": 0.724,
      "step": 380
    },
    {
      "epoch": 0.02462121212121212,
      "grad_norm": 0.6732079982757568,
      "learning_rate": 8.180862250262882e-05,
      "loss": 0.688,
      "step": 390
    },
    {
      "epoch": 0.025252525252525252,
      "grad_norm": 1.0824929475784302,
      "learning_rate": 8.391167192429022e-05,
      "loss": 0.6987,
      "step": 400
    },
    {
      "epoch": 0.025883838383838384,
      "grad_norm": 0.48610448837280273,
      "learning_rate": 8.601472134595163e-05,
      "loss": 0.8898,
      "step": 410
    },
    {
      "epoch": 0.026515151515151516,
      "grad_norm": 0.5487117171287537,
      "learning_rate": 8.811777076761303e-05,
      "loss": 0.7905,
      "step": 420
    },
    {
      "epoch": 0.027146464646464648,
      "grad_norm": 0.5478590726852417,
      "learning_rate": 9.022082018927446e-05,
      "loss": 0.7159,
      "step": 430
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 0.6264445781707764,
      "learning_rate": 9.232386961093586e-05,
      "loss": 0.6454,
      "step": 440
    },
    {
      "epoch": 0.028409090909090908,
      "grad_norm": 1.202105164527893,
      "learning_rate": 9.442691903259728e-05,
      "loss": 0.6948,
      "step": 450
    },
    {
      "epoch": 0.02904040404040404,
      "grad_norm": 0.4790206551551819,
      "learning_rate": 9.652996845425868e-05,
      "loss": 0.9209,
      "step": 460
    },
    {
      "epoch": 0.029671717171717172,
      "grad_norm": 0.5063254237174988,
      "learning_rate": 9.863301787592008e-05,
      "loss": 0.8042,
      "step": 470
    },
    {
      "epoch": 0.030303030303030304,
      "grad_norm": 0.5612941384315491,
      "learning_rate": 0.0001007360672975815,
      "loss": 0.6803,
      "step": 480
    },
    {
      "epoch": 0.030934343434343436,
      "grad_norm": 0.567492663860321,
      "learning_rate": 0.00010283911671924291,
      "loss": 0.6397,
      "step": 490
    },
    {
      "epoch": 0.03156565656565657,
      "grad_norm": 1.1327983140945435,
      "learning_rate": 0.00010494216614090431,
      "loss": 0.701,
      "step": 500
    },
    {
      "epoch": 0.032196969696969696,
      "grad_norm": 0.49899259209632874,
      "learning_rate": 0.00010704521556256572,
      "loss": 0.9254,
      "step": 510
    },
    {
      "epoch": 0.03282828282828283,
      "grad_norm": 0.5031077265739441,
      "learning_rate": 0.00010914826498422714,
      "loss": 0.7712,
      "step": 520
    },
    {
      "epoch": 0.03345959595959596,
      "grad_norm": 0.5419979095458984,
      "learning_rate": 0.00011125131440588854,
      "loss": 0.731,
      "step": 530
    },
    {
      "epoch": 0.03409090909090909,
      "grad_norm": 0.622744083404541,
      "learning_rate": 0.00011335436382754996,
      "loss": 0.6472,
      "step": 540
    },
    {
      "epoch": 0.034722222222222224,
      "grad_norm": 1.0638457536697388,
      "learning_rate": 0.00011545741324921136,
      "loss": 0.6757,
      "step": 550
    },
    {
      "epoch": 0.03535353535353535,
      "grad_norm": 0.39383000135421753,
      "learning_rate": 0.00011756046267087277,
      "loss": 0.9569,
      "step": 560
    },
    {
      "epoch": 0.03598484848484849,
      "grad_norm": 0.4730018973350525,
      "learning_rate": 0.00011966351209253419,
      "loss": 0.7767,
      "step": 570
    },
    {
      "epoch": 0.036616161616161616,
      "grad_norm": 0.4968392848968506,
      "learning_rate": 0.00012176656151419559,
      "loss": 0.6895,
      "step": 580
    },
    {
      "epoch": 0.037247474747474744,
      "grad_norm": 0.5624285936355591,
      "learning_rate": 0.000123869610935857,
      "loss": 0.6517,
      "step": 590
    },
    {
      "epoch": 0.03787878787878788,
      "grad_norm": 1.023213505744934,
      "learning_rate": 0.0001259726603575184,
      "loss": 0.6725,
      "step": 600
    },
    {
      "epoch": 0.03851010101010101,
      "grad_norm": 0.43401893973350525,
      "learning_rate": 0.00012807570977917983,
      "loss": 0.9104,
      "step": 610
    },
    {
      "epoch": 0.039141414141414144,
      "grad_norm": 0.45413336157798767,
      "learning_rate": 0.00013017875920084122,
      "loss": 0.7729,
      "step": 620
    },
    {
      "epoch": 0.03977272727272727,
      "grad_norm": 0.5161787867546082,
      "learning_rate": 0.00013228180862250263,
      "loss": 0.6932,
      "step": 630
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": 0.5412288904190063,
      "learning_rate": 0.00013438485804416405,
      "loss": 0.6584,
      "step": 640
    },
    {
      "epoch": 0.041035353535353536,
      "grad_norm": 0.8710629343986511,
      "learning_rate": 0.00013648790746582546,
      "loss": 0.6475,
      "step": 650
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.3990229368209839,
      "learning_rate": 0.00013859095688748688,
      "loss": 0.8839,
      "step": 660
    },
    {
      "epoch": 0.0422979797979798,
      "grad_norm": 0.4656022787094116,
      "learning_rate": 0.00014069400630914826,
      "loss": 0.7698,
      "step": 670
    },
    {
      "epoch": 0.04292929292929293,
      "grad_norm": 0.5120764374732971,
      "learning_rate": 0.00014279705573080968,
      "loss": 0.7184,
      "step": 680
    },
    {
      "epoch": 0.043560606060606064,
      "grad_norm": 0.5285665392875671,
      "learning_rate": 0.0001449001051524711,
      "loss": 0.6276,
      "step": 690
    },
    {
      "epoch": 0.04419191919191919,
      "grad_norm": 0.8821708559989929,
      "learning_rate": 0.0001470031545741325,
      "loss": 0.7025,
      "step": 700
    },
    {
      "epoch": 0.04482323232323232,
      "grad_norm": 0.41884952783584595,
      "learning_rate": 0.00014910620399579392,
      "loss": 0.883,
      "step": 710
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.4143523573875427,
      "learning_rate": 0.0001512092534174553,
      "loss": 0.8106,
      "step": 720
    },
    {
      "epoch": 0.046085858585858584,
      "grad_norm": 0.4880114793777466,
      "learning_rate": 0.00015331230283911672,
      "loss": 0.733,
      "step": 730
    },
    {
      "epoch": 0.04671717171717172,
      "grad_norm": 0.4987323582172394,
      "learning_rate": 0.00015541535226077814,
      "loss": 0.6369,
      "step": 740
    },
    {
      "epoch": 0.04734848484848485,
      "grad_norm": 0.8602477312088013,
      "learning_rate": 0.00015751840168243955,
      "loss": 0.6624,
      "step": 750
    },
    {
      "epoch": 0.047979797979797977,
      "grad_norm": 0.39679214358329773,
      "learning_rate": 0.00015962145110410097,
      "loss": 0.8926,
      "step": 760
    },
    {
      "epoch": 0.04861111111111111,
      "grad_norm": 0.40994685888290405,
      "learning_rate": 0.00016172450052576236,
      "loss": 0.7803,
      "step": 770
    },
    {
      "epoch": 0.04924242424242424,
      "grad_norm": 0.4929552674293518,
      "learning_rate": 0.00016382754994742377,
      "loss": 0.7299,
      "step": 780
    },
    {
      "epoch": 0.049873737373737376,
      "grad_norm": 0.4801204204559326,
      "learning_rate": 0.00016593059936908516,
      "loss": 0.5913,
      "step": 790
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 0.8023887872695923,
      "learning_rate": 0.0001680336487907466,
      "loss": 0.7091,
      "step": 800
    },
    {
      "epoch": 0.05113636363636364,
      "grad_norm": 0.39392536878585815,
      "learning_rate": 0.00017013669821240801,
      "loss": 0.9108,
      "step": 810
    },
    {
      "epoch": 0.05176767676767677,
      "grad_norm": 0.44109266996383667,
      "learning_rate": 0.0001722397476340694,
      "loss": 0.784,
      "step": 820
    },
    {
      "epoch": 0.052398989898989896,
      "grad_norm": 0.45967403054237366,
      "learning_rate": 0.00017434279705573082,
      "loss": 0.6844,
      "step": 830
    },
    {
      "epoch": 0.05303030303030303,
      "grad_norm": 0.5077601075172424,
      "learning_rate": 0.00017644584647739223,
      "loss": 0.6371,
      "step": 840
    },
    {
      "epoch": 0.05366161616161616,
      "grad_norm": 0.9186100363731384,
      "learning_rate": 0.00017854889589905365,
      "loss": 0.6856,
      "step": 850
    },
    {
      "epoch": 0.054292929292929296,
      "grad_norm": 0.37829434871673584,
      "learning_rate": 0.00018065194532071506,
      "loss": 0.9513,
      "step": 860
    },
    {
      "epoch": 0.054924242424242424,
      "grad_norm": 0.43521374464035034,
      "learning_rate": 0.00018275499474237645,
      "loss": 0.7637,
      "step": 870
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 0.4747503399848938,
      "learning_rate": 0.00018485804416403786,
      "loss": 0.7121,
      "step": 880
    },
    {
      "epoch": 0.05618686868686869,
      "grad_norm": 0.5159175992012024,
      "learning_rate": 0.00018696109358569928,
      "loss": 0.6259,
      "step": 890
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 0.8636043667793274,
      "learning_rate": 0.0001890641430073607,
      "loss": 0.6775,
      "step": 900
    },
    {
      "epoch": 0.05744949494949495,
      "grad_norm": 0.38597747683525085,
      "learning_rate": 0.0001911671924290221,
      "loss": 0.904,
      "step": 910
    },
    {
      "epoch": 0.05808080808080808,
      "grad_norm": 0.43279117345809937,
      "learning_rate": 0.0001932702418506835,
      "loss": 0.779,
      "step": 920
    },
    {
      "epoch": 0.058712121212121215,
      "grad_norm": 0.4871803820133209,
      "learning_rate": 0.0001953732912723449,
      "loss": 0.7087,
      "step": 930
    },
    {
      "epoch": 0.059343434343434344,
      "grad_norm": 0.5029152035713196,
      "learning_rate": 0.00019747634069400632,
      "loss": 0.6251,
      "step": 940
    },
    {
      "epoch": 0.05997474747474747,
      "grad_norm": 0.8505828380584717,
      "learning_rate": 0.00019957939011566774,
      "loss": 0.6129,
      "step": 950
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": 0.3888472318649292,
      "learning_rate": 0.00019999996655333316,
      "loss": 0.8999,
      "step": 960
    },
    {
      "epoch": 0.061237373737373736,
      "grad_norm": 0.41575539112091064,
      "learning_rate": 0.00019999983067628733,
      "loss": 0.7818,
      "step": 970
    },
    {
      "epoch": 0.06186868686868687,
      "grad_norm": 0.45895668864250183,
      "learning_rate": 0.0001999995902785878,
      "loss": 0.6867,
      "step": 980
    },
    {
      "epoch": 0.0625,
      "grad_norm": 0.5550145506858826,
      "learning_rate": 0.0001999992453604858,
      "loss": 0.5901,
      "step": 990
    },
    {
      "epoch": 0.06313131313131314,
      "grad_norm": 0.9265154600143433,
      "learning_rate": 0.00019999879592234187,
      "loss": 0.6879,
      "step": 1000
    },
    {
      "epoch": 0.06313131313131314,
      "eval_loss": 0.7599223852157593,
      "eval_runtime": 43.7005,
      "eval_samples_per_second": 58.581,
      "eval_steps_per_second": 7.323,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 31680,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.125527469031424e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}

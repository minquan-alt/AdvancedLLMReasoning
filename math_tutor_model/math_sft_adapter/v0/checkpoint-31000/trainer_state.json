{
  "best_global_step": 31000,
  "best_metric": 0.5063484907150269,
  "best_model_checkpoint": "math_tutor_model/checkpoint-31000",
  "epoch": 1.9570707070707072,
  "eval_steps": 1000,
  "global_step": 31000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 6.313131313131313e-05,
      "grad_norm": 0.5092648267745972,
      "learning_rate": 0.0,
      "loss": 1.1781,
      "step": 1
    },
    {
      "epoch": 0.0006313131313131314,
      "grad_norm": 1.0038855075836182,
      "learning_rate": 1.8927444794952682e-06,
      "loss": 1.4598,
      "step": 10
    },
    {
      "epoch": 0.0012626262626262627,
      "grad_norm": 1.1605165004730225,
      "learning_rate": 3.995793901156677e-06,
      "loss": 1.6342,
      "step": 20
    },
    {
      "epoch": 0.001893939393939394,
      "grad_norm": 1.1726101636886597,
      "learning_rate": 6.098843322818087e-06,
      "loss": 1.6964,
      "step": 30
    },
    {
      "epoch": 0.0025252525252525255,
      "grad_norm": 1.453029990196228,
      "learning_rate": 8.201892744479495e-06,
      "loss": 1.7211,
      "step": 40
    },
    {
      "epoch": 0.0031565656565656565,
      "grad_norm": 1.8017903566360474,
      "learning_rate": 1.0304942166140905e-05,
      "loss": 1.7319,
      "step": 50
    },
    {
      "epoch": 0.003787878787878788,
      "grad_norm": 0.5794760584831238,
      "learning_rate": 1.2407991587802314e-05,
      "loss": 1.2407,
      "step": 60
    },
    {
      "epoch": 0.004419191919191919,
      "grad_norm": 0.7006651163101196,
      "learning_rate": 1.4511041009463724e-05,
      "loss": 1.3017,
      "step": 70
    },
    {
      "epoch": 0.005050505050505051,
      "grad_norm": 0.7171847224235535,
      "learning_rate": 1.661409043112513e-05,
      "loss": 1.2685,
      "step": 80
    },
    {
      "epoch": 0.005681818181818182,
      "grad_norm": 0.7153325080871582,
      "learning_rate": 1.871713985278654e-05,
      "loss": 1.178,
      "step": 90
    },
    {
      "epoch": 0.006313131313131313,
      "grad_norm": 1.319507122039795,
      "learning_rate": 2.0820189274447953e-05,
      "loss": 1.1983,
      "step": 100
    },
    {
      "epoch": 0.006944444444444444,
      "grad_norm": 0.4877150058746338,
      "learning_rate": 2.292323869610936e-05,
      "loss": 1.147,
      "step": 110
    },
    {
      "epoch": 0.007575757575757576,
      "grad_norm": 0.4692564010620117,
      "learning_rate": 2.5026288117770768e-05,
      "loss": 1.1314,
      "step": 120
    },
    {
      "epoch": 0.008207070707070708,
      "grad_norm": 0.5800968408584595,
      "learning_rate": 2.7129337539432176e-05,
      "loss": 1.1279,
      "step": 130
    },
    {
      "epoch": 0.008838383838383838,
      "grad_norm": 0.6486352682113647,
      "learning_rate": 2.9232386961093587e-05,
      "loss": 1.0681,
      "step": 140
    },
    {
      "epoch": 0.00946969696969697,
      "grad_norm": 1.0840227603912354,
      "learning_rate": 3.1335436382754995e-05,
      "loss": 1.101,
      "step": 150
    },
    {
      "epoch": 0.010101010101010102,
      "grad_norm": 0.46684345602989197,
      "learning_rate": 3.34384858044164e-05,
      "loss": 1.0765,
      "step": 160
    },
    {
      "epoch": 0.010732323232323232,
      "grad_norm": 0.5504432916641235,
      "learning_rate": 3.554153522607782e-05,
      "loss": 1.1021,
      "step": 170
    },
    {
      "epoch": 0.011363636363636364,
      "grad_norm": 0.5505733489990234,
      "learning_rate": 3.7644584647739225e-05,
      "loss": 1.0749,
      "step": 180
    },
    {
      "epoch": 0.011994949494949494,
      "grad_norm": 0.6061124801635742,
      "learning_rate": 3.974763406940063e-05,
      "loss": 1.0484,
      "step": 190
    },
    {
      "epoch": 0.012626262626262626,
      "grad_norm": 1.1371119022369385,
      "learning_rate": 4.185068349106204e-05,
      "loss": 1.059,
      "step": 200
    },
    {
      "epoch": 0.013257575757575758,
      "grad_norm": 0.5298175811767578,
      "learning_rate": 4.395373291272345e-05,
      "loss": 1.0797,
      "step": 210
    },
    {
      "epoch": 0.013888888888888888,
      "grad_norm": 0.5940582156181335,
      "learning_rate": 4.6056782334384864e-05,
      "loss": 1.0629,
      "step": 220
    },
    {
      "epoch": 0.01452020202020202,
      "grad_norm": 0.6109808683395386,
      "learning_rate": 4.815983175604627e-05,
      "loss": 1.0332,
      "step": 230
    },
    {
      "epoch": 0.015151515151515152,
      "grad_norm": 0.6498274207115173,
      "learning_rate": 5.026288117770768e-05,
      "loss": 1.0109,
      "step": 240
    },
    {
      "epoch": 0.015782828282828284,
      "grad_norm": 1.179053544998169,
      "learning_rate": 5.236593059936909e-05,
      "loss": 0.9896,
      "step": 250
    },
    {
      "epoch": 0.016414141414141416,
      "grad_norm": 0.5732084512710571,
      "learning_rate": 5.44689800210305e-05,
      "loss": 1.0723,
      "step": 260
    },
    {
      "epoch": 0.017045454545454544,
      "grad_norm": 0.559108316898346,
      "learning_rate": 5.657202944269191e-05,
      "loss": 1.0365,
      "step": 270
    },
    {
      "epoch": 0.017676767676767676,
      "grad_norm": 0.5784690976142883,
      "learning_rate": 5.867507886435332e-05,
      "loss": 1.0032,
      "step": 280
    },
    {
      "epoch": 0.018308080808080808,
      "grad_norm": 0.7329161167144775,
      "learning_rate": 6.0778128286014725e-05,
      "loss": 0.9627,
      "step": 290
    },
    {
      "epoch": 0.01893939393939394,
      "grad_norm": 0.9231019616127014,
      "learning_rate": 6.288117770767613e-05,
      "loss": 0.9993,
      "step": 300
    },
    {
      "epoch": 0.019570707070707072,
      "grad_norm": 0.5076531767845154,
      "learning_rate": 6.498422712933754e-05,
      "loss": 1.0374,
      "step": 310
    },
    {
      "epoch": 0.020202020202020204,
      "grad_norm": 0.5314714908599854,
      "learning_rate": 6.708727655099896e-05,
      "loss": 1.0332,
      "step": 320
    },
    {
      "epoch": 0.020833333333333332,
      "grad_norm": 0.5895927548408508,
      "learning_rate": 6.919032597266037e-05,
      "loss": 1.0028,
      "step": 330
    },
    {
      "epoch": 0.021464646464646464,
      "grad_norm": 0.6265683770179749,
      "learning_rate": 7.129337539432177e-05,
      "loss": 0.985,
      "step": 340
    },
    {
      "epoch": 0.022095959595959596,
      "grad_norm": 0.9994589686393738,
      "learning_rate": 7.339642481598317e-05,
      "loss": 0.9914,
      "step": 350
    },
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 0.4712381958961487,
      "learning_rate": 7.549947423764459e-05,
      "loss": 1.0135,
      "step": 360
    },
    {
      "epoch": 0.02335858585858586,
      "grad_norm": 0.5213924646377563,
      "learning_rate": 7.760252365930599e-05,
      "loss": 1.0187,
      "step": 370
    },
    {
      "epoch": 0.023989898989898988,
      "grad_norm": 0.552244246006012,
      "learning_rate": 7.970557308096742e-05,
      "loss": 1.0142,
      "step": 380
    },
    {
      "epoch": 0.02462121212121212,
      "grad_norm": 0.605170488357544,
      "learning_rate": 8.180862250262882e-05,
      "loss": 0.9547,
      "step": 390
    },
    {
      "epoch": 0.025252525252525252,
      "grad_norm": 0.8086289763450623,
      "learning_rate": 8.391167192429022e-05,
      "loss": 0.9541,
      "step": 400
    },
    {
      "epoch": 0.025883838383838384,
      "grad_norm": 0.4483409821987152,
      "learning_rate": 8.601472134595163e-05,
      "loss": 1.0191,
      "step": 410
    },
    {
      "epoch": 0.026515151515151516,
      "grad_norm": 0.4984561502933502,
      "learning_rate": 8.811777076761303e-05,
      "loss": 1.0208,
      "step": 420
    },
    {
      "epoch": 0.027146464646464648,
      "grad_norm": 0.49999457597732544,
      "learning_rate": 9.022082018927446e-05,
      "loss": 0.9828,
      "step": 430
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 0.5872477889060974,
      "learning_rate": 9.232386961093586e-05,
      "loss": 0.9625,
      "step": 440
    },
    {
      "epoch": 0.028409090909090908,
      "grad_norm": 0.9023650288581848,
      "learning_rate": 9.442691903259728e-05,
      "loss": 0.9486,
      "step": 450
    },
    {
      "epoch": 0.02904040404040404,
      "grad_norm": 0.4345221519470215,
      "learning_rate": 9.652996845425868e-05,
      "loss": 1.0117,
      "step": 460
    },
    {
      "epoch": 0.029671717171717172,
      "grad_norm": 0.4707091152667999,
      "learning_rate": 9.863301787592008e-05,
      "loss": 1.0279,
      "step": 470
    },
    {
      "epoch": 0.030303030303030304,
      "grad_norm": 0.47164538502693176,
      "learning_rate": 0.0001007360672975815,
      "loss": 0.9662,
      "step": 480
    },
    {
      "epoch": 0.030934343434343436,
      "grad_norm": 0.5436370968818665,
      "learning_rate": 0.00010283911671924291,
      "loss": 0.939,
      "step": 490
    },
    {
      "epoch": 0.03156565656565657,
      "grad_norm": 0.9068610668182373,
      "learning_rate": 0.00010494216614090431,
      "loss": 0.9748,
      "step": 500
    },
    {
      "epoch": 0.032196969696969696,
      "grad_norm": 0.4267598092556,
      "learning_rate": 0.00010704521556256572,
      "loss": 1.0205,
      "step": 510
    },
    {
      "epoch": 0.03282828282828283,
      "grad_norm": 0.440646231174469,
      "learning_rate": 0.00010914826498422714,
      "loss": 1.0077,
      "step": 520
    },
    {
      "epoch": 0.03345959595959596,
      "grad_norm": 0.4694534242153168,
      "learning_rate": 0.00011125131440588854,
      "loss": 0.9798,
      "step": 530
    },
    {
      "epoch": 0.03409090909090909,
      "grad_norm": 0.5469862818717957,
      "learning_rate": 0.00011335436382754996,
      "loss": 0.9485,
      "step": 540
    },
    {
      "epoch": 0.034722222222222224,
      "grad_norm": 0.873009204864502,
      "learning_rate": 0.00011545741324921136,
      "loss": 0.9576,
      "step": 550
    },
    {
      "epoch": 0.03535353535353535,
      "grad_norm": 0.4294698238372803,
      "learning_rate": 0.00011756046267087277,
      "loss": 0.9826,
      "step": 560
    },
    {
      "epoch": 0.03598484848484849,
      "grad_norm": 0.4311674237251282,
      "learning_rate": 0.00011966351209253419,
      "loss": 0.9763,
      "step": 570
    },
    {
      "epoch": 0.036616161616161616,
      "grad_norm": 0.46870291233062744,
      "learning_rate": 0.00012176656151419559,
      "loss": 0.9651,
      "step": 580
    },
    {
      "epoch": 0.037247474747474744,
      "grad_norm": 0.5374376773834229,
      "learning_rate": 0.000123869610935857,
      "loss": 0.9501,
      "step": 590
    },
    {
      "epoch": 0.03787878787878788,
      "grad_norm": 0.8207476735115051,
      "learning_rate": 0.0001259726603575184,
      "loss": 0.9524,
      "step": 600
    },
    {
      "epoch": 0.03851010101010101,
      "grad_norm": 0.4167443513870239,
      "learning_rate": 0.00012807570977917983,
      "loss": 1.0122,
      "step": 610
    },
    {
      "epoch": 0.039141414141414144,
      "grad_norm": 0.46308737993240356,
      "learning_rate": 0.00013017875920084122,
      "loss": 0.9779,
      "step": 620
    },
    {
      "epoch": 0.03977272727272727,
      "grad_norm": 0.4526059329509735,
      "learning_rate": 0.00013228180862250263,
      "loss": 0.9835,
      "step": 630
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": 0.5314608216285706,
      "learning_rate": 0.00013438485804416405,
      "loss": 0.9387,
      "step": 640
    },
    {
      "epoch": 0.041035353535353536,
      "grad_norm": 0.7409462332725525,
      "learning_rate": 0.00013648790746582546,
      "loss": 0.9155,
      "step": 650
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.39500120282173157,
      "learning_rate": 0.00013859095688748688,
      "loss": 1.045,
      "step": 660
    },
    {
      "epoch": 0.0422979797979798,
      "grad_norm": 0.4251002073287964,
      "learning_rate": 0.00014069400630914826,
      "loss": 1.0002,
      "step": 670
    },
    {
      "epoch": 0.04292929292929293,
      "grad_norm": 0.4866493344306946,
      "learning_rate": 0.00014279705573080968,
      "loss": 0.97,
      "step": 680
    },
    {
      "epoch": 0.043560606060606064,
      "grad_norm": 0.4947895407676697,
      "learning_rate": 0.0001449001051524711,
      "loss": 0.9361,
      "step": 690
    },
    {
      "epoch": 0.04419191919191919,
      "grad_norm": 0.8517065644264221,
      "learning_rate": 0.0001470031545741325,
      "loss": 0.9254,
      "step": 700
    },
    {
      "epoch": 0.04482323232323232,
      "grad_norm": 0.3971863389015198,
      "learning_rate": 0.00014910620399579392,
      "loss": 1.0174,
      "step": 710
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.4516993761062622,
      "learning_rate": 0.0001512092534174553,
      "loss": 0.9711,
      "step": 720
    },
    {
      "epoch": 0.046085858585858584,
      "grad_norm": 0.4635412395000458,
      "learning_rate": 0.00015331230283911672,
      "loss": 0.9625,
      "step": 730
    },
    {
      "epoch": 0.04671717171717172,
      "grad_norm": 0.5018364787101746,
      "learning_rate": 0.00015541535226077814,
      "loss": 0.9047,
      "step": 740
    },
    {
      "epoch": 0.04734848484848485,
      "grad_norm": 0.785271167755127,
      "learning_rate": 0.00015751840168243955,
      "loss": 0.9308,
      "step": 750
    },
    {
      "epoch": 0.047979797979797977,
      "grad_norm": 0.40073466300964355,
      "learning_rate": 0.00015962145110410097,
      "loss": 1.0348,
      "step": 760
    },
    {
      "epoch": 0.04861111111111111,
      "grad_norm": 0.42523932456970215,
      "learning_rate": 0.00016172450052576236,
      "loss": 0.9751,
      "step": 770
    },
    {
      "epoch": 0.04924242424242424,
      "grad_norm": 0.47073236107826233,
      "learning_rate": 0.00016382754994742377,
      "loss": 0.9462,
      "step": 780
    },
    {
      "epoch": 0.049873737373737376,
      "grad_norm": 0.5140103101730347,
      "learning_rate": 0.00016593059936908516,
      "loss": 0.8898,
      "step": 790
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 0.7719871997833252,
      "learning_rate": 0.0001680336487907466,
      "loss": 0.9388,
      "step": 800
    },
    {
      "epoch": 0.05113636363636364,
      "grad_norm": 0.40267521142959595,
      "learning_rate": 0.00017013669821240801,
      "loss": 0.972,
      "step": 810
    },
    {
      "epoch": 0.05176767676767677,
      "grad_norm": 0.4087556004524231,
      "learning_rate": 0.0001722397476340694,
      "loss": 0.9786,
      "step": 820
    },
    {
      "epoch": 0.052398989898989896,
      "grad_norm": 0.45229318737983704,
      "learning_rate": 0.00017434279705573082,
      "loss": 0.9305,
      "step": 830
    },
    {
      "epoch": 0.05303030303030303,
      "grad_norm": 0.5276629328727722,
      "learning_rate": 0.00017644584647739223,
      "loss": 0.9125,
      "step": 840
    },
    {
      "epoch": 0.05366161616161616,
      "grad_norm": 0.7445554733276367,
      "learning_rate": 0.00017854889589905365,
      "loss": 0.9355,
      "step": 850
    },
    {
      "epoch": 0.054292929292929296,
      "grad_norm": 0.3979644775390625,
      "learning_rate": 0.00018065194532071506,
      "loss": 0.9907,
      "step": 860
    },
    {
      "epoch": 0.054924242424242424,
      "grad_norm": 0.44829729199409485,
      "learning_rate": 0.00018275499474237645,
      "loss": 0.9574,
      "step": 870
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 0.44900932908058167,
      "learning_rate": 0.00018485804416403786,
      "loss": 0.9373,
      "step": 880
    },
    {
      "epoch": 0.05618686868686869,
      "grad_norm": 0.4998871982097626,
      "learning_rate": 0.00018696109358569928,
      "loss": 0.9219,
      "step": 890
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 0.7167801856994629,
      "learning_rate": 0.0001890641430073607,
      "loss": 0.8876,
      "step": 900
    },
    {
      "epoch": 0.05744949494949495,
      "grad_norm": 0.4088430106639862,
      "learning_rate": 0.0001911671924290221,
      "loss": 0.9739,
      "step": 910
    },
    {
      "epoch": 0.05808080808080808,
      "grad_norm": 0.4394177496433258,
      "learning_rate": 0.0001932702418506835,
      "loss": 0.9804,
      "step": 920
    },
    {
      "epoch": 0.058712121212121215,
      "grad_norm": 0.4614415168762207,
      "learning_rate": 0.0001953732912723449,
      "loss": 0.9021,
      "step": 930
    },
    {
      "epoch": 0.059343434343434344,
      "grad_norm": 0.5111036896705627,
      "learning_rate": 0.00019747634069400632,
      "loss": 0.8827,
      "step": 940
    },
    {
      "epoch": 0.05997474747474747,
      "grad_norm": 0.7363557815551758,
      "learning_rate": 0.00019957939011566774,
      "loss": 0.8885,
      "step": 950
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": 0.39827895164489746,
      "learning_rate": 0.00019999996655333316,
      "loss": 0.9972,
      "step": 960
    },
    {
      "epoch": 0.061237373737373736,
      "grad_norm": 0.4305219054222107,
      "learning_rate": 0.00019999983067628733,
      "loss": 0.9757,
      "step": 970
    },
    {
      "epoch": 0.06186868686868687,
      "grad_norm": 0.4447081685066223,
      "learning_rate": 0.0001999995902785878,
      "loss": 0.9514,
      "step": 980
    },
    {
      "epoch": 0.0625,
      "grad_norm": 0.48579245805740356,
      "learning_rate": 0.0001999992453604858,
      "loss": 0.8882,
      "step": 990
    },
    {
      "epoch": 0.06313131313131314,
      "grad_norm": 0.7275524139404297,
      "learning_rate": 0.00019999879592234187,
      "loss": 0.9124,
      "step": 1000
    },
    {
      "epoch": 0.06313131313131314,
      "eval_loss": 0.9467018246650696,
      "eval_runtime": 27.2337,
      "eval_samples_per_second": 94.001,
      "eval_steps_per_second": 11.75,
      "step": 1000
    },
    {
      "epoch": 0.06376262626262626,
      "grad_norm": 0.40324586629867554,
      "learning_rate": 0.00019999824196462578,
      "loss": 1.0089,
      "step": 1010
    },
    {
      "epoch": 0.06439393939393939,
      "grad_norm": 0.42818593978881836,
      "learning_rate": 0.00019999758348791647,
      "loss": 0.9418,
      "step": 1020
    },
    {
      "epoch": 0.06502525252525253,
      "grad_norm": 0.43285858631134033,
      "learning_rate": 0.00019999682049290227,
      "loss": 0.9111,
      "step": 1030
    },
    {
      "epoch": 0.06565656565656566,
      "grad_norm": 0.49966171383857727,
      "learning_rate": 0.00019999595298038057,
      "loss": 0.8638,
      "step": 1040
    },
    {
      "epoch": 0.06628787878787878,
      "grad_norm": 0.7247596383094788,
      "learning_rate": 0.00019999498095125818,
      "loss": 0.8962,
      "step": 1050
    },
    {
      "epoch": 0.06691919191919192,
      "grad_norm": 0.3946726322174072,
      "learning_rate": 0.00019999390440655107,
      "loss": 0.9597,
      "step": 1060
    },
    {
      "epoch": 0.06755050505050506,
      "grad_norm": 0.4311758577823639,
      "learning_rate": 0.00019999272334738442,
      "loss": 0.9449,
      "step": 1070
    },
    {
      "epoch": 0.06818181818181818,
      "grad_norm": 0.4666595458984375,
      "learning_rate": 0.0001999914377749927,
      "loss": 0.9137,
      "step": 1080
    },
    {
      "epoch": 0.06881313131313131,
      "grad_norm": 0.4912818372249603,
      "learning_rate": 0.00019999004769071955,
      "loss": 0.8788,
      "step": 1090
    },
    {
      "epoch": 0.06944444444444445,
      "grad_norm": 0.7350306510925293,
      "learning_rate": 0.00019998855309601797,
      "loss": 0.873,
      "step": 1100
    },
    {
      "epoch": 0.07007575757575757,
      "grad_norm": 0.4092772305011749,
      "learning_rate": 0.00019998695399245012,
      "loss": 1.0458,
      "step": 1110
    },
    {
      "epoch": 0.0707070707070707,
      "grad_norm": 0.4206644892692566,
      "learning_rate": 0.00019998525038168735,
      "loss": 0.957,
      "step": 1120
    },
    {
      "epoch": 0.07133838383838384,
      "grad_norm": 0.502644956111908,
      "learning_rate": 0.00019998344226551028,
      "loss": 0.8965,
      "step": 1130
    },
    {
      "epoch": 0.07196969696969698,
      "grad_norm": 0.5151838660240173,
      "learning_rate": 0.00019998152964580883,
      "loss": 0.8693,
      "step": 1140
    },
    {
      "epoch": 0.0726010101010101,
      "grad_norm": 0.7897934317588806,
      "learning_rate": 0.00019997951252458205,
      "loss": 0.9066,
      "step": 1150
    },
    {
      "epoch": 0.07323232323232323,
      "grad_norm": 0.3789682686328888,
      "learning_rate": 0.00019997739090393823,
      "loss": 0.9943,
      "step": 1160
    },
    {
      "epoch": 0.07386363636363637,
      "grad_norm": 0.43282124400138855,
      "learning_rate": 0.00019997516478609498,
      "loss": 0.9281,
      "step": 1170
    },
    {
      "epoch": 0.07449494949494949,
      "grad_norm": 0.4386129677295685,
      "learning_rate": 0.00019997283417337895,
      "loss": 0.8972,
      "step": 1180
    },
    {
      "epoch": 0.07512626262626262,
      "grad_norm": 0.4949673116207123,
      "learning_rate": 0.00019997039906822624,
      "loss": 0.867,
      "step": 1190
    },
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 0.7160463333129883,
      "learning_rate": 0.00019996785947318192,
      "loss": 0.8943,
      "step": 1200
    },
    {
      "epoch": 0.0763888888888889,
      "grad_norm": 0.42326855659484863,
      "learning_rate": 0.00019996521539090046,
      "loss": 1.0329,
      "step": 1210
    },
    {
      "epoch": 0.07702020202020202,
      "grad_norm": 0.416482537984848,
      "learning_rate": 0.00019996246682414548,
      "loss": 0.9281,
      "step": 1220
    },
    {
      "epoch": 0.07765151515151515,
      "grad_norm": 0.48489615321159363,
      "learning_rate": 0.0001999596137757898,
      "loss": 0.9044,
      "step": 1230
    },
    {
      "epoch": 0.07828282828282829,
      "grad_norm": 0.5203187465667725,
      "learning_rate": 0.0001999566562488154,
      "loss": 0.8642,
      "step": 1240
    },
    {
      "epoch": 0.07891414141414141,
      "grad_norm": 0.759473443031311,
      "learning_rate": 0.0001999535942463136,
      "loss": 0.8423,
      "step": 1250
    },
    {
      "epoch": 0.07954545454545454,
      "grad_norm": 0.3947957456111908,
      "learning_rate": 0.00019995042777148477,
      "loss": 0.9748,
      "step": 1260
    },
    {
      "epoch": 0.08017676767676768,
      "grad_norm": 0.447480171918869,
      "learning_rate": 0.00019994715682763854,
      "loss": 0.9358,
      "step": 1270
    },
    {
      "epoch": 0.08080808080808081,
      "grad_norm": 0.45288851857185364,
      "learning_rate": 0.00019994378141819378,
      "loss": 0.8749,
      "step": 1280
    },
    {
      "epoch": 0.08143939393939394,
      "grad_norm": 0.5199007391929626,
      "learning_rate": 0.0001999403015466784,
      "loss": 0.8671,
      "step": 1290
    },
    {
      "epoch": 0.08207070707070707,
      "grad_norm": 0.8685715794563293,
      "learning_rate": 0.0001999367172167297,
      "loss": 0.8648,
      "step": 1300
    },
    {
      "epoch": 0.08270202020202021,
      "grad_norm": 0.379661500453949,
      "learning_rate": 0.00019993302843209393,
      "loss": 0.9641,
      "step": 1310
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.4346831440925598,
      "learning_rate": 0.00019992923519662674,
      "loss": 0.9395,
      "step": 1320
    },
    {
      "epoch": 0.08396464646464646,
      "grad_norm": 0.49057310819625854,
      "learning_rate": 0.00019992533751429282,
      "loss": 0.8662,
      "step": 1330
    },
    {
      "epoch": 0.0845959595959596,
      "grad_norm": 0.5500571727752686,
      "learning_rate": 0.00019992133538916608,
      "loss": 0.8518,
      "step": 1340
    },
    {
      "epoch": 0.08522727272727272,
      "grad_norm": 0.7858998775482178,
      "learning_rate": 0.0001999172288254295,
      "loss": 0.8731,
      "step": 1350
    },
    {
      "epoch": 0.08585858585858586,
      "grad_norm": 0.40742629766464233,
      "learning_rate": 0.00019991301782737537,
      "loss": 0.9738,
      "step": 1360
    },
    {
      "epoch": 0.08648989898989899,
      "grad_norm": 0.44539931416511536,
      "learning_rate": 0.00019990870239940502,
      "loss": 0.926,
      "step": 1370
    },
    {
      "epoch": 0.08712121212121213,
      "grad_norm": 0.5004927515983582,
      "learning_rate": 0.000199904282546029,
      "loss": 0.8581,
      "step": 1380
    },
    {
      "epoch": 0.08775252525252525,
      "grad_norm": 0.5613665580749512,
      "learning_rate": 0.00019989975827186695,
      "loss": 0.835,
      "step": 1390
    },
    {
      "epoch": 0.08838383838383838,
      "grad_norm": 0.7562660574913025,
      "learning_rate": 0.00019989512958164772,
      "loss": 0.8334,
      "step": 1400
    },
    {
      "epoch": 0.08901515151515152,
      "grad_norm": 0.40560752153396606,
      "learning_rate": 0.0001998903964802092,
      "loss": 0.9439,
      "step": 1410
    },
    {
      "epoch": 0.08964646464646464,
      "grad_norm": 0.48586180806159973,
      "learning_rate": 0.0001998855589724985,
      "loss": 0.9081,
      "step": 1420
    },
    {
      "epoch": 0.09027777777777778,
      "grad_norm": 0.5029296875,
      "learning_rate": 0.0001998806170635718,
      "loss": 0.866,
      "step": 1430
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.5713523626327515,
      "learning_rate": 0.0001998755707585945,
      "loss": 0.8891,
      "step": 1440
    },
    {
      "epoch": 0.09154040404040405,
      "grad_norm": 0.8226764798164368,
      "learning_rate": 0.00019987042006284095,
      "loss": 0.8434,
      "step": 1450
    },
    {
      "epoch": 0.09217171717171717,
      "grad_norm": 0.4211214780807495,
      "learning_rate": 0.00019986516498169473,
      "loss": 0.9317,
      "step": 1460
    },
    {
      "epoch": 0.0928030303030303,
      "grad_norm": 0.49744337797164917,
      "learning_rate": 0.0001998598055206485,
      "loss": 0.9114,
      "step": 1470
    },
    {
      "epoch": 0.09343434343434344,
      "grad_norm": 0.5054945945739746,
      "learning_rate": 0.00019985434168530398,
      "loss": 0.873,
      "step": 1480
    },
    {
      "epoch": 0.09406565656565656,
      "grad_norm": 0.5643303394317627,
      "learning_rate": 0.0001998487734813721,
      "loss": 0.8299,
      "step": 1490
    },
    {
      "epoch": 0.0946969696969697,
      "grad_norm": 0.6976872086524963,
      "learning_rate": 0.0001998431009146727,
      "loss": 0.8433,
      "step": 1500
    },
    {
      "epoch": 0.09532828282828283,
      "grad_norm": 0.4267701208591461,
      "learning_rate": 0.00019983732399113483,
      "loss": 0.9523,
      "step": 1510
    },
    {
      "epoch": 0.09595959595959595,
      "grad_norm": 0.4605022072792053,
      "learning_rate": 0.0001998314427167966,
      "loss": 0.9245,
      "step": 1520
    },
    {
      "epoch": 0.09659090909090909,
      "grad_norm": 0.518143355846405,
      "learning_rate": 0.00019982545709780515,
      "loss": 0.8612,
      "step": 1530
    },
    {
      "epoch": 0.09722222222222222,
      "grad_norm": 0.6061258912086487,
      "learning_rate": 0.00019981936714041668,
      "loss": 0.8259,
      "step": 1540
    },
    {
      "epoch": 0.09785353535353536,
      "grad_norm": 0.7591531872749329,
      "learning_rate": 0.00019981317285099653,
      "loss": 0.8042,
      "step": 1550
    },
    {
      "epoch": 0.09848484848484848,
      "grad_norm": 0.3919331133365631,
      "learning_rate": 0.0001998068742360189,
      "loss": 0.9391,
      "step": 1560
    },
    {
      "epoch": 0.09911616161616162,
      "grad_norm": 0.5004214644432068,
      "learning_rate": 0.00019980047130206728,
      "loss": 0.9158,
      "step": 1570
    },
    {
      "epoch": 0.09974747474747475,
      "grad_norm": 0.5116076469421387,
      "learning_rate": 0.000199793964055834,
      "loss": 0.8546,
      "step": 1580
    },
    {
      "epoch": 0.10037878787878787,
      "grad_norm": 0.591215968132019,
      "learning_rate": 0.0001997873525041205,
      "loss": 0.8346,
      "step": 1590
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 0.8753274083137512,
      "learning_rate": 0.00019978063665383725,
      "loss": 0.8634,
      "step": 1600
    },
    {
      "epoch": 0.10164141414141414,
      "grad_norm": 0.43350327014923096,
      "learning_rate": 0.0001997738165120037,
      "loss": 0.9331,
      "step": 1610
    },
    {
      "epoch": 0.10227272727272728,
      "grad_norm": 0.5097168684005737,
      "learning_rate": 0.00019976689208574827,
      "loss": 0.9076,
      "step": 1620
    },
    {
      "epoch": 0.1029040404040404,
      "grad_norm": 0.5247926115989685,
      "learning_rate": 0.00019975986338230853,
      "loss": 0.8207,
      "step": 1630
    },
    {
      "epoch": 0.10353535353535354,
      "grad_norm": 0.6446250677108765,
      "learning_rate": 0.00019975273040903087,
      "loss": 0.8078,
      "step": 1640
    },
    {
      "epoch": 0.10416666666666667,
      "grad_norm": 0.8003672361373901,
      "learning_rate": 0.00019974549317337075,
      "loss": 0.8195,
      "step": 1650
    },
    {
      "epoch": 0.10479797979797979,
      "grad_norm": 0.4129176139831543,
      "learning_rate": 0.00019973815168289257,
      "loss": 0.9432,
      "step": 1660
    },
    {
      "epoch": 0.10542929292929293,
      "grad_norm": 0.45286381244659424,
      "learning_rate": 0.00019973070594526973,
      "loss": 0.9106,
      "step": 1670
    },
    {
      "epoch": 0.10606060606060606,
      "grad_norm": 0.5159316062927246,
      "learning_rate": 0.00019972315596828458,
      "loss": 0.8676,
      "step": 1680
    },
    {
      "epoch": 0.10669191919191919,
      "grad_norm": 0.5524932742118835,
      "learning_rate": 0.0001997155017598284,
      "loss": 0.8116,
      "step": 1690
    },
    {
      "epoch": 0.10732323232323232,
      "grad_norm": 0.8455977439880371,
      "learning_rate": 0.0001997077433279015,
      "loss": 0.8098,
      "step": 1700
    },
    {
      "epoch": 0.10795454545454546,
      "grad_norm": 0.42986762523651123,
      "learning_rate": 0.00019969988068061295,
      "loss": 0.9245,
      "step": 1710
    },
    {
      "epoch": 0.10858585858585859,
      "grad_norm": 0.49853822588920593,
      "learning_rate": 0.00019969191382618095,
      "loss": 0.8926,
      "step": 1720
    },
    {
      "epoch": 0.10921717171717171,
      "grad_norm": 0.5442687273025513,
      "learning_rate": 0.00019968384277293247,
      "loss": 0.8423,
      "step": 1730
    },
    {
      "epoch": 0.10984848484848485,
      "grad_norm": 0.5966998934745789,
      "learning_rate": 0.00019967566752930347,
      "loss": 0.8233,
      "step": 1740
    },
    {
      "epoch": 0.11047979797979798,
      "grad_norm": 0.827552080154419,
      "learning_rate": 0.00019966738810383875,
      "loss": 0.7682,
      "step": 1750
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.43787339329719543,
      "learning_rate": 0.00019965900450519207,
      "loss": 0.9355,
      "step": 1760
    },
    {
      "epoch": 0.11174242424242424,
      "grad_norm": 0.4841938614845276,
      "learning_rate": 0.000199650516742126,
      "loss": 0.8863,
      "step": 1770
    },
    {
      "epoch": 0.11237373737373738,
      "grad_norm": 0.5904253125190735,
      "learning_rate": 0.00019964192482351204,
      "loss": 0.8489,
      "step": 1780
    },
    {
      "epoch": 0.11300505050505051,
      "grad_norm": 0.6222173571586609,
      "learning_rate": 0.00019963322875833056,
      "loss": 0.8154,
      "step": 1790
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 0.8397778868675232,
      "learning_rate": 0.0001996244285556707,
      "loss": 0.7851,
      "step": 1800
    },
    {
      "epoch": 0.11426767676767677,
      "grad_norm": 0.43562033772468567,
      "learning_rate": 0.00019961552422473057,
      "loss": 0.9448,
      "step": 1810
    },
    {
      "epoch": 0.1148989898989899,
      "grad_norm": 0.49730274081230164,
      "learning_rate": 0.000199606515774817,
      "loss": 0.914,
      "step": 1820
    },
    {
      "epoch": 0.11553030303030302,
      "grad_norm": 0.5812826752662659,
      "learning_rate": 0.00019959740321534572,
      "loss": 0.8188,
      "step": 1830
    },
    {
      "epoch": 0.11616161616161616,
      "grad_norm": 0.6531727910041809,
      "learning_rate": 0.00019958818655584125,
      "loss": 0.7757,
      "step": 1840
    },
    {
      "epoch": 0.1167929292929293,
      "grad_norm": 0.7923195362091064,
      "learning_rate": 0.0001995788658059369,
      "loss": 0.7818,
      "step": 1850
    },
    {
      "epoch": 0.11742424242424243,
      "grad_norm": 0.45921364426612854,
      "learning_rate": 0.00019956944097537484,
      "loss": 0.8977,
      "step": 1860
    },
    {
      "epoch": 0.11805555555555555,
      "grad_norm": 0.4942459166049957,
      "learning_rate": 0.00019955991207400595,
      "loss": 0.8633,
      "step": 1870
    },
    {
      "epoch": 0.11868686868686869,
      "grad_norm": 0.550398588180542,
      "learning_rate": 0.0001995502791117899,
      "loss": 0.8287,
      "step": 1880
    },
    {
      "epoch": 0.11931818181818182,
      "grad_norm": 0.5691207647323608,
      "learning_rate": 0.0001995405420987952,
      "loss": 0.7913,
      "step": 1890
    },
    {
      "epoch": 0.11994949494949494,
      "grad_norm": 0.8962611556053162,
      "learning_rate": 0.000199530701045199,
      "loss": 0.7856,
      "step": 1900
    },
    {
      "epoch": 0.12058080808080808,
      "grad_norm": 0.4272255301475525,
      "learning_rate": 0.00019952075596128726,
      "loss": 0.9429,
      "step": 1910
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 0.5055891871452332,
      "learning_rate": 0.0001995107068574547,
      "loss": 0.8689,
      "step": 1920
    },
    {
      "epoch": 0.12184343434343434,
      "grad_norm": 0.5636619925498962,
      "learning_rate": 0.00019950055374420468,
      "loss": 0.8015,
      "step": 1930
    },
    {
      "epoch": 0.12247474747474747,
      "grad_norm": 0.5823515057563782,
      "learning_rate": 0.00019949029663214937,
      "loss": 0.7786,
      "step": 1940
    },
    {
      "epoch": 0.12310606060606061,
      "grad_norm": 0.8790888786315918,
      "learning_rate": 0.00019947993553200955,
      "loss": 0.7672,
      "step": 1950
    },
    {
      "epoch": 0.12373737373737374,
      "grad_norm": 0.4872295558452606,
      "learning_rate": 0.00019946947045461472,
      "loss": 0.9216,
      "step": 1960
    },
    {
      "epoch": 0.12436868686868686,
      "grad_norm": 0.5439364314079285,
      "learning_rate": 0.00019945890141090307,
      "loss": 0.8626,
      "step": 1970
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.6327049732208252,
      "learning_rate": 0.0001994482284119215,
      "loss": 0.8191,
      "step": 1980
    },
    {
      "epoch": 0.12563131313131312,
      "grad_norm": 0.6687145233154297,
      "learning_rate": 0.0001994374514688255,
      "loss": 0.7795,
      "step": 1990
    },
    {
      "epoch": 0.12626262626262627,
      "grad_norm": 0.9320880174636841,
      "learning_rate": 0.00019942657059287918,
      "loss": 0.7792,
      "step": 2000
    },
    {
      "epoch": 0.12626262626262627,
      "eval_loss": 0.8338238000869751,
      "eval_runtime": 27.276,
      "eval_samples_per_second": 93.855,
      "eval_steps_per_second": 11.732,
      "step": 2000
    },
    {
      "epoch": 0.1268939393939394,
      "grad_norm": 0.4458157420158386,
      "learning_rate": 0.00019941558579545534,
      "loss": 0.9383,
      "step": 2010
    },
    {
      "epoch": 0.1275252525252525,
      "grad_norm": 0.5609799027442932,
      "learning_rate": 0.00019940449708803537,
      "loss": 0.8643,
      "step": 2020
    },
    {
      "epoch": 0.12815656565656566,
      "grad_norm": 0.5868834853172302,
      "learning_rate": 0.00019939330448220932,
      "loss": 0.838,
      "step": 2030
    },
    {
      "epoch": 0.12878787878787878,
      "grad_norm": 0.5802881121635437,
      "learning_rate": 0.00019938200798967577,
      "loss": 0.7569,
      "step": 2040
    },
    {
      "epoch": 0.1294191919191919,
      "grad_norm": 0.8532348275184631,
      "learning_rate": 0.00019937060762224192,
      "loss": 0.7822,
      "step": 2050
    },
    {
      "epoch": 0.13005050505050506,
      "grad_norm": 0.44485417008399963,
      "learning_rate": 0.00019935910339182348,
      "loss": 0.9307,
      "step": 2060
    },
    {
      "epoch": 0.13068181818181818,
      "grad_norm": 0.5271552205085754,
      "learning_rate": 0.00019934749531044484,
      "loss": 0.8602,
      "step": 2070
    },
    {
      "epoch": 0.13131313131313133,
      "grad_norm": 0.5830236077308655,
      "learning_rate": 0.0001993357833902388,
      "loss": 0.7825,
      "step": 2080
    },
    {
      "epoch": 0.13194444444444445,
      "grad_norm": 0.6748720407485962,
      "learning_rate": 0.0001993239676434468,
      "loss": 0.7441,
      "step": 2090
    },
    {
      "epoch": 0.13257575757575757,
      "grad_norm": 0.8696335554122925,
      "learning_rate": 0.00019931204808241873,
      "loss": 0.7536,
      "step": 2100
    },
    {
      "epoch": 0.13320707070707072,
      "grad_norm": 0.4964369237422943,
      "learning_rate": 0.00019930002471961301,
      "loss": 0.9012,
      "step": 2110
    },
    {
      "epoch": 0.13383838383838384,
      "grad_norm": 0.5407465696334839,
      "learning_rate": 0.00019928789756759661,
      "loss": 0.8672,
      "step": 2120
    },
    {
      "epoch": 0.13446969696969696,
      "grad_norm": 0.6225587725639343,
      "learning_rate": 0.00019927566663904486,
      "loss": 0.7965,
      "step": 2130
    },
    {
      "epoch": 0.1351010101010101,
      "grad_norm": 0.671110212802887,
      "learning_rate": 0.0001992633319467417,
      "loss": 0.7521,
      "step": 2140
    },
    {
      "epoch": 0.13573232323232323,
      "grad_norm": 0.9717885851860046,
      "learning_rate": 0.00019925089350357938,
      "loss": 0.7638,
      "step": 2150
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 0.4905236065387726,
      "learning_rate": 0.00019923835132255874,
      "loss": 0.9222,
      "step": 2160
    },
    {
      "epoch": 0.1369949494949495,
      "grad_norm": 0.5381935834884644,
      "learning_rate": 0.00019922570541678887,
      "loss": 0.8454,
      "step": 2170
    },
    {
      "epoch": 0.13762626262626262,
      "grad_norm": 0.5398241281509399,
      "learning_rate": 0.00019921295579948748,
      "loss": 0.8094,
      "step": 2180
    },
    {
      "epoch": 0.13825757575757575,
      "grad_norm": 1.054864764213562,
      "learning_rate": 0.00019920010248398052,
      "loss": 0.7368,
      "step": 2190
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 0.9540656805038452,
      "learning_rate": 0.0001991871454837024,
      "loss": 0.7676,
      "step": 2200
    },
    {
      "epoch": 0.13952020202020202,
      "grad_norm": 0.5193989276885986,
      "learning_rate": 0.00019917408481219585,
      "loss": 0.8816,
      "step": 2210
    },
    {
      "epoch": 0.14015151515151514,
      "grad_norm": 0.5837627053260803,
      "learning_rate": 0.00019916092048311205,
      "loss": 0.8781,
      "step": 2220
    },
    {
      "epoch": 0.1407828282828283,
      "grad_norm": 0.6785120368003845,
      "learning_rate": 0.0001991476525102104,
      "loss": 0.8121,
      "step": 2230
    },
    {
      "epoch": 0.1414141414141414,
      "grad_norm": 0.6777743697166443,
      "learning_rate": 0.00019913428090735877,
      "loss": 0.7435,
      "step": 2240
    },
    {
      "epoch": 0.14204545454545456,
      "grad_norm": 1.073117733001709,
      "learning_rate": 0.00019912080568853323,
      "loss": 0.758,
      "step": 2250
    },
    {
      "epoch": 0.14267676767676768,
      "grad_norm": 0.4643658995628357,
      "learning_rate": 0.0001991072268678182,
      "loss": 0.9034,
      "step": 2260
    },
    {
      "epoch": 0.1433080808080808,
      "grad_norm": 0.5185081362724304,
      "learning_rate": 0.00019909354445940634,
      "loss": 0.8541,
      "step": 2270
    },
    {
      "epoch": 0.14393939393939395,
      "grad_norm": 0.6076844930648804,
      "learning_rate": 0.00019907975847759866,
      "loss": 0.7912,
      "step": 2280
    },
    {
      "epoch": 0.14457070707070707,
      "grad_norm": 0.6883999109268188,
      "learning_rate": 0.00019906586893680438,
      "loss": 0.744,
      "step": 2290
    },
    {
      "epoch": 0.1452020202020202,
      "grad_norm": 0.9529832005500793,
      "learning_rate": 0.00019905187585154095,
      "loss": 0.7741,
      "step": 2300
    },
    {
      "epoch": 0.14583333333333334,
      "grad_norm": 0.4924416244029999,
      "learning_rate": 0.00019903777923643406,
      "loss": 0.9186,
      "step": 2310
    },
    {
      "epoch": 0.14646464646464646,
      "grad_norm": 0.5256723165512085,
      "learning_rate": 0.00019902357910621762,
      "loss": 0.8366,
      "step": 2320
    },
    {
      "epoch": 0.14709595959595959,
      "grad_norm": 0.5781369209289551,
      "learning_rate": 0.00019900927547573366,
      "loss": 0.7758,
      "step": 2330
    },
    {
      "epoch": 0.14772727272727273,
      "grad_norm": 0.6617695689201355,
      "learning_rate": 0.00019899486835993257,
      "loss": 0.7545,
      "step": 2340
    },
    {
      "epoch": 0.14835858585858586,
      "grad_norm": 1.0167388916015625,
      "learning_rate": 0.0001989803577738727,
      "loss": 0.7427,
      "step": 2350
    },
    {
      "epoch": 0.14898989898989898,
      "grad_norm": 0.46141016483306885,
      "learning_rate": 0.00019896574373272065,
      "loss": 0.9331,
      "step": 2360
    },
    {
      "epoch": 0.14962121212121213,
      "grad_norm": 0.5681381821632385,
      "learning_rate": 0.00019895102625175118,
      "loss": 0.818,
      "step": 2370
    },
    {
      "epoch": 0.15025252525252525,
      "grad_norm": 0.6212285757064819,
      "learning_rate": 0.00019893620534634706,
      "loss": 0.7782,
      "step": 2380
    },
    {
      "epoch": 0.15088383838383837,
      "grad_norm": 0.7176325917243958,
      "learning_rate": 0.00019892128103199928,
      "loss": 0.7306,
      "step": 2390
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 0.991489827632904,
      "learning_rate": 0.0001989062533243068,
      "loss": 0.7485,
      "step": 2400
    },
    {
      "epoch": 0.15214646464646464,
      "grad_norm": 0.5086601376533508,
      "learning_rate": 0.00019889112223897676,
      "loss": 0.9004,
      "step": 2410
    },
    {
      "epoch": 0.1527777777777778,
      "grad_norm": 0.5726848244667053,
      "learning_rate": 0.0001988758877918243,
      "loss": 0.8408,
      "step": 2420
    },
    {
      "epoch": 0.1534090909090909,
      "grad_norm": 0.637711226940155,
      "learning_rate": 0.0001988605499987725,
      "loss": 0.7949,
      "step": 2430
    },
    {
      "epoch": 0.15404040404040403,
      "grad_norm": 0.71788489818573,
      "learning_rate": 0.00019884510887585263,
      "loss": 0.7077,
      "step": 2440
    },
    {
      "epoch": 0.15467171717171718,
      "grad_norm": 0.9684906005859375,
      "learning_rate": 0.00019882956443920388,
      "loss": 0.7392,
      "step": 2450
    },
    {
      "epoch": 0.1553030303030303,
      "grad_norm": 0.4676392376422882,
      "learning_rate": 0.00019881391670507342,
      "loss": 0.9023,
      "step": 2460
    },
    {
      "epoch": 0.15593434343434343,
      "grad_norm": 0.5791764855384827,
      "learning_rate": 0.00019879816568981636,
      "loss": 0.8158,
      "step": 2470
    },
    {
      "epoch": 0.15656565656565657,
      "grad_norm": 0.674104630947113,
      "learning_rate": 0.0001987823114098958,
      "loss": 0.7772,
      "step": 2480
    },
    {
      "epoch": 0.1571969696969697,
      "grad_norm": 0.7449119091033936,
      "learning_rate": 0.0001987663538818828,
      "loss": 0.7139,
      "step": 2490
    },
    {
      "epoch": 0.15782828282828282,
      "grad_norm": 0.9963280558586121,
      "learning_rate": 0.00019875029312245625,
      "loss": 0.7251,
      "step": 2500
    },
    {
      "epoch": 0.15845959595959597,
      "grad_norm": 0.4839588403701782,
      "learning_rate": 0.00019873412914840304,
      "loss": 0.9355,
      "step": 2510
    },
    {
      "epoch": 0.1590909090909091,
      "grad_norm": 0.5863730311393738,
      "learning_rate": 0.00019871786197661785,
      "loss": 0.8123,
      "step": 2520
    },
    {
      "epoch": 0.1597222222222222,
      "grad_norm": 0.6110161542892456,
      "learning_rate": 0.00019870149162410326,
      "loss": 0.7631,
      "step": 2530
    },
    {
      "epoch": 0.16035353535353536,
      "grad_norm": 0.7272172570228577,
      "learning_rate": 0.00019868501810796975,
      "loss": 0.7259,
      "step": 2540
    },
    {
      "epoch": 0.16098484848484848,
      "grad_norm": 0.9596837759017944,
      "learning_rate": 0.00019866844144543553,
      "loss": 0.7127,
      "step": 2550
    },
    {
      "epoch": 0.16161616161616163,
      "grad_norm": 0.5496588945388794,
      "learning_rate": 0.0001986517616538267,
      "loss": 0.8754,
      "step": 2560
    },
    {
      "epoch": 0.16224747474747475,
      "grad_norm": 0.5752465128898621,
      "learning_rate": 0.00019863497875057705,
      "loss": 0.8076,
      "step": 2570
    },
    {
      "epoch": 0.16287878787878787,
      "grad_norm": 0.6438614726066589,
      "learning_rate": 0.00019861809275322826,
      "loss": 0.7749,
      "step": 2580
    },
    {
      "epoch": 0.16351010101010102,
      "grad_norm": 0.8031302094459534,
      "learning_rate": 0.00019860110367942975,
      "loss": 0.692,
      "step": 2590
    },
    {
      "epoch": 0.16414141414141414,
      "grad_norm": 0.9156016111373901,
      "learning_rate": 0.00019858401154693858,
      "loss": 0.7,
      "step": 2600
    },
    {
      "epoch": 0.16477272727272727,
      "grad_norm": 0.496735155582428,
      "learning_rate": 0.0001985668163736196,
      "loss": 0.9039,
      "step": 2610
    },
    {
      "epoch": 0.16540404040404041,
      "grad_norm": 0.5905826687812805,
      "learning_rate": 0.00019854951817744536,
      "loss": 0.8377,
      "step": 2620
    },
    {
      "epoch": 0.16603535353535354,
      "grad_norm": 0.6868301033973694,
      "learning_rate": 0.00019853211697649608,
      "loss": 0.7556,
      "step": 2630
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.7296802401542664,
      "learning_rate": 0.0001985146127889596,
      "loss": 0.6795,
      "step": 2640
    },
    {
      "epoch": 0.1672979797979798,
      "grad_norm": 1.0678026676177979,
      "learning_rate": 0.00019849700563313153,
      "loss": 0.7152,
      "step": 2650
    },
    {
      "epoch": 0.16792929292929293,
      "grad_norm": 0.4883025586605072,
      "learning_rate": 0.00019847929552741495,
      "loss": 0.8714,
      "step": 2660
    },
    {
      "epoch": 0.16856060606060605,
      "grad_norm": 0.5960721373558044,
      "learning_rate": 0.00019846148249032063,
      "loss": 0.8231,
      "step": 2670
    },
    {
      "epoch": 0.1691919191919192,
      "grad_norm": 0.64022296667099,
      "learning_rate": 0.00019844356654046688,
      "loss": 0.7734,
      "step": 2680
    },
    {
      "epoch": 0.16982323232323232,
      "grad_norm": 0.7293738126754761,
      "learning_rate": 0.00019842554769657962,
      "loss": 0.6963,
      "step": 2690
    },
    {
      "epoch": 0.17045454545454544,
      "grad_norm": 1.0133296251296997,
      "learning_rate": 0.0001984074259774923,
      "loss": 0.7068,
      "step": 2700
    },
    {
      "epoch": 0.1710858585858586,
      "grad_norm": 0.4781786799430847,
      "learning_rate": 0.00019838920140214587,
      "loss": 0.8699,
      "step": 2710
    },
    {
      "epoch": 0.1717171717171717,
      "grad_norm": 0.5547852516174316,
      "learning_rate": 0.00019837087398958883,
      "loss": 0.817,
      "step": 2720
    },
    {
      "epoch": 0.17234848484848486,
      "grad_norm": 0.6851038932800293,
      "learning_rate": 0.00019835244375897713,
      "loss": 0.7674,
      "step": 2730
    },
    {
      "epoch": 0.17297979797979798,
      "grad_norm": 0.8076152205467224,
      "learning_rate": 0.00019833391072957422,
      "loss": 0.7121,
      "step": 2740
    },
    {
      "epoch": 0.1736111111111111,
      "grad_norm": 1.0549875497817993,
      "learning_rate": 0.00019831527492075092,
      "loss": 0.6929,
      "step": 2750
    },
    {
      "epoch": 0.17424242424242425,
      "grad_norm": 0.5288456082344055,
      "learning_rate": 0.00019829653635198563,
      "loss": 0.8682,
      "step": 2760
    },
    {
      "epoch": 0.17487373737373738,
      "grad_norm": 0.6235291361808777,
      "learning_rate": 0.00019827769504286396,
      "loss": 0.8012,
      "step": 2770
    },
    {
      "epoch": 0.1755050505050505,
      "grad_norm": 0.6597782969474792,
      "learning_rate": 0.00019825875101307905,
      "loss": 0.766,
      "step": 2780
    },
    {
      "epoch": 0.17613636363636365,
      "grad_norm": 0.7904951572418213,
      "learning_rate": 0.00019823970428243135,
      "loss": 0.712,
      "step": 2790
    },
    {
      "epoch": 0.17676767676767677,
      "grad_norm": 1.2259427309036255,
      "learning_rate": 0.00019822055487082866,
      "loss": 0.7038,
      "step": 2800
    },
    {
      "epoch": 0.1773989898989899,
      "grad_norm": 0.5086365342140198,
      "learning_rate": 0.00019820130279828613,
      "loss": 0.8943,
      "step": 2810
    },
    {
      "epoch": 0.17803030303030304,
      "grad_norm": 0.5791149139404297,
      "learning_rate": 0.00019818194808492615,
      "loss": 0.8077,
      "step": 2820
    },
    {
      "epoch": 0.17866161616161616,
      "grad_norm": 0.7340189218521118,
      "learning_rate": 0.00019816249075097845,
      "loss": 0.7248,
      "step": 2830
    },
    {
      "epoch": 0.17929292929292928,
      "grad_norm": 0.8152987360954285,
      "learning_rate": 0.00019814293081677994,
      "loss": 0.677,
      "step": 2840
    },
    {
      "epoch": 0.17992424242424243,
      "grad_norm": 1.044371485710144,
      "learning_rate": 0.0001981232683027749,
      "loss": 0.6892,
      "step": 2850
    },
    {
      "epoch": 0.18055555555555555,
      "grad_norm": 0.49501755833625793,
      "learning_rate": 0.00019810350322951474,
      "loss": 0.8991,
      "step": 2860
    },
    {
      "epoch": 0.18118686868686867,
      "grad_norm": 0.602576732635498,
      "learning_rate": 0.00019808363561765806,
      "loss": 0.7898,
      "step": 2870
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.6292011737823486,
      "learning_rate": 0.00019806366548797067,
      "loss": 0.756,
      "step": 2880
    },
    {
      "epoch": 0.18244949494949494,
      "grad_norm": 0.834084153175354,
      "learning_rate": 0.00019804359286132548,
      "loss": 0.6917,
      "step": 2890
    },
    {
      "epoch": 0.1830808080808081,
      "grad_norm": 0.9863002896308899,
      "learning_rate": 0.0001980234177587026,
      "loss": 0.7159,
      "step": 2900
    },
    {
      "epoch": 0.18371212121212122,
      "grad_norm": 0.5218273401260376,
      "learning_rate": 0.00019800314020118918,
      "loss": 0.8608,
      "step": 2910
    },
    {
      "epoch": 0.18434343434343434,
      "grad_norm": 0.5750924944877625,
      "learning_rate": 0.00019798276020997952,
      "loss": 0.7912,
      "step": 2920
    },
    {
      "epoch": 0.1849747474747475,
      "grad_norm": 0.6217162013053894,
      "learning_rate": 0.00019796227780637498,
      "loss": 0.7214,
      "step": 2930
    },
    {
      "epoch": 0.1856060606060606,
      "grad_norm": 0.7451394200325012,
      "learning_rate": 0.0001979416930117839,
      "loss": 0.6686,
      "step": 2940
    },
    {
      "epoch": 0.18623737373737373,
      "grad_norm": 1.078221321105957,
      "learning_rate": 0.00019792100584772166,
      "loss": 0.6891,
      "step": 2950
    },
    {
      "epoch": 0.18686868686868688,
      "grad_norm": 0.5199106335639954,
      "learning_rate": 0.00019790021633581071,
      "loss": 0.8727,
      "step": 2960
    },
    {
      "epoch": 0.1875,
      "grad_norm": 0.6874793767929077,
      "learning_rate": 0.0001978793244977804,
      "loss": 0.7838,
      "step": 2970
    },
    {
      "epoch": 0.18813131313131312,
      "grad_norm": 0.7335466742515564,
      "learning_rate": 0.00019785833035546702,
      "loss": 0.7198,
      "step": 2980
    },
    {
      "epoch": 0.18876262626262627,
      "grad_norm": 0.7366064786911011,
      "learning_rate": 0.00019783723393081387,
      "loss": 0.6815,
      "step": 2990
    },
    {
      "epoch": 0.1893939393939394,
      "grad_norm": 1.0605926513671875,
      "learning_rate": 0.00019781603524587107,
      "loss": 0.6666,
      "step": 3000
    },
    {
      "epoch": 0.1893939393939394,
      "eval_loss": 0.745069682598114,
      "eval_runtime": 27.261,
      "eval_samples_per_second": 93.907,
      "eval_steps_per_second": 11.738,
      "step": 3000
    },
    {
      "epoch": 0.1900252525252525,
      "grad_norm": 0.5447154641151428,
      "learning_rate": 0.00019779473432279568,
      "loss": 0.902,
      "step": 3010
    },
    {
      "epoch": 0.19065656565656566,
      "grad_norm": 0.6627722382545471,
      "learning_rate": 0.00019777333118385163,
      "loss": 0.7788,
      "step": 3020
    },
    {
      "epoch": 0.19128787878787878,
      "grad_norm": 0.6715835332870483,
      "learning_rate": 0.00019775182585140958,
      "loss": 0.7167,
      "step": 3030
    },
    {
      "epoch": 0.1919191919191919,
      "grad_norm": 0.7608147859573364,
      "learning_rate": 0.0001977302183479472,
      "loss": 0.6747,
      "step": 3040
    },
    {
      "epoch": 0.19255050505050506,
      "grad_norm": 1.015800952911377,
      "learning_rate": 0.00019770850869604872,
      "loss": 0.6537,
      "step": 3050
    },
    {
      "epoch": 0.19318181818181818,
      "grad_norm": 0.5336358547210693,
      "learning_rate": 0.0001976866969184053,
      "loss": 0.8782,
      "step": 3060
    },
    {
      "epoch": 0.19381313131313133,
      "grad_norm": 0.6922327280044556,
      "learning_rate": 0.00019766478303781475,
      "loss": 0.7973,
      "step": 3070
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 0.6918286681175232,
      "learning_rate": 0.0001976427670771817,
      "loss": 0.721,
      "step": 3080
    },
    {
      "epoch": 0.19507575757575757,
      "grad_norm": 0.7608218789100647,
      "learning_rate": 0.0001976206490595174,
      "loss": 0.6621,
      "step": 3090
    },
    {
      "epoch": 0.19570707070707072,
      "grad_norm": 1.1376641988754272,
      "learning_rate": 0.00019759842900793974,
      "loss": 0.6817,
      "step": 3100
    },
    {
      "epoch": 0.19633838383838384,
      "grad_norm": 0.5511743426322937,
      "learning_rate": 0.00019757610694567338,
      "loss": 0.8621,
      "step": 3110
    },
    {
      "epoch": 0.19696969696969696,
      "grad_norm": 0.6059994101524353,
      "learning_rate": 0.00019755368289604944,
      "loss": 0.7733,
      "step": 3120
    },
    {
      "epoch": 0.1976010101010101,
      "grad_norm": 0.6792340278625488,
      "learning_rate": 0.0001975311568825058,
      "loss": 0.7338,
      "step": 3130
    },
    {
      "epoch": 0.19823232323232323,
      "grad_norm": 0.7740210294723511,
      "learning_rate": 0.00019750852892858677,
      "loss": 0.6456,
      "step": 3140
    },
    {
      "epoch": 0.19886363636363635,
      "grad_norm": 0.9879257678985596,
      "learning_rate": 0.00019748579905794333,
      "loss": 0.6095,
      "step": 3150
    },
    {
      "epoch": 0.1994949494949495,
      "grad_norm": 0.5154120922088623,
      "learning_rate": 0.0001974629672943329,
      "loss": 0.8742,
      "step": 3160
    },
    {
      "epoch": 0.20012626262626262,
      "grad_norm": 0.6250808238983154,
      "learning_rate": 0.00019744003366161942,
      "loss": 0.791,
      "step": 3170
    },
    {
      "epoch": 0.20075757575757575,
      "grad_norm": 0.6933863162994385,
      "learning_rate": 0.00019741699818377333,
      "loss": 0.7154,
      "step": 3180
    },
    {
      "epoch": 0.2013888888888889,
      "grad_norm": 0.8763179779052734,
      "learning_rate": 0.0001973938608848715,
      "loss": 0.6208,
      "step": 3190
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 1.149739146232605,
      "learning_rate": 0.00019737062178909725,
      "loss": 0.659,
      "step": 3200
    },
    {
      "epoch": 0.20265151515151514,
      "grad_norm": 0.5820778012275696,
      "learning_rate": 0.00019734728092074024,
      "loss": 0.8547,
      "step": 3210
    },
    {
      "epoch": 0.2032828282828283,
      "grad_norm": 0.655501127243042,
      "learning_rate": 0.00019732383830419658,
      "loss": 0.7587,
      "step": 3220
    },
    {
      "epoch": 0.2039141414141414,
      "grad_norm": 0.695408284664154,
      "learning_rate": 0.00019730029396396862,
      "loss": 0.7177,
      "step": 3230
    },
    {
      "epoch": 0.20454545454545456,
      "grad_norm": 0.7577276825904846,
      "learning_rate": 0.00019727664792466515,
      "loss": 0.6552,
      "step": 3240
    },
    {
      "epoch": 0.20517676767676768,
      "grad_norm": 0.9298124313354492,
      "learning_rate": 0.0001972529002110012,
      "loss": 0.6579,
      "step": 3250
    },
    {
      "epoch": 0.2058080808080808,
      "grad_norm": 0.5072958469390869,
      "learning_rate": 0.00019722905084779808,
      "loss": 0.8608,
      "step": 3260
    },
    {
      "epoch": 0.20643939393939395,
      "grad_norm": 0.5833141803741455,
      "learning_rate": 0.0001972050998599833,
      "loss": 0.7793,
      "step": 3270
    },
    {
      "epoch": 0.20707070707070707,
      "grad_norm": 0.6601421236991882,
      "learning_rate": 0.00019718104727259073,
      "loss": 0.7038,
      "step": 3280
    },
    {
      "epoch": 0.2077020202020202,
      "grad_norm": 0.8299331665039062,
      "learning_rate": 0.00019715689311076024,
      "loss": 0.6527,
      "step": 3290
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 1.1553014516830444,
      "learning_rate": 0.000197132637399738,
      "loss": 0.6747,
      "step": 3300
    },
    {
      "epoch": 0.20896464646464646,
      "grad_norm": 0.6042424440383911,
      "learning_rate": 0.00019710828016487628,
      "loss": 0.9149,
      "step": 3310
    },
    {
      "epoch": 0.20959595959595959,
      "grad_norm": 0.6279816031455994,
      "learning_rate": 0.00019708382143163343,
      "loss": 0.751,
      "step": 3320
    },
    {
      "epoch": 0.21022727272727273,
      "grad_norm": 0.7412064075469971,
      "learning_rate": 0.000197059261225574,
      "loss": 0.7027,
      "step": 3330
    },
    {
      "epoch": 0.21085858585858586,
      "grad_norm": 0.8499515652656555,
      "learning_rate": 0.00019703459957236844,
      "loss": 0.6421,
      "step": 3340
    },
    {
      "epoch": 0.21148989898989898,
      "grad_norm": 1.0128357410430908,
      "learning_rate": 0.00019700983649779334,
      "loss": 0.5927,
      "step": 3350
    },
    {
      "epoch": 0.21212121212121213,
      "grad_norm": 0.5070518851280212,
      "learning_rate": 0.0001969849720277313,
      "loss": 0.9043,
      "step": 3360
    },
    {
      "epoch": 0.21275252525252525,
      "grad_norm": 0.6357104778289795,
      "learning_rate": 0.0001969600061881708,
      "loss": 0.7667,
      "step": 3370
    },
    {
      "epoch": 0.21338383838383837,
      "grad_norm": 0.6981188058853149,
      "learning_rate": 0.00019693493900520644,
      "loss": 0.7097,
      "step": 3380
    },
    {
      "epoch": 0.21401515151515152,
      "grad_norm": 0.8290572166442871,
      "learning_rate": 0.0001969097705050386,
      "loss": 0.6576,
      "step": 3390
    },
    {
      "epoch": 0.21464646464646464,
      "grad_norm": 0.9649792313575745,
      "learning_rate": 0.00019688450071397357,
      "loss": 0.635,
      "step": 3400
    },
    {
      "epoch": 0.2152777777777778,
      "grad_norm": 0.5470774173736572,
      "learning_rate": 0.00019685912965842359,
      "loss": 0.8291,
      "step": 3410
    },
    {
      "epoch": 0.2159090909090909,
      "grad_norm": 0.6061720252037048,
      "learning_rate": 0.00019683365736490672,
      "loss": 0.744,
      "step": 3420
    },
    {
      "epoch": 0.21654040404040403,
      "grad_norm": 0.7339971661567688,
      "learning_rate": 0.00019680808386004673,
      "loss": 0.6862,
      "step": 3430
    },
    {
      "epoch": 0.21717171717171718,
      "grad_norm": 0.742709219455719,
      "learning_rate": 0.00019678240917057335,
      "loss": 0.6256,
      "step": 3440
    },
    {
      "epoch": 0.2178030303030303,
      "grad_norm": 1.038543701171875,
      "learning_rate": 0.0001967566333233219,
      "loss": 0.632,
      "step": 3450
    },
    {
      "epoch": 0.21843434343434343,
      "grad_norm": 0.6171247959136963,
      "learning_rate": 0.0001967307563452336,
      "loss": 0.8551,
      "step": 3460
    },
    {
      "epoch": 0.21906565656565657,
      "grad_norm": 0.6691005229949951,
      "learning_rate": 0.00019670477826335517,
      "loss": 0.7501,
      "step": 3470
    },
    {
      "epoch": 0.2196969696969697,
      "grad_norm": 0.7246281504631042,
      "learning_rate": 0.00019667869910483922,
      "loss": 0.7102,
      "step": 3480
    },
    {
      "epoch": 0.22032828282828282,
      "grad_norm": 0.8003141283988953,
      "learning_rate": 0.00019665251889694387,
      "loss": 0.6457,
      "step": 3490
    },
    {
      "epoch": 0.22095959595959597,
      "grad_norm": 1.2344048023223877,
      "learning_rate": 0.00019662623766703285,
      "loss": 0.6386,
      "step": 3500
    },
    {
      "epoch": 0.2215909090909091,
      "grad_norm": 0.5584490895271301,
      "learning_rate": 0.00019659985544257557,
      "loss": 0.8473,
      "step": 3510
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.7575691342353821,
      "learning_rate": 0.00019657337225114692,
      "loss": 0.7638,
      "step": 3520
    },
    {
      "epoch": 0.22285353535353536,
      "grad_norm": 0.6677626371383667,
      "learning_rate": 0.0001965467881204274,
      "loss": 0.6938,
      "step": 3530
    },
    {
      "epoch": 0.22348484848484848,
      "grad_norm": 0.8051018714904785,
      "learning_rate": 0.00019652010307820287,
      "loss": 0.625,
      "step": 3540
    },
    {
      "epoch": 0.22411616161616163,
      "grad_norm": 1.150392770767212,
      "learning_rate": 0.00019649331715236484,
      "loss": 0.6314,
      "step": 3550
    },
    {
      "epoch": 0.22474747474747475,
      "grad_norm": 0.5936555862426758,
      "learning_rate": 0.00019646643037091017,
      "loss": 0.8472,
      "step": 3560
    },
    {
      "epoch": 0.22537878787878787,
      "grad_norm": 0.7444565296173096,
      "learning_rate": 0.00019643944276194112,
      "loss": 0.7804,
      "step": 3570
    },
    {
      "epoch": 0.22601010101010102,
      "grad_norm": 0.6727052330970764,
      "learning_rate": 0.0001964123543536654,
      "loss": 0.6928,
      "step": 3580
    },
    {
      "epoch": 0.22664141414141414,
      "grad_norm": 0.7139135003089905,
      "learning_rate": 0.00019638516517439598,
      "loss": 0.5986,
      "step": 3590
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 1.0766022205352783,
      "learning_rate": 0.0001963578752525513,
      "loss": 0.611,
      "step": 3600
    },
    {
      "epoch": 0.22790404040404041,
      "grad_norm": 0.5333102941513062,
      "learning_rate": 0.00019633048461665492,
      "loss": 0.8331,
      "step": 3610
    },
    {
      "epoch": 0.22853535353535354,
      "grad_norm": 0.6479005813598633,
      "learning_rate": 0.00019630299329533583,
      "loss": 0.7427,
      "step": 3620
    },
    {
      "epoch": 0.22916666666666666,
      "grad_norm": 0.7891281247138977,
      "learning_rate": 0.00019627540131732815,
      "loss": 0.6872,
      "step": 3630
    },
    {
      "epoch": 0.2297979797979798,
      "grad_norm": 0.744083821773529,
      "learning_rate": 0.0001962477087114713,
      "loss": 0.6245,
      "step": 3640
    },
    {
      "epoch": 0.23042929292929293,
      "grad_norm": 1.1561157703399658,
      "learning_rate": 0.00019621991550670975,
      "loss": 0.646,
      "step": 3650
    },
    {
      "epoch": 0.23106060606060605,
      "grad_norm": 0.548424482345581,
      "learning_rate": 0.0001961920217320932,
      "loss": 0.8397,
      "step": 3660
    },
    {
      "epoch": 0.2316919191919192,
      "grad_norm": 0.6671515107154846,
      "learning_rate": 0.0001961640274167765,
      "loss": 0.7579,
      "step": 3670
    },
    {
      "epoch": 0.23232323232323232,
      "grad_norm": 0.697683572769165,
      "learning_rate": 0.0001961359325900195,
      "loss": 0.6858,
      "step": 3680
    },
    {
      "epoch": 0.23295454545454544,
      "grad_norm": 0.8648586869239807,
      "learning_rate": 0.0001961077372811872,
      "loss": 0.6409,
      "step": 3690
    },
    {
      "epoch": 0.2335858585858586,
      "grad_norm": 0.9259229302406311,
      "learning_rate": 0.0001960794415197495,
      "loss": 0.635,
      "step": 3700
    },
    {
      "epoch": 0.2342171717171717,
      "grad_norm": 0.5201980471611023,
      "learning_rate": 0.00019605104533528138,
      "loss": 0.8651,
      "step": 3710
    },
    {
      "epoch": 0.23484848484848486,
      "grad_norm": 0.7483479380607605,
      "learning_rate": 0.00019602254875746283,
      "loss": 0.749,
      "step": 3720
    },
    {
      "epoch": 0.23547979797979798,
      "grad_norm": 0.6824462413787842,
      "learning_rate": 0.00019599395181607864,
      "loss": 0.6461,
      "step": 3730
    },
    {
      "epoch": 0.2361111111111111,
      "grad_norm": 0.8410384654998779,
      "learning_rate": 0.00019596525454101856,
      "loss": 0.6197,
      "step": 3740
    },
    {
      "epoch": 0.23674242424242425,
      "grad_norm": 1.0816935300827026,
      "learning_rate": 0.0001959364569622773,
      "loss": 0.6296,
      "step": 3750
    },
    {
      "epoch": 0.23737373737373738,
      "grad_norm": 0.583977460861206,
      "learning_rate": 0.00019590755910995426,
      "loss": 0.8402,
      "step": 3760
    },
    {
      "epoch": 0.2380050505050505,
      "grad_norm": 0.6941959857940674,
      "learning_rate": 0.00019587856101425377,
      "loss": 0.7222,
      "step": 3770
    },
    {
      "epoch": 0.23863636363636365,
      "grad_norm": 0.7717662453651428,
      "learning_rate": 0.00019584946270548482,
      "loss": 0.6863,
      "step": 3780
    },
    {
      "epoch": 0.23926767676767677,
      "grad_norm": 0.8349329829216003,
      "learning_rate": 0.00019582026421406125,
      "loss": 0.5904,
      "step": 3790
    },
    {
      "epoch": 0.2398989898989899,
      "grad_norm": 1.1655611991882324,
      "learning_rate": 0.00019579096557050156,
      "loss": 0.623,
      "step": 3800
    },
    {
      "epoch": 0.24053030303030304,
      "grad_norm": 0.5716366767883301,
      "learning_rate": 0.0001957615668054289,
      "loss": 0.8073,
      "step": 3810
    },
    {
      "epoch": 0.24116161616161616,
      "grad_norm": 0.7168810963630676,
      "learning_rate": 0.00019573206794957116,
      "loss": 0.7278,
      "step": 3820
    },
    {
      "epoch": 0.24179292929292928,
      "grad_norm": 0.7578932642936707,
      "learning_rate": 0.00019570246903376071,
      "loss": 0.6611,
      "step": 3830
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 0.7495874166488647,
      "learning_rate": 0.00019567277008893466,
      "loss": 0.6034,
      "step": 3840
    },
    {
      "epoch": 0.24305555555555555,
      "grad_norm": 1.0915462970733643,
      "learning_rate": 0.0001956429711461346,
      "loss": 0.6146,
      "step": 3850
    },
    {
      "epoch": 0.24368686868686867,
      "grad_norm": 0.6142016053199768,
      "learning_rate": 0.00019561307223650654,
      "loss": 0.8049,
      "step": 3860
    },
    {
      "epoch": 0.24431818181818182,
      "grad_norm": 0.7064968943595886,
      "learning_rate": 0.00019558307339130116,
      "loss": 0.7363,
      "step": 3870
    },
    {
      "epoch": 0.24494949494949494,
      "grad_norm": 0.785517156124115,
      "learning_rate": 0.00019555297464187343,
      "loss": 0.6719,
      "step": 3880
    },
    {
      "epoch": 0.2455808080808081,
      "grad_norm": 0.7869837284088135,
      "learning_rate": 0.0001955227760196829,
      "loss": 0.5953,
      "step": 3890
    },
    {
      "epoch": 0.24621212121212122,
      "grad_norm": 1.0495184659957886,
      "learning_rate": 0.00019549247755629333,
      "loss": 0.6269,
      "step": 3900
    },
    {
      "epoch": 0.24684343434343434,
      "grad_norm": 0.5724454522132874,
      "learning_rate": 0.00019546207928337298,
      "loss": 0.8508,
      "step": 3910
    },
    {
      "epoch": 0.2474747474747475,
      "grad_norm": 0.7092358469963074,
      "learning_rate": 0.00019543158123269439,
      "loss": 0.7218,
      "step": 3920
    },
    {
      "epoch": 0.2481060606060606,
      "grad_norm": 0.8124663233757019,
      "learning_rate": 0.00019540098343613435,
      "loss": 0.6789,
      "step": 3930
    },
    {
      "epoch": 0.24873737373737373,
      "grad_norm": 0.8590080142021179,
      "learning_rate": 0.0001953702859256739,
      "loss": 0.6081,
      "step": 3940
    },
    {
      "epoch": 0.24936868686868688,
      "grad_norm": 1.1656885147094727,
      "learning_rate": 0.00019533948873339836,
      "loss": 0.5964,
      "step": 3950
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.5682050585746765,
      "learning_rate": 0.00019530859189149723,
      "loss": 0.8059,
      "step": 3960
    },
    {
      "epoch": 0.25063131313131315,
      "grad_norm": 0.7570399045944214,
      "learning_rate": 0.00019527759543226414,
      "loss": 0.7325,
      "step": 3970
    },
    {
      "epoch": 0.25126262626262624,
      "grad_norm": 0.7688345909118652,
      "learning_rate": 0.00019524649938809681,
      "loss": 0.6367,
      "step": 3980
    },
    {
      "epoch": 0.2518939393939394,
      "grad_norm": 0.8515782952308655,
      "learning_rate": 0.00019521530379149716,
      "loss": 0.6034,
      "step": 3990
    },
    {
      "epoch": 0.25252525252525254,
      "grad_norm": 1.068707823753357,
      "learning_rate": 0.00019518400867507102,
      "loss": 0.6117,
      "step": 4000
    },
    {
      "epoch": 0.25252525252525254,
      "eval_loss": 0.6772322654724121,
      "eval_runtime": 27.3052,
      "eval_samples_per_second": 93.755,
      "eval_steps_per_second": 11.719,
      "step": 4000
    },
    {
      "epoch": 0.25315656565656564,
      "grad_norm": 0.5219240784645081,
      "learning_rate": 0.0001951526140715283,
      "loss": 0.889,
      "step": 4010
    },
    {
      "epoch": 0.2537878787878788,
      "grad_norm": 0.7831730842590332,
      "learning_rate": 0.00019512112001368297,
      "loss": 0.7098,
      "step": 4020
    },
    {
      "epoch": 0.25441919191919193,
      "grad_norm": 0.7411784529685974,
      "learning_rate": 0.0001950895265344528,
      "loss": 0.6524,
      "step": 4030
    },
    {
      "epoch": 0.255050505050505,
      "grad_norm": 0.764510989189148,
      "learning_rate": 0.00019505783366685958,
      "loss": 0.6029,
      "step": 4040
    },
    {
      "epoch": 0.2556818181818182,
      "grad_norm": 1.09827721118927,
      "learning_rate": 0.00019502604144402903,
      "loss": 0.5922,
      "step": 4050
    },
    {
      "epoch": 0.2563131313131313,
      "grad_norm": 0.5714800953865051,
      "learning_rate": 0.00019499414989919054,
      "loss": 0.8305,
      "step": 4060
    },
    {
      "epoch": 0.2569444444444444,
      "grad_norm": 0.792681097984314,
      "learning_rate": 0.00019496215906567748,
      "loss": 0.706,
      "step": 4070
    },
    {
      "epoch": 0.25757575757575757,
      "grad_norm": 0.7366827726364136,
      "learning_rate": 0.0001949300689769269,
      "loss": 0.6661,
      "step": 4080
    },
    {
      "epoch": 0.2582070707070707,
      "grad_norm": 0.830299973487854,
      "learning_rate": 0.0001948978796664797,
      "loss": 0.5909,
      "step": 4090
    },
    {
      "epoch": 0.2588383838383838,
      "grad_norm": 1.0126402378082275,
      "learning_rate": 0.00019486559116798028,
      "loss": 0.6126,
      "step": 4100
    },
    {
      "epoch": 0.25946969696969696,
      "grad_norm": 0.582241952419281,
      "learning_rate": 0.00019483320351517698,
      "loss": 0.8272,
      "step": 4110
    },
    {
      "epoch": 0.2601010101010101,
      "grad_norm": 0.6968902945518494,
      "learning_rate": 0.00019480071674192158,
      "loss": 0.7031,
      "step": 4120
    },
    {
      "epoch": 0.26073232323232326,
      "grad_norm": 0.727138340473175,
      "learning_rate": 0.00019476813088216955,
      "loss": 0.6279,
      "step": 4130
    },
    {
      "epoch": 0.26136363636363635,
      "grad_norm": 0.8846218585968018,
      "learning_rate": 0.00019473544596997986,
      "loss": 0.604,
      "step": 4140
    },
    {
      "epoch": 0.2619949494949495,
      "grad_norm": 1.0743659734725952,
      "learning_rate": 0.0001947026620395151,
      "loss": 0.5853,
      "step": 4150
    },
    {
      "epoch": 0.26262626262626265,
      "grad_norm": 0.5679904222488403,
      "learning_rate": 0.00019466977912504127,
      "loss": 0.8055,
      "step": 4160
    },
    {
      "epoch": 0.26325757575757575,
      "grad_norm": 0.6614130735397339,
      "learning_rate": 0.00019463679726092791,
      "loss": 0.7486,
      "step": 4170
    },
    {
      "epoch": 0.2638888888888889,
      "grad_norm": 0.7180368900299072,
      "learning_rate": 0.0001946037164816479,
      "loss": 0.6361,
      "step": 4180
    },
    {
      "epoch": 0.26452020202020204,
      "grad_norm": 0.7655189633369446,
      "learning_rate": 0.00019457053682177754,
      "loss": 0.5966,
      "step": 4190
    },
    {
      "epoch": 0.26515151515151514,
      "grad_norm": 0.9534170627593994,
      "learning_rate": 0.00019453725831599652,
      "loss": 0.5772,
      "step": 4200
    },
    {
      "epoch": 0.2657828282828283,
      "grad_norm": 0.5781427621841431,
      "learning_rate": 0.0001945038809990878,
      "loss": 0.8491,
      "step": 4210
    },
    {
      "epoch": 0.26641414141414144,
      "grad_norm": 0.6894657611846924,
      "learning_rate": 0.0001944704049059376,
      "loss": 0.7078,
      "step": 4220
    },
    {
      "epoch": 0.26704545454545453,
      "grad_norm": 0.7195695042610168,
      "learning_rate": 0.00019443683007153544,
      "loss": 0.6465,
      "step": 4230
    },
    {
      "epoch": 0.2676767676767677,
      "grad_norm": 0.7929624915122986,
      "learning_rate": 0.00019440315653097398,
      "loss": 0.5807,
      "step": 4240
    },
    {
      "epoch": 0.26830808080808083,
      "grad_norm": 1.247113585472107,
      "learning_rate": 0.00019436938431944916,
      "loss": 0.5965,
      "step": 4250
    },
    {
      "epoch": 0.2689393939393939,
      "grad_norm": 0.5307172536849976,
      "learning_rate": 0.0001943355134722599,
      "loss": 0.8116,
      "step": 4260
    },
    {
      "epoch": 0.26957070707070707,
      "grad_norm": 0.673069417476654,
      "learning_rate": 0.00019430154402480832,
      "loss": 0.7112,
      "step": 4270
    },
    {
      "epoch": 0.2702020202020202,
      "grad_norm": 0.7150174379348755,
      "learning_rate": 0.0001942674760125996,
      "loss": 0.6553,
      "step": 4280
    },
    {
      "epoch": 0.2708333333333333,
      "grad_norm": 0.8584343194961548,
      "learning_rate": 0.00019423330947124183,
      "loss": 0.5793,
      "step": 4290
    },
    {
      "epoch": 0.27146464646464646,
      "grad_norm": 1.003257393836975,
      "learning_rate": 0.00019419904443644624,
      "loss": 0.6084,
      "step": 4300
    },
    {
      "epoch": 0.2720959595959596,
      "grad_norm": 0.6080260872840881,
      "learning_rate": 0.00019416468094402687,
      "loss": 0.8042,
      "step": 4310
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.7347806096076965,
      "learning_rate": 0.00019413021902990078,
      "loss": 0.7033,
      "step": 4320
    },
    {
      "epoch": 0.27335858585858586,
      "grad_norm": 0.7934287190437317,
      "learning_rate": 0.00019409565873008782,
      "loss": 0.6367,
      "step": 4330
    },
    {
      "epoch": 0.273989898989899,
      "grad_norm": 0.8434942960739136,
      "learning_rate": 0.0001940610000807107,
      "loss": 0.5901,
      "step": 4340
    },
    {
      "epoch": 0.2746212121212121,
      "grad_norm": 1.2004021406173706,
      "learning_rate": 0.00019402624311799495,
      "loss": 0.6021,
      "step": 4350
    },
    {
      "epoch": 0.27525252525252525,
      "grad_norm": 0.5505009293556213,
      "learning_rate": 0.00019399138787826883,
      "loss": 0.8679,
      "step": 4360
    },
    {
      "epoch": 0.2758838383838384,
      "grad_norm": 0.6756744384765625,
      "learning_rate": 0.0001939564343979633,
      "loss": 0.7198,
      "step": 4370
    },
    {
      "epoch": 0.2765151515151515,
      "grad_norm": 0.7191880941390991,
      "learning_rate": 0.00019392138271361205,
      "loss": 0.6194,
      "step": 4380
    },
    {
      "epoch": 0.27714646464646464,
      "grad_norm": 0.7674593329429626,
      "learning_rate": 0.00019388623286185138,
      "loss": 0.5826,
      "step": 4390
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 1.0534693002700806,
      "learning_rate": 0.00019385098487942023,
      "loss": 0.6015,
      "step": 4400
    },
    {
      "epoch": 0.2784090909090909,
      "grad_norm": 0.5751586556434631,
      "learning_rate": 0.00019381563880316004,
      "loss": 0.8411,
      "step": 4410
    },
    {
      "epoch": 0.27904040404040403,
      "grad_norm": 0.7208975553512573,
      "learning_rate": 0.0001937801946700149,
      "loss": 0.6886,
      "step": 4420
    },
    {
      "epoch": 0.2796717171717172,
      "grad_norm": 0.7053536772727966,
      "learning_rate": 0.00019374465251703122,
      "loss": 0.6338,
      "step": 4430
    },
    {
      "epoch": 0.2803030303030303,
      "grad_norm": 0.7985022068023682,
      "learning_rate": 0.00019370901238135804,
      "loss": 0.5766,
      "step": 4440
    },
    {
      "epoch": 0.2809343434343434,
      "grad_norm": 1.1297340393066406,
      "learning_rate": 0.00019367327430024663,
      "loss": 0.5979,
      "step": 4450
    },
    {
      "epoch": 0.2815656565656566,
      "grad_norm": 0.604833722114563,
      "learning_rate": 0.00019363743831105081,
      "loss": 0.8053,
      "step": 4460
    },
    {
      "epoch": 0.2821969696969697,
      "grad_norm": 0.7244064211845398,
      "learning_rate": 0.00019360150445122665,
      "loss": 0.7158,
      "step": 4470
    },
    {
      "epoch": 0.2828282828282828,
      "grad_norm": 0.8539688587188721,
      "learning_rate": 0.0001935654727583325,
      "loss": 0.6034,
      "step": 4480
    },
    {
      "epoch": 0.28345959595959597,
      "grad_norm": 0.8163095116615295,
      "learning_rate": 0.00019352934327002892,
      "loss": 0.569,
      "step": 4490
    },
    {
      "epoch": 0.2840909090909091,
      "grad_norm": 1.0585854053497314,
      "learning_rate": 0.00019349311602407884,
      "loss": 0.59,
      "step": 4500
    },
    {
      "epoch": 0.2847222222222222,
      "grad_norm": 0.5627225041389465,
      "learning_rate": 0.00019345679105834727,
      "loss": 0.792,
      "step": 4510
    },
    {
      "epoch": 0.28535353535353536,
      "grad_norm": 0.6342817544937134,
      "learning_rate": 0.00019342036841080132,
      "loss": 0.6857,
      "step": 4520
    },
    {
      "epoch": 0.2859848484848485,
      "grad_norm": 0.8010412454605103,
      "learning_rate": 0.00019338384811951027,
      "loss": 0.6352,
      "step": 4530
    },
    {
      "epoch": 0.2866161616161616,
      "grad_norm": 0.9310699701309204,
      "learning_rate": 0.00019334723022264544,
      "loss": 0.5966,
      "step": 4540
    },
    {
      "epoch": 0.28724747474747475,
      "grad_norm": 0.9700534343719482,
      "learning_rate": 0.00019331051475848018,
      "loss": 0.5774,
      "step": 4550
    },
    {
      "epoch": 0.2878787878787879,
      "grad_norm": 0.5599976181983948,
      "learning_rate": 0.0001932737017653897,
      "loss": 0.8336,
      "step": 4560
    },
    {
      "epoch": 0.288510101010101,
      "grad_norm": 0.6462521553039551,
      "learning_rate": 0.00019323679128185135,
      "loss": 0.6961,
      "step": 4570
    },
    {
      "epoch": 0.28914141414141414,
      "grad_norm": 0.7330341935157776,
      "learning_rate": 0.00019319978334644426,
      "loss": 0.6463,
      "step": 4580
    },
    {
      "epoch": 0.2897727272727273,
      "grad_norm": 0.8686737418174744,
      "learning_rate": 0.00019316267799784938,
      "loss": 0.5649,
      "step": 4590
    },
    {
      "epoch": 0.2904040404040404,
      "grad_norm": 1.0145928859710693,
      "learning_rate": 0.00019312547527484958,
      "loss": 0.5921,
      "step": 4600
    },
    {
      "epoch": 0.29103535353535354,
      "grad_norm": 0.6002787351608276,
      "learning_rate": 0.00019308817521632943,
      "loss": 0.8042,
      "step": 4610
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.6557075381278992,
      "learning_rate": 0.00019305077786127526,
      "loss": 0.681,
      "step": 4620
    },
    {
      "epoch": 0.2922979797979798,
      "grad_norm": 0.726617693901062,
      "learning_rate": 0.00019301328324877512,
      "loss": 0.6145,
      "step": 4630
    },
    {
      "epoch": 0.29292929292929293,
      "grad_norm": 0.8333576321601868,
      "learning_rate": 0.00019297569141801867,
      "loss": 0.5771,
      "step": 4640
    },
    {
      "epoch": 0.2935606060606061,
      "grad_norm": 1.144715428352356,
      "learning_rate": 0.00019293800240829717,
      "loss": 0.5777,
      "step": 4650
    },
    {
      "epoch": 0.29419191919191917,
      "grad_norm": 0.5324021577835083,
      "learning_rate": 0.00019290021625900354,
      "loss": 0.7965,
      "step": 4660
    },
    {
      "epoch": 0.2948232323232323,
      "grad_norm": 0.7485097646713257,
      "learning_rate": 0.00019286233300963218,
      "loss": 0.7075,
      "step": 4670
    },
    {
      "epoch": 0.29545454545454547,
      "grad_norm": 0.668709933757782,
      "learning_rate": 0.00019282435269977894,
      "loss": 0.6295,
      "step": 4680
    },
    {
      "epoch": 0.29608585858585856,
      "grad_norm": 0.7289692759513855,
      "learning_rate": 0.00019278627536914117,
      "loss": 0.565,
      "step": 4690
    },
    {
      "epoch": 0.2967171717171717,
      "grad_norm": 1.241212010383606,
      "learning_rate": 0.00019274810105751762,
      "loss": 0.5714,
      "step": 4700
    },
    {
      "epoch": 0.29734848484848486,
      "grad_norm": 0.5874489545822144,
      "learning_rate": 0.0001927098298048084,
      "loss": 0.7991,
      "step": 4710
    },
    {
      "epoch": 0.29797979797979796,
      "grad_norm": 0.6207696199417114,
      "learning_rate": 0.00019267146165101491,
      "loss": 0.727,
      "step": 4720
    },
    {
      "epoch": 0.2986111111111111,
      "grad_norm": 0.6582732796669006,
      "learning_rate": 0.0001926329966362399,
      "loss": 0.6191,
      "step": 4730
    },
    {
      "epoch": 0.29924242424242425,
      "grad_norm": 0.7452285289764404,
      "learning_rate": 0.0001925944348006873,
      "loss": 0.5626,
      "step": 4740
    },
    {
      "epoch": 0.29987373737373735,
      "grad_norm": 1.1364666223526,
      "learning_rate": 0.00019255577618466227,
      "loss": 0.5828,
      "step": 4750
    },
    {
      "epoch": 0.3005050505050505,
      "grad_norm": 0.5877828001976013,
      "learning_rate": 0.0001925170208285711,
      "loss": 0.7884,
      "step": 4760
    },
    {
      "epoch": 0.30113636363636365,
      "grad_norm": 0.7253538370132446,
      "learning_rate": 0.00019247816877292125,
      "loss": 0.6988,
      "step": 4770
    },
    {
      "epoch": 0.30176767676767674,
      "grad_norm": 0.7966983914375305,
      "learning_rate": 0.0001924392200583212,
      "loss": 0.6379,
      "step": 4780
    },
    {
      "epoch": 0.3023989898989899,
      "grad_norm": 0.7604701519012451,
      "learning_rate": 0.00019240017472548044,
      "loss": 0.5318,
      "step": 4790
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 1.0633622407913208,
      "learning_rate": 0.0001923610328152095,
      "loss": 0.5949,
      "step": 4800
    },
    {
      "epoch": 0.3036616161616162,
      "grad_norm": 0.5759729743003845,
      "learning_rate": 0.00019232179436841983,
      "loss": 0.7733,
      "step": 4810
    },
    {
      "epoch": 0.3042929292929293,
      "grad_norm": 0.7012568116188049,
      "learning_rate": 0.00019228245942612374,
      "loss": 0.7112,
      "step": 4820
    },
    {
      "epoch": 0.30492424242424243,
      "grad_norm": 0.7440206408500671,
      "learning_rate": 0.0001922430280294345,
      "loss": 0.6041,
      "step": 4830
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 0.8046401143074036,
      "learning_rate": 0.0001922035002195661,
      "loss": 0.555,
      "step": 4840
    },
    {
      "epoch": 0.3061868686868687,
      "grad_norm": 1.043587565422058,
      "learning_rate": 0.00019216387603783334,
      "loss": 0.5325,
      "step": 4850
    },
    {
      "epoch": 0.3068181818181818,
      "grad_norm": 0.5920006632804871,
      "learning_rate": 0.00019212415552565174,
      "loss": 0.8003,
      "step": 4860
    },
    {
      "epoch": 0.307449494949495,
      "grad_norm": 0.6786617636680603,
      "learning_rate": 0.00019208433872453754,
      "loss": 0.696,
      "step": 4870
    },
    {
      "epoch": 0.30808080808080807,
      "grad_norm": 0.8274611830711365,
      "learning_rate": 0.00019204442567610756,
      "loss": 0.6497,
      "step": 4880
    },
    {
      "epoch": 0.3087121212121212,
      "grad_norm": 0.9361644983291626,
      "learning_rate": 0.00019200441642207923,
      "loss": 0.5727,
      "step": 4890
    },
    {
      "epoch": 0.30934343434343436,
      "grad_norm": 0.9931752681732178,
      "learning_rate": 0.0001919643110042706,
      "loss": 0.5839,
      "step": 4900
    },
    {
      "epoch": 0.30997474747474746,
      "grad_norm": 0.5650510191917419,
      "learning_rate": 0.00019192410946460015,
      "loss": 0.7536,
      "step": 4910
    },
    {
      "epoch": 0.3106060606060606,
      "grad_norm": 0.7109998464584351,
      "learning_rate": 0.00019188381184508688,
      "loss": 0.6898,
      "step": 4920
    },
    {
      "epoch": 0.31123737373737376,
      "grad_norm": 0.6318044662475586,
      "learning_rate": 0.0001918434181878502,
      "loss": 0.6105,
      "step": 4930
    },
    {
      "epoch": 0.31186868686868685,
      "grad_norm": 0.8525688648223877,
      "learning_rate": 0.00019180292853510992,
      "loss": 0.5463,
      "step": 4940
    },
    {
      "epoch": 0.3125,
      "grad_norm": 1.1676466464996338,
      "learning_rate": 0.00019176234292918608,
      "loss": 0.6016,
      "step": 4950
    },
    {
      "epoch": 0.31313131313131315,
      "grad_norm": 0.5535843968391418,
      "learning_rate": 0.00019172166141249915,
      "loss": 0.7952,
      "step": 4960
    },
    {
      "epoch": 0.31376262626262624,
      "grad_norm": 0.6939905285835266,
      "learning_rate": 0.00019168088402756985,
      "loss": 0.6837,
      "step": 4970
    },
    {
      "epoch": 0.3143939393939394,
      "grad_norm": 0.7587629556655884,
      "learning_rate": 0.0001916400108170189,
      "loss": 0.6063,
      "step": 4980
    },
    {
      "epoch": 0.31502525252525254,
      "grad_norm": 0.8732277154922485,
      "learning_rate": 0.00019159904182356746,
      "loss": 0.5479,
      "step": 4990
    },
    {
      "epoch": 0.31565656565656564,
      "grad_norm": 1.0229947566986084,
      "learning_rate": 0.00019155797709003656,
      "loss": 0.5747,
      "step": 5000
    },
    {
      "epoch": 0.31565656565656564,
      "eval_loss": 0.6354244947433472,
      "eval_runtime": 28.38,
      "eval_samples_per_second": 90.204,
      "eval_steps_per_second": 11.276,
      "step": 5000
    },
    {
      "epoch": 0.3162878787878788,
      "grad_norm": 0.6578483581542969,
      "learning_rate": 0.00019151681665934746,
      "loss": 0.8051,
      "step": 5010
    },
    {
      "epoch": 0.31691919191919193,
      "grad_norm": 0.6700741052627563,
      "learning_rate": 0.00019147556057452135,
      "loss": 0.6832,
      "step": 5020
    },
    {
      "epoch": 0.317550505050505,
      "grad_norm": 0.6822919845581055,
      "learning_rate": 0.00019143420887867945,
      "loss": 0.6239,
      "step": 5030
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 0.8895336389541626,
      "learning_rate": 0.00019139276161504287,
      "loss": 0.5422,
      "step": 5040
    },
    {
      "epoch": 0.3188131313131313,
      "grad_norm": 1.083667278289795,
      "learning_rate": 0.00019135121882693268,
      "loss": 0.5851,
      "step": 5050
    },
    {
      "epoch": 0.3194444444444444,
      "grad_norm": 0.5383729338645935,
      "learning_rate": 0.00019130958055776969,
      "loss": 0.7796,
      "step": 5060
    },
    {
      "epoch": 0.32007575757575757,
      "grad_norm": 0.7698951363563538,
      "learning_rate": 0.00019126784685107463,
      "loss": 0.6807,
      "step": 5070
    },
    {
      "epoch": 0.3207070707070707,
      "grad_norm": 0.8183860778808594,
      "learning_rate": 0.00019122601775046789,
      "loss": 0.6165,
      "step": 5080
    },
    {
      "epoch": 0.3213383838383838,
      "grad_norm": 0.7684093117713928,
      "learning_rate": 0.00019118409329966956,
      "loss": 0.562,
      "step": 5090
    },
    {
      "epoch": 0.32196969696969696,
      "grad_norm": 0.9316547513008118,
      "learning_rate": 0.00019114207354249948,
      "loss": 0.5644,
      "step": 5100
    },
    {
      "epoch": 0.3226010101010101,
      "grad_norm": 0.5924767255783081,
      "learning_rate": 0.00019109995852287698,
      "loss": 0.7886,
      "step": 5110
    },
    {
      "epoch": 0.32323232323232326,
      "grad_norm": 0.7118251323699951,
      "learning_rate": 0.0001910577482848211,
      "loss": 0.6822,
      "step": 5120
    },
    {
      "epoch": 0.32386363636363635,
      "grad_norm": 0.7546359300613403,
      "learning_rate": 0.0001910154428724503,
      "loss": 0.6195,
      "step": 5130
    },
    {
      "epoch": 0.3244949494949495,
      "grad_norm": 0.8794420957565308,
      "learning_rate": 0.00019097304232998255,
      "loss": 0.5468,
      "step": 5140
    },
    {
      "epoch": 0.32512626262626265,
      "grad_norm": 0.9442066550254822,
      "learning_rate": 0.00019093054670173522,
      "loss": 0.5689,
      "step": 5150
    },
    {
      "epoch": 0.32575757575757575,
      "grad_norm": 0.6172857880592346,
      "learning_rate": 0.00019088795603212517,
      "loss": 0.8134,
      "step": 5160
    },
    {
      "epoch": 0.3263888888888889,
      "grad_norm": 0.650283932685852,
      "learning_rate": 0.00019084527036566847,
      "loss": 0.6556,
      "step": 5170
    },
    {
      "epoch": 0.32702020202020204,
      "grad_norm": 0.7220401763916016,
      "learning_rate": 0.0001908024897469805,
      "loss": 0.6053,
      "step": 5180
    },
    {
      "epoch": 0.32765151515151514,
      "grad_norm": 0.8287461996078491,
      "learning_rate": 0.00019075961422077597,
      "loss": 0.5468,
      "step": 5190
    },
    {
      "epoch": 0.3282828282828283,
      "grad_norm": 1.1781922578811646,
      "learning_rate": 0.00019071664383186874,
      "loss": 0.5837,
      "step": 5200
    },
    {
      "epoch": 0.32891414141414144,
      "grad_norm": 0.6113929152488708,
      "learning_rate": 0.00019067357862517177,
      "loss": 0.7827,
      "step": 5210
    },
    {
      "epoch": 0.32954545454545453,
      "grad_norm": 0.7292771339416504,
      "learning_rate": 0.00019063041864569722,
      "loss": 0.6508,
      "step": 5220
    },
    {
      "epoch": 0.3301767676767677,
      "grad_norm": 0.6693084239959717,
      "learning_rate": 0.00019058716393855624,
      "loss": 0.6278,
      "step": 5230
    },
    {
      "epoch": 0.33080808080808083,
      "grad_norm": 0.8752071261405945,
      "learning_rate": 0.000190543814548959,
      "loss": 0.5606,
      "step": 5240
    },
    {
      "epoch": 0.3314393939393939,
      "grad_norm": 0.9859668016433716,
      "learning_rate": 0.00019050037052221463,
      "loss": 0.5579,
      "step": 5250
    },
    {
      "epoch": 0.33207070707070707,
      "grad_norm": 0.5765371918678284,
      "learning_rate": 0.00019045683190373124,
      "loss": 0.805,
      "step": 5260
    },
    {
      "epoch": 0.3327020202020202,
      "grad_norm": 0.7346866130828857,
      "learning_rate": 0.00019041319873901573,
      "loss": 0.6739,
      "step": 5270
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.718723714351654,
      "learning_rate": 0.00019036947107367382,
      "loss": 0.5789,
      "step": 5280
    },
    {
      "epoch": 0.33396464646464646,
      "grad_norm": 0.8555094599723816,
      "learning_rate": 0.00019032564895341006,
      "loss": 0.577,
      "step": 5290
    },
    {
      "epoch": 0.3345959595959596,
      "grad_norm": 1.1615906953811646,
      "learning_rate": 0.00019028173242402767,
      "loss": 0.5765,
      "step": 5300
    },
    {
      "epoch": 0.3352272727272727,
      "grad_norm": 0.6241433620452881,
      "learning_rate": 0.0001902377215314286,
      "loss": 0.7686,
      "step": 5310
    },
    {
      "epoch": 0.33585858585858586,
      "grad_norm": 0.7163758873939514,
      "learning_rate": 0.00019019361632161336,
      "loss": 0.677,
      "step": 5320
    },
    {
      "epoch": 0.336489898989899,
      "grad_norm": 0.7867381572723389,
      "learning_rate": 0.00019014941684068114,
      "loss": 0.596,
      "step": 5330
    },
    {
      "epoch": 0.3371212121212121,
      "grad_norm": 0.839661717414856,
      "learning_rate": 0.00019010512313482955,
      "loss": 0.5634,
      "step": 5340
    },
    {
      "epoch": 0.33775252525252525,
      "grad_norm": 1.001997709274292,
      "learning_rate": 0.00019006073525035477,
      "loss": 0.5794,
      "step": 5350
    },
    {
      "epoch": 0.3383838383838384,
      "grad_norm": 0.6121259331703186,
      "learning_rate": 0.0001900162532336514,
      "loss": 0.7722,
      "step": 5360
    },
    {
      "epoch": 0.3390151515151515,
      "grad_norm": 0.6729865670204163,
      "learning_rate": 0.00018997167713121236,
      "loss": 0.6536,
      "step": 5370
    },
    {
      "epoch": 0.33964646464646464,
      "grad_norm": 0.7232084274291992,
      "learning_rate": 0.000189927006989629,
      "loss": 0.5816,
      "step": 5380
    },
    {
      "epoch": 0.3402777777777778,
      "grad_norm": 0.8457463383674622,
      "learning_rate": 0.0001898822428555909,
      "loss": 0.5316,
      "step": 5390
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 1.1690783500671387,
      "learning_rate": 0.00018983738477588595,
      "loss": 0.564,
      "step": 5400
    },
    {
      "epoch": 0.34154040404040403,
      "grad_norm": 0.6278010010719299,
      "learning_rate": 0.00018979243279740015,
      "loss": 0.7921,
      "step": 5410
    },
    {
      "epoch": 0.3421717171717172,
      "grad_norm": 0.6660869121551514,
      "learning_rate": 0.00018974738696711768,
      "loss": 0.6724,
      "step": 5420
    },
    {
      "epoch": 0.3428030303030303,
      "grad_norm": 0.7201115489006042,
      "learning_rate": 0.00018970224733212083,
      "loss": 0.5853,
      "step": 5430
    },
    {
      "epoch": 0.3434343434343434,
      "grad_norm": 0.9530907273292542,
      "learning_rate": 0.0001896570139395899,
      "loss": 0.5394,
      "step": 5440
    },
    {
      "epoch": 0.3440656565656566,
      "grad_norm": 0.9547246694564819,
      "learning_rate": 0.00018961168683680326,
      "loss": 0.5302,
      "step": 5450
    },
    {
      "epoch": 0.3446969696969697,
      "grad_norm": 0.6238282322883606,
      "learning_rate": 0.0001895662660711371,
      "loss": 0.7731,
      "step": 5460
    },
    {
      "epoch": 0.3453282828282828,
      "grad_norm": 0.6518881320953369,
      "learning_rate": 0.00018952075169006568,
      "loss": 0.6653,
      "step": 5470
    },
    {
      "epoch": 0.34595959595959597,
      "grad_norm": 0.7803547978401184,
      "learning_rate": 0.00018947514374116089,
      "loss": 0.6125,
      "step": 5480
    },
    {
      "epoch": 0.3465909090909091,
      "grad_norm": 0.7857193946838379,
      "learning_rate": 0.00018942944227209264,
      "loss": 0.5539,
      "step": 5490
    },
    {
      "epoch": 0.3472222222222222,
      "grad_norm": 1.122987985610962,
      "learning_rate": 0.0001893836473306284,
      "loss": 0.5603,
      "step": 5500
    },
    {
      "epoch": 0.34785353535353536,
      "grad_norm": 0.5832096934318542,
      "learning_rate": 0.00018933775896463347,
      "loss": 0.7777,
      "step": 5510
    },
    {
      "epoch": 0.3484848484848485,
      "grad_norm": 0.6410543918609619,
      "learning_rate": 0.00018929177722207076,
      "loss": 0.6832,
      "step": 5520
    },
    {
      "epoch": 0.3491161616161616,
      "grad_norm": 0.7499108910560608,
      "learning_rate": 0.00018924570215100075,
      "loss": 0.5925,
      "step": 5530
    },
    {
      "epoch": 0.34974747474747475,
      "grad_norm": 0.7321015000343323,
      "learning_rate": 0.0001891995337995815,
      "loss": 0.5456,
      "step": 5540
    },
    {
      "epoch": 0.3503787878787879,
      "grad_norm": 1.081998348236084,
      "learning_rate": 0.00018915327221606854,
      "loss": 0.5583,
      "step": 5550
    },
    {
      "epoch": 0.351010101010101,
      "grad_norm": 0.5806440711021423,
      "learning_rate": 0.0001891069174488149,
      "loss": 0.7725,
      "step": 5560
    },
    {
      "epoch": 0.35164141414141414,
      "grad_norm": 0.6845955848693848,
      "learning_rate": 0.0001890604695462709,
      "loss": 0.6665,
      "step": 5570
    },
    {
      "epoch": 0.3522727272727273,
      "grad_norm": 0.7361495494842529,
      "learning_rate": 0.0001890139285569843,
      "loss": 0.589,
      "step": 5580
    },
    {
      "epoch": 0.3529040404040404,
      "grad_norm": 0.7890605926513672,
      "learning_rate": 0.00018896729452960015,
      "loss": 0.542,
      "step": 5590
    },
    {
      "epoch": 0.35353535353535354,
      "grad_norm": 1.4187060594558716,
      "learning_rate": 0.00018892056751286073,
      "loss": 0.5633,
      "step": 5600
    },
    {
      "epoch": 0.3541666666666667,
      "grad_norm": 0.573908269405365,
      "learning_rate": 0.0001888737475556055,
      "loss": 0.7618,
      "step": 5610
    },
    {
      "epoch": 0.3547979797979798,
      "grad_norm": 0.6761958003044128,
      "learning_rate": 0.00018882683470677103,
      "loss": 0.6585,
      "step": 5620
    },
    {
      "epoch": 0.35542929292929293,
      "grad_norm": 0.7293843030929565,
      "learning_rate": 0.00018877982901539103,
      "loss": 0.6277,
      "step": 5630
    },
    {
      "epoch": 0.3560606060606061,
      "grad_norm": 0.8363537192344666,
      "learning_rate": 0.00018873273053059627,
      "loss": 0.5663,
      "step": 5640
    },
    {
      "epoch": 0.35669191919191917,
      "grad_norm": 1.3587478399276733,
      "learning_rate": 0.00018868553930161447,
      "loss": 0.5326,
      "step": 5650
    },
    {
      "epoch": 0.3573232323232323,
      "grad_norm": 0.6028680801391602,
      "learning_rate": 0.00018863825537777026,
      "loss": 0.7792,
      "step": 5660
    },
    {
      "epoch": 0.35795454545454547,
      "grad_norm": 0.6457834839820862,
      "learning_rate": 0.00018859087880848525,
      "loss": 0.661,
      "step": 5670
    },
    {
      "epoch": 0.35858585858585856,
      "grad_norm": 0.7663736939430237,
      "learning_rate": 0.0001885434096432778,
      "loss": 0.5807,
      "step": 5680
    },
    {
      "epoch": 0.3592171717171717,
      "grad_norm": 0.8832402229309082,
      "learning_rate": 0.00018849584793176303,
      "loss": 0.5543,
      "step": 5690
    },
    {
      "epoch": 0.35984848484848486,
      "grad_norm": 1.1062904596328735,
      "learning_rate": 0.00018844819372365286,
      "loss": 0.5572,
      "step": 5700
    },
    {
      "epoch": 0.36047979797979796,
      "grad_norm": 0.5973621606826782,
      "learning_rate": 0.0001884004470687559,
      "loss": 0.7711,
      "step": 5710
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 0.6439493298530579,
      "learning_rate": 0.00018835260801697734,
      "loss": 0.6446,
      "step": 5720
    },
    {
      "epoch": 0.36174242424242425,
      "grad_norm": 0.6515389680862427,
      "learning_rate": 0.00018830467661831891,
      "loss": 0.5828,
      "step": 5730
    },
    {
      "epoch": 0.36237373737373735,
      "grad_norm": 0.797014057636261,
      "learning_rate": 0.00018825665292287894,
      "loss": 0.5391,
      "step": 5740
    },
    {
      "epoch": 0.3630050505050505,
      "grad_norm": 0.9291923642158508,
      "learning_rate": 0.0001882085369808522,
      "loss": 0.5817,
      "step": 5750
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.5489802360534668,
      "learning_rate": 0.00018816032884252988,
      "loss": 0.7893,
      "step": 5760
    },
    {
      "epoch": 0.36426767676767674,
      "grad_norm": 0.7280834317207336,
      "learning_rate": 0.0001881120285582995,
      "loss": 0.6722,
      "step": 5770
    },
    {
      "epoch": 0.3648989898989899,
      "grad_norm": 0.861813485622406,
      "learning_rate": 0.00018806363617864493,
      "loss": 0.5882,
      "step": 5780
    },
    {
      "epoch": 0.36553030303030304,
      "grad_norm": 0.8888502717018127,
      "learning_rate": 0.00018801515175414629,
      "loss": 0.5327,
      "step": 5790
    },
    {
      "epoch": 0.3661616161616162,
      "grad_norm": 1.318236231803894,
      "learning_rate": 0.00018796657533547988,
      "loss": 0.5337,
      "step": 5800
    },
    {
      "epoch": 0.3667929292929293,
      "grad_norm": 0.5998983979225159,
      "learning_rate": 0.0001879179069734182,
      "loss": 0.8128,
      "step": 5810
    },
    {
      "epoch": 0.36742424242424243,
      "grad_norm": 0.6808114647865295,
      "learning_rate": 0.00018786914671882983,
      "loss": 0.668,
      "step": 5820
    },
    {
      "epoch": 0.3680555555555556,
      "grad_norm": 0.7133130431175232,
      "learning_rate": 0.0001878202946226794,
      "loss": 0.5761,
      "step": 5830
    },
    {
      "epoch": 0.3686868686868687,
      "grad_norm": 0.8375285267829895,
      "learning_rate": 0.00018777135073602748,
      "loss": 0.5452,
      "step": 5840
    },
    {
      "epoch": 0.3693181818181818,
      "grad_norm": 0.9664574265480042,
      "learning_rate": 0.00018772231511003068,
      "loss": 0.5618,
      "step": 5850
    },
    {
      "epoch": 0.369949494949495,
      "grad_norm": 0.5635879039764404,
      "learning_rate": 0.0001876731877959414,
      "loss": 0.7609,
      "step": 5860
    },
    {
      "epoch": 0.37058080808080807,
      "grad_norm": 0.66936194896698,
      "learning_rate": 0.00018762396884510797,
      "loss": 0.6753,
      "step": 5870
    },
    {
      "epoch": 0.3712121212121212,
      "grad_norm": 0.7512418031692505,
      "learning_rate": 0.0001875746583089744,
      "loss": 0.5749,
      "step": 5880
    },
    {
      "epoch": 0.37184343434343436,
      "grad_norm": 0.9039126038551331,
      "learning_rate": 0.0001875252562390805,
      "loss": 0.5294,
      "step": 5890
    },
    {
      "epoch": 0.37247474747474746,
      "grad_norm": 1.146936297416687,
      "learning_rate": 0.00018747576268706172,
      "loss": 0.557,
      "step": 5900
    },
    {
      "epoch": 0.3731060606060606,
      "grad_norm": 0.5891149044036865,
      "learning_rate": 0.0001874261777046491,
      "loss": 0.787,
      "step": 5910
    },
    {
      "epoch": 0.37373737373737376,
      "grad_norm": 0.6318565011024475,
      "learning_rate": 0.00018737650134366927,
      "loss": 0.6267,
      "step": 5920
    },
    {
      "epoch": 0.37436868686868685,
      "grad_norm": 0.7535669207572937,
      "learning_rate": 0.00018732673365604447,
      "loss": 0.57,
      "step": 5930
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.7772605419158936,
      "learning_rate": 0.0001872768746937922,
      "loss": 0.5439,
      "step": 5940
    },
    {
      "epoch": 0.37563131313131315,
      "grad_norm": 1.2650660276412964,
      "learning_rate": 0.00018722692450902551,
      "loss": 0.5343,
      "step": 5950
    },
    {
      "epoch": 0.37626262626262624,
      "grad_norm": 0.5863489508628845,
      "learning_rate": 0.0001871768831539527,
      "loss": 0.7943,
      "step": 5960
    },
    {
      "epoch": 0.3768939393939394,
      "grad_norm": 0.6829921007156372,
      "learning_rate": 0.00018712675068087746,
      "loss": 0.6401,
      "step": 5970
    },
    {
      "epoch": 0.37752525252525254,
      "grad_norm": 0.9169061779975891,
      "learning_rate": 0.00018707652714219868,
      "loss": 0.59,
      "step": 5980
    },
    {
      "epoch": 0.37815656565656564,
      "grad_norm": 0.7714381814002991,
      "learning_rate": 0.00018702621259041036,
      "loss": 0.5238,
      "step": 5990
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 0.9657641053199768,
      "learning_rate": 0.00018697580707810173,
      "loss": 0.5391,
      "step": 6000
    },
    {
      "epoch": 0.3787878787878788,
      "eval_loss": 0.6083608865737915,
      "eval_runtime": 27.409,
      "eval_samples_per_second": 93.4,
      "eval_steps_per_second": 11.675,
      "step": 6000
    },
    {
      "epoch": 0.37941919191919193,
      "grad_norm": 0.5433776378631592,
      "learning_rate": 0.00018692531065795702,
      "loss": 0.7675,
      "step": 6010
    },
    {
      "epoch": 0.380050505050505,
      "grad_norm": 0.6648391485214233,
      "learning_rate": 0.00018687472338275557,
      "loss": 0.6569,
      "step": 6020
    },
    {
      "epoch": 0.3806818181818182,
      "grad_norm": 0.6911960244178772,
      "learning_rate": 0.00018682404530537155,
      "loss": 0.5856,
      "step": 6030
    },
    {
      "epoch": 0.3813131313131313,
      "grad_norm": 0.7883504033088684,
      "learning_rate": 0.00018677327647877412,
      "loss": 0.5385,
      "step": 6040
    },
    {
      "epoch": 0.3819444444444444,
      "grad_norm": 1.1119394302368164,
      "learning_rate": 0.00018672241695602733,
      "loss": 0.5584,
      "step": 6050
    },
    {
      "epoch": 0.38257575757575757,
      "grad_norm": 0.5715657472610474,
      "learning_rate": 0.0001866714667902899,
      "loss": 0.7979,
      "step": 6060
    },
    {
      "epoch": 0.3832070707070707,
      "grad_norm": 0.6526241898536682,
      "learning_rate": 0.00018662042603481542,
      "loss": 0.6476,
      "step": 6070
    },
    {
      "epoch": 0.3838383838383838,
      "grad_norm": 0.7770770192146301,
      "learning_rate": 0.00018656929474295209,
      "loss": 0.5941,
      "step": 6080
    },
    {
      "epoch": 0.38446969696969696,
      "grad_norm": 0.9009290933609009,
      "learning_rate": 0.00018651807296814278,
      "loss": 0.529,
      "step": 6090
    },
    {
      "epoch": 0.3851010101010101,
      "grad_norm": 1.033205270767212,
      "learning_rate": 0.0001864667607639249,
      "loss": 0.5541,
      "step": 6100
    },
    {
      "epoch": 0.38573232323232326,
      "grad_norm": 0.5942338705062866,
      "learning_rate": 0.0001864153581839304,
      "loss": 0.7525,
      "step": 6110
    },
    {
      "epoch": 0.38636363636363635,
      "grad_norm": 0.7236207127571106,
      "learning_rate": 0.00018636386528188568,
      "loss": 0.6637,
      "step": 6120
    },
    {
      "epoch": 0.3869949494949495,
      "grad_norm": 0.7482415437698364,
      "learning_rate": 0.00018631228211161152,
      "loss": 0.6017,
      "step": 6130
    },
    {
      "epoch": 0.38762626262626265,
      "grad_norm": 0.8524355292320251,
      "learning_rate": 0.00018626060872702313,
      "loss": 0.5475,
      "step": 6140
    },
    {
      "epoch": 0.38825757575757575,
      "grad_norm": 1.0196795463562012,
      "learning_rate": 0.00018620884518212995,
      "loss": 0.5287,
      "step": 6150
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.5488223433494568,
      "learning_rate": 0.00018615699153103562,
      "loss": 0.7936,
      "step": 6160
    },
    {
      "epoch": 0.38952020202020204,
      "grad_norm": 0.7334930896759033,
      "learning_rate": 0.00018610504782793808,
      "loss": 0.6859,
      "step": 6170
    },
    {
      "epoch": 0.39015151515151514,
      "grad_norm": 0.825121283531189,
      "learning_rate": 0.00018605301412712922,
      "loss": 0.5811,
      "step": 6180
    },
    {
      "epoch": 0.3907828282828283,
      "grad_norm": 0.8506012558937073,
      "learning_rate": 0.0001860008904829952,
      "loss": 0.5203,
      "step": 6190
    },
    {
      "epoch": 0.39141414141414144,
      "grad_norm": 1.061792254447937,
      "learning_rate": 0.00018594867695001605,
      "loss": 0.5837,
      "step": 6200
    },
    {
      "epoch": 0.39204545454545453,
      "grad_norm": 0.5806285738945007,
      "learning_rate": 0.00018589637358276578,
      "loss": 0.77,
      "step": 6210
    },
    {
      "epoch": 0.3926767676767677,
      "grad_norm": 0.6656145453453064,
      "learning_rate": 0.0001858439804359123,
      "loss": 0.6648,
      "step": 6220
    },
    {
      "epoch": 0.39330808080808083,
      "grad_norm": 0.7021386027336121,
      "learning_rate": 0.00018579149756421735,
      "loss": 0.592,
      "step": 6230
    },
    {
      "epoch": 0.3939393939393939,
      "grad_norm": 0.9116240739822388,
      "learning_rate": 0.0001857389250225365,
      "loss": 0.5052,
      "step": 6240
    },
    {
      "epoch": 0.39457070707070707,
      "grad_norm": 1.116866111755371,
      "learning_rate": 0.00018568626286581897,
      "loss": 0.5371,
      "step": 6250
    },
    {
      "epoch": 0.3952020202020202,
      "grad_norm": 0.6154735088348389,
      "learning_rate": 0.0001856335111491077,
      "loss": 0.7853,
      "step": 6260
    },
    {
      "epoch": 0.3958333333333333,
      "grad_norm": 0.6758896112442017,
      "learning_rate": 0.00018558066992753925,
      "loss": 0.6464,
      "step": 6270
    },
    {
      "epoch": 0.39646464646464646,
      "grad_norm": 0.7455413937568665,
      "learning_rate": 0.00018552773925634367,
      "loss": 0.5678,
      "step": 6280
    },
    {
      "epoch": 0.3970959595959596,
      "grad_norm": 0.7612907886505127,
      "learning_rate": 0.00018547471919084453,
      "loss": 0.5226,
      "step": 6290
    },
    {
      "epoch": 0.3977272727272727,
      "grad_norm": 1.1246331930160522,
      "learning_rate": 0.00018542160978645886,
      "loss": 0.5358,
      "step": 6300
    },
    {
      "epoch": 0.39835858585858586,
      "grad_norm": 0.6579055190086365,
      "learning_rate": 0.00018536841109869704,
      "loss": 0.7494,
      "step": 6310
    },
    {
      "epoch": 0.398989898989899,
      "grad_norm": 0.7879274487495422,
      "learning_rate": 0.00018531512318316283,
      "loss": 0.6677,
      "step": 6320
    },
    {
      "epoch": 0.3996212121212121,
      "grad_norm": 0.6887534856796265,
      "learning_rate": 0.0001852617460955531,
      "loss": 0.5894,
      "step": 6330
    },
    {
      "epoch": 0.40025252525252525,
      "grad_norm": 0.7911218404769897,
      "learning_rate": 0.00018520827989165813,
      "loss": 0.5258,
      "step": 6340
    },
    {
      "epoch": 0.4008838383838384,
      "grad_norm": 1.0992769002914429,
      "learning_rate": 0.0001851547246273612,
      "loss": 0.5665,
      "step": 6350
    },
    {
      "epoch": 0.4015151515151515,
      "grad_norm": 0.581439733505249,
      "learning_rate": 0.00018510108035863868,
      "loss": 0.7713,
      "step": 6360
    },
    {
      "epoch": 0.40214646464646464,
      "grad_norm": 0.6784172654151917,
      "learning_rate": 0.00018504734714156008,
      "loss": 0.6303,
      "step": 6370
    },
    {
      "epoch": 0.4027777777777778,
      "grad_norm": 0.7228202223777771,
      "learning_rate": 0.00018499352503228774,
      "loss": 0.5847,
      "step": 6380
    },
    {
      "epoch": 0.4034090909090909,
      "grad_norm": 0.8072268962860107,
      "learning_rate": 0.000184939614087077,
      "loss": 0.508,
      "step": 6390
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 0.9974951148033142,
      "learning_rate": 0.00018488561436227603,
      "loss": 0.5387,
      "step": 6400
    },
    {
      "epoch": 0.4046717171717172,
      "grad_norm": 0.6997259259223938,
      "learning_rate": 0.0001848315259143258,
      "loss": 0.8198,
      "step": 6410
    },
    {
      "epoch": 0.4053030303030303,
      "grad_norm": 0.7578186988830566,
      "learning_rate": 0.00018477734879976,
      "loss": 0.6636,
      "step": 6420
    },
    {
      "epoch": 0.4059343434343434,
      "grad_norm": 0.6908935904502869,
      "learning_rate": 0.00018472308307520497,
      "loss": 0.5877,
      "step": 6430
    },
    {
      "epoch": 0.4065656565656566,
      "grad_norm": 0.823662519454956,
      "learning_rate": 0.0001846687287973797,
      "loss": 0.5326,
      "step": 6440
    },
    {
      "epoch": 0.4071969696969697,
      "grad_norm": 1.0582275390625,
      "learning_rate": 0.0001846142860230958,
      "loss": 0.5284,
      "step": 6450
    },
    {
      "epoch": 0.4078282828282828,
      "grad_norm": 0.611077070236206,
      "learning_rate": 0.00018455975480925722,
      "loss": 0.8242,
      "step": 6460
    },
    {
      "epoch": 0.40845959595959597,
      "grad_norm": 0.7780469655990601,
      "learning_rate": 0.0001845051352128605,
      "loss": 0.6677,
      "step": 6470
    },
    {
      "epoch": 0.4090909090909091,
      "grad_norm": 0.7325640320777893,
      "learning_rate": 0.00018445042729099445,
      "loss": 0.5845,
      "step": 6480
    },
    {
      "epoch": 0.4097222222222222,
      "grad_norm": 0.7049621343612671,
      "learning_rate": 0.00018439563110084033,
      "loss": 0.5487,
      "step": 6490
    },
    {
      "epoch": 0.41035353535353536,
      "grad_norm": 1.2103140354156494,
      "learning_rate": 0.00018434074669967148,
      "loss": 0.5398,
      "step": 6500
    },
    {
      "epoch": 0.4109848484848485,
      "grad_norm": 0.594965398311615,
      "learning_rate": 0.00018428577414485357,
      "loss": 0.7614,
      "step": 6510
    },
    {
      "epoch": 0.4116161616161616,
      "grad_norm": 0.623036801815033,
      "learning_rate": 0.00018423071349384435,
      "loss": 0.6692,
      "step": 6520
    },
    {
      "epoch": 0.41224747474747475,
      "grad_norm": 0.7474129796028137,
      "learning_rate": 0.00018417556480419372,
      "loss": 0.5522,
      "step": 6530
    },
    {
      "epoch": 0.4128787878787879,
      "grad_norm": 0.7588824033737183,
      "learning_rate": 0.00018412032813354347,
      "loss": 0.5217,
      "step": 6540
    },
    {
      "epoch": 0.413510101010101,
      "grad_norm": 0.9659335613250732,
      "learning_rate": 0.0001840650035396275,
      "loss": 0.5261,
      "step": 6550
    },
    {
      "epoch": 0.41414141414141414,
      "grad_norm": 0.5676929950714111,
      "learning_rate": 0.0001840095910802715,
      "loss": 0.7588,
      "step": 6560
    },
    {
      "epoch": 0.4147727272727273,
      "grad_norm": 0.6372143626213074,
      "learning_rate": 0.00018395409081339305,
      "loss": 0.6365,
      "step": 6570
    },
    {
      "epoch": 0.4154040404040404,
      "grad_norm": 0.7602266669273376,
      "learning_rate": 0.00018389850279700148,
      "loss": 0.5762,
      "step": 6580
    },
    {
      "epoch": 0.41603535353535354,
      "grad_norm": 0.8911465406417847,
      "learning_rate": 0.00018384282708919784,
      "loss": 0.5141,
      "step": 6590
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.9443477988243103,
      "learning_rate": 0.00018378706374817485,
      "loss": 0.5335,
      "step": 6600
    },
    {
      "epoch": 0.4172979797979798,
      "grad_norm": 0.6008862257003784,
      "learning_rate": 0.00018373121283221682,
      "loss": 0.7393,
      "step": 6610
    },
    {
      "epoch": 0.41792929292929293,
      "grad_norm": 0.7743738889694214,
      "learning_rate": 0.00018367527439969958,
      "loss": 0.6205,
      "step": 6620
    },
    {
      "epoch": 0.4185606060606061,
      "grad_norm": 0.7291122674942017,
      "learning_rate": 0.00018361924850909044,
      "loss": 0.567,
      "step": 6630
    },
    {
      "epoch": 0.41919191919191917,
      "grad_norm": 0.7600089311599731,
      "learning_rate": 0.00018356313521894816,
      "loss": 0.5141,
      "step": 6640
    },
    {
      "epoch": 0.4198232323232323,
      "grad_norm": 1.0716909170150757,
      "learning_rate": 0.00018350693458792279,
      "loss": 0.5423,
      "step": 6650
    },
    {
      "epoch": 0.42045454545454547,
      "grad_norm": 0.6891409754753113,
      "learning_rate": 0.0001834506466747557,
      "loss": 0.7655,
      "step": 6660
    },
    {
      "epoch": 0.42108585858585856,
      "grad_norm": 0.6870277523994446,
      "learning_rate": 0.0001833942715382795,
      "loss": 0.64,
      "step": 6670
    },
    {
      "epoch": 0.4217171717171717,
      "grad_norm": 0.6293309926986694,
      "learning_rate": 0.00018333780923741788,
      "loss": 0.5451,
      "step": 6680
    },
    {
      "epoch": 0.42234848484848486,
      "grad_norm": 0.8664209246635437,
      "learning_rate": 0.0001832812598311858,
      "loss": 0.5201,
      "step": 6690
    },
    {
      "epoch": 0.42297979797979796,
      "grad_norm": 0.9683524966239929,
      "learning_rate": 0.00018322462337868914,
      "loss": 0.5334,
      "step": 6700
    },
    {
      "epoch": 0.4236111111111111,
      "grad_norm": 0.6050415635108948,
      "learning_rate": 0.00018316789993912477,
      "loss": 0.7712,
      "step": 6710
    },
    {
      "epoch": 0.42424242424242425,
      "grad_norm": 0.7022687792778015,
      "learning_rate": 0.0001831110895717805,
      "loss": 0.6321,
      "step": 6720
    },
    {
      "epoch": 0.42487373737373735,
      "grad_norm": 0.6981886029243469,
      "learning_rate": 0.00018305419233603508,
      "loss": 0.5857,
      "step": 6730
    },
    {
      "epoch": 0.4255050505050505,
      "grad_norm": 0.8014115691184998,
      "learning_rate": 0.00018299720829135786,
      "loss": 0.5373,
      "step": 6740
    },
    {
      "epoch": 0.42613636363636365,
      "grad_norm": 0.9578883647918701,
      "learning_rate": 0.00018294013749730904,
      "loss": 0.5681,
      "step": 6750
    },
    {
      "epoch": 0.42676767676767674,
      "grad_norm": 0.6189029216766357,
      "learning_rate": 0.00018288298001353957,
      "loss": 0.7301,
      "step": 6760
    },
    {
      "epoch": 0.4273989898989899,
      "grad_norm": 0.6787287592887878,
      "learning_rate": 0.00018282573589979085,
      "loss": 0.6215,
      "step": 6770
    },
    {
      "epoch": 0.42803030303030304,
      "grad_norm": 0.8055798411369324,
      "learning_rate": 0.00018276840521589497,
      "loss": 0.566,
      "step": 6780
    },
    {
      "epoch": 0.4286616161616162,
      "grad_norm": 0.9095854759216309,
      "learning_rate": 0.0001827109880217744,
      "loss": 0.5247,
      "step": 6790
    },
    {
      "epoch": 0.4292929292929293,
      "grad_norm": 0.9432606101036072,
      "learning_rate": 0.0001826534843774421,
      "loss": 0.5338,
      "step": 6800
    },
    {
      "epoch": 0.42992424242424243,
      "grad_norm": 0.5806302428245544,
      "learning_rate": 0.0001825958943430013,
      "loss": 0.718,
      "step": 6810
    },
    {
      "epoch": 0.4305555555555556,
      "grad_norm": 0.7472066283226013,
      "learning_rate": 0.00018253821797864562,
      "loss": 0.6416,
      "step": 6820
    },
    {
      "epoch": 0.4311868686868687,
      "grad_norm": 0.7274330258369446,
      "learning_rate": 0.00018248045534465884,
      "loss": 0.5518,
      "step": 6830
    },
    {
      "epoch": 0.4318181818181818,
      "grad_norm": 0.777045488357544,
      "learning_rate": 0.00018242260650141502,
      "loss": 0.5344,
      "step": 6840
    },
    {
      "epoch": 0.432449494949495,
      "grad_norm": 1.1557681560516357,
      "learning_rate": 0.0001823646715093782,
      "loss": 0.5489,
      "step": 6850
    },
    {
      "epoch": 0.43308080808080807,
      "grad_norm": 0.5989468693733215,
      "learning_rate": 0.00018230665042910248,
      "loss": 0.7759,
      "step": 6860
    },
    {
      "epoch": 0.4337121212121212,
      "grad_norm": 0.6763079166412354,
      "learning_rate": 0.00018224854332123206,
      "loss": 0.6468,
      "step": 6870
    },
    {
      "epoch": 0.43434343434343436,
      "grad_norm": 0.8058540225028992,
      "learning_rate": 0.0001821903502465009,
      "loss": 0.5782,
      "step": 6880
    },
    {
      "epoch": 0.43497474747474746,
      "grad_norm": 0.8007886409759521,
      "learning_rate": 0.00018213207126573292,
      "loss": 0.5087,
      "step": 6890
    },
    {
      "epoch": 0.4356060606060606,
      "grad_norm": 1.3041198253631592,
      "learning_rate": 0.00018207370643984178,
      "loss": 0.5437,
      "step": 6900
    },
    {
      "epoch": 0.43623737373737376,
      "grad_norm": 0.6729303002357483,
      "learning_rate": 0.0001820152558298309,
      "loss": 0.7478,
      "step": 6910
    },
    {
      "epoch": 0.43686868686868685,
      "grad_norm": 0.7168794274330139,
      "learning_rate": 0.00018195671949679333,
      "loss": 0.6327,
      "step": 6920
    },
    {
      "epoch": 0.4375,
      "grad_norm": 0.7596614956855774,
      "learning_rate": 0.0001818980975019117,
      "loss": 0.5482,
      "step": 6930
    },
    {
      "epoch": 0.43813131313131315,
      "grad_norm": 0.8045967817306519,
      "learning_rate": 0.00018183938990645827,
      "loss": 0.5066,
      "step": 6940
    },
    {
      "epoch": 0.43876262626262624,
      "grad_norm": 1.0041868686676025,
      "learning_rate": 0.00018178059677179467,
      "loss": 0.5669,
      "step": 6950
    },
    {
      "epoch": 0.4393939393939394,
      "grad_norm": 0.6689700484275818,
      "learning_rate": 0.00018172171815937195,
      "loss": 0.7325,
      "step": 6960
    },
    {
      "epoch": 0.44002525252525254,
      "grad_norm": 0.7198066115379333,
      "learning_rate": 0.00018166275413073062,
      "loss": 0.6622,
      "step": 6970
    },
    {
      "epoch": 0.44065656565656564,
      "grad_norm": 0.8159673810005188,
      "learning_rate": 0.00018160370474750023,
      "loss": 0.5726,
      "step": 6980
    },
    {
      "epoch": 0.4412878787878788,
      "grad_norm": 0.6863812804222107,
      "learning_rate": 0.0001815445700713998,
      "loss": 0.5362,
      "step": 6990
    },
    {
      "epoch": 0.44191919191919193,
      "grad_norm": 0.9821881651878357,
      "learning_rate": 0.00018148535016423734,
      "loss": 0.5441,
      "step": 7000
    },
    {
      "epoch": 0.44191919191919193,
      "eval_loss": 0.5908827781677246,
      "eval_runtime": 27.3746,
      "eval_samples_per_second": 93.517,
      "eval_steps_per_second": 11.69,
      "step": 7000
    },
    {
      "epoch": 0.442550505050505,
      "grad_norm": 0.6582397818565369,
      "learning_rate": 0.00018142604508791,
      "loss": 0.719,
      "step": 7010
    },
    {
      "epoch": 0.4431818181818182,
      "grad_norm": 0.6876886487007141,
      "learning_rate": 0.00018136665490440393,
      "loss": 0.6558,
      "step": 7020
    },
    {
      "epoch": 0.4438131313131313,
      "grad_norm": 0.6641818881034851,
      "learning_rate": 0.00018130717967579423,
      "loss": 0.564,
      "step": 7030
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.7168505191802979,
      "learning_rate": 0.00018124761946424492,
      "loss": 0.5157,
      "step": 7040
    },
    {
      "epoch": 0.44507575757575757,
      "grad_norm": 0.944426417350769,
      "learning_rate": 0.00018118797433200882,
      "loss": 0.5181,
      "step": 7050
    },
    {
      "epoch": 0.4457070707070707,
      "grad_norm": 0.660180926322937,
      "learning_rate": 0.00018112824434142753,
      "loss": 0.831,
      "step": 7060
    },
    {
      "epoch": 0.4463383838383838,
      "grad_norm": 0.6853012442588806,
      "learning_rate": 0.00018106842955493133,
      "loss": 0.6381,
      "step": 7070
    },
    {
      "epoch": 0.44696969696969696,
      "grad_norm": 0.6295440793037415,
      "learning_rate": 0.00018100853003503916,
      "loss": 0.5765,
      "step": 7080
    },
    {
      "epoch": 0.4476010101010101,
      "grad_norm": 0.7571380734443665,
      "learning_rate": 0.00018094854584435843,
      "loss": 0.493,
      "step": 7090
    },
    {
      "epoch": 0.44823232323232326,
      "grad_norm": 0.8866646885871887,
      "learning_rate": 0.00018088847704558517,
      "loss": 0.545,
      "step": 7100
    },
    {
      "epoch": 0.44886363636363635,
      "grad_norm": 0.6081569790840149,
      "learning_rate": 0.00018082832370150374,
      "loss": 0.7195,
      "step": 7110
    },
    {
      "epoch": 0.4494949494949495,
      "grad_norm": 0.7159175872802734,
      "learning_rate": 0.00018076808587498696,
      "loss": 0.6225,
      "step": 7120
    },
    {
      "epoch": 0.45012626262626265,
      "grad_norm": 0.6093949675559998,
      "learning_rate": 0.00018070776362899587,
      "loss": 0.5654,
      "step": 7130
    },
    {
      "epoch": 0.45075757575757575,
      "grad_norm": 0.8400552868843079,
      "learning_rate": 0.0001806473570265798,
      "loss": 0.5062,
      "step": 7140
    },
    {
      "epoch": 0.4513888888888889,
      "grad_norm": 0.963050127029419,
      "learning_rate": 0.00018058686613087624,
      "loss": 0.5249,
      "step": 7150
    },
    {
      "epoch": 0.45202020202020204,
      "grad_norm": 0.5821149349212646,
      "learning_rate": 0.00018052629100511077,
      "loss": 0.7557,
      "step": 7160
    },
    {
      "epoch": 0.45265151515151514,
      "grad_norm": 0.644615888595581,
      "learning_rate": 0.00018046563171259701,
      "loss": 0.667,
      "step": 7170
    },
    {
      "epoch": 0.4532828282828283,
      "grad_norm": 0.6924688220024109,
      "learning_rate": 0.00018040488831673655,
      "loss": 0.5744,
      "step": 7180
    },
    {
      "epoch": 0.45391414141414144,
      "grad_norm": 0.8942870497703552,
      "learning_rate": 0.00018034406088101893,
      "loss": 0.5427,
      "step": 7190
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.9686787128448486,
      "learning_rate": 0.00018028314946902144,
      "loss": 0.5296,
      "step": 7200
    },
    {
      "epoch": 0.4551767676767677,
      "grad_norm": 0.5707201361656189,
      "learning_rate": 0.00018022215414440924,
      "loss": 0.7414,
      "step": 7210
    },
    {
      "epoch": 0.45580808080808083,
      "grad_norm": 0.618965744972229,
      "learning_rate": 0.00018016107497093514,
      "loss": 0.6463,
      "step": 7220
    },
    {
      "epoch": 0.4564393939393939,
      "grad_norm": 0.7574859261512756,
      "learning_rate": 0.00018009991201243955,
      "loss": 0.5733,
      "step": 7230
    },
    {
      "epoch": 0.45707070707070707,
      "grad_norm": 0.6828228235244751,
      "learning_rate": 0.00018003866533285054,
      "loss": 0.4784,
      "step": 7240
    },
    {
      "epoch": 0.4577020202020202,
      "grad_norm": 1.163494348526001,
      "learning_rate": 0.00017997733499618365,
      "loss": 0.5776,
      "step": 7250
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.5958918333053589,
      "learning_rate": 0.00017991592106654186,
      "loss": 0.7468,
      "step": 7260
    },
    {
      "epoch": 0.45896464646464646,
      "grad_norm": 0.6819302439689636,
      "learning_rate": 0.00017985442360811553,
      "loss": 0.6071,
      "step": 7270
    },
    {
      "epoch": 0.4595959595959596,
      "grad_norm": 0.8120288252830505,
      "learning_rate": 0.00017979284268518228,
      "loss": 0.5528,
      "step": 7280
    },
    {
      "epoch": 0.4602272727272727,
      "grad_norm": 0.7200056910514832,
      "learning_rate": 0.00017973117836210702,
      "loss": 0.5421,
      "step": 7290
    },
    {
      "epoch": 0.46085858585858586,
      "grad_norm": 1.1278009414672852,
      "learning_rate": 0.00017966943070334184,
      "loss": 0.5113,
      "step": 7300
    },
    {
      "epoch": 0.461489898989899,
      "grad_norm": 0.6265709400177002,
      "learning_rate": 0.00017960759977342586,
      "loss": 0.7388,
      "step": 7310
    },
    {
      "epoch": 0.4621212121212121,
      "grad_norm": 0.6382631063461304,
      "learning_rate": 0.0001795456856369853,
      "loss": 0.6246,
      "step": 7320
    },
    {
      "epoch": 0.46275252525252525,
      "grad_norm": 0.7256936430931091,
      "learning_rate": 0.00017948368835873332,
      "loss": 0.5802,
      "step": 7330
    },
    {
      "epoch": 0.4633838383838384,
      "grad_norm": 0.755691647529602,
      "learning_rate": 0.00017942160800347,
      "loss": 0.5068,
      "step": 7340
    },
    {
      "epoch": 0.4640151515151515,
      "grad_norm": 1.0924580097198486,
      "learning_rate": 0.00017935944463608227,
      "loss": 0.5494,
      "step": 7350
    },
    {
      "epoch": 0.46464646464646464,
      "grad_norm": 0.669704794883728,
      "learning_rate": 0.00017929719832154376,
      "loss": 0.7261,
      "step": 7360
    },
    {
      "epoch": 0.4652777777777778,
      "grad_norm": 0.6547823548316956,
      "learning_rate": 0.00017923486912491482,
      "loss": 0.6524,
      "step": 7370
    },
    {
      "epoch": 0.4659090909090909,
      "grad_norm": 0.692944347858429,
      "learning_rate": 0.0001791724571113425,
      "loss": 0.5674,
      "step": 7380
    },
    {
      "epoch": 0.46654040404040403,
      "grad_norm": 0.6874088048934937,
      "learning_rate": 0.0001791099623460603,
      "loss": 0.4861,
      "step": 7390
    },
    {
      "epoch": 0.4671717171717172,
      "grad_norm": 1.0708526372909546,
      "learning_rate": 0.00017904738489438836,
      "loss": 0.5307,
      "step": 7400
    },
    {
      "epoch": 0.4678030303030303,
      "grad_norm": 0.6836604475975037,
      "learning_rate": 0.00017898472482173302,
      "loss": 0.7085,
      "step": 7410
    },
    {
      "epoch": 0.4684343434343434,
      "grad_norm": 0.6248346567153931,
      "learning_rate": 0.0001789219821935872,
      "loss": 0.6257,
      "step": 7420
    },
    {
      "epoch": 0.4690656565656566,
      "grad_norm": 0.7317250370979309,
      "learning_rate": 0.00017885915707552998,
      "loss": 0.5405,
      "step": 7430
    },
    {
      "epoch": 0.4696969696969697,
      "grad_norm": 0.862128734588623,
      "learning_rate": 0.0001787962495332267,
      "loss": 0.501,
      "step": 7440
    },
    {
      "epoch": 0.4703282828282828,
      "grad_norm": 0.889229416847229,
      "learning_rate": 0.00017873325963242888,
      "loss": 0.5304,
      "step": 7450
    },
    {
      "epoch": 0.47095959595959597,
      "grad_norm": 0.582894504070282,
      "learning_rate": 0.00017867018743897406,
      "loss": 0.7615,
      "step": 7460
    },
    {
      "epoch": 0.4715909090909091,
      "grad_norm": 0.6623352766036987,
      "learning_rate": 0.0001786070330187858,
      "loss": 0.63,
      "step": 7470
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 0.7342755794525146,
      "learning_rate": 0.00017854379643787363,
      "loss": 0.5656,
      "step": 7480
    },
    {
      "epoch": 0.47285353535353536,
      "grad_norm": 0.8783890604972839,
      "learning_rate": 0.000178480477762333,
      "loss": 0.5225,
      "step": 7490
    },
    {
      "epoch": 0.4734848484848485,
      "grad_norm": 1.0121986865997314,
      "learning_rate": 0.00017841707705834505,
      "loss": 0.5334,
      "step": 7500
    },
    {
      "epoch": 0.4741161616161616,
      "grad_norm": 0.572057843208313,
      "learning_rate": 0.00017835359439217677,
      "loss": 0.7659,
      "step": 7510
    },
    {
      "epoch": 0.47474747474747475,
      "grad_norm": 0.6950221061706543,
      "learning_rate": 0.00017829002983018075,
      "loss": 0.6317,
      "step": 7520
    },
    {
      "epoch": 0.4753787878787879,
      "grad_norm": 0.654387891292572,
      "learning_rate": 0.0001782263834387952,
      "loss": 0.6124,
      "step": 7530
    },
    {
      "epoch": 0.476010101010101,
      "grad_norm": 0.7840420007705688,
      "learning_rate": 0.00017816265528454382,
      "loss": 0.5061,
      "step": 7540
    },
    {
      "epoch": 0.47664141414141414,
      "grad_norm": 1.003827452659607,
      "learning_rate": 0.0001780988454340359,
      "loss": 0.5301,
      "step": 7550
    },
    {
      "epoch": 0.4772727272727273,
      "grad_norm": 0.6165429353713989,
      "learning_rate": 0.00017803495395396593,
      "loss": 0.7529,
      "step": 7560
    },
    {
      "epoch": 0.4779040404040404,
      "grad_norm": 0.6123965382575989,
      "learning_rate": 0.0001779709809111139,
      "loss": 0.6411,
      "step": 7570
    },
    {
      "epoch": 0.47853535353535354,
      "grad_norm": 0.7383981347084045,
      "learning_rate": 0.00017790692637234488,
      "loss": 0.5393,
      "step": 7580
    },
    {
      "epoch": 0.4791666666666667,
      "grad_norm": 0.6301671266555786,
      "learning_rate": 0.00017784279040460924,
      "loss": 0.5123,
      "step": 7590
    },
    {
      "epoch": 0.4797979797979798,
      "grad_norm": 0.9712590575218201,
      "learning_rate": 0.00017777857307494247,
      "loss": 0.5312,
      "step": 7600
    },
    {
      "epoch": 0.48042929292929293,
      "grad_norm": 0.6300521492958069,
      "learning_rate": 0.000177714274450465,
      "loss": 0.7522,
      "step": 7610
    },
    {
      "epoch": 0.4810606060606061,
      "grad_norm": 0.7592799067497253,
      "learning_rate": 0.00017764989459838232,
      "loss": 0.6261,
      "step": 7620
    },
    {
      "epoch": 0.48169191919191917,
      "grad_norm": 0.7232942581176758,
      "learning_rate": 0.00017758543358598476,
      "loss": 0.5591,
      "step": 7630
    },
    {
      "epoch": 0.4823232323232323,
      "grad_norm": 0.813723623752594,
      "learning_rate": 0.00017752089148064752,
      "loss": 0.5076,
      "step": 7640
    },
    {
      "epoch": 0.48295454545454547,
      "grad_norm": 1.0034205913543701,
      "learning_rate": 0.00017745626834983055,
      "loss": 0.5091,
      "step": 7650
    },
    {
      "epoch": 0.48358585858585856,
      "grad_norm": 0.6554344296455383,
      "learning_rate": 0.00017739156426107845,
      "loss": 0.7771,
      "step": 7660
    },
    {
      "epoch": 0.4842171717171717,
      "grad_norm": 0.6211978793144226,
      "learning_rate": 0.00017732677928202053,
      "loss": 0.6419,
      "step": 7670
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 0.7142544388771057,
      "learning_rate": 0.00017726191348037054,
      "loss": 0.5554,
      "step": 7680
    },
    {
      "epoch": 0.48547979797979796,
      "grad_norm": 0.7731212377548218,
      "learning_rate": 0.00017719696692392677,
      "loss": 0.5322,
      "step": 7690
    },
    {
      "epoch": 0.4861111111111111,
      "grad_norm": 1.2062458992004395,
      "learning_rate": 0.0001771319396805719,
      "loss": 0.5178,
      "step": 7700
    },
    {
      "epoch": 0.48674242424242425,
      "grad_norm": 0.606095016002655,
      "learning_rate": 0.00017706683181827295,
      "loss": 0.7375,
      "step": 7710
    },
    {
      "epoch": 0.48737373737373735,
      "grad_norm": 0.6507496237754822,
      "learning_rate": 0.00017700164340508117,
      "loss": 0.6423,
      "step": 7720
    },
    {
      "epoch": 0.4880050505050505,
      "grad_norm": 0.689579963684082,
      "learning_rate": 0.0001769363745091321,
      "loss": 0.5534,
      "step": 7730
    },
    {
      "epoch": 0.48863636363636365,
      "grad_norm": 0.7607659101486206,
      "learning_rate": 0.00017687102519864525,
      "loss": 0.5178,
      "step": 7740
    },
    {
      "epoch": 0.48926767676767674,
      "grad_norm": 0.9309207797050476,
      "learning_rate": 0.0001768055955419243,
      "loss": 0.5524,
      "step": 7750
    },
    {
      "epoch": 0.4898989898989899,
      "grad_norm": 0.6069095134735107,
      "learning_rate": 0.0001767400856073569,
      "loss": 0.7089,
      "step": 7760
    },
    {
      "epoch": 0.49053030303030304,
      "grad_norm": 0.6496742367744446,
      "learning_rate": 0.00017667449546341453,
      "loss": 0.6324,
      "step": 7770
    },
    {
      "epoch": 0.4911616161616162,
      "grad_norm": 0.6555066704750061,
      "learning_rate": 0.00017660882517865254,
      "loss": 0.548,
      "step": 7780
    },
    {
      "epoch": 0.4917929292929293,
      "grad_norm": 0.764835000038147,
      "learning_rate": 0.00017654307482171014,
      "loss": 0.5077,
      "step": 7790
    },
    {
      "epoch": 0.49242424242424243,
      "grad_norm": 1.2813174724578857,
      "learning_rate": 0.00017647724446131005,
      "loss": 0.5318,
      "step": 7800
    },
    {
      "epoch": 0.4930555555555556,
      "grad_norm": 0.5997570753097534,
      "learning_rate": 0.00017641133416625878,
      "loss": 0.7279,
      "step": 7810
    },
    {
      "epoch": 0.4936868686868687,
      "grad_norm": 0.691418468952179,
      "learning_rate": 0.00017634534400544631,
      "loss": 0.6005,
      "step": 7820
    },
    {
      "epoch": 0.4943181818181818,
      "grad_norm": 0.7245383262634277,
      "learning_rate": 0.00017627927404784607,
      "loss": 0.5336,
      "step": 7830
    },
    {
      "epoch": 0.494949494949495,
      "grad_norm": 0.728950023651123,
      "learning_rate": 0.00017621312436251496,
      "loss": 0.517,
      "step": 7840
    },
    {
      "epoch": 0.49558080808080807,
      "grad_norm": 0.8812928795814514,
      "learning_rate": 0.00017614689501859316,
      "loss": 0.5181,
      "step": 7850
    },
    {
      "epoch": 0.4962121212121212,
      "grad_norm": 0.5982614159584045,
      "learning_rate": 0.00017608058608530413,
      "loss": 0.7642,
      "step": 7860
    },
    {
      "epoch": 0.49684343434343436,
      "grad_norm": 0.7403950095176697,
      "learning_rate": 0.00017601419763195453,
      "loss": 0.6235,
      "step": 7870
    },
    {
      "epoch": 0.49747474747474746,
      "grad_norm": 0.712908923625946,
      "learning_rate": 0.0001759477297279341,
      "loss": 0.5889,
      "step": 7880
    },
    {
      "epoch": 0.4981060606060606,
      "grad_norm": 0.804527997970581,
      "learning_rate": 0.00017588118244271568,
      "loss": 0.5289,
      "step": 7890
    },
    {
      "epoch": 0.49873737373737376,
      "grad_norm": 0.8421348333358765,
      "learning_rate": 0.00017581455584585507,
      "loss": 0.5182,
      "step": 7900
    },
    {
      "epoch": 0.49936868686868685,
      "grad_norm": 0.6108242869377136,
      "learning_rate": 0.00017574785000699084,
      "loss": 0.7236,
      "step": 7910
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.6591709852218628,
      "learning_rate": 0.0001756810649958446,
      "loss": 0.6464,
      "step": 7920
    },
    {
      "epoch": 0.5006313131313131,
      "grad_norm": 0.6823734641075134,
      "learning_rate": 0.00017561420088222054,
      "loss": 0.5448,
      "step": 7930
    },
    {
      "epoch": 0.5012626262626263,
      "grad_norm": 0.7356827259063721,
      "learning_rate": 0.0001755472577360056,
      "loss": 0.511,
      "step": 7940
    },
    {
      "epoch": 0.5018939393939394,
      "grad_norm": 1.0888158082962036,
      "learning_rate": 0.0001754802356271693,
      "loss": 0.5342,
      "step": 7950
    },
    {
      "epoch": 0.5025252525252525,
      "grad_norm": 0.5929847955703735,
      "learning_rate": 0.00017541313462576368,
      "loss": 0.7431,
      "step": 7960
    },
    {
      "epoch": 0.5031565656565656,
      "grad_norm": 0.6430888175964355,
      "learning_rate": 0.0001753459548019233,
      "loss": 0.6264,
      "step": 7970
    },
    {
      "epoch": 0.5037878787878788,
      "grad_norm": 0.7002361416816711,
      "learning_rate": 0.0001752786962258651,
      "loss": 0.5578,
      "step": 7980
    },
    {
      "epoch": 0.5044191919191919,
      "grad_norm": 0.7845878601074219,
      "learning_rate": 0.00017521135896788828,
      "loss": 0.5156,
      "step": 7990
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 1.0626643896102905,
      "learning_rate": 0.00017514394309837424,
      "loss": 0.54,
      "step": 8000
    },
    {
      "epoch": 0.5050505050505051,
      "eval_loss": 0.578924834728241,
      "eval_runtime": 27.148,
      "eval_samples_per_second": 94.298,
      "eval_steps_per_second": 11.787,
      "step": 8000
    },
    {
      "epoch": 0.5056818181818182,
      "grad_norm": 0.5671148896217346,
      "learning_rate": 0.0001750764486877867,
      "loss": 0.7472,
      "step": 8010
    },
    {
      "epoch": 0.5063131313131313,
      "grad_norm": 0.6959950923919678,
      "learning_rate": 0.0001750088758066713,
      "loss": 0.6157,
      "step": 8020
    },
    {
      "epoch": 0.5069444444444444,
      "grad_norm": 0.700547993183136,
      "learning_rate": 0.00017494122452565582,
      "loss": 0.5707,
      "step": 8030
    },
    {
      "epoch": 0.5075757575757576,
      "grad_norm": 0.8388614058494568,
      "learning_rate": 0.00017487349491544996,
      "loss": 0.5223,
      "step": 8040
    },
    {
      "epoch": 0.5082070707070707,
      "grad_norm": 1.037367820739746,
      "learning_rate": 0.00017480568704684521,
      "loss": 0.4973,
      "step": 8050
    },
    {
      "epoch": 0.5088383838383839,
      "grad_norm": 0.6163510084152222,
      "learning_rate": 0.00017473780099071498,
      "loss": 0.7425,
      "step": 8060
    },
    {
      "epoch": 0.509469696969697,
      "grad_norm": 0.6549428701400757,
      "learning_rate": 0.0001746698368180143,
      "loss": 0.6162,
      "step": 8070
    },
    {
      "epoch": 0.51010101010101,
      "grad_norm": 0.7457553744316101,
      "learning_rate": 0.0001746017945997799,
      "loss": 0.5671,
      "step": 8080
    },
    {
      "epoch": 0.5107323232323232,
      "grad_norm": 0.746569812297821,
      "learning_rate": 0.00017453367440713007,
      "loss": 0.5014,
      "step": 8090
    },
    {
      "epoch": 0.5113636363636364,
      "grad_norm": 0.9903282523155212,
      "learning_rate": 0.00017446547631126463,
      "loss": 0.5287,
      "step": 8100
    },
    {
      "epoch": 0.5119949494949495,
      "grad_norm": 0.6073612570762634,
      "learning_rate": 0.00017439720038346472,
      "loss": 0.7083,
      "step": 8110
    },
    {
      "epoch": 0.5126262626262627,
      "grad_norm": 0.6428342461585999,
      "learning_rate": 0.00017432884669509299,
      "loss": 0.6185,
      "step": 8120
    },
    {
      "epoch": 0.5132575757575758,
      "grad_norm": 0.6305633187294006,
      "learning_rate": 0.0001742604153175932,
      "loss": 0.5426,
      "step": 8130
    },
    {
      "epoch": 0.5138888888888888,
      "grad_norm": 0.7640303373336792,
      "learning_rate": 0.00017419190632249053,
      "loss": 0.5176,
      "step": 8140
    },
    {
      "epoch": 0.514520202020202,
      "grad_norm": 1.0350719690322876,
      "learning_rate": 0.000174123319781391,
      "loss": 0.5282,
      "step": 8150
    },
    {
      "epoch": 0.5151515151515151,
      "grad_norm": 0.5674313306808472,
      "learning_rate": 0.0001740546557659819,
      "loss": 0.7461,
      "step": 8160
    },
    {
      "epoch": 0.5157828282828283,
      "grad_norm": 0.6418477296829224,
      "learning_rate": 0.00017398591434803143,
      "loss": 0.5668,
      "step": 8170
    },
    {
      "epoch": 0.5164141414141414,
      "grad_norm": 0.7291843295097351,
      "learning_rate": 0.0001739170955993887,
      "loss": 0.5473,
      "step": 8180
    },
    {
      "epoch": 0.5170454545454546,
      "grad_norm": 0.7252343893051147,
      "learning_rate": 0.0001738481995919836,
      "loss": 0.4877,
      "step": 8190
    },
    {
      "epoch": 0.5176767676767676,
      "grad_norm": 0.9164720773696899,
      "learning_rate": 0.00017377922639782685,
      "loss": 0.5385,
      "step": 8200
    },
    {
      "epoch": 0.5183080808080808,
      "grad_norm": 0.6269335150718689,
      "learning_rate": 0.00017371017608900982,
      "loss": 0.7553,
      "step": 8210
    },
    {
      "epoch": 0.5189393939393939,
      "grad_norm": 0.7063830494880676,
      "learning_rate": 0.0001736410487377044,
      "loss": 0.6116,
      "step": 8220
    },
    {
      "epoch": 0.5195707070707071,
      "grad_norm": 0.6731562614440918,
      "learning_rate": 0.0001735718444161631,
      "loss": 0.5472,
      "step": 8230
    },
    {
      "epoch": 0.5202020202020202,
      "grad_norm": 0.790433406829834,
      "learning_rate": 0.00017350256319671888,
      "loss": 0.499,
      "step": 8240
    },
    {
      "epoch": 0.5208333333333334,
      "grad_norm": 0.9409703016281128,
      "learning_rate": 0.000173433205151785,
      "loss": 0.516,
      "step": 8250
    },
    {
      "epoch": 0.5214646464646465,
      "grad_norm": 0.621878445148468,
      "learning_rate": 0.0001733637703538551,
      "loss": 0.7558,
      "step": 8260
    },
    {
      "epoch": 0.5220959595959596,
      "grad_norm": 0.6025867462158203,
      "learning_rate": 0.000173294258875503,
      "loss": 0.6437,
      "step": 8270
    },
    {
      "epoch": 0.5227272727272727,
      "grad_norm": 0.6046354174613953,
      "learning_rate": 0.0001732246707893827,
      "loss": 0.5597,
      "step": 8280
    },
    {
      "epoch": 0.5233585858585859,
      "grad_norm": 0.8065158128738403,
      "learning_rate": 0.0001731550061682282,
      "loss": 0.494,
      "step": 8290
    },
    {
      "epoch": 0.523989898989899,
      "grad_norm": 1.2688246965408325,
      "learning_rate": 0.00017308526508485352,
      "loss": 0.5417,
      "step": 8300
    },
    {
      "epoch": 0.5246212121212122,
      "grad_norm": 0.5910103917121887,
      "learning_rate": 0.0001730154476121527,
      "loss": 0.7436,
      "step": 8310
    },
    {
      "epoch": 0.5252525252525253,
      "grad_norm": 0.6910600662231445,
      "learning_rate": 0.00017294555382309947,
      "loss": 0.6249,
      "step": 8320
    },
    {
      "epoch": 0.5258838383838383,
      "grad_norm": 0.7428235411643982,
      "learning_rate": 0.00017287558379074747,
      "loss": 0.5597,
      "step": 8330
    },
    {
      "epoch": 0.5265151515151515,
      "grad_norm": 0.770065426826477,
      "learning_rate": 0.0001728055375882299,
      "loss": 0.4918,
      "step": 8340
    },
    {
      "epoch": 0.5271464646464646,
      "grad_norm": 1.0323365926742554,
      "learning_rate": 0.00017273541528875966,
      "loss": 0.5268,
      "step": 8350
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 0.6374819874763489,
      "learning_rate": 0.00017266521696562913,
      "loss": 0.7487,
      "step": 8360
    },
    {
      "epoch": 0.5284090909090909,
      "grad_norm": 0.6149225234985352,
      "learning_rate": 0.0001725949426922102,
      "loss": 0.6105,
      "step": 8370
    },
    {
      "epoch": 0.5290404040404041,
      "grad_norm": 0.806321918964386,
      "learning_rate": 0.00017252459254195413,
      "loss": 0.5591,
      "step": 8380
    },
    {
      "epoch": 0.5296717171717171,
      "grad_norm": 0.7144362330436707,
      "learning_rate": 0.00017245416658839152,
      "loss": 0.5154,
      "step": 8390
    },
    {
      "epoch": 0.5303030303030303,
      "grad_norm": 1.1171679496765137,
      "learning_rate": 0.00017238366490513206,
      "loss": 0.5103,
      "step": 8400
    },
    {
      "epoch": 0.5309343434343434,
      "grad_norm": 0.6402524709701538,
      "learning_rate": 0.00017231308756586477,
      "loss": 0.7245,
      "step": 8410
    },
    {
      "epoch": 0.5315656565656566,
      "grad_norm": 0.6471856832504272,
      "learning_rate": 0.00017224243464435766,
      "loss": 0.6019,
      "step": 8420
    },
    {
      "epoch": 0.5321969696969697,
      "grad_norm": 0.7572510242462158,
      "learning_rate": 0.00017217170621445775,
      "loss": 0.5278,
      "step": 8430
    },
    {
      "epoch": 0.5328282828282829,
      "grad_norm": 0.813193678855896,
      "learning_rate": 0.00017210090235009098,
      "loss": 0.4922,
      "step": 8440
    },
    {
      "epoch": 0.5334595959595959,
      "grad_norm": 0.9495216608047485,
      "learning_rate": 0.00017203002312526214,
      "loss": 0.5123,
      "step": 8450
    },
    {
      "epoch": 0.5340909090909091,
      "grad_norm": 0.5641337633132935,
      "learning_rate": 0.00017195906861405477,
      "loss": 0.71,
      "step": 8460
    },
    {
      "epoch": 0.5347222222222222,
      "grad_norm": 0.6673982739448547,
      "learning_rate": 0.00017188803889063112,
      "loss": 0.626,
      "step": 8470
    },
    {
      "epoch": 0.5353535353535354,
      "grad_norm": 0.7623387575149536,
      "learning_rate": 0.00017181693402923206,
      "loss": 0.5514,
      "step": 8480
    },
    {
      "epoch": 0.5359848484848485,
      "grad_norm": 0.7562404274940491,
      "learning_rate": 0.00017174575410417697,
      "loss": 0.4833,
      "step": 8490
    },
    {
      "epoch": 0.5366161616161617,
      "grad_norm": 0.9768145084381104,
      "learning_rate": 0.0001716744991898637,
      "loss": 0.5138,
      "step": 8500
    },
    {
      "epoch": 0.5372474747474747,
      "grad_norm": 0.607541024684906,
      "learning_rate": 0.00017160316936076848,
      "loss": 0.7708,
      "step": 8510
    },
    {
      "epoch": 0.5378787878787878,
      "grad_norm": 0.5920299291610718,
      "learning_rate": 0.00017153176469144585,
      "loss": 0.6266,
      "step": 8520
    },
    {
      "epoch": 0.538510101010101,
      "grad_norm": 0.6860084533691406,
      "learning_rate": 0.00017146028525652856,
      "loss": 0.5409,
      "step": 8530
    },
    {
      "epoch": 0.5391414141414141,
      "grad_norm": 0.8015006184577942,
      "learning_rate": 0.0001713887311307275,
      "loss": 0.5071,
      "step": 8540
    },
    {
      "epoch": 0.5397727272727273,
      "grad_norm": 0.9435932636260986,
      "learning_rate": 0.00017131710238883164,
      "loss": 0.5027,
      "step": 8550
    },
    {
      "epoch": 0.5404040404040404,
      "grad_norm": 0.5997703671455383,
      "learning_rate": 0.00017124539910570792,
      "loss": 0.7374,
      "step": 8560
    },
    {
      "epoch": 0.5410353535353535,
      "grad_norm": 0.6288837194442749,
      "learning_rate": 0.0001711736213563012,
      "loss": 0.6025,
      "step": 8570
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.7528879642486572,
      "learning_rate": 0.00017110176921563425,
      "loss": 0.5339,
      "step": 8580
    },
    {
      "epoch": 0.5422979797979798,
      "grad_norm": 0.7620011568069458,
      "learning_rate": 0.00017102984275880746,
      "loss": 0.5084,
      "step": 8590
    },
    {
      "epoch": 0.5429292929292929,
      "grad_norm": 0.9728717803955078,
      "learning_rate": 0.00017095784206099896,
      "loss": 0.524,
      "step": 8600
    },
    {
      "epoch": 0.5435606060606061,
      "grad_norm": 0.5647729635238647,
      "learning_rate": 0.00017088576719746453,
      "loss": 0.7584,
      "step": 8610
    },
    {
      "epoch": 0.5441919191919192,
      "grad_norm": 0.7118963003158569,
      "learning_rate": 0.00017081361824353736,
      "loss": 0.6467,
      "step": 8620
    },
    {
      "epoch": 0.5448232323232324,
      "grad_norm": 0.664786159992218,
      "learning_rate": 0.00017074139527462818,
      "loss": 0.5284,
      "step": 8630
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.8275582790374756,
      "learning_rate": 0.000170669098366225,
      "loss": 0.4749,
      "step": 8640
    },
    {
      "epoch": 0.5460858585858586,
      "grad_norm": 1.2185934782028198,
      "learning_rate": 0.00017059672759389317,
      "loss": 0.5081,
      "step": 8650
    },
    {
      "epoch": 0.5467171717171717,
      "grad_norm": 0.5786290764808655,
      "learning_rate": 0.0001705242830332752,
      "loss": 0.7057,
      "step": 8660
    },
    {
      "epoch": 0.5473484848484849,
      "grad_norm": 0.6911237239837646,
      "learning_rate": 0.00017045176476009074,
      "loss": 0.6291,
      "step": 8670
    },
    {
      "epoch": 0.547979797979798,
      "grad_norm": 0.6757599115371704,
      "learning_rate": 0.00017037917285013654,
      "loss": 0.5353,
      "step": 8680
    },
    {
      "epoch": 0.5486111111111112,
      "grad_norm": 0.7431146502494812,
      "learning_rate": 0.00017030650737928627,
      "loss": 0.4639,
      "step": 8690
    },
    {
      "epoch": 0.5492424242424242,
      "grad_norm": 0.941423237323761,
      "learning_rate": 0.00017023376842349041,
      "loss": 0.5136,
      "step": 8700
    },
    {
      "epoch": 0.5498737373737373,
      "grad_norm": 0.5689589977264404,
      "learning_rate": 0.00017016095605877637,
      "loss": 0.7326,
      "step": 8710
    },
    {
      "epoch": 0.5505050505050505,
      "grad_norm": 0.6286278367042542,
      "learning_rate": 0.00017008807036124828,
      "loss": 0.6207,
      "step": 8720
    },
    {
      "epoch": 0.5511363636363636,
      "grad_norm": 0.7922292947769165,
      "learning_rate": 0.0001700151114070868,
      "loss": 0.5461,
      "step": 8730
    },
    {
      "epoch": 0.5517676767676768,
      "grad_norm": 0.8212761878967285,
      "learning_rate": 0.00016994207927254924,
      "loss": 0.4865,
      "step": 8740
    },
    {
      "epoch": 0.55239898989899,
      "grad_norm": 1.0219744443893433,
      "learning_rate": 0.00016986897403396944,
      "loss": 0.5152,
      "step": 8750
    },
    {
      "epoch": 0.553030303030303,
      "grad_norm": 0.6037240624427795,
      "learning_rate": 0.00016979579576775758,
      "loss": 0.7185,
      "step": 8760
    },
    {
      "epoch": 0.5536616161616161,
      "grad_norm": 0.6580737233161926,
      "learning_rate": 0.00016972254455040021,
      "loss": 0.6025,
      "step": 8770
    },
    {
      "epoch": 0.5542929292929293,
      "grad_norm": 0.6297159194946289,
      "learning_rate": 0.0001696492204584601,
      "loss": 0.5651,
      "step": 8780
    },
    {
      "epoch": 0.5549242424242424,
      "grad_norm": 0.894807755947113,
      "learning_rate": 0.00016957582356857617,
      "loss": 0.4631,
      "step": 8790
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.0152623653411865,
      "learning_rate": 0.00016950235395746347,
      "loss": 0.5219,
      "step": 8800
    },
    {
      "epoch": 0.5561868686868687,
      "grad_norm": 0.6217525005340576,
      "learning_rate": 0.0001694288117019131,
      "loss": 0.7262,
      "step": 8810
    },
    {
      "epoch": 0.5568181818181818,
      "grad_norm": 0.6197936534881592,
      "learning_rate": 0.000169355196878792,
      "loss": 0.6071,
      "step": 8820
    },
    {
      "epoch": 0.5574494949494949,
      "grad_norm": 0.7360895872116089,
      "learning_rate": 0.00016928150956504293,
      "loss": 0.5464,
      "step": 8830
    },
    {
      "epoch": 0.5580808080808081,
      "grad_norm": 0.7620795369148254,
      "learning_rate": 0.0001692077498376846,
      "loss": 0.4894,
      "step": 8840
    },
    {
      "epoch": 0.5587121212121212,
      "grad_norm": 0.9170685410499573,
      "learning_rate": 0.00016913391777381124,
      "loss": 0.5227,
      "step": 8850
    },
    {
      "epoch": 0.5593434343434344,
      "grad_norm": 0.5853192806243896,
      "learning_rate": 0.00016906001345059273,
      "loss": 0.7288,
      "step": 8860
    },
    {
      "epoch": 0.5599747474747475,
      "grad_norm": 0.628969132900238,
      "learning_rate": 0.00016898603694527443,
      "loss": 0.6257,
      "step": 8870
    },
    {
      "epoch": 0.5606060606060606,
      "grad_norm": 0.7234898209571838,
      "learning_rate": 0.00016891198833517729,
      "loss": 0.5436,
      "step": 8880
    },
    {
      "epoch": 0.5612373737373737,
      "grad_norm": 0.7205088138580322,
      "learning_rate": 0.00016883786769769752,
      "loss": 0.5183,
      "step": 8890
    },
    {
      "epoch": 0.5618686868686869,
      "grad_norm": 1.038011908531189,
      "learning_rate": 0.00016876367511030655,
      "loss": 0.531,
      "step": 8900
    },
    {
      "epoch": 0.5625,
      "grad_norm": 0.6014278531074524,
      "learning_rate": 0.00016868941065055116,
      "loss": 0.7461,
      "step": 8910
    },
    {
      "epoch": 0.5631313131313131,
      "grad_norm": 0.6033473014831543,
      "learning_rate": 0.00016861507439605317,
      "loss": 0.625,
      "step": 8920
    },
    {
      "epoch": 0.5637626262626263,
      "grad_norm": 0.6435050368309021,
      "learning_rate": 0.00016854066642450942,
      "loss": 0.5486,
      "step": 8930
    },
    {
      "epoch": 0.5643939393939394,
      "grad_norm": 0.7559719085693359,
      "learning_rate": 0.00016846618681369178,
      "loss": 0.4738,
      "step": 8940
    },
    {
      "epoch": 0.5650252525252525,
      "grad_norm": 0.8655114769935608,
      "learning_rate": 0.00016839163564144694,
      "loss": 0.524,
      "step": 8950
    },
    {
      "epoch": 0.5656565656565656,
      "grad_norm": 0.5815051794052124,
      "learning_rate": 0.0001683170129856964,
      "loss": 0.7318,
      "step": 8960
    },
    {
      "epoch": 0.5662878787878788,
      "grad_norm": 0.6429320573806763,
      "learning_rate": 0.00016824231892443635,
      "loss": 0.6209,
      "step": 8970
    },
    {
      "epoch": 0.5669191919191919,
      "grad_norm": 0.6432978510856628,
      "learning_rate": 0.0001681675535357377,
      "loss": 0.54,
      "step": 8980
    },
    {
      "epoch": 0.5675505050505051,
      "grad_norm": 0.8134156465530396,
      "learning_rate": 0.00016809271689774584,
      "loss": 0.5091,
      "step": 8990
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 0.9552921652793884,
      "learning_rate": 0.00016801780908868062,
      "loss": 0.4853,
      "step": 9000
    },
    {
      "epoch": 0.5681818181818182,
      "eval_loss": 0.5697404146194458,
      "eval_runtime": 28.5065,
      "eval_samples_per_second": 89.804,
      "eval_steps_per_second": 11.225,
      "step": 9000
    },
    {
      "epoch": 0.5688131313131313,
      "grad_norm": 0.5849342942237854,
      "learning_rate": 0.00016794283018683632,
      "loss": 0.7374,
      "step": 9010
    },
    {
      "epoch": 0.5694444444444444,
      "grad_norm": 0.7163851857185364,
      "learning_rate": 0.00016786778027058153,
      "loss": 0.6094,
      "step": 9020
    },
    {
      "epoch": 0.5700757575757576,
      "grad_norm": 0.6832115054130554,
      "learning_rate": 0.00016779265941835897,
      "loss": 0.5534,
      "step": 9030
    },
    {
      "epoch": 0.5707070707070707,
      "grad_norm": 0.7660689949989319,
      "learning_rate": 0.00016771746770868567,
      "loss": 0.4896,
      "step": 9040
    },
    {
      "epoch": 0.5713383838383839,
      "grad_norm": 1.063767671585083,
      "learning_rate": 0.00016764220522015263,
      "loss": 0.5359,
      "step": 9050
    },
    {
      "epoch": 0.571969696969697,
      "grad_norm": 0.5511501431465149,
      "learning_rate": 0.0001675668720314248,
      "loss": 0.7329,
      "step": 9060
    },
    {
      "epoch": 0.57260101010101,
      "grad_norm": 0.6513356566429138,
      "learning_rate": 0.00016749146822124097,
      "loss": 0.6245,
      "step": 9070
    },
    {
      "epoch": 0.5732323232323232,
      "grad_norm": 0.5953903198242188,
      "learning_rate": 0.00016741599386841397,
      "loss": 0.547,
      "step": 9080
    },
    {
      "epoch": 0.5738636363636364,
      "grad_norm": 0.7221503257751465,
      "learning_rate": 0.00016734044905183012,
      "loss": 0.4586,
      "step": 9090
    },
    {
      "epoch": 0.5744949494949495,
      "grad_norm": 1.1477705240249634,
      "learning_rate": 0.00016726483385044958,
      "loss": 0.5489,
      "step": 9100
    },
    {
      "epoch": 0.5751262626262627,
      "grad_norm": 0.5694388151168823,
      "learning_rate": 0.0001671891483433059,
      "loss": 0.7114,
      "step": 9110
    },
    {
      "epoch": 0.5757575757575758,
      "grad_norm": 0.6186876893043518,
      "learning_rate": 0.00016711339260950622,
      "loss": 0.5986,
      "step": 9120
    },
    {
      "epoch": 0.5763888888888888,
      "grad_norm": 0.6753848195075989,
      "learning_rate": 0.0001670375667282311,
      "loss": 0.5232,
      "step": 9130
    },
    {
      "epoch": 0.577020202020202,
      "grad_norm": 0.702938973903656,
      "learning_rate": 0.00016696167077873435,
      "loss": 0.4887,
      "step": 9140
    },
    {
      "epoch": 0.5776515151515151,
      "grad_norm": 1.0157321691513062,
      "learning_rate": 0.00016688570484034307,
      "loss": 0.519,
      "step": 9150
    },
    {
      "epoch": 0.5782828282828283,
      "grad_norm": 0.5609768033027649,
      "learning_rate": 0.00016680966899245748,
      "loss": 0.7024,
      "step": 9160
    },
    {
      "epoch": 0.5789141414141414,
      "grad_norm": 0.569517970085144,
      "learning_rate": 0.00016673356331455084,
      "loss": 0.6071,
      "step": 9170
    },
    {
      "epoch": 0.5795454545454546,
      "grad_norm": 0.699206531047821,
      "learning_rate": 0.00016665738788616946,
      "loss": 0.5404,
      "step": 9180
    },
    {
      "epoch": 0.5801767676767676,
      "grad_norm": 0.6626868844032288,
      "learning_rate": 0.0001665811427869326,
      "loss": 0.4741,
      "step": 9190
    },
    {
      "epoch": 0.5808080808080808,
      "grad_norm": 1.057674765586853,
      "learning_rate": 0.00016650482809653217,
      "loss": 0.514,
      "step": 9200
    },
    {
      "epoch": 0.5814393939393939,
      "grad_norm": 0.6003877520561218,
      "learning_rate": 0.000166428443894733,
      "loss": 0.7363,
      "step": 9210
    },
    {
      "epoch": 0.5820707070707071,
      "grad_norm": 0.5828176140785217,
      "learning_rate": 0.00016635199026137243,
      "loss": 0.6037,
      "step": 9220
    },
    {
      "epoch": 0.5827020202020202,
      "grad_norm": 0.747687816619873,
      "learning_rate": 0.00016627546727636044,
      "loss": 0.5642,
      "step": 9230
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.7262148857116699,
      "learning_rate": 0.00016619887501967954,
      "loss": 0.4979,
      "step": 9240
    },
    {
      "epoch": 0.5839646464646465,
      "grad_norm": 0.9663421511650085,
      "learning_rate": 0.00016612221357138453,
      "loss": 0.5092,
      "step": 9250
    },
    {
      "epoch": 0.5845959595959596,
      "grad_norm": 0.6312105059623718,
      "learning_rate": 0.00016604548301160264,
      "loss": 0.8135,
      "step": 9260
    },
    {
      "epoch": 0.5852272727272727,
      "grad_norm": 0.6083599925041199,
      "learning_rate": 0.00016596868342053325,
      "loss": 0.616,
      "step": 9270
    },
    {
      "epoch": 0.5858585858585859,
      "grad_norm": 0.7082657814025879,
      "learning_rate": 0.000165891814878448,
      "loss": 0.532,
      "step": 9280
    },
    {
      "epoch": 0.586489898989899,
      "grad_norm": 0.761286199092865,
      "learning_rate": 0.00016581487746569043,
      "loss": 0.4988,
      "step": 9290
    },
    {
      "epoch": 0.5871212121212122,
      "grad_norm": 1.0681735277175903,
      "learning_rate": 0.0001657378712626762,
      "loss": 0.5055,
      "step": 9300
    },
    {
      "epoch": 0.5877525252525253,
      "grad_norm": 0.6230157613754272,
      "learning_rate": 0.00016566079634989289,
      "loss": 0.7132,
      "step": 9310
    },
    {
      "epoch": 0.5883838383838383,
      "grad_norm": 0.6026455163955688,
      "learning_rate": 0.00016558365280789977,
      "loss": 0.5804,
      "step": 9320
    },
    {
      "epoch": 0.5890151515151515,
      "grad_norm": 0.646434485912323,
      "learning_rate": 0.00016550644071732794,
      "loss": 0.534,
      "step": 9330
    },
    {
      "epoch": 0.5896464646464646,
      "grad_norm": 0.66474449634552,
      "learning_rate": 0.0001654291601588801,
      "loss": 0.496,
      "step": 9340
    },
    {
      "epoch": 0.5902777777777778,
      "grad_norm": 0.8743157386779785,
      "learning_rate": 0.00016535181121333058,
      "loss": 0.5129,
      "step": 9350
    },
    {
      "epoch": 0.5909090909090909,
      "grad_norm": 0.5828522443771362,
      "learning_rate": 0.00016527439396152508,
      "loss": 0.7057,
      "step": 9360
    },
    {
      "epoch": 0.5915404040404041,
      "grad_norm": 0.6677644848823547,
      "learning_rate": 0.0001651969084843808,
      "loss": 0.6276,
      "step": 9370
    },
    {
      "epoch": 0.5921717171717171,
      "grad_norm": 0.7089948654174805,
      "learning_rate": 0.00016511935486288618,
      "loss": 0.523,
      "step": 9380
    },
    {
      "epoch": 0.5928030303030303,
      "grad_norm": 0.7703858613967896,
      "learning_rate": 0.0001650417331781009,
      "loss": 0.4543,
      "step": 9390
    },
    {
      "epoch": 0.5934343434343434,
      "grad_norm": 1.0024311542510986,
      "learning_rate": 0.0001649640435111558,
      "loss": 0.5082,
      "step": 9400
    },
    {
      "epoch": 0.5940656565656566,
      "grad_norm": 0.5712293982505798,
      "learning_rate": 0.00016488628594325277,
      "loss": 0.7182,
      "step": 9410
    },
    {
      "epoch": 0.5946969696969697,
      "grad_norm": 0.7216587662696838,
      "learning_rate": 0.0001648084605556647,
      "loss": 0.6093,
      "step": 9420
    },
    {
      "epoch": 0.5953282828282829,
      "grad_norm": 0.5914806723594666,
      "learning_rate": 0.00016473056742973526,
      "loss": 0.5013,
      "step": 9430
    },
    {
      "epoch": 0.5959595959595959,
      "grad_norm": 0.7751336693763733,
      "learning_rate": 0.00016465260664687902,
      "loss": 0.4687,
      "step": 9440
    },
    {
      "epoch": 0.5965909090909091,
      "grad_norm": 1.0089976787567139,
      "learning_rate": 0.0001645745782885813,
      "loss": 0.5082,
      "step": 9450
    },
    {
      "epoch": 0.5972222222222222,
      "grad_norm": 0.5451928973197937,
      "learning_rate": 0.00016449648243639788,
      "loss": 0.7138,
      "step": 9460
    },
    {
      "epoch": 0.5978535353535354,
      "grad_norm": 0.6148408055305481,
      "learning_rate": 0.0001644183191719553,
      "loss": 0.6298,
      "step": 9470
    },
    {
      "epoch": 0.5984848484848485,
      "grad_norm": 0.6403055191040039,
      "learning_rate": 0.00016434008857695037,
      "loss": 0.5521,
      "step": 9480
    },
    {
      "epoch": 0.5991161616161617,
      "grad_norm": 0.7750465273857117,
      "learning_rate": 0.0001642617907331504,
      "loss": 0.4663,
      "step": 9490
    },
    {
      "epoch": 0.5997474747474747,
      "grad_norm": 1.1629151105880737,
      "learning_rate": 0.00016418342572239292,
      "loss": 0.5095,
      "step": 9500
    },
    {
      "epoch": 0.6003787878787878,
      "grad_norm": 0.5770272016525269,
      "learning_rate": 0.0001641049936265857,
      "loss": 0.7128,
      "step": 9510
    },
    {
      "epoch": 0.601010101010101,
      "grad_norm": 0.6337581872940063,
      "learning_rate": 0.00016402649452770666,
      "loss": 0.6233,
      "step": 9520
    },
    {
      "epoch": 0.6016414141414141,
      "grad_norm": 0.6559898257255554,
      "learning_rate": 0.00016394792850780364,
      "loss": 0.5374,
      "step": 9530
    },
    {
      "epoch": 0.6022727272727273,
      "grad_norm": 0.7238504886627197,
      "learning_rate": 0.00016386929564899457,
      "loss": 0.4797,
      "step": 9540
    },
    {
      "epoch": 0.6029040404040404,
      "grad_norm": 0.9948258399963379,
      "learning_rate": 0.0001637905960334671,
      "loss": 0.5225,
      "step": 9550
    },
    {
      "epoch": 0.6035353535353535,
      "grad_norm": 0.6306954622268677,
      "learning_rate": 0.00016371182974347876,
      "loss": 0.7366,
      "step": 9560
    },
    {
      "epoch": 0.6041666666666666,
      "grad_norm": 0.6813060641288757,
      "learning_rate": 0.0001636329968613567,
      "loss": 0.6037,
      "step": 9570
    },
    {
      "epoch": 0.6047979797979798,
      "grad_norm": 0.6454476714134216,
      "learning_rate": 0.00016355409746949778,
      "loss": 0.5347,
      "step": 9580
    },
    {
      "epoch": 0.6054292929292929,
      "grad_norm": 0.8190798759460449,
      "learning_rate": 0.0001634751316503682,
      "loss": 0.4916,
      "step": 9590
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.1511801481246948,
      "learning_rate": 0.00016339609948650376,
      "loss": 0.4965,
      "step": 9600
    },
    {
      "epoch": 0.6066919191919192,
      "grad_norm": 0.6270524859428406,
      "learning_rate": 0.0001633170010605095,
      "loss": 0.7242,
      "step": 9610
    },
    {
      "epoch": 0.6073232323232324,
      "grad_norm": 0.6262259483337402,
      "learning_rate": 0.00016323783645505975,
      "loss": 0.6118,
      "step": 9620
    },
    {
      "epoch": 0.6079545454545454,
      "grad_norm": 0.6198704242706299,
      "learning_rate": 0.000163158605752898,
      "loss": 0.5308,
      "step": 9630
    },
    {
      "epoch": 0.6085858585858586,
      "grad_norm": 0.7219989895820618,
      "learning_rate": 0.0001630793090368369,
      "loss": 0.4594,
      "step": 9640
    },
    {
      "epoch": 0.6092171717171717,
      "grad_norm": 0.8701320290565491,
      "learning_rate": 0.00016299994638975797,
      "loss": 0.532,
      "step": 9650
    },
    {
      "epoch": 0.6098484848484849,
      "grad_norm": 0.5924901962280273,
      "learning_rate": 0.0001629205178946118,
      "loss": 0.7544,
      "step": 9660
    },
    {
      "epoch": 0.610479797979798,
      "grad_norm": 0.6924330592155457,
      "learning_rate": 0.00016284102363441758,
      "loss": 0.6051,
      "step": 9670
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.5664271116256714,
      "learning_rate": 0.00016276146369226347,
      "loss": 0.5428,
      "step": 9680
    },
    {
      "epoch": 0.6117424242424242,
      "grad_norm": 0.7779406309127808,
      "learning_rate": 0.00016268183815130614,
      "loss": 0.4939,
      "step": 9690
    },
    {
      "epoch": 0.6123737373737373,
      "grad_norm": 0.9932751655578613,
      "learning_rate": 0.00016260214709477088,
      "loss": 0.5183,
      "step": 9700
    },
    {
      "epoch": 0.6130050505050505,
      "grad_norm": 0.624411404132843,
      "learning_rate": 0.00016252239060595146,
      "loss": 0.6997,
      "step": 9710
    },
    {
      "epoch": 0.6136363636363636,
      "grad_norm": 0.6652482151985168,
      "learning_rate": 0.00016244256876821,
      "loss": 0.6061,
      "step": 9720
    },
    {
      "epoch": 0.6142676767676768,
      "grad_norm": 0.7317819595336914,
      "learning_rate": 0.00016236268166497695,
      "loss": 0.5343,
      "step": 9730
    },
    {
      "epoch": 0.61489898989899,
      "grad_norm": 0.7673357129096985,
      "learning_rate": 0.000162282729379751,
      "loss": 0.4947,
      "step": 9740
    },
    {
      "epoch": 0.615530303030303,
      "grad_norm": 0.9375787973403931,
      "learning_rate": 0.0001622027119960989,
      "loss": 0.5405,
      "step": 9750
    },
    {
      "epoch": 0.6161616161616161,
      "grad_norm": 0.6180186867713928,
      "learning_rate": 0.0001621226295976555,
      "loss": 0.73,
      "step": 9760
    },
    {
      "epoch": 0.6167929292929293,
      "grad_norm": 0.639901876449585,
      "learning_rate": 0.00016204248226812365,
      "loss": 0.6033,
      "step": 9770
    },
    {
      "epoch": 0.6174242424242424,
      "grad_norm": 0.8158813118934631,
      "learning_rate": 0.0001619622700912739,
      "loss": 0.5281,
      "step": 9780
    },
    {
      "epoch": 0.6180555555555556,
      "grad_norm": 0.6952929496765137,
      "learning_rate": 0.00016188199315094473,
      "loss": 0.4966,
      "step": 9790
    },
    {
      "epoch": 0.6186868686868687,
      "grad_norm": 1.1230703592300415,
      "learning_rate": 0.0001618016515310423,
      "loss": 0.5465,
      "step": 9800
    },
    {
      "epoch": 0.6193181818181818,
      "grad_norm": 0.5628705024719238,
      "learning_rate": 0.0001617212453155403,
      "loss": 0.6899,
      "step": 9810
    },
    {
      "epoch": 0.6199494949494949,
      "grad_norm": 0.5926527380943298,
      "learning_rate": 0.00016164077458847995,
      "loss": 0.5677,
      "step": 9820
    },
    {
      "epoch": 0.6205808080808081,
      "grad_norm": 0.6544759273529053,
      "learning_rate": 0.00016156023943396998,
      "loss": 0.5603,
      "step": 9830
    },
    {
      "epoch": 0.6212121212121212,
      "grad_norm": 0.722484827041626,
      "learning_rate": 0.0001614796399361864,
      "loss": 0.5152,
      "step": 9840
    },
    {
      "epoch": 0.6218434343434344,
      "grad_norm": 1.075011134147644,
      "learning_rate": 0.00016139897617937238,
      "loss": 0.5327,
      "step": 9850
    },
    {
      "epoch": 0.6224747474747475,
      "grad_norm": 0.6388146281242371,
      "learning_rate": 0.00016131824824783847,
      "loss": 0.694,
      "step": 9860
    },
    {
      "epoch": 0.6231060606060606,
      "grad_norm": 0.6580607891082764,
      "learning_rate": 0.00016123745622596212,
      "loss": 0.6024,
      "step": 9870
    },
    {
      "epoch": 0.6237373737373737,
      "grad_norm": 0.6531664133071899,
      "learning_rate": 0.0001611566001981878,
      "loss": 0.5339,
      "step": 9880
    },
    {
      "epoch": 0.6243686868686869,
      "grad_norm": 0.9387347102165222,
      "learning_rate": 0.00016107568024902697,
      "loss": 0.465,
      "step": 9890
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.9779948592185974,
      "learning_rate": 0.00016099469646305777,
      "loss": 0.5205,
      "step": 9900
    },
    {
      "epoch": 0.6256313131313131,
      "grad_norm": 0.5690833330154419,
      "learning_rate": 0.00016091364892492516,
      "loss": 0.7166,
      "step": 9910
    },
    {
      "epoch": 0.6262626262626263,
      "grad_norm": 0.618030309677124,
      "learning_rate": 0.00016083253771934065,
      "loss": 0.5875,
      "step": 9920
    },
    {
      "epoch": 0.6268939393939394,
      "grad_norm": 0.6901810765266418,
      "learning_rate": 0.00016075136293108245,
      "loss": 0.5289,
      "step": 9930
    },
    {
      "epoch": 0.6275252525252525,
      "grad_norm": 0.7188361287117004,
      "learning_rate": 0.000160670124644995,
      "loss": 0.4809,
      "step": 9940
    },
    {
      "epoch": 0.6281565656565656,
      "grad_norm": 1.02596914768219,
      "learning_rate": 0.00016058882294598932,
      "loss": 0.5085,
      "step": 9950
    },
    {
      "epoch": 0.6287878787878788,
      "grad_norm": 0.5997493267059326,
      "learning_rate": 0.00016050745791904256,
      "loss": 0.6954,
      "step": 9960
    },
    {
      "epoch": 0.6294191919191919,
      "grad_norm": 0.6015306711196899,
      "learning_rate": 0.00016042602964919816,
      "loss": 0.6095,
      "step": 9970
    },
    {
      "epoch": 0.6300505050505051,
      "grad_norm": 0.7047749757766724,
      "learning_rate": 0.0001603445382215656,
      "loss": 0.5287,
      "step": 9980
    },
    {
      "epoch": 0.6306818181818182,
      "grad_norm": 0.6820768117904663,
      "learning_rate": 0.00016026298372132046,
      "loss": 0.4862,
      "step": 9990
    },
    {
      "epoch": 0.6313131313131313,
      "grad_norm": 0.9814989566802979,
      "learning_rate": 0.00016018136623370408,
      "loss": 0.5095,
      "step": 10000
    },
    {
      "epoch": 0.6313131313131313,
      "eval_loss": 0.5610395669937134,
      "eval_runtime": 29.0275,
      "eval_samples_per_second": 88.192,
      "eval_steps_per_second": 11.024,
      "step": 10000
    },
    {
      "epoch": 0.6319444444444444,
      "grad_norm": 0.5336341261863708,
      "learning_rate": 0.00016009968584402383,
      "loss": 0.7298,
      "step": 10010
    },
    {
      "epoch": 0.6325757575757576,
      "grad_norm": 0.7084565758705139,
      "learning_rate": 0.00016001794263765265,
      "loss": 0.6319,
      "step": 10020
    },
    {
      "epoch": 0.6332070707070707,
      "grad_norm": 0.6133972406387329,
      "learning_rate": 0.0001599361367000293,
      "loss": 0.5176,
      "step": 10030
    },
    {
      "epoch": 0.6338383838383839,
      "grad_norm": 0.7794643044471741,
      "learning_rate": 0.000159854268116658,
      "loss": 0.4928,
      "step": 10040
    },
    {
      "epoch": 0.634469696969697,
      "grad_norm": 0.8817728161811829,
      "learning_rate": 0.00015977233697310838,
      "loss": 0.5165,
      "step": 10050
    },
    {
      "epoch": 0.63510101010101,
      "grad_norm": 0.5256279110908508,
      "learning_rate": 0.00015969034335501568,
      "loss": 0.7081,
      "step": 10060
    },
    {
      "epoch": 0.6357323232323232,
      "grad_norm": 0.6603567004203796,
      "learning_rate": 0.00015960828734808027,
      "loss": 0.6066,
      "step": 10070
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.7120043635368347,
      "learning_rate": 0.00015952616903806775,
      "loss": 0.5262,
      "step": 10080
    },
    {
      "epoch": 0.6369949494949495,
      "grad_norm": 0.7762681841850281,
      "learning_rate": 0.00015944398851080885,
      "loss": 0.5054,
      "step": 10090
    },
    {
      "epoch": 0.6376262626262627,
      "grad_norm": 1.0158042907714844,
      "learning_rate": 0.00015936174585219937,
      "loss": 0.4961,
      "step": 10100
    },
    {
      "epoch": 0.6382575757575758,
      "grad_norm": 0.5520707368850708,
      "learning_rate": 0.00015927944114820005,
      "loss": 0.7019,
      "step": 10110
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 0.623985230922699,
      "learning_rate": 0.00015919707448483638,
      "loss": 0.6219,
      "step": 10120
    },
    {
      "epoch": 0.639520202020202,
      "grad_norm": 0.6146858334541321,
      "learning_rate": 0.0001591146459481987,
      "loss": 0.5262,
      "step": 10130
    },
    {
      "epoch": 0.6401515151515151,
      "grad_norm": 0.8028244972229004,
      "learning_rate": 0.00015903215562444202,
      "loss": 0.4579,
      "step": 10140
    },
    {
      "epoch": 0.6407828282828283,
      "grad_norm": 1.1887600421905518,
      "learning_rate": 0.00015894960359978593,
      "loss": 0.5283,
      "step": 10150
    },
    {
      "epoch": 0.6414141414141414,
      "grad_norm": 0.6019163131713867,
      "learning_rate": 0.00015886698996051446,
      "loss": 0.7141,
      "step": 10160
    },
    {
      "epoch": 0.6420454545454546,
      "grad_norm": 0.612490713596344,
      "learning_rate": 0.00015878431479297603,
      "loss": 0.6034,
      "step": 10170
    },
    {
      "epoch": 0.6426767676767676,
      "grad_norm": 0.6465587019920349,
      "learning_rate": 0.0001587015781835835,
      "loss": 0.5416,
      "step": 10180
    },
    {
      "epoch": 0.6433080808080808,
      "grad_norm": 0.7560814619064331,
      "learning_rate": 0.00015861878021881384,
      "loss": 0.5156,
      "step": 10190
    },
    {
      "epoch": 0.6439393939393939,
      "grad_norm": 1.0162643194198608,
      "learning_rate": 0.0001585359209852081,
      "loss": 0.5106,
      "step": 10200
    },
    {
      "epoch": 0.6445707070707071,
      "grad_norm": 0.5484115481376648,
      "learning_rate": 0.00015845300056937153,
      "loss": 0.6798,
      "step": 10210
    },
    {
      "epoch": 0.6452020202020202,
      "grad_norm": 0.6867944002151489,
      "learning_rate": 0.0001583700190579732,
      "loss": 0.5895,
      "step": 10220
    },
    {
      "epoch": 0.6458333333333334,
      "grad_norm": 0.6822202801704407,
      "learning_rate": 0.00015828697653774606,
      "loss": 0.5187,
      "step": 10230
    },
    {
      "epoch": 0.6464646464646465,
      "grad_norm": 0.6740832328796387,
      "learning_rate": 0.00015820387309548685,
      "loss": 0.4817,
      "step": 10240
    },
    {
      "epoch": 0.6470959595959596,
      "grad_norm": 1.0034559965133667,
      "learning_rate": 0.00015812070881805602,
      "loss": 0.4928,
      "step": 10250
    },
    {
      "epoch": 0.6477272727272727,
      "grad_norm": 0.5202574729919434,
      "learning_rate": 0.00015803748379237747,
      "loss": 0.7267,
      "step": 10260
    },
    {
      "epoch": 0.6483585858585859,
      "grad_norm": 0.6594498157501221,
      "learning_rate": 0.00015795419810543882,
      "loss": 0.6229,
      "step": 10270
    },
    {
      "epoch": 0.648989898989899,
      "grad_norm": 0.6646615266799927,
      "learning_rate": 0.00015787085184429086,
      "loss": 0.5175,
      "step": 10280
    },
    {
      "epoch": 0.6496212121212122,
      "grad_norm": 0.6308152079582214,
      "learning_rate": 0.0001577874450960478,
      "loss": 0.4874,
      "step": 10290
    },
    {
      "epoch": 0.6502525252525253,
      "grad_norm": 0.9893712401390076,
      "learning_rate": 0.00015770397794788706,
      "loss": 0.5198,
      "step": 10300
    },
    {
      "epoch": 0.6508838383838383,
      "grad_norm": 0.5140767097473145,
      "learning_rate": 0.00015762045048704927,
      "loss": 0.7319,
      "step": 10310
    },
    {
      "epoch": 0.6515151515151515,
      "grad_norm": 0.6555214524269104,
      "learning_rate": 0.00015753686280083797,
      "loss": 0.6023,
      "step": 10320
    },
    {
      "epoch": 0.6521464646464646,
      "grad_norm": 0.6270751357078552,
      "learning_rate": 0.00015745321497661973,
      "loss": 0.5599,
      "step": 10330
    },
    {
      "epoch": 0.6527777777777778,
      "grad_norm": 0.6615898609161377,
      "learning_rate": 0.00015736950710182392,
      "loss": 0.4872,
      "step": 10340
    },
    {
      "epoch": 0.6534090909090909,
      "grad_norm": 1.1644892692565918,
      "learning_rate": 0.00015728573926394271,
      "loss": 0.518,
      "step": 10350
    },
    {
      "epoch": 0.6540404040404041,
      "grad_norm": 0.5682229995727539,
      "learning_rate": 0.00015720191155053098,
      "loss": 0.6618,
      "step": 10360
    },
    {
      "epoch": 0.6546717171717171,
      "grad_norm": 0.628243625164032,
      "learning_rate": 0.0001571180240492061,
      "loss": 0.6051,
      "step": 10370
    },
    {
      "epoch": 0.6553030303030303,
      "grad_norm": 0.6722463965415955,
      "learning_rate": 0.00015703407684764802,
      "loss": 0.5316,
      "step": 10380
    },
    {
      "epoch": 0.6559343434343434,
      "grad_norm": 0.7588673233985901,
      "learning_rate": 0.00015695007003359908,
      "loss": 0.4777,
      "step": 10390
    },
    {
      "epoch": 0.6565656565656566,
      "grad_norm": 0.9397499561309814,
      "learning_rate": 0.0001568660036948638,
      "loss": 0.5357,
      "step": 10400
    },
    {
      "epoch": 0.6571969696969697,
      "grad_norm": 0.5240222811698914,
      "learning_rate": 0.0001567818779193091,
      "loss": 0.6932,
      "step": 10410
    },
    {
      "epoch": 0.6578282828282829,
      "grad_norm": 0.6276808977127075,
      "learning_rate": 0.0001566976927948639,
      "loss": 0.6016,
      "step": 10420
    },
    {
      "epoch": 0.6584595959595959,
      "grad_norm": 0.6330903768539429,
      "learning_rate": 0.0001566134484095192,
      "loss": 0.5264,
      "step": 10430
    },
    {
      "epoch": 0.6590909090909091,
      "grad_norm": 0.8061608672142029,
      "learning_rate": 0.00015652914485132796,
      "loss": 0.4742,
      "step": 10440
    },
    {
      "epoch": 0.6597222222222222,
      "grad_norm": 0.8812556862831116,
      "learning_rate": 0.00015644478220840492,
      "loss": 0.503,
      "step": 10450
    },
    {
      "epoch": 0.6603535353535354,
      "grad_norm": 0.6100068688392639,
      "learning_rate": 0.00015636036056892663,
      "loss": 0.7505,
      "step": 10460
    },
    {
      "epoch": 0.6609848484848485,
      "grad_norm": 0.6305376291275024,
      "learning_rate": 0.00015627588002113134,
      "loss": 0.6018,
      "step": 10470
    },
    {
      "epoch": 0.6616161616161617,
      "grad_norm": 0.6960409283638,
      "learning_rate": 0.00015619134065331873,
      "loss": 0.5286,
      "step": 10480
    },
    {
      "epoch": 0.6622474747474747,
      "grad_norm": 0.6642200350761414,
      "learning_rate": 0.0001561067425538501,
      "loss": 0.4955,
      "step": 10490
    },
    {
      "epoch": 0.6628787878787878,
      "grad_norm": 1.1106765270233154,
      "learning_rate": 0.00015602208581114808,
      "loss": 0.5066,
      "step": 10500
    },
    {
      "epoch": 0.663510101010101,
      "grad_norm": 0.5917830467224121,
      "learning_rate": 0.00015593737051369655,
      "loss": 0.6852,
      "step": 10510
    },
    {
      "epoch": 0.6641414141414141,
      "grad_norm": 0.5990296006202698,
      "learning_rate": 0.00015585259675004076,
      "loss": 0.617,
      "step": 10520
    },
    {
      "epoch": 0.6647727272727273,
      "grad_norm": 0.6183884143829346,
      "learning_rate": 0.00015576776460878686,
      "loss": 0.5169,
      "step": 10530
    },
    {
      "epoch": 0.6654040404040404,
      "grad_norm": 0.8281865119934082,
      "learning_rate": 0.0001556828741786021,
      "loss": 0.4973,
      "step": 10540
    },
    {
      "epoch": 0.6660353535353535,
      "grad_norm": 1.2316936254501343,
      "learning_rate": 0.00015559792554821472,
      "loss": 0.494,
      "step": 10550
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.5816935896873474,
      "learning_rate": 0.00015551291880641372,
      "loss": 0.6913,
      "step": 10560
    },
    {
      "epoch": 0.6672979797979798,
      "grad_norm": 0.6800694465637207,
      "learning_rate": 0.00015542785404204883,
      "loss": 0.6264,
      "step": 10570
    },
    {
      "epoch": 0.6679292929292929,
      "grad_norm": 0.6122955083847046,
      "learning_rate": 0.00015534273134403047,
      "loss": 0.5368,
      "step": 10580
    },
    {
      "epoch": 0.6685606060606061,
      "grad_norm": 0.7313991785049438,
      "learning_rate": 0.00015525755080132955,
      "loss": 0.4949,
      "step": 10590
    },
    {
      "epoch": 0.6691919191919192,
      "grad_norm": 1.056698203086853,
      "learning_rate": 0.0001551723125029775,
      "loss": 0.5143,
      "step": 10600
    },
    {
      "epoch": 0.6698232323232324,
      "grad_norm": 0.6068392992019653,
      "learning_rate": 0.00015508701653806615,
      "loss": 0.6976,
      "step": 10610
    },
    {
      "epoch": 0.6704545454545454,
      "grad_norm": 0.6738645434379578,
      "learning_rate": 0.0001550016629957475,
      "loss": 0.6286,
      "step": 10620
    },
    {
      "epoch": 0.6710858585858586,
      "grad_norm": 0.6442766785621643,
      "learning_rate": 0.0001549162519652338,
      "loss": 0.5254,
      "step": 10630
    },
    {
      "epoch": 0.6717171717171717,
      "grad_norm": 0.6953415870666504,
      "learning_rate": 0.00015483078353579734,
      "loss": 0.5102,
      "step": 10640
    },
    {
      "epoch": 0.6723484848484849,
      "grad_norm": 1.2428793907165527,
      "learning_rate": 0.00015474525779677047,
      "loss": 0.5192,
      "step": 10650
    },
    {
      "epoch": 0.672979797979798,
      "grad_norm": 0.5626111030578613,
      "learning_rate": 0.00015465967483754538,
      "loss": 0.7237,
      "step": 10660
    },
    {
      "epoch": 0.6736111111111112,
      "grad_norm": 0.6394316554069519,
      "learning_rate": 0.00015457403474757405,
      "loss": 0.5996,
      "step": 10670
    },
    {
      "epoch": 0.6742424242424242,
      "grad_norm": 0.6563475131988525,
      "learning_rate": 0.0001544883376163683,
      "loss": 0.533,
      "step": 10680
    },
    {
      "epoch": 0.6748737373737373,
      "grad_norm": 0.6722785830497742,
      "learning_rate": 0.00015440258353349945,
      "loss": 0.4636,
      "step": 10690
    },
    {
      "epoch": 0.6755050505050505,
      "grad_norm": 1.0582404136657715,
      "learning_rate": 0.00015431677258859837,
      "loss": 0.5013,
      "step": 10700
    },
    {
      "epoch": 0.6761363636363636,
      "grad_norm": 0.5443518161773682,
      "learning_rate": 0.00015423090487135535,
      "loss": 0.6876,
      "step": 10710
    },
    {
      "epoch": 0.6767676767676768,
      "grad_norm": 0.6639506816864014,
      "learning_rate": 0.00015414498047152007,
      "loss": 0.5983,
      "step": 10720
    },
    {
      "epoch": 0.67739898989899,
      "grad_norm": 0.6784919500350952,
      "learning_rate": 0.00015405899947890146,
      "loss": 0.5219,
      "step": 10730
    },
    {
      "epoch": 0.678030303030303,
      "grad_norm": 0.7205167412757874,
      "learning_rate": 0.0001539729619833675,
      "loss": 0.4646,
      "step": 10740
    },
    {
      "epoch": 0.6786616161616161,
      "grad_norm": 0.9414646029472351,
      "learning_rate": 0.00015388686807484532,
      "loss": 0.5044,
      "step": 10750
    },
    {
      "epoch": 0.6792929292929293,
      "grad_norm": 0.6037731766700745,
      "learning_rate": 0.000153800717843321,
      "loss": 0.7222,
      "step": 10760
    },
    {
      "epoch": 0.6799242424242424,
      "grad_norm": 0.6716630458831787,
      "learning_rate": 0.00015371451137883945,
      "loss": 0.5928,
      "step": 10770
    },
    {
      "epoch": 0.6805555555555556,
      "grad_norm": 0.7048764824867249,
      "learning_rate": 0.00015362824877150443,
      "loss": 0.5107,
      "step": 10780
    },
    {
      "epoch": 0.6811868686868687,
      "grad_norm": 0.7398087978363037,
      "learning_rate": 0.0001535419301114783,
      "loss": 0.4836,
      "step": 10790
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 1.035003900527954,
      "learning_rate": 0.00015345555548898206,
      "loss": 0.4798,
      "step": 10800
    },
    {
      "epoch": 0.6824494949494949,
      "grad_norm": 0.5533728003501892,
      "learning_rate": 0.00015336912499429515,
      "loss": 0.6955,
      "step": 10810
    },
    {
      "epoch": 0.6830808080808081,
      "grad_norm": 0.6299276947975159,
      "learning_rate": 0.00015328263871775544,
      "loss": 0.5719,
      "step": 10820
    },
    {
      "epoch": 0.6837121212121212,
      "grad_norm": 0.7040059566497803,
      "learning_rate": 0.0001531960967497592,
      "loss": 0.541,
      "step": 10830
    },
    {
      "epoch": 0.6843434343434344,
      "grad_norm": 0.6807965040206909,
      "learning_rate": 0.00015310949918076068,
      "loss": 0.4852,
      "step": 10840
    },
    {
      "epoch": 0.6849747474747475,
      "grad_norm": 1.003161072731018,
      "learning_rate": 0.00015302284610127246,
      "loss": 0.505,
      "step": 10850
    },
    {
      "epoch": 0.6856060606060606,
      "grad_norm": 0.6331389546394348,
      "learning_rate": 0.00015293613760186502,
      "loss": 0.6787,
      "step": 10860
    },
    {
      "epoch": 0.6862373737373737,
      "grad_norm": 0.5756049752235413,
      "learning_rate": 0.00015284937377316684,
      "loss": 0.5905,
      "step": 10870
    },
    {
      "epoch": 0.6868686868686869,
      "grad_norm": 0.6996668577194214,
      "learning_rate": 0.00015276255470586417,
      "loss": 0.523,
      "step": 10880
    },
    {
      "epoch": 0.6875,
      "grad_norm": 0.6445791721343994,
      "learning_rate": 0.00015267568049070102,
      "loss": 0.4926,
      "step": 10890
    },
    {
      "epoch": 0.6881313131313131,
      "grad_norm": 0.9679751396179199,
      "learning_rate": 0.00015258875121847902,
      "loss": 0.5181,
      "step": 10900
    },
    {
      "epoch": 0.6887626262626263,
      "grad_norm": 0.5702676177024841,
      "learning_rate": 0.00015250176698005744,
      "loss": 0.7372,
      "step": 10910
    },
    {
      "epoch": 0.6893939393939394,
      "grad_norm": 0.5687804222106934,
      "learning_rate": 0.00015241472786635288,
      "loss": 0.5875,
      "step": 10920
    },
    {
      "epoch": 0.6900252525252525,
      "grad_norm": 0.6807616353034973,
      "learning_rate": 0.0001523276339683393,
      "loss": 0.5296,
      "step": 10930
    },
    {
      "epoch": 0.6906565656565656,
      "grad_norm": 0.761805534362793,
      "learning_rate": 0.0001522404853770481,
      "loss": 0.4721,
      "step": 10940
    },
    {
      "epoch": 0.6912878787878788,
      "grad_norm": 0.8681610822677612,
      "learning_rate": 0.00015215328218356756,
      "loss": 0.5146,
      "step": 10950
    },
    {
      "epoch": 0.6919191919191919,
      "grad_norm": 0.534610390663147,
      "learning_rate": 0.00015206602447904327,
      "loss": 0.7234,
      "step": 10960
    },
    {
      "epoch": 0.6925505050505051,
      "grad_norm": 0.7093101739883423,
      "learning_rate": 0.00015197871235467765,
      "loss": 0.5945,
      "step": 10970
    },
    {
      "epoch": 0.6931818181818182,
      "grad_norm": 0.5722554922103882,
      "learning_rate": 0.00015189134590173014,
      "loss": 0.503,
      "step": 10980
    },
    {
      "epoch": 0.6938131313131313,
      "grad_norm": 0.626319944858551,
      "learning_rate": 0.00015180392521151677,
      "loss": 0.4936,
      "step": 10990
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 0.9075873494148254,
      "learning_rate": 0.0001517164503754105,
      "loss": 0.5148,
      "step": 11000
    },
    {
      "epoch": 0.6944444444444444,
      "eval_loss": 0.5536175966262817,
      "eval_runtime": 27.2874,
      "eval_samples_per_second": 93.816,
      "eval_steps_per_second": 11.727,
      "step": 11000
    },
    {
      "epoch": 0.6950757575757576,
      "grad_norm": 0.5669497847557068,
      "learning_rate": 0.00015162892148484067,
      "loss": 0.7557,
      "step": 11010
    },
    {
      "epoch": 0.6957070707070707,
      "grad_norm": 0.6150569915771484,
      "learning_rate": 0.00015154133863129324,
      "loss": 0.5929,
      "step": 11020
    },
    {
      "epoch": 0.6963383838383839,
      "grad_norm": 0.6415058970451355,
      "learning_rate": 0.0001514537019063105,
      "loss": 0.5317,
      "step": 11030
    },
    {
      "epoch": 0.696969696969697,
      "grad_norm": 0.683363676071167,
      "learning_rate": 0.0001513660114014911,
      "loss": 0.4707,
      "step": 11040
    },
    {
      "epoch": 0.69760101010101,
      "grad_norm": 0.8701376914978027,
      "learning_rate": 0.00015127826720848995,
      "loss": 0.5041,
      "step": 11050
    },
    {
      "epoch": 0.6982323232323232,
      "grad_norm": 0.5593884587287903,
      "learning_rate": 0.00015119046941901787,
      "loss": 0.7082,
      "step": 11060
    },
    {
      "epoch": 0.6988636363636364,
      "grad_norm": 0.6436285376548767,
      "learning_rate": 0.00015110261812484197,
      "loss": 0.6188,
      "step": 11070
    },
    {
      "epoch": 0.6994949494949495,
      "grad_norm": 0.618478536605835,
      "learning_rate": 0.00015101471341778507,
      "loss": 0.5296,
      "step": 11080
    },
    {
      "epoch": 0.7001262626262627,
      "grad_norm": 0.7757560610771179,
      "learning_rate": 0.0001509267553897259,
      "loss": 0.4889,
      "step": 11090
    },
    {
      "epoch": 0.7007575757575758,
      "grad_norm": 0.9628687500953674,
      "learning_rate": 0.000150838744132599,
      "loss": 0.5275,
      "step": 11100
    },
    {
      "epoch": 0.7013888888888888,
      "grad_norm": 0.5580108165740967,
      "learning_rate": 0.0001507506797383944,
      "loss": 0.7123,
      "step": 11110
    },
    {
      "epoch": 0.702020202020202,
      "grad_norm": 0.6466276049613953,
      "learning_rate": 0.00015066256229915776,
      "loss": 0.6289,
      "step": 11120
    },
    {
      "epoch": 0.7026515151515151,
      "grad_norm": 0.6653864979743958,
      "learning_rate": 0.00015057439190699018,
      "loss": 0.5231,
      "step": 11130
    },
    {
      "epoch": 0.7032828282828283,
      "grad_norm": 0.5739415287971497,
      "learning_rate": 0.000150486168654048,
      "loss": 0.4803,
      "step": 11140
    },
    {
      "epoch": 0.7039141414141414,
      "grad_norm": 0.8773326873779297,
      "learning_rate": 0.00015039789263254303,
      "loss": 0.4854,
      "step": 11150
    },
    {
      "epoch": 0.7045454545454546,
      "grad_norm": 0.5206267237663269,
      "learning_rate": 0.00015030956393474198,
      "loss": 0.6819,
      "step": 11160
    },
    {
      "epoch": 0.7051767676767676,
      "grad_norm": 0.6649022698402405,
      "learning_rate": 0.00015022118265296678,
      "loss": 0.5799,
      "step": 11170
    },
    {
      "epoch": 0.7058080808080808,
      "grad_norm": 0.6916127800941467,
      "learning_rate": 0.00015013274887959433,
      "loss": 0.5404,
      "step": 11180
    },
    {
      "epoch": 0.7064393939393939,
      "grad_norm": 0.7013433575630188,
      "learning_rate": 0.00015004426270705625,
      "loss": 0.468,
      "step": 11190
    },
    {
      "epoch": 0.7070707070707071,
      "grad_norm": 0.8981716632843018,
      "learning_rate": 0.00014995572422783911,
      "loss": 0.5149,
      "step": 11200
    },
    {
      "epoch": 0.7077020202020202,
      "grad_norm": 0.5433361530303955,
      "learning_rate": 0.000149867133534484,
      "loss": 0.7165,
      "step": 11210
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.6685206294059753,
      "learning_rate": 0.0001497784907195867,
      "loss": 0.5895,
      "step": 11220
    },
    {
      "epoch": 0.7089646464646465,
      "grad_norm": 0.6134523749351501,
      "learning_rate": 0.00014968979587579736,
      "loss": 0.5353,
      "step": 11230
    },
    {
      "epoch": 0.7095959595959596,
      "grad_norm": 0.7647712230682373,
      "learning_rate": 0.0001496010490958206,
      "loss": 0.4698,
      "step": 11240
    },
    {
      "epoch": 0.7102272727272727,
      "grad_norm": 0.798601508140564,
      "learning_rate": 0.00014951225047241535,
      "loss": 0.5211,
      "step": 11250
    },
    {
      "epoch": 0.7108585858585859,
      "grad_norm": 0.4866088032722473,
      "learning_rate": 0.00014942340009839458,
      "loss": 0.7436,
      "step": 11260
    },
    {
      "epoch": 0.711489898989899,
      "grad_norm": 0.626154363155365,
      "learning_rate": 0.00014933449806662545,
      "loss": 0.6272,
      "step": 11270
    },
    {
      "epoch": 0.7121212121212122,
      "grad_norm": 0.6184539198875427,
      "learning_rate": 0.00014924554447002915,
      "loss": 0.5137,
      "step": 11280
    },
    {
      "epoch": 0.7127525252525253,
      "grad_norm": 0.6623725891113281,
      "learning_rate": 0.00014915653940158067,
      "loss": 0.4836,
      "step": 11290
    },
    {
      "epoch": 0.7133838383838383,
      "grad_norm": 0.8842839002609253,
      "learning_rate": 0.00014906748295430888,
      "loss": 0.4814,
      "step": 11300
    },
    {
      "epoch": 0.7140151515151515,
      "grad_norm": 0.5735744833946228,
      "learning_rate": 0.0001489783752212963,
      "loss": 0.727,
      "step": 11310
    },
    {
      "epoch": 0.7146464646464646,
      "grad_norm": 0.6141955256462097,
      "learning_rate": 0.00014888921629567907,
      "loss": 0.5834,
      "step": 11320
    },
    {
      "epoch": 0.7152777777777778,
      "grad_norm": 0.6844936609268188,
      "learning_rate": 0.00014880000627064686,
      "loss": 0.5246,
      "step": 11330
    },
    {
      "epoch": 0.7159090909090909,
      "grad_norm": 0.733518123626709,
      "learning_rate": 0.00014871074523944267,
      "loss": 0.4672,
      "step": 11340
    },
    {
      "epoch": 0.7165404040404041,
      "grad_norm": 1.1112340688705444,
      "learning_rate": 0.000148621433295363,
      "loss": 0.5202,
      "step": 11350
    },
    {
      "epoch": 0.7171717171717171,
      "grad_norm": 0.5119219422340393,
      "learning_rate": 0.0001485320705317573,
      "loss": 0.7271,
      "step": 11360
    },
    {
      "epoch": 0.7178030303030303,
      "grad_norm": 0.5304273962974548,
      "learning_rate": 0.00014844265704202836,
      "loss": 0.6109,
      "step": 11370
    },
    {
      "epoch": 0.7184343434343434,
      "grad_norm": 0.6327852010726929,
      "learning_rate": 0.00014835319291963184,
      "loss": 0.5172,
      "step": 11380
    },
    {
      "epoch": 0.7190656565656566,
      "grad_norm": 0.6390203833580017,
      "learning_rate": 0.0001482636782580764,
      "loss": 0.4875,
      "step": 11390
    },
    {
      "epoch": 0.7196969696969697,
      "grad_norm": 0.8727097511291504,
      "learning_rate": 0.00014817411315092358,
      "loss": 0.4922,
      "step": 11400
    },
    {
      "epoch": 0.7203282828282829,
      "grad_norm": 0.6278547644615173,
      "learning_rate": 0.00014808449769178745,
      "loss": 0.7282,
      "step": 11410
    },
    {
      "epoch": 0.7209595959595959,
      "grad_norm": 0.5897693634033203,
      "learning_rate": 0.00014799483197433494,
      "loss": 0.6083,
      "step": 11420
    },
    {
      "epoch": 0.7215909090909091,
      "grad_norm": 0.6279316544532776,
      "learning_rate": 0.00014790511609228535,
      "loss": 0.5558,
      "step": 11430
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.6644487977027893,
      "learning_rate": 0.00014781535013941047,
      "loss": 0.4749,
      "step": 11440
    },
    {
      "epoch": 0.7228535353535354,
      "grad_norm": 0.9821993112564087,
      "learning_rate": 0.0001477255342095344,
      "loss": 0.5105,
      "step": 11450
    },
    {
      "epoch": 0.7234848484848485,
      "grad_norm": 0.5877029299736023,
      "learning_rate": 0.0001476356683965336,
      "loss": 0.6872,
      "step": 11460
    },
    {
      "epoch": 0.7241161616161617,
      "grad_norm": 0.5246282815933228,
      "learning_rate": 0.0001475457527943364,
      "loss": 0.5902,
      "step": 11470
    },
    {
      "epoch": 0.7247474747474747,
      "grad_norm": 0.6859436631202698,
      "learning_rate": 0.00014745578749692347,
      "loss": 0.5319,
      "step": 11480
    },
    {
      "epoch": 0.7253787878787878,
      "grad_norm": 0.8613813519477844,
      "learning_rate": 0.0001473657725983272,
      "loss": 0.4815,
      "step": 11490
    },
    {
      "epoch": 0.726010101010101,
      "grad_norm": 0.9561315178871155,
      "learning_rate": 0.00014727570819263197,
      "loss": 0.4961,
      "step": 11500
    },
    {
      "epoch": 0.7266414141414141,
      "grad_norm": 0.5420469641685486,
      "learning_rate": 0.00014718559437397382,
      "loss": 0.7254,
      "step": 11510
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.6113565564155579,
      "learning_rate": 0.00014709543123654043,
      "loss": 0.5903,
      "step": 11520
    },
    {
      "epoch": 0.7279040404040404,
      "grad_norm": 0.6398712396621704,
      "learning_rate": 0.00014700521887457116,
      "loss": 0.5249,
      "step": 11530
    },
    {
      "epoch": 0.7285353535353535,
      "grad_norm": 0.6719386577606201,
      "learning_rate": 0.00014691495738235658,
      "loss": 0.4956,
      "step": 11540
    },
    {
      "epoch": 0.7291666666666666,
      "grad_norm": 0.997478723526001,
      "learning_rate": 0.00014682464685423886,
      "loss": 0.4873,
      "step": 11550
    },
    {
      "epoch": 0.7297979797979798,
      "grad_norm": 0.5434187054634094,
      "learning_rate": 0.00014673428738461126,
      "loss": 0.7253,
      "step": 11560
    },
    {
      "epoch": 0.7304292929292929,
      "grad_norm": 0.5597427487373352,
      "learning_rate": 0.00014664387906791827,
      "loss": 0.6037,
      "step": 11570
    },
    {
      "epoch": 0.7310606060606061,
      "grad_norm": 0.6033618450164795,
      "learning_rate": 0.00014655342199865543,
      "loss": 0.539,
      "step": 11580
    },
    {
      "epoch": 0.7316919191919192,
      "grad_norm": 0.7257830500602722,
      "learning_rate": 0.0001464629162713692,
      "loss": 0.4878,
      "step": 11590
    },
    {
      "epoch": 0.7323232323232324,
      "grad_norm": 0.9129477739334106,
      "learning_rate": 0.0001463723619806569,
      "loss": 0.5223,
      "step": 11600
    },
    {
      "epoch": 0.7329545454545454,
      "grad_norm": 0.5395910739898682,
      "learning_rate": 0.00014628175922116665,
      "loss": 0.7283,
      "step": 11610
    },
    {
      "epoch": 0.7335858585858586,
      "grad_norm": 0.5802341103553772,
      "learning_rate": 0.0001461911080875972,
      "loss": 0.6097,
      "step": 11620
    },
    {
      "epoch": 0.7342171717171717,
      "grad_norm": 0.6748054623603821,
      "learning_rate": 0.00014610040867469787,
      "loss": 0.5147,
      "step": 11630
    },
    {
      "epoch": 0.7348484848484849,
      "grad_norm": 0.7468414306640625,
      "learning_rate": 0.00014600966107726845,
      "loss": 0.4543,
      "step": 11640
    },
    {
      "epoch": 0.735479797979798,
      "grad_norm": 0.8454912900924683,
      "learning_rate": 0.00014591886539015906,
      "loss": 0.4943,
      "step": 11650
    },
    {
      "epoch": 0.7361111111111112,
      "grad_norm": 0.5255289673805237,
      "learning_rate": 0.00014582802170827014,
      "loss": 0.7149,
      "step": 11660
    },
    {
      "epoch": 0.7367424242424242,
      "grad_norm": 0.6096526980400085,
      "learning_rate": 0.00014573713012655222,
      "loss": 0.5878,
      "step": 11670
    },
    {
      "epoch": 0.7373737373737373,
      "grad_norm": 0.6521674394607544,
      "learning_rate": 0.00014564619074000596,
      "loss": 0.5429,
      "step": 11680
    },
    {
      "epoch": 0.7380050505050505,
      "grad_norm": 0.6331238150596619,
      "learning_rate": 0.00014555520364368195,
      "loss": 0.4553,
      "step": 11690
    },
    {
      "epoch": 0.7386363636363636,
      "grad_norm": 0.8516479134559631,
      "learning_rate": 0.00014546416893268069,
      "loss": 0.4902,
      "step": 11700
    },
    {
      "epoch": 0.7392676767676768,
      "grad_norm": 0.5228622555732727,
      "learning_rate": 0.00014537308670215237,
      "loss": 0.7259,
      "step": 11710
    },
    {
      "epoch": 0.73989898989899,
      "grad_norm": 0.5902215838432312,
      "learning_rate": 0.00014528195704729692,
      "loss": 0.584,
      "step": 11720
    },
    {
      "epoch": 0.740530303030303,
      "grad_norm": 0.5938458442687988,
      "learning_rate": 0.00014519078006336382,
      "loss": 0.5415,
      "step": 11730
    },
    {
      "epoch": 0.7411616161616161,
      "grad_norm": 0.6663249135017395,
      "learning_rate": 0.000145099555845652,
      "loss": 0.4952,
      "step": 11740
    },
    {
      "epoch": 0.7417929292929293,
      "grad_norm": 0.79927659034729,
      "learning_rate": 0.0001450082844895098,
      "loss": 0.4779,
      "step": 11750
    },
    {
      "epoch": 0.7424242424242424,
      "grad_norm": 0.5725183486938477,
      "learning_rate": 0.0001449169660903347,
      "loss": 0.7106,
      "step": 11760
    },
    {
      "epoch": 0.7430555555555556,
      "grad_norm": 0.5095925331115723,
      "learning_rate": 0.00014482560074357358,
      "loss": 0.5896,
      "step": 11770
    },
    {
      "epoch": 0.7436868686868687,
      "grad_norm": 0.6830637454986572,
      "learning_rate": 0.00014473418854472217,
      "loss": 0.5074,
      "step": 11780
    },
    {
      "epoch": 0.7443181818181818,
      "grad_norm": 0.6794175505638123,
      "learning_rate": 0.0001446427295893253,
      "loss": 0.487,
      "step": 11790
    },
    {
      "epoch": 0.7449494949494949,
      "grad_norm": 0.7801890969276428,
      "learning_rate": 0.0001445512239729766,
      "loss": 0.5374,
      "step": 11800
    },
    {
      "epoch": 0.7455808080808081,
      "grad_norm": 0.6427426338195801,
      "learning_rate": 0.00014445967179131853,
      "loss": 0.7148,
      "step": 11810
    },
    {
      "epoch": 0.7462121212121212,
      "grad_norm": 0.5752976536750793,
      "learning_rate": 0.00014436807314004217,
      "loss": 0.591,
      "step": 11820
    },
    {
      "epoch": 0.7468434343434344,
      "grad_norm": 0.6040766835212708,
      "learning_rate": 0.00014427642811488722,
      "loss": 0.5396,
      "step": 11830
    },
    {
      "epoch": 0.7474747474747475,
      "grad_norm": 0.6243247389793396,
      "learning_rate": 0.00014418473681164178,
      "loss": 0.482,
      "step": 11840
    },
    {
      "epoch": 0.7481060606060606,
      "grad_norm": 1.0154458284378052,
      "learning_rate": 0.0001440929993261424,
      "loss": 0.5051,
      "step": 11850
    },
    {
      "epoch": 0.7487373737373737,
      "grad_norm": 0.5102674961090088,
      "learning_rate": 0.00014400121575427384,
      "loss": 0.72,
      "step": 11860
    },
    {
      "epoch": 0.7493686868686869,
      "grad_norm": 0.5849292278289795,
      "learning_rate": 0.00014390938619196906,
      "loss": 0.6115,
      "step": 11870
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.6396389603614807,
      "learning_rate": 0.0001438175107352091,
      "loss": 0.5408,
      "step": 11880
    },
    {
      "epoch": 0.7506313131313131,
      "grad_norm": 0.7971886396408081,
      "learning_rate": 0.0001437255894800229,
      "loss": 0.4784,
      "step": 11890
    },
    {
      "epoch": 0.7512626262626263,
      "grad_norm": 0.9011407494544983,
      "learning_rate": 0.0001436336225224874,
      "loss": 0.5111,
      "step": 11900
    },
    {
      "epoch": 0.7518939393939394,
      "grad_norm": 0.5310609936714172,
      "learning_rate": 0.0001435416099587271,
      "loss": 0.6994,
      "step": 11910
    },
    {
      "epoch": 0.7525252525252525,
      "grad_norm": 0.5607309937477112,
      "learning_rate": 0.00014344955188491442,
      "loss": 0.5688,
      "step": 11920
    },
    {
      "epoch": 0.7531565656565656,
      "grad_norm": 0.5848428010940552,
      "learning_rate": 0.00014335744839726917,
      "loss": 0.5006,
      "step": 11930
    },
    {
      "epoch": 0.7537878787878788,
      "grad_norm": 0.592444896697998,
      "learning_rate": 0.0001432652995920587,
      "loss": 0.4825,
      "step": 11940
    },
    {
      "epoch": 0.7544191919191919,
      "grad_norm": 0.866573691368103,
      "learning_rate": 0.00014317310556559765,
      "loss": 0.5108,
      "step": 11950
    },
    {
      "epoch": 0.7550505050505051,
      "grad_norm": 0.5196856260299683,
      "learning_rate": 0.0001430808664142481,
      "loss": 0.6852,
      "step": 11960
    },
    {
      "epoch": 0.7556818181818182,
      "grad_norm": 0.6228311657905579,
      "learning_rate": 0.00014298858223441902,
      "loss": 0.5908,
      "step": 11970
    },
    {
      "epoch": 0.7563131313131313,
      "grad_norm": 0.6251500248908997,
      "learning_rate": 0.00014289625312256674,
      "loss": 0.549,
      "step": 11980
    },
    {
      "epoch": 0.7569444444444444,
      "grad_norm": 0.7026849389076233,
      "learning_rate": 0.00014280387917519436,
      "loss": 0.4672,
      "step": 11990
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 1.2605352401733398,
      "learning_rate": 0.00014271146048885187,
      "loss": 0.5001,
      "step": 12000
    },
    {
      "epoch": 0.7575757575757576,
      "eval_loss": 0.5488002896308899,
      "eval_runtime": 27.4117,
      "eval_samples_per_second": 93.391,
      "eval_steps_per_second": 11.674,
      "step": 12000
    },
    {
      "epoch": 0.7582070707070707,
      "grad_norm": 0.5645745396614075,
      "learning_rate": 0.0001426189971601361,
      "loss": 0.7286,
      "step": 12010
    },
    {
      "epoch": 0.7588383838383839,
      "grad_norm": 0.5522037744522095,
      "learning_rate": 0.00014252648928569044,
      "loss": 0.5943,
      "step": 12020
    },
    {
      "epoch": 0.759469696969697,
      "grad_norm": 0.6184792518615723,
      "learning_rate": 0.00014243393696220492,
      "loss": 0.5137,
      "step": 12030
    },
    {
      "epoch": 0.76010101010101,
      "grad_norm": 0.681661069393158,
      "learning_rate": 0.000142341340286416,
      "loss": 0.4683,
      "step": 12040
    },
    {
      "epoch": 0.7607323232323232,
      "grad_norm": 0.986609697341919,
      "learning_rate": 0.00014224869935510647,
      "loss": 0.5219,
      "step": 12050
    },
    {
      "epoch": 0.7613636363636364,
      "grad_norm": 0.6418775320053101,
      "learning_rate": 0.00014215601426510543,
      "loss": 0.6941,
      "step": 12060
    },
    {
      "epoch": 0.7619949494949495,
      "grad_norm": 0.6295611262321472,
      "learning_rate": 0.0001420632851132881,
      "loss": 0.6144,
      "step": 12070
    },
    {
      "epoch": 0.7626262626262627,
      "grad_norm": 0.6387905478477478,
      "learning_rate": 0.00014197051199657575,
      "loss": 0.5285,
      "step": 12080
    },
    {
      "epoch": 0.7632575757575758,
      "grad_norm": 0.728402853012085,
      "learning_rate": 0.0001418776950119356,
      "loss": 0.4642,
      "step": 12090
    },
    {
      "epoch": 0.7638888888888888,
      "grad_norm": 0.8517777919769287,
      "learning_rate": 0.00014178483425638077,
      "loss": 0.4885,
      "step": 12100
    },
    {
      "epoch": 0.764520202020202,
      "grad_norm": 0.5316308736801147,
      "learning_rate": 0.00014169192982697003,
      "loss": 0.681,
      "step": 12110
    },
    {
      "epoch": 0.7651515151515151,
      "grad_norm": 0.5937336087226868,
      "learning_rate": 0.00014159898182080792,
      "loss": 0.6051,
      "step": 12120
    },
    {
      "epoch": 0.7657828282828283,
      "grad_norm": 0.6459532976150513,
      "learning_rate": 0.00014150599033504446,
      "loss": 0.5254,
      "step": 12130
    },
    {
      "epoch": 0.7664141414141414,
      "grad_norm": 0.5941194295883179,
      "learning_rate": 0.00014141295546687513,
      "loss": 0.5091,
      "step": 12140
    },
    {
      "epoch": 0.7670454545454546,
      "grad_norm": 0.8536360859870911,
      "learning_rate": 0.00014131987731354068,
      "loss": 0.5124,
      "step": 12150
    },
    {
      "epoch": 0.7676767676767676,
      "grad_norm": 0.5732446908950806,
      "learning_rate": 0.00014122675597232726,
      "loss": 0.6856,
      "step": 12160
    },
    {
      "epoch": 0.7683080808080808,
      "grad_norm": 0.5303078889846802,
      "learning_rate": 0.00014113359154056603,
      "loss": 0.5942,
      "step": 12170
    },
    {
      "epoch": 0.7689393939393939,
      "grad_norm": 0.7021543979644775,
      "learning_rate": 0.00014104038411563323,
      "loss": 0.5466,
      "step": 12180
    },
    {
      "epoch": 0.7695707070707071,
      "grad_norm": 0.6265504956245422,
      "learning_rate": 0.00014094713379495,
      "loss": 0.4745,
      "step": 12190
    },
    {
      "epoch": 0.7702020202020202,
      "grad_norm": 0.9447016716003418,
      "learning_rate": 0.00014085384067598242,
      "loss": 0.4748,
      "step": 12200
    },
    {
      "epoch": 0.7708333333333334,
      "grad_norm": 0.5965749025344849,
      "learning_rate": 0.00014076050485624118,
      "loss": 0.658,
      "step": 12210
    },
    {
      "epoch": 0.7714646464646465,
      "grad_norm": 0.5718294382095337,
      "learning_rate": 0.00014066712643328165,
      "loss": 0.5762,
      "step": 12220
    },
    {
      "epoch": 0.7720959595959596,
      "grad_norm": 0.6793681383132935,
      "learning_rate": 0.0001405737055047038,
      "loss": 0.506,
      "step": 12230
    },
    {
      "epoch": 0.7727272727272727,
      "grad_norm": 0.6456235647201538,
      "learning_rate": 0.00014048024216815186,
      "loss": 0.4678,
      "step": 12240
    },
    {
      "epoch": 0.7733585858585859,
      "grad_norm": 0.8622037172317505,
      "learning_rate": 0.00014038673652131455,
      "loss": 0.4841,
      "step": 12250
    },
    {
      "epoch": 0.773989898989899,
      "grad_norm": 0.5506232380867004,
      "learning_rate": 0.00014029318866192475,
      "loss": 0.6949,
      "step": 12260
    },
    {
      "epoch": 0.7746212121212122,
      "grad_norm": 0.6098310947418213,
      "learning_rate": 0.00014019959868775947,
      "loss": 0.5957,
      "step": 12270
    },
    {
      "epoch": 0.7752525252525253,
      "grad_norm": 0.6406723856925964,
      "learning_rate": 0.00014010596669663966,
      "loss": 0.5323,
      "step": 12280
    },
    {
      "epoch": 0.7758838383838383,
      "grad_norm": 0.6908765435218811,
      "learning_rate": 0.00014001229278643034,
      "loss": 0.4707,
      "step": 12290
    },
    {
      "epoch": 0.7765151515151515,
      "grad_norm": 1.0876790285110474,
      "learning_rate": 0.0001399185770550402,
      "loss": 0.4851,
      "step": 12300
    },
    {
      "epoch": 0.7771464646464646,
      "grad_norm": 0.5588963627815247,
      "learning_rate": 0.00013982481960042172,
      "loss": 0.7212,
      "step": 12310
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.6591330766677856,
      "learning_rate": 0.00013973102052057095,
      "loss": 0.5803,
      "step": 12320
    },
    {
      "epoch": 0.7784090909090909,
      "grad_norm": 0.6867309808731079,
      "learning_rate": 0.0001396371799135275,
      "loss": 0.5323,
      "step": 12330
    },
    {
      "epoch": 0.7790404040404041,
      "grad_norm": 0.6864200234413147,
      "learning_rate": 0.00013954329787737438,
      "loss": 0.4558,
      "step": 12340
    },
    {
      "epoch": 0.7796717171717171,
      "grad_norm": 0.9081438183784485,
      "learning_rate": 0.0001394493745102378,
      "loss": 0.5044,
      "step": 12350
    },
    {
      "epoch": 0.7803030303030303,
      "grad_norm": 0.5118538737297058,
      "learning_rate": 0.0001393554099102873,
      "loss": 0.689,
      "step": 12360
    },
    {
      "epoch": 0.7809343434343434,
      "grad_norm": 0.5430775284767151,
      "learning_rate": 0.00013926140417573542,
      "loss": 0.584,
      "step": 12370
    },
    {
      "epoch": 0.7815656565656566,
      "grad_norm": 0.6723099946975708,
      "learning_rate": 0.00013916735740483776,
      "loss": 0.5386,
      "step": 12380
    },
    {
      "epoch": 0.7821969696969697,
      "grad_norm": 0.5744951367378235,
      "learning_rate": 0.00013907326969589276,
      "loss": 0.4758,
      "step": 12390
    },
    {
      "epoch": 0.7828282828282829,
      "grad_norm": 0.9172720313072205,
      "learning_rate": 0.00013897914114724172,
      "loss": 0.471,
      "step": 12400
    },
    {
      "epoch": 0.7834595959595959,
      "grad_norm": 0.5147091746330261,
      "learning_rate": 0.00013888497185726855,
      "loss": 0.7219,
      "step": 12410
    },
    {
      "epoch": 0.7840909090909091,
      "grad_norm": 0.5618786811828613,
      "learning_rate": 0.00013879076192439976,
      "loss": 0.548,
      "step": 12420
    },
    {
      "epoch": 0.7847222222222222,
      "grad_norm": 0.7563232183456421,
      "learning_rate": 0.0001386965114471044,
      "loss": 0.5082,
      "step": 12430
    },
    {
      "epoch": 0.7853535353535354,
      "grad_norm": 0.6989296674728394,
      "learning_rate": 0.0001386022205238938,
      "loss": 0.4616,
      "step": 12440
    },
    {
      "epoch": 0.7859848484848485,
      "grad_norm": 0.9477042555809021,
      "learning_rate": 0.00013850788925332167,
      "loss": 0.5033,
      "step": 12450
    },
    {
      "epoch": 0.7866161616161617,
      "grad_norm": 0.5188520550727844,
      "learning_rate": 0.0001384135177339838,
      "loss": 0.7367,
      "step": 12460
    },
    {
      "epoch": 0.7872474747474747,
      "grad_norm": 0.6310461163520813,
      "learning_rate": 0.0001383191060645181,
      "loss": 0.5923,
      "step": 12470
    },
    {
      "epoch": 0.7878787878787878,
      "grad_norm": 0.6548859477043152,
      "learning_rate": 0.00013822465434360442,
      "loss": 0.5358,
      "step": 12480
    },
    {
      "epoch": 0.788510101010101,
      "grad_norm": 0.714531660079956,
      "learning_rate": 0.00013813016266996447,
      "loss": 0.5047,
      "step": 12490
    },
    {
      "epoch": 0.7891414141414141,
      "grad_norm": 1.0106661319732666,
      "learning_rate": 0.00013803563114236176,
      "loss": 0.5008,
      "step": 12500
    },
    {
      "epoch": 0.7897727272727273,
      "grad_norm": 0.5742725729942322,
      "learning_rate": 0.00013794105985960147,
      "loss": 0.7452,
      "step": 12510
    },
    {
      "epoch": 0.7904040404040404,
      "grad_norm": 0.560047447681427,
      "learning_rate": 0.00013784644892053018,
      "loss": 0.5736,
      "step": 12520
    },
    {
      "epoch": 0.7910353535353535,
      "grad_norm": 0.6286509037017822,
      "learning_rate": 0.00013775179842403612,
      "loss": 0.5113,
      "step": 12530
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.646783173084259,
      "learning_rate": 0.00013765710846904873,
      "loss": 0.4944,
      "step": 12540
    },
    {
      "epoch": 0.7922979797979798,
      "grad_norm": 0.9490503668785095,
      "learning_rate": 0.00013756237915453875,
      "loss": 0.5015,
      "step": 12550
    },
    {
      "epoch": 0.7929292929292929,
      "grad_norm": 0.5031042098999023,
      "learning_rate": 0.00013746761057951805,
      "loss": 0.7144,
      "step": 12560
    },
    {
      "epoch": 0.7935606060606061,
      "grad_norm": 0.6712832450866699,
      "learning_rate": 0.00013737280284303955,
      "loss": 0.5637,
      "step": 12570
    },
    {
      "epoch": 0.7941919191919192,
      "grad_norm": 0.6306891441345215,
      "learning_rate": 0.00013727795604419706,
      "loss": 0.5267,
      "step": 12580
    },
    {
      "epoch": 0.7948232323232324,
      "grad_norm": 0.6102079153060913,
      "learning_rate": 0.00013718307028212523,
      "loss": 0.4712,
      "step": 12590
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 0.7925893664360046,
      "learning_rate": 0.00013708814565599947,
      "loss": 0.489,
      "step": 12600
    },
    {
      "epoch": 0.7960858585858586,
      "grad_norm": 0.5026618242263794,
      "learning_rate": 0.0001369931822650358,
      "loss": 0.6712,
      "step": 12610
    },
    {
      "epoch": 0.7967171717171717,
      "grad_norm": 0.5962971448898315,
      "learning_rate": 0.0001368981802084907,
      "loss": 0.5865,
      "step": 12620
    },
    {
      "epoch": 0.7973484848484849,
      "grad_norm": 0.6364347338676453,
      "learning_rate": 0.00013680313958566113,
      "loss": 0.5288,
      "step": 12630
    },
    {
      "epoch": 0.797979797979798,
      "grad_norm": 0.7442088723182678,
      "learning_rate": 0.00013670806049588438,
      "loss": 0.4674,
      "step": 12640
    },
    {
      "epoch": 0.7986111111111112,
      "grad_norm": 1.0201624631881714,
      "learning_rate": 0.00013661294303853784,
      "loss": 0.5076,
      "step": 12650
    },
    {
      "epoch": 0.7992424242424242,
      "grad_norm": 0.5383306741714478,
      "learning_rate": 0.0001365177873130391,
      "loss": 0.6732,
      "step": 12660
    },
    {
      "epoch": 0.7998737373737373,
      "grad_norm": 0.5189357995986938,
      "learning_rate": 0.00013642259341884572,
      "loss": 0.5956,
      "step": 12670
    },
    {
      "epoch": 0.8005050505050505,
      "grad_norm": 0.6594438552856445,
      "learning_rate": 0.00013632736145545514,
      "loss": 0.5339,
      "step": 12680
    },
    {
      "epoch": 0.8011363636363636,
      "grad_norm": 0.7534450888633728,
      "learning_rate": 0.00013623209152240458,
      "loss": 0.476,
      "step": 12690
    },
    {
      "epoch": 0.8017676767676768,
      "grad_norm": 0.8171694874763489,
      "learning_rate": 0.00013613678371927098,
      "loss": 0.475,
      "step": 12700
    },
    {
      "epoch": 0.80239898989899,
      "grad_norm": 0.55400550365448,
      "learning_rate": 0.00013604143814567086,
      "loss": 0.6714,
      "step": 12710
    },
    {
      "epoch": 0.803030303030303,
      "grad_norm": 0.5969735980033875,
      "learning_rate": 0.0001359460549012602,
      "loss": 0.586,
      "step": 12720
    },
    {
      "epoch": 0.8036616161616161,
      "grad_norm": 0.722767174243927,
      "learning_rate": 0.00013585063408573435,
      "loss": 0.5353,
      "step": 12730
    },
    {
      "epoch": 0.8042929292929293,
      "grad_norm": 0.5883948802947998,
      "learning_rate": 0.00013575517579882793,
      "loss": 0.4684,
      "step": 12740
    },
    {
      "epoch": 0.8049242424242424,
      "grad_norm": 0.6717745065689087,
      "learning_rate": 0.00013565968014031478,
      "loss": 0.5007,
      "step": 12750
    },
    {
      "epoch": 0.8055555555555556,
      "grad_norm": 0.5491935610771179,
      "learning_rate": 0.00013556414721000768,
      "loss": 0.6848,
      "step": 12760
    },
    {
      "epoch": 0.8061868686868687,
      "grad_norm": 0.5545650720596313,
      "learning_rate": 0.00013546857710775852,
      "loss": 0.5758,
      "step": 12770
    },
    {
      "epoch": 0.8068181818181818,
      "grad_norm": 0.6105465888977051,
      "learning_rate": 0.00013537296993345794,
      "loss": 0.5103,
      "step": 12780
    },
    {
      "epoch": 0.8074494949494949,
      "grad_norm": 0.5794836282730103,
      "learning_rate": 0.00013527732578703534,
      "loss": 0.496,
      "step": 12790
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 0.7226268649101257,
      "learning_rate": 0.00013518164476845876,
      "loss": 0.5097,
      "step": 12800
    },
    {
      "epoch": 0.8087121212121212,
      "grad_norm": 0.5381549000740051,
      "learning_rate": 0.00013508592697773485,
      "loss": 0.6794,
      "step": 12810
    },
    {
      "epoch": 0.8093434343434344,
      "grad_norm": 0.5560536980628967,
      "learning_rate": 0.0001349901725149086,
      "loss": 0.5897,
      "step": 12820
    },
    {
      "epoch": 0.8099747474747475,
      "grad_norm": 0.6649383306503296,
      "learning_rate": 0.00013489438148006332,
      "loss": 0.5246,
      "step": 12830
    },
    {
      "epoch": 0.8106060606060606,
      "grad_norm": 0.6250474452972412,
      "learning_rate": 0.0001347985539733207,
      "loss": 0.4657,
      "step": 12840
    },
    {
      "epoch": 0.8112373737373737,
      "grad_norm": 0.8857137560844421,
      "learning_rate": 0.00013470269009484037,
      "loss": 0.4965,
      "step": 12850
    },
    {
      "epoch": 0.8118686868686869,
      "grad_norm": 0.5481236577033997,
      "learning_rate": 0.00013460678994482012,
      "loss": 0.6911,
      "step": 12860
    },
    {
      "epoch": 0.8125,
      "grad_norm": 0.6298750042915344,
      "learning_rate": 0.0001345108536234955,
      "loss": 0.592,
      "step": 12870
    },
    {
      "epoch": 0.8131313131313131,
      "grad_norm": 0.5944226384162903,
      "learning_rate": 0.00013441488123114003,
      "loss": 0.5121,
      "step": 12880
    },
    {
      "epoch": 0.8137626262626263,
      "grad_norm": 0.6628309488296509,
      "learning_rate": 0.0001343188728680648,
      "loss": 0.4686,
      "step": 12890
    },
    {
      "epoch": 0.8143939393939394,
      "grad_norm": 0.91483074426651,
      "learning_rate": 0.00013422282863461855,
      "loss": 0.4935,
      "step": 12900
    },
    {
      "epoch": 0.8150252525252525,
      "grad_norm": 0.5855256915092468,
      "learning_rate": 0.00013412674863118751,
      "loss": 0.6623,
      "step": 12910
    },
    {
      "epoch": 0.8156565656565656,
      "grad_norm": 0.56207674741745,
      "learning_rate": 0.00013403063295819537,
      "loss": 0.6096,
      "step": 12920
    },
    {
      "epoch": 0.8162878787878788,
      "grad_norm": 0.6711302399635315,
      "learning_rate": 0.00013393448171610296,
      "loss": 0.5158,
      "step": 12930
    },
    {
      "epoch": 0.8169191919191919,
      "grad_norm": 0.6422381401062012,
      "learning_rate": 0.00013383829500540831,
      "loss": 0.4757,
      "step": 12940
    },
    {
      "epoch": 0.8175505050505051,
      "grad_norm": 1.0287357568740845,
      "learning_rate": 0.0001337420729266467,
      "loss": 0.4787,
      "step": 12950
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.5496835708618164,
      "learning_rate": 0.00013364581558039011,
      "loss": 0.695,
      "step": 12960
    },
    {
      "epoch": 0.8188131313131313,
      "grad_norm": 0.5053057670593262,
      "learning_rate": 0.00013354952306724761,
      "loss": 0.6016,
      "step": 12970
    },
    {
      "epoch": 0.8194444444444444,
      "grad_norm": 0.6105124950408936,
      "learning_rate": 0.0001334531954878649,
      "loss": 0.502,
      "step": 12980
    },
    {
      "epoch": 0.8200757575757576,
      "grad_norm": 0.6431969404220581,
      "learning_rate": 0.0001333568329429244,
      "loss": 0.4541,
      "step": 12990
    },
    {
      "epoch": 0.8207070707070707,
      "grad_norm": 0.9895679354667664,
      "learning_rate": 0.00013326043553314498,
      "loss": 0.4847,
      "step": 13000
    },
    {
      "epoch": 0.8207070707070707,
      "eval_loss": 0.5436650514602661,
      "eval_runtime": 27.3386,
      "eval_samples_per_second": 93.64,
      "eval_steps_per_second": 11.705,
      "step": 13000
    },
    {
      "epoch": 0.8213383838383839,
      "grad_norm": 0.570975124835968,
      "learning_rate": 0.00013316400335928208,
      "loss": 0.666,
      "step": 13010
    },
    {
      "epoch": 0.821969696969697,
      "grad_norm": 0.6430348753929138,
      "learning_rate": 0.00013306753652212732,
      "loss": 0.5703,
      "step": 13020
    },
    {
      "epoch": 0.82260101010101,
      "grad_norm": 0.5882304906845093,
      "learning_rate": 0.00013297103512250875,
      "loss": 0.508,
      "step": 13030
    },
    {
      "epoch": 0.8232323232323232,
      "grad_norm": 0.602371096611023,
      "learning_rate": 0.00013287449926129038,
      "loss": 0.4555,
      "step": 13040
    },
    {
      "epoch": 0.8238636363636364,
      "grad_norm": 0.8860539793968201,
      "learning_rate": 0.00013277792903937228,
      "loss": 0.4852,
      "step": 13050
    },
    {
      "epoch": 0.8244949494949495,
      "grad_norm": 0.5680156946182251,
      "learning_rate": 0.00013268132455769046,
      "loss": 0.7124,
      "step": 13060
    },
    {
      "epoch": 0.8251262626262627,
      "grad_norm": 0.5839475393295288,
      "learning_rate": 0.00013258468591721673,
      "loss": 0.5731,
      "step": 13070
    },
    {
      "epoch": 0.8257575757575758,
      "grad_norm": 0.5819215774536133,
      "learning_rate": 0.0001324880132189586,
      "loss": 0.5285,
      "step": 13080
    },
    {
      "epoch": 0.8263888888888888,
      "grad_norm": 0.6980578899383545,
      "learning_rate": 0.00013239130656395918,
      "loss": 0.477,
      "step": 13090
    },
    {
      "epoch": 0.827020202020202,
      "grad_norm": 0.7298089861869812,
      "learning_rate": 0.0001322945660532971,
      "loss": 0.4857,
      "step": 13100
    },
    {
      "epoch": 0.8276515151515151,
      "grad_norm": 0.4927089214324951,
      "learning_rate": 0.00013219779178808624,
      "loss": 0.6936,
      "step": 13110
    },
    {
      "epoch": 0.8282828282828283,
      "grad_norm": 0.5642773509025574,
      "learning_rate": 0.000132100983869476,
      "loss": 0.5669,
      "step": 13120
    },
    {
      "epoch": 0.8289141414141414,
      "grad_norm": 0.5860962867736816,
      "learning_rate": 0.00013200414239865072,
      "loss": 0.505,
      "step": 13130
    },
    {
      "epoch": 0.8295454545454546,
      "grad_norm": 0.6136366724967957,
      "learning_rate": 0.00013190726747682997,
      "loss": 0.46,
      "step": 13140
    },
    {
      "epoch": 0.8301767676767676,
      "grad_norm": 0.8588699698448181,
      "learning_rate": 0.0001318103592052682,
      "loss": 0.4981,
      "step": 13150
    },
    {
      "epoch": 0.8308080808080808,
      "grad_norm": 0.47105473279953003,
      "learning_rate": 0.00013171341768525476,
      "loss": 0.6996,
      "step": 13160
    },
    {
      "epoch": 0.8314393939393939,
      "grad_norm": 0.4970909357070923,
      "learning_rate": 0.00013161644301811368,
      "loss": 0.5664,
      "step": 13170
    },
    {
      "epoch": 0.8320707070707071,
      "grad_norm": 0.6332927346229553,
      "learning_rate": 0.00013151943530520375,
      "loss": 0.5144,
      "step": 13180
    },
    {
      "epoch": 0.8327020202020202,
      "grad_norm": 0.7213175892829895,
      "learning_rate": 0.0001314223946479182,
      "loss": 0.4847,
      "step": 13190
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.1082409620285034,
      "learning_rate": 0.00013132532114768473,
      "loss": 0.5012,
      "step": 13200
    },
    {
      "epoch": 0.8339646464646465,
      "grad_norm": 0.5897113084793091,
      "learning_rate": 0.00013122821490596536,
      "loss": 0.6578,
      "step": 13210
    },
    {
      "epoch": 0.8345959595959596,
      "grad_norm": 0.6115443706512451,
      "learning_rate": 0.00013113107602425638,
      "loss": 0.5628,
      "step": 13220
    },
    {
      "epoch": 0.8352272727272727,
      "grad_norm": 0.5538140535354614,
      "learning_rate": 0.00013103390460408816,
      "loss": 0.5204,
      "step": 13230
    },
    {
      "epoch": 0.8358585858585859,
      "grad_norm": 0.6424857974052429,
      "learning_rate": 0.000130936700747025,
      "loss": 0.4687,
      "step": 13240
    },
    {
      "epoch": 0.836489898989899,
      "grad_norm": 0.8103302717208862,
      "learning_rate": 0.00013083946455466527,
      "loss": 0.4964,
      "step": 13250
    },
    {
      "epoch": 0.8371212121212122,
      "grad_norm": 0.49981462955474854,
      "learning_rate": 0.000130742196128641,
      "loss": 0.6709,
      "step": 13260
    },
    {
      "epoch": 0.8377525252525253,
      "grad_norm": 0.5487580299377441,
      "learning_rate": 0.00013064489557061795,
      "loss": 0.587,
      "step": 13270
    },
    {
      "epoch": 0.8383838383838383,
      "grad_norm": 0.5711541175842285,
      "learning_rate": 0.0001305475629822955,
      "loss": 0.5027,
      "step": 13280
    },
    {
      "epoch": 0.8390151515151515,
      "grad_norm": 0.6735122203826904,
      "learning_rate": 0.00013045019846540644,
      "loss": 0.4665,
      "step": 13290
    },
    {
      "epoch": 0.8396464646464646,
      "grad_norm": 0.8079771399497986,
      "learning_rate": 0.000130352802121717,
      "loss": 0.4833,
      "step": 13300
    },
    {
      "epoch": 0.8402777777777778,
      "grad_norm": 0.5294227004051208,
      "learning_rate": 0.00013025537405302666,
      "loss": 0.6935,
      "step": 13310
    },
    {
      "epoch": 0.8409090909090909,
      "grad_norm": 0.6122360229492188,
      "learning_rate": 0.00013015791436116802,
      "loss": 0.5911,
      "step": 13320
    },
    {
      "epoch": 0.8415404040404041,
      "grad_norm": 0.5899481773376465,
      "learning_rate": 0.00013006042314800677,
      "loss": 0.513,
      "step": 13330
    },
    {
      "epoch": 0.8421717171717171,
      "grad_norm": 0.7136526703834534,
      "learning_rate": 0.00012996290051544155,
      "loss": 0.4558,
      "step": 13340
    },
    {
      "epoch": 0.8428030303030303,
      "grad_norm": 0.8597419857978821,
      "learning_rate": 0.0001298653465654038,
      "loss": 0.5138,
      "step": 13350
    },
    {
      "epoch": 0.8434343434343434,
      "grad_norm": 0.574180006980896,
      "learning_rate": 0.00012976776139985772,
      "loss": 0.705,
      "step": 13360
    },
    {
      "epoch": 0.8440656565656566,
      "grad_norm": 0.5843489766120911,
      "learning_rate": 0.00012967014512080014,
      "loss": 0.6028,
      "step": 13370
    },
    {
      "epoch": 0.8446969696969697,
      "grad_norm": 0.6220381855964661,
      "learning_rate": 0.00012957249783026044,
      "loss": 0.4781,
      "step": 13380
    },
    {
      "epoch": 0.8453282828282829,
      "grad_norm": 0.5946854948997498,
      "learning_rate": 0.00012947481963030032,
      "loss": 0.4858,
      "step": 13390
    },
    {
      "epoch": 0.8459595959595959,
      "grad_norm": 0.703422486782074,
      "learning_rate": 0.0001293771106230139,
      "loss": 0.5043,
      "step": 13400
    },
    {
      "epoch": 0.8465909090909091,
      "grad_norm": 0.4881921112537384,
      "learning_rate": 0.00012927937091052744,
      "loss": 0.7302,
      "step": 13410
    },
    {
      "epoch": 0.8472222222222222,
      "grad_norm": 0.49822503328323364,
      "learning_rate": 0.00012918160059499924,
      "loss": 0.5781,
      "step": 13420
    },
    {
      "epoch": 0.8478535353535354,
      "grad_norm": 0.5858906507492065,
      "learning_rate": 0.0001290837997786197,
      "loss": 0.5006,
      "step": 13430
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 0.6391187310218811,
      "learning_rate": 0.00012898596856361104,
      "loss": 0.4934,
      "step": 13440
    },
    {
      "epoch": 0.8491161616161617,
      "grad_norm": 0.8443765640258789,
      "learning_rate": 0.00012888810705222726,
      "loss": 0.4853,
      "step": 13450
    },
    {
      "epoch": 0.8497474747474747,
      "grad_norm": 0.5210794806480408,
      "learning_rate": 0.00012879021534675398,
      "loss": 0.682,
      "step": 13460
    },
    {
      "epoch": 0.8503787878787878,
      "grad_norm": 0.5949555039405823,
      "learning_rate": 0.00012869229354950853,
      "loss": 0.5867,
      "step": 13470
    },
    {
      "epoch": 0.851010101010101,
      "grad_norm": 0.7064134478569031,
      "learning_rate": 0.00012859434176283946,
      "loss": 0.5328,
      "step": 13480
    },
    {
      "epoch": 0.8516414141414141,
      "grad_norm": 0.7347269058227539,
      "learning_rate": 0.00012849636008912686,
      "loss": 0.478,
      "step": 13490
    },
    {
      "epoch": 0.8522727272727273,
      "grad_norm": 0.8329459428787231,
      "learning_rate": 0.00012839834863078198,
      "loss": 0.5101,
      "step": 13500
    },
    {
      "epoch": 0.8529040404040404,
      "grad_norm": 0.5271977782249451,
      "learning_rate": 0.00012830030749024721,
      "loss": 0.704,
      "step": 13510
    },
    {
      "epoch": 0.8535353535353535,
      "grad_norm": 0.6087992191314697,
      "learning_rate": 0.00012820223676999597,
      "loss": 0.5843,
      "step": 13520
    },
    {
      "epoch": 0.8541666666666666,
      "grad_norm": 0.6319032311439514,
      "learning_rate": 0.00012810413657253259,
      "loss": 0.5037,
      "step": 13530
    },
    {
      "epoch": 0.8547979797979798,
      "grad_norm": 0.7325848340988159,
      "learning_rate": 0.0001280060070003922,
      "loss": 0.4697,
      "step": 13540
    },
    {
      "epoch": 0.8554292929292929,
      "grad_norm": 0.968131422996521,
      "learning_rate": 0.00012790784815614064,
      "loss": 0.502,
      "step": 13550
    },
    {
      "epoch": 0.8560606060606061,
      "grad_norm": 0.4842863976955414,
      "learning_rate": 0.00012780966014237435,
      "loss": 0.6401,
      "step": 13560
    },
    {
      "epoch": 0.8566919191919192,
      "grad_norm": 0.5574069619178772,
      "learning_rate": 0.00012771144306172026,
      "loss": 0.5768,
      "step": 13570
    },
    {
      "epoch": 0.8573232323232324,
      "grad_norm": 0.6657356023788452,
      "learning_rate": 0.0001276131970168357,
      "loss": 0.5202,
      "step": 13580
    },
    {
      "epoch": 0.8579545454545454,
      "grad_norm": 0.5505519509315491,
      "learning_rate": 0.00012751492211040823,
      "loss": 0.464,
      "step": 13590
    },
    {
      "epoch": 0.8585858585858586,
      "grad_norm": 0.793126106262207,
      "learning_rate": 0.00012741661844515562,
      "loss": 0.4569,
      "step": 13600
    },
    {
      "epoch": 0.8592171717171717,
      "grad_norm": 0.6048237681388855,
      "learning_rate": 0.00012731828612382566,
      "loss": 0.683,
      "step": 13610
    },
    {
      "epoch": 0.8598484848484849,
      "grad_norm": 0.6151214838027954,
      "learning_rate": 0.0001272199252491961,
      "loss": 0.5807,
      "step": 13620
    },
    {
      "epoch": 0.860479797979798,
      "grad_norm": 0.6144343018531799,
      "learning_rate": 0.00012712153592407457,
      "loss": 0.5144,
      "step": 13630
    },
    {
      "epoch": 0.8611111111111112,
      "grad_norm": 0.6900295615196228,
      "learning_rate": 0.00012702311825129838,
      "loss": 0.4729,
      "step": 13640
    },
    {
      "epoch": 0.8617424242424242,
      "grad_norm": 1.061104416847229,
      "learning_rate": 0.00012692467233373452,
      "loss": 0.4958,
      "step": 13650
    },
    {
      "epoch": 0.8623737373737373,
      "grad_norm": 0.4880428612232208,
      "learning_rate": 0.00012682619827427945,
      "loss": 0.6769,
      "step": 13660
    },
    {
      "epoch": 0.8630050505050505,
      "grad_norm": 0.5422914028167725,
      "learning_rate": 0.00012672769617585916,
      "loss": 0.5952,
      "step": 13670
    },
    {
      "epoch": 0.8636363636363636,
      "grad_norm": 0.6391079425811768,
      "learning_rate": 0.00012662916614142875,
      "loss": 0.5044,
      "step": 13680
    },
    {
      "epoch": 0.8642676767676768,
      "grad_norm": 0.7158488035202026,
      "learning_rate": 0.0001265306082739727,
      "loss": 0.4573,
      "step": 13690
    },
    {
      "epoch": 0.86489898989899,
      "grad_norm": 0.869753897190094,
      "learning_rate": 0.00012643202267650447,
      "loss": 0.5021,
      "step": 13700
    },
    {
      "epoch": 0.865530303030303,
      "grad_norm": 0.5354065299034119,
      "learning_rate": 0.0001263334094520666,
      "loss": 0.7041,
      "step": 13710
    },
    {
      "epoch": 0.8661616161616161,
      "grad_norm": 0.629118025302887,
      "learning_rate": 0.00012623476870373041,
      "loss": 0.59,
      "step": 13720
    },
    {
      "epoch": 0.8667929292929293,
      "grad_norm": 0.5187672972679138,
      "learning_rate": 0.00012613610053459606,
      "loss": 0.5362,
      "step": 13730
    },
    {
      "epoch": 0.8674242424242424,
      "grad_norm": 0.6497418880462646,
      "learning_rate": 0.00012603740504779228,
      "loss": 0.4755,
      "step": 13740
    },
    {
      "epoch": 0.8680555555555556,
      "grad_norm": 0.8548673987388611,
      "learning_rate": 0.00012593868234647647,
      "loss": 0.4884,
      "step": 13750
    },
    {
      "epoch": 0.8686868686868687,
      "grad_norm": 0.5416344404220581,
      "learning_rate": 0.00012583993253383443,
      "loss": 0.6911,
      "step": 13760
    },
    {
      "epoch": 0.8693181818181818,
      "grad_norm": 0.5683988928794861,
      "learning_rate": 0.00012574115571308022,
      "loss": 0.5756,
      "step": 13770
    },
    {
      "epoch": 0.8699494949494949,
      "grad_norm": 0.5452135801315308,
      "learning_rate": 0.00012564235198745625,
      "loss": 0.497,
      "step": 13780
    },
    {
      "epoch": 0.8705808080808081,
      "grad_norm": 0.6996577382087708,
      "learning_rate": 0.00012554352146023295,
      "loss": 0.4614,
      "step": 13790
    },
    {
      "epoch": 0.8712121212121212,
      "grad_norm": 0.7608072757720947,
      "learning_rate": 0.00012544466423470885,
      "loss": 0.4751,
      "step": 13800
    },
    {
      "epoch": 0.8718434343434344,
      "grad_norm": 0.49376335740089417,
      "learning_rate": 0.0001253457804142103,
      "loss": 0.6714,
      "step": 13810
    },
    {
      "epoch": 0.8724747474747475,
      "grad_norm": 0.6153733134269714,
      "learning_rate": 0.00012524687010209153,
      "loss": 0.5546,
      "step": 13820
    },
    {
      "epoch": 0.8731060606060606,
      "grad_norm": 0.56177818775177,
      "learning_rate": 0.00012514793340173442,
      "loss": 0.5136,
      "step": 13830
    },
    {
      "epoch": 0.8737373737373737,
      "grad_norm": 0.6128756403923035,
      "learning_rate": 0.0001250489704165484,
      "loss": 0.4344,
      "step": 13840
    },
    {
      "epoch": 0.8743686868686869,
      "grad_norm": 0.9419592618942261,
      "learning_rate": 0.00012494998124997042,
      "loss": 0.4982,
      "step": 13850
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.4976339042186737,
      "learning_rate": 0.00012485096600546477,
      "loss": 0.6766,
      "step": 13860
    },
    {
      "epoch": 0.8756313131313131,
      "grad_norm": 0.5971490740776062,
      "learning_rate": 0.00012475192478652303,
      "loss": 0.5705,
      "step": 13870
    },
    {
      "epoch": 0.8762626262626263,
      "grad_norm": 0.5859614014625549,
      "learning_rate": 0.00012465285769666388,
      "loss": 0.5159,
      "step": 13880
    },
    {
      "epoch": 0.8768939393939394,
      "grad_norm": 0.6968825459480286,
      "learning_rate": 0.00012455376483943312,
      "loss": 0.4646,
      "step": 13890
    },
    {
      "epoch": 0.8775252525252525,
      "grad_norm": 0.8528317213058472,
      "learning_rate": 0.0001244546463184033,
      "loss": 0.491,
      "step": 13900
    },
    {
      "epoch": 0.8781565656565656,
      "grad_norm": 0.5519726872444153,
      "learning_rate": 0.00012435550223717406,
      "loss": 0.7192,
      "step": 13910
    },
    {
      "epoch": 0.8787878787878788,
      "grad_norm": 0.5444708466529846,
      "learning_rate": 0.00012425633269937155,
      "loss": 0.5734,
      "step": 13920
    },
    {
      "epoch": 0.8794191919191919,
      "grad_norm": 0.6926050782203674,
      "learning_rate": 0.00012415713780864862,
      "loss": 0.5002,
      "step": 13930
    },
    {
      "epoch": 0.8800505050505051,
      "grad_norm": 0.6504577994346619,
      "learning_rate": 0.00012405791766868457,
      "loss": 0.4793,
      "step": 13940
    },
    {
      "epoch": 0.8806818181818182,
      "grad_norm": 0.7725946307182312,
      "learning_rate": 0.00012395867238318512,
      "loss": 0.4958,
      "step": 13950
    },
    {
      "epoch": 0.8813131313131313,
      "grad_norm": 0.505342960357666,
      "learning_rate": 0.00012385940205588227,
      "loss": 0.7111,
      "step": 13960
    },
    {
      "epoch": 0.8819444444444444,
      "grad_norm": 0.5202058553695679,
      "learning_rate": 0.0001237601067905342,
      "loss": 0.5532,
      "step": 13970
    },
    {
      "epoch": 0.8825757575757576,
      "grad_norm": 0.5917003154754639,
      "learning_rate": 0.00012366078669092512,
      "loss": 0.5185,
      "step": 13980
    },
    {
      "epoch": 0.8832070707070707,
      "grad_norm": 0.7779576182365417,
      "learning_rate": 0.00012356144186086528,
      "loss": 0.4797,
      "step": 13990
    },
    {
      "epoch": 0.8838383838383839,
      "grad_norm": 0.8982733488082886,
      "learning_rate": 0.00012346207240419068,
      "loss": 0.4702,
      "step": 14000
    },
    {
      "epoch": 0.8838383838383839,
      "eval_loss": 0.5381790399551392,
      "eval_runtime": 27.3168,
      "eval_samples_per_second": 93.715,
      "eval_steps_per_second": 11.714,
      "step": 14000
    },
    {
      "epoch": 0.884469696969697,
      "grad_norm": 0.5488813519477844,
      "learning_rate": 0.00012336267842476308,
      "loss": 0.6652,
      "step": 14010
    },
    {
      "epoch": 0.88510101010101,
      "grad_norm": 0.5034664273262024,
      "learning_rate": 0.00012326326002646992,
      "loss": 0.5497,
      "step": 14020
    },
    {
      "epoch": 0.8857323232323232,
      "grad_norm": 0.686495840549469,
      "learning_rate": 0.00012316381731322413,
      "loss": 0.5007,
      "step": 14030
    },
    {
      "epoch": 0.8863636363636364,
      "grad_norm": 0.6189455389976501,
      "learning_rate": 0.00012306435038896409,
      "loss": 0.4801,
      "step": 14040
    },
    {
      "epoch": 0.8869949494949495,
      "grad_norm": 0.8829519748687744,
      "learning_rate": 0.00012296485935765338,
      "loss": 0.4981,
      "step": 14050
    },
    {
      "epoch": 0.8876262626262627,
      "grad_norm": 0.5497207641601562,
      "learning_rate": 0.00012286534432328096,
      "loss": 0.6854,
      "step": 14060
    },
    {
      "epoch": 0.8882575757575758,
      "grad_norm": 0.5339611768722534,
      "learning_rate": 0.00012276580538986067,
      "loss": 0.5828,
      "step": 14070
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.6294302940368652,
      "learning_rate": 0.00012266624266143148,
      "loss": 0.5223,
      "step": 14080
    },
    {
      "epoch": 0.889520202020202,
      "grad_norm": 0.6116486191749573,
      "learning_rate": 0.00012256665624205717,
      "loss": 0.4826,
      "step": 14090
    },
    {
      "epoch": 0.8901515151515151,
      "grad_norm": 0.8612177968025208,
      "learning_rate": 0.00012246704623582636,
      "loss": 0.5053,
      "step": 14100
    },
    {
      "epoch": 0.8907828282828283,
      "grad_norm": 0.456590861082077,
      "learning_rate": 0.00012236741274685212,
      "loss": 0.7215,
      "step": 14110
    },
    {
      "epoch": 0.8914141414141414,
      "grad_norm": 0.5905047059059143,
      "learning_rate": 0.00012226775587927227,
      "loss": 0.566,
      "step": 14120
    },
    {
      "epoch": 0.8920454545454546,
      "grad_norm": 0.6151363849639893,
      "learning_rate": 0.00012216807573724905,
      "loss": 0.5153,
      "step": 14130
    },
    {
      "epoch": 0.8926767676767676,
      "grad_norm": 0.6458349823951721,
      "learning_rate": 0.00012206837242496887,
      "loss": 0.4441,
      "step": 14140
    },
    {
      "epoch": 0.8933080808080808,
      "grad_norm": 0.9071994423866272,
      "learning_rate": 0.00012196864604664253,
      "loss": 0.4802,
      "step": 14150
    },
    {
      "epoch": 0.8939393939393939,
      "grad_norm": 0.5433645248413086,
      "learning_rate": 0.00012186889670650486,
      "loss": 0.6503,
      "step": 14160
    },
    {
      "epoch": 0.8945707070707071,
      "grad_norm": 0.5147403478622437,
      "learning_rate": 0.00012176912450881468,
      "loss": 0.5781,
      "step": 14170
    },
    {
      "epoch": 0.8952020202020202,
      "grad_norm": 0.661317765712738,
      "learning_rate": 0.00012166932955785471,
      "loss": 0.511,
      "step": 14180
    },
    {
      "epoch": 0.8958333333333334,
      "grad_norm": 0.557945728302002,
      "learning_rate": 0.00012156951195793152,
      "loss": 0.4796,
      "step": 14190
    },
    {
      "epoch": 0.8964646464646465,
      "grad_norm": 0.8176923990249634,
      "learning_rate": 0.0001214696718133752,
      "loss": 0.5001,
      "step": 14200
    },
    {
      "epoch": 0.8970959595959596,
      "grad_norm": 0.5267322063446045,
      "learning_rate": 0.0001213698092285396,
      "loss": 0.6907,
      "step": 14210
    },
    {
      "epoch": 0.8977272727272727,
      "grad_norm": 0.544486939907074,
      "learning_rate": 0.00012126992430780187,
      "loss": 0.5823,
      "step": 14220
    },
    {
      "epoch": 0.8983585858585859,
      "grad_norm": 0.5694468021392822,
      "learning_rate": 0.00012117001715556255,
      "loss": 0.512,
      "step": 14230
    },
    {
      "epoch": 0.898989898989899,
      "grad_norm": 0.6034800410270691,
      "learning_rate": 0.00012107008787624546,
      "loss": 0.4601,
      "step": 14240
    },
    {
      "epoch": 0.8996212121212122,
      "grad_norm": 0.8275291919708252,
      "learning_rate": 0.00012097013657429746,
      "loss": 0.5022,
      "step": 14250
    },
    {
      "epoch": 0.9002525252525253,
      "grad_norm": 0.5511261820793152,
      "learning_rate": 0.00012087016335418855,
      "loss": 0.7062,
      "step": 14260
    },
    {
      "epoch": 0.9008838383838383,
      "grad_norm": 0.5683846473693848,
      "learning_rate": 0.00012077016832041151,
      "loss": 0.5973,
      "step": 14270
    },
    {
      "epoch": 0.9015151515151515,
      "grad_norm": 0.6322275400161743,
      "learning_rate": 0.00012067015157748203,
      "loss": 0.5224,
      "step": 14280
    },
    {
      "epoch": 0.9021464646464646,
      "grad_norm": 0.6139160990715027,
      "learning_rate": 0.00012057011322993838,
      "loss": 0.4987,
      "step": 14290
    },
    {
      "epoch": 0.9027777777777778,
      "grad_norm": 0.7266422510147095,
      "learning_rate": 0.00012047005338234155,
      "loss": 0.4974,
      "step": 14300
    },
    {
      "epoch": 0.9034090909090909,
      "grad_norm": 0.462640643119812,
      "learning_rate": 0.00012036997213927484,
      "loss": 0.6615,
      "step": 14310
    },
    {
      "epoch": 0.9040404040404041,
      "grad_norm": 0.5398191809654236,
      "learning_rate": 0.00012026986960534406,
      "loss": 0.5694,
      "step": 14320
    },
    {
      "epoch": 0.9046717171717171,
      "grad_norm": 0.5277344584465027,
      "learning_rate": 0.00012016974588517717,
      "loss": 0.5249,
      "step": 14330
    },
    {
      "epoch": 0.9053030303030303,
      "grad_norm": 0.6248831152915955,
      "learning_rate": 0.00012006960108342436,
      "loss": 0.4707,
      "step": 14340
    },
    {
      "epoch": 0.9059343434343434,
      "grad_norm": 0.8225948810577393,
      "learning_rate": 0.00011996943530475779,
      "loss": 0.4836,
      "step": 14350
    },
    {
      "epoch": 0.9065656565656566,
      "grad_norm": 0.4618719816207886,
      "learning_rate": 0.0001198692486538715,
      "loss": 0.6792,
      "step": 14360
    },
    {
      "epoch": 0.9071969696969697,
      "grad_norm": 0.5547179579734802,
      "learning_rate": 0.00011976904123548151,
      "loss": 0.5934,
      "step": 14370
    },
    {
      "epoch": 0.9078282828282829,
      "grad_norm": 0.618104875087738,
      "learning_rate": 0.00011966881315432539,
      "loss": 0.5284,
      "step": 14380
    },
    {
      "epoch": 0.9084595959595959,
      "grad_norm": 0.6479926705360413,
      "learning_rate": 0.00011956856451516239,
      "loss": 0.4558,
      "step": 14390
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.9186997413635254,
      "learning_rate": 0.00011946829542277313,
      "loss": 0.4623,
      "step": 14400
    },
    {
      "epoch": 0.9097222222222222,
      "grad_norm": 0.5533633828163147,
      "learning_rate": 0.00011936800598195983,
      "loss": 0.6737,
      "step": 14410
    },
    {
      "epoch": 0.9103535353535354,
      "grad_norm": 0.5928512811660767,
      "learning_rate": 0.00011926769629754582,
      "loss": 0.5795,
      "step": 14420
    },
    {
      "epoch": 0.9109848484848485,
      "grad_norm": 0.575854480266571,
      "learning_rate": 0.00011916736647437552,
      "loss": 0.5221,
      "step": 14430
    },
    {
      "epoch": 0.9116161616161617,
      "grad_norm": 0.7077135443687439,
      "learning_rate": 0.00011906701661731461,
      "loss": 0.4629,
      "step": 14440
    },
    {
      "epoch": 0.9122474747474747,
      "grad_norm": 0.8214622139930725,
      "learning_rate": 0.00011896664683124949,
      "loss": 0.4955,
      "step": 14450
    },
    {
      "epoch": 0.9128787878787878,
      "grad_norm": 0.5220351815223694,
      "learning_rate": 0.00011886625722108759,
      "loss": 0.6517,
      "step": 14460
    },
    {
      "epoch": 0.913510101010101,
      "grad_norm": 0.5915391445159912,
      "learning_rate": 0.0001187658478917569,
      "loss": 0.5759,
      "step": 14470
    },
    {
      "epoch": 0.9141414141414141,
      "grad_norm": 0.6620791554450989,
      "learning_rate": 0.00011866541894820613,
      "loss": 0.4947,
      "step": 14480
    },
    {
      "epoch": 0.9147727272727273,
      "grad_norm": 0.5614414215087891,
      "learning_rate": 0.00011856497049540438,
      "loss": 0.4506,
      "step": 14490
    },
    {
      "epoch": 0.9154040404040404,
      "grad_norm": 0.9742050170898438,
      "learning_rate": 0.0001184645026383413,
      "loss": 0.502,
      "step": 14500
    },
    {
      "epoch": 0.9160353535353535,
      "grad_norm": 0.5164632201194763,
      "learning_rate": 0.00011836401548202665,
      "loss": 0.6741,
      "step": 14510
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.5404476523399353,
      "learning_rate": 0.0001182635091314905,
      "loss": 0.5718,
      "step": 14520
    },
    {
      "epoch": 0.9172979797979798,
      "grad_norm": 0.5572554469108582,
      "learning_rate": 0.00011816298369178287,
      "loss": 0.5107,
      "step": 14530
    },
    {
      "epoch": 0.9179292929292929,
      "grad_norm": 0.6357147693634033,
      "learning_rate": 0.00011806243926797378,
      "loss": 0.4625,
      "step": 14540
    },
    {
      "epoch": 0.9185606060606061,
      "grad_norm": 0.7537611722946167,
      "learning_rate": 0.00011796187596515316,
      "loss": 0.5,
      "step": 14550
    },
    {
      "epoch": 0.9191919191919192,
      "grad_norm": 0.4805369973182678,
      "learning_rate": 0.00011786129388843055,
      "loss": 0.6912,
      "step": 14560
    },
    {
      "epoch": 0.9198232323232324,
      "grad_norm": 0.5741963386535645,
      "learning_rate": 0.00011776069314293523,
      "loss": 0.5709,
      "step": 14570
    },
    {
      "epoch": 0.9204545454545454,
      "grad_norm": 0.6011599898338318,
      "learning_rate": 0.00011766007383381586,
      "loss": 0.5097,
      "step": 14580
    },
    {
      "epoch": 0.9210858585858586,
      "grad_norm": 0.5827755331993103,
      "learning_rate": 0.00011755943606624064,
      "loss": 0.454,
      "step": 14590
    },
    {
      "epoch": 0.9217171717171717,
      "grad_norm": 0.8556448817253113,
      "learning_rate": 0.00011745877994539696,
      "loss": 0.4919,
      "step": 14600
    },
    {
      "epoch": 0.9223484848484849,
      "grad_norm": 0.5549802184104919,
      "learning_rate": 0.00011735810557649148,
      "loss": 0.6932,
      "step": 14610
    },
    {
      "epoch": 0.922979797979798,
      "grad_norm": 0.5364561676979065,
      "learning_rate": 0.00011725741306474982,
      "loss": 0.567,
      "step": 14620
    },
    {
      "epoch": 0.9236111111111112,
      "grad_norm": 0.556452214717865,
      "learning_rate": 0.0001171567025154167,
      "loss": 0.519,
      "step": 14630
    },
    {
      "epoch": 0.9242424242424242,
      "grad_norm": 0.6402288675308228,
      "learning_rate": 0.00011705597403375559,
      "loss": 0.4512,
      "step": 14640
    },
    {
      "epoch": 0.9248737373737373,
      "grad_norm": 0.8149012923240662,
      "learning_rate": 0.00011695522772504872,
      "loss": 0.4862,
      "step": 14650
    },
    {
      "epoch": 0.9255050505050505,
      "grad_norm": 0.5451039671897888,
      "learning_rate": 0.000116854463694597,
      "loss": 0.6921,
      "step": 14660
    },
    {
      "epoch": 0.9261363636363636,
      "grad_norm": 0.54402095079422,
      "learning_rate": 0.00011675368204771979,
      "loss": 0.5974,
      "step": 14670
    },
    {
      "epoch": 0.9267676767676768,
      "grad_norm": 0.6317455768585205,
      "learning_rate": 0.00011665288288975498,
      "loss": 0.4963,
      "step": 14680
    },
    {
      "epoch": 0.92739898989899,
      "grad_norm": 0.7216183543205261,
      "learning_rate": 0.0001165520663260586,
      "loss": 0.4683,
      "step": 14690
    },
    {
      "epoch": 0.928030303030303,
      "grad_norm": 0.9398926496505737,
      "learning_rate": 0.00011645123246200502,
      "loss": 0.4859,
      "step": 14700
    },
    {
      "epoch": 0.9286616161616161,
      "grad_norm": 0.5385030508041382,
      "learning_rate": 0.0001163503814029866,
      "loss": 0.6986,
      "step": 14710
    },
    {
      "epoch": 0.9292929292929293,
      "grad_norm": 0.4764665961265564,
      "learning_rate": 0.0001162495132544137,
      "loss": 0.5763,
      "step": 14720
    },
    {
      "epoch": 0.9299242424242424,
      "grad_norm": 0.6534984111785889,
      "learning_rate": 0.00011614862812171458,
      "loss": 0.5249,
      "step": 14730
    },
    {
      "epoch": 0.9305555555555556,
      "grad_norm": 0.7361531853675842,
      "learning_rate": 0.00011604772611033522,
      "loss": 0.4381,
      "step": 14740
    },
    {
      "epoch": 0.9311868686868687,
      "grad_norm": 0.9915294051170349,
      "learning_rate": 0.0001159468073257392,
      "loss": 0.4866,
      "step": 14750
    },
    {
      "epoch": 0.9318181818181818,
      "grad_norm": 0.5383821725845337,
      "learning_rate": 0.0001158458718734077,
      "loss": 0.6949,
      "step": 14760
    },
    {
      "epoch": 0.9324494949494949,
      "grad_norm": 0.5385206341743469,
      "learning_rate": 0.00011574491985883931,
      "loss": 0.5794,
      "step": 14770
    },
    {
      "epoch": 0.9330808080808081,
      "grad_norm": 0.491055428981781,
      "learning_rate": 0.00011564395138754984,
      "loss": 0.5361,
      "step": 14780
    },
    {
      "epoch": 0.9337121212121212,
      "grad_norm": 0.65193110704422,
      "learning_rate": 0.00011554296656507246,
      "loss": 0.464,
      "step": 14790
    },
    {
      "epoch": 0.9343434343434344,
      "grad_norm": 0.6822025775909424,
      "learning_rate": 0.0001154419654969573,
      "loss": 0.4678,
      "step": 14800
    },
    {
      "epoch": 0.9349747474747475,
      "grad_norm": 0.5224807858467102,
      "learning_rate": 0.00011534094828877154,
      "loss": 0.6618,
      "step": 14810
    },
    {
      "epoch": 0.9356060606060606,
      "grad_norm": 0.5200857520103455,
      "learning_rate": 0.00011523991504609917,
      "loss": 0.5798,
      "step": 14820
    },
    {
      "epoch": 0.9362373737373737,
      "grad_norm": 0.5767768025398254,
      "learning_rate": 0.00011513886587454101,
      "loss": 0.5196,
      "step": 14830
    },
    {
      "epoch": 0.9368686868686869,
      "grad_norm": 0.6002743244171143,
      "learning_rate": 0.00011503780087971448,
      "loss": 0.4831,
      "step": 14840
    },
    {
      "epoch": 0.9375,
      "grad_norm": 0.9267683625221252,
      "learning_rate": 0.00011493672016725356,
      "loss": 0.5216,
      "step": 14850
    },
    {
      "epoch": 0.9381313131313131,
      "grad_norm": 0.5254136919975281,
      "learning_rate": 0.00011483562384280864,
      "loss": 0.6639,
      "step": 14860
    },
    {
      "epoch": 0.9387626262626263,
      "grad_norm": 0.4923168122768402,
      "learning_rate": 0.00011473451201204644,
      "loss": 0.5771,
      "step": 14870
    },
    {
      "epoch": 0.9393939393939394,
      "grad_norm": 0.5771089792251587,
      "learning_rate": 0.00011463338478064989,
      "loss": 0.5242,
      "step": 14880
    },
    {
      "epoch": 0.9400252525252525,
      "grad_norm": 0.7255411148071289,
      "learning_rate": 0.00011453224225431797,
      "loss": 0.4769,
      "step": 14890
    },
    {
      "epoch": 0.9406565656565656,
      "grad_norm": 0.99202561378479,
      "learning_rate": 0.00011443108453876579,
      "loss": 0.4713,
      "step": 14900
    },
    {
      "epoch": 0.9412878787878788,
      "grad_norm": 0.4952683448791504,
      "learning_rate": 0.00011432991173972412,
      "loss": 0.6963,
      "step": 14910
    },
    {
      "epoch": 0.9419191919191919,
      "grad_norm": 0.5967009663581848,
      "learning_rate": 0.0001142287239629397,
      "loss": 0.5778,
      "step": 14920
    },
    {
      "epoch": 0.9425505050505051,
      "grad_norm": 0.5679914951324463,
      "learning_rate": 0.00011412752131417477,
      "loss": 0.4945,
      "step": 14930
    },
    {
      "epoch": 0.9431818181818182,
      "grad_norm": 0.5813766717910767,
      "learning_rate": 0.00011402630389920723,
      "loss": 0.4618,
      "step": 14940
    },
    {
      "epoch": 0.9438131313131313,
      "grad_norm": 0.9143353700637817,
      "learning_rate": 0.0001139250718238303,
      "loss": 0.4805,
      "step": 14950
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 0.5133100748062134,
      "learning_rate": 0.00011382382519385266,
      "loss": 0.687,
      "step": 14960
    },
    {
      "epoch": 0.9450757575757576,
      "grad_norm": 0.5652880668640137,
      "learning_rate": 0.00011372256411509808,
      "loss": 0.5919,
      "step": 14970
    },
    {
      "epoch": 0.9457070707070707,
      "grad_norm": 0.5932576656341553,
      "learning_rate": 0.00011362128869340549,
      "loss": 0.5125,
      "step": 14980
    },
    {
      "epoch": 0.9463383838383839,
      "grad_norm": 0.5702904462814331,
      "learning_rate": 0.00011351999903462882,
      "loss": 0.4658,
      "step": 14990
    },
    {
      "epoch": 0.946969696969697,
      "grad_norm": 0.8629094958305359,
      "learning_rate": 0.00011341869524463684,
      "loss": 0.4919,
      "step": 15000
    },
    {
      "epoch": 0.946969696969697,
      "eval_loss": 0.5343874096870422,
      "eval_runtime": 34.9815,
      "eval_samples_per_second": 73.182,
      "eval_steps_per_second": 9.148,
      "step": 15000
    },
    {
      "epoch": 0.94760101010101,
      "grad_norm": 0.5599154233932495,
      "learning_rate": 0.00011331737742931314,
      "loss": 0.6312,
      "step": 15010
    },
    {
      "epoch": 0.9482323232323232,
      "grad_norm": 0.5498698949813843,
      "learning_rate": 0.00011321604569455591,
      "loss": 0.597,
      "step": 15020
    },
    {
      "epoch": 0.9488636363636364,
      "grad_norm": 0.5120929479598999,
      "learning_rate": 0.00011311470014627792,
      "loss": 0.5219,
      "step": 15030
    },
    {
      "epoch": 0.9494949494949495,
      "grad_norm": 0.6730033159255981,
      "learning_rate": 0.00011301334089040641,
      "loss": 0.474,
      "step": 15040
    },
    {
      "epoch": 0.9501262626262627,
      "grad_norm": 0.6906611323356628,
      "learning_rate": 0.00011291196803288291,
      "loss": 0.5015,
      "step": 15050
    },
    {
      "epoch": 0.9507575757575758,
      "grad_norm": 0.5144419074058533,
      "learning_rate": 0.00011281058167966313,
      "loss": 0.7179,
      "step": 15060
    },
    {
      "epoch": 0.9513888888888888,
      "grad_norm": 0.4716389775276184,
      "learning_rate": 0.000112709181936717,
      "loss": 0.5866,
      "step": 15070
    },
    {
      "epoch": 0.952020202020202,
      "grad_norm": 0.5781723260879517,
      "learning_rate": 0.00011260776891002831,
      "loss": 0.5063,
      "step": 15080
    },
    {
      "epoch": 0.9526515151515151,
      "grad_norm": 0.5660507082939148,
      "learning_rate": 0.00011250634270559484,
      "loss": 0.4691,
      "step": 15090
    },
    {
      "epoch": 0.9532828282828283,
      "grad_norm": 1.0068069696426392,
      "learning_rate": 0.00011240490342942806,
      "loss": 0.4736,
      "step": 15100
    },
    {
      "epoch": 0.9539141414141414,
      "grad_norm": 0.5630481243133545,
      "learning_rate": 0.00011230345118755319,
      "loss": 0.6877,
      "step": 15110
    },
    {
      "epoch": 0.9545454545454546,
      "grad_norm": 0.47467324137687683,
      "learning_rate": 0.00011220198608600894,
      "loss": 0.569,
      "step": 15120
    },
    {
      "epoch": 0.9551767676767676,
      "grad_norm": 0.5961889028549194,
      "learning_rate": 0.00011210050823084746,
      "loss": 0.4938,
      "step": 15130
    },
    {
      "epoch": 0.9558080808080808,
      "grad_norm": 0.6286396980285645,
      "learning_rate": 0.00011199901772813426,
      "loss": 0.459,
      "step": 15140
    },
    {
      "epoch": 0.9564393939393939,
      "grad_norm": 0.912581741809845,
      "learning_rate": 0.00011189751468394805,
      "loss": 0.4969,
      "step": 15150
    },
    {
      "epoch": 0.9570707070707071,
      "grad_norm": 0.47330549359321594,
      "learning_rate": 0.00011179599920438066,
      "loss": 0.7107,
      "step": 15160
    },
    {
      "epoch": 0.9577020202020202,
      "grad_norm": 0.5293593406677246,
      "learning_rate": 0.00011169447139553691,
      "loss": 0.5461,
      "step": 15170
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 0.6116522550582886,
      "learning_rate": 0.00011159293136353452,
      "loss": 0.519,
      "step": 15180
    },
    {
      "epoch": 0.9589646464646465,
      "grad_norm": 0.6685961484909058,
      "learning_rate": 0.00011149137921450396,
      "loss": 0.4755,
      "step": 15190
    },
    {
      "epoch": 0.9595959595959596,
      "grad_norm": 0.7400134205818176,
      "learning_rate": 0.00011138981505458841,
      "loss": 0.4717,
      "step": 15200
    },
    {
      "epoch": 0.9602272727272727,
      "grad_norm": 0.5707361698150635,
      "learning_rate": 0.00011128823898994351,
      "loss": 0.6926,
      "step": 15210
    },
    {
      "epoch": 0.9608585858585859,
      "grad_norm": 0.4697893261909485,
      "learning_rate": 0.00011118665112673749,
      "loss": 0.5718,
      "step": 15220
    },
    {
      "epoch": 0.961489898989899,
      "grad_norm": 0.5628228783607483,
      "learning_rate": 0.00011108505157115079,
      "loss": 0.52,
      "step": 15230
    },
    {
      "epoch": 0.9621212121212122,
      "grad_norm": 0.5467842817306519,
      "learning_rate": 0.00011098344042937605,
      "loss": 0.4415,
      "step": 15240
    },
    {
      "epoch": 0.9627525252525253,
      "grad_norm": 1.0815882682800293,
      "learning_rate": 0.0001108818178076182,
      "loss": 0.4878,
      "step": 15250
    },
    {
      "epoch": 0.9633838383838383,
      "grad_norm": 0.5728668570518494,
      "learning_rate": 0.00011078018381209392,
      "loss": 0.6607,
      "step": 15260
    },
    {
      "epoch": 0.9640151515151515,
      "grad_norm": 0.5732964277267456,
      "learning_rate": 0.000110678538549032,
      "loss": 0.5734,
      "step": 15270
    },
    {
      "epoch": 0.9646464646464646,
      "grad_norm": 0.6603596806526184,
      "learning_rate": 0.00011057688212467287,
      "loss": 0.5012,
      "step": 15280
    },
    {
      "epoch": 0.9652777777777778,
      "grad_norm": 0.6326664090156555,
      "learning_rate": 0.00011047521464526871,
      "loss": 0.4366,
      "step": 15290
    },
    {
      "epoch": 0.9659090909090909,
      "grad_norm": 0.8376827836036682,
      "learning_rate": 0.00011037353621708315,
      "loss": 0.4915,
      "step": 15300
    },
    {
      "epoch": 0.9665404040404041,
      "grad_norm": 0.49920955300331116,
      "learning_rate": 0.00011027184694639139,
      "loss": 0.7325,
      "step": 15310
    },
    {
      "epoch": 0.9671717171717171,
      "grad_norm": 0.5716894865036011,
      "learning_rate": 0.00011017014693947987,
      "loss": 0.5838,
      "step": 15320
    },
    {
      "epoch": 0.9678030303030303,
      "grad_norm": 0.6328772306442261,
      "learning_rate": 0.00011006843630264628,
      "loss": 0.5176,
      "step": 15330
    },
    {
      "epoch": 0.9684343434343434,
      "grad_norm": 0.5542431473731995,
      "learning_rate": 0.00010996671514219943,
      "loss": 0.4696,
      "step": 15340
    },
    {
      "epoch": 0.9690656565656566,
      "grad_norm": 0.9654923677444458,
      "learning_rate": 0.00010986498356445912,
      "loss": 0.4745,
      "step": 15350
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 0.5879325270652771,
      "learning_rate": 0.00010976324167575604,
      "loss": 0.6721,
      "step": 15360
    },
    {
      "epoch": 0.9703282828282829,
      "grad_norm": 0.5818185806274414,
      "learning_rate": 0.00010966148958243165,
      "loss": 0.5694,
      "step": 15370
    },
    {
      "epoch": 0.9709595959595959,
      "grad_norm": 0.5020058751106262,
      "learning_rate": 0.00010955972739083811,
      "loss": 0.5238,
      "step": 15380
    },
    {
      "epoch": 0.9715909090909091,
      "grad_norm": 0.6357765793800354,
      "learning_rate": 0.00010945795520733808,
      "loss": 0.4887,
      "step": 15390
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 0.9034340381622314,
      "learning_rate": 0.0001093561731383047,
      "loss": 0.4974,
      "step": 15400
    },
    {
      "epoch": 0.9728535353535354,
      "grad_norm": 0.4953339099884033,
      "learning_rate": 0.00010925438129012142,
      "loss": 0.6714,
      "step": 15410
    },
    {
      "epoch": 0.9734848484848485,
      "grad_norm": 0.47056546807289124,
      "learning_rate": 0.00010915257976918196,
      "loss": 0.5771,
      "step": 15420
    },
    {
      "epoch": 0.9741161616161617,
      "grad_norm": 0.6618484854698181,
      "learning_rate": 0.00010905076868189008,
      "loss": 0.4996,
      "step": 15430
    },
    {
      "epoch": 0.9747474747474747,
      "grad_norm": 0.578514039516449,
      "learning_rate": 0.00010894894813465961,
      "loss": 0.4526,
      "step": 15440
    },
    {
      "epoch": 0.9753787878787878,
      "grad_norm": 0.9848448038101196,
      "learning_rate": 0.00010884711823391418,
      "loss": 0.4923,
      "step": 15450
    },
    {
      "epoch": 0.976010101010101,
      "grad_norm": 0.48116615414619446,
      "learning_rate": 0.00010874527908608732,
      "loss": 0.6554,
      "step": 15460
    },
    {
      "epoch": 0.9766414141414141,
      "grad_norm": 0.5076225996017456,
      "learning_rate": 0.00010864343079762209,
      "loss": 0.5719,
      "step": 15470
    },
    {
      "epoch": 0.9772727272727273,
      "grad_norm": 0.5784098505973816,
      "learning_rate": 0.00010854157347497118,
      "loss": 0.5018,
      "step": 15480
    },
    {
      "epoch": 0.9779040404040404,
      "grad_norm": 0.6003864407539368,
      "learning_rate": 0.00010843970722459675,
      "loss": 0.4768,
      "step": 15490
    },
    {
      "epoch": 0.9785353535353535,
      "grad_norm": 0.7369610071182251,
      "learning_rate": 0.00010833783215297019,
      "loss": 0.4608,
      "step": 15500
    },
    {
      "epoch": 0.9791666666666666,
      "grad_norm": 0.4849420189857483,
      "learning_rate": 0.00010823594836657223,
      "loss": 0.6452,
      "step": 15510
    },
    {
      "epoch": 0.9797979797979798,
      "grad_norm": 0.4751661419868469,
      "learning_rate": 0.00010813405597189259,
      "loss": 0.5624,
      "step": 15520
    },
    {
      "epoch": 0.9804292929292929,
      "grad_norm": 0.5551533102989197,
      "learning_rate": 0.0001080321550754301,
      "loss": 0.4927,
      "step": 15530
    },
    {
      "epoch": 0.9810606060606061,
      "grad_norm": 0.6343836784362793,
      "learning_rate": 0.0001079302457836924,
      "loss": 0.4646,
      "step": 15540
    },
    {
      "epoch": 0.9816919191919192,
      "grad_norm": 0.8081687092781067,
      "learning_rate": 0.00010782832820319593,
      "loss": 0.4902,
      "step": 15550
    },
    {
      "epoch": 0.9823232323232324,
      "grad_norm": 0.6123889088630676,
      "learning_rate": 0.00010772640244046576,
      "loss": 0.6844,
      "step": 15560
    },
    {
      "epoch": 0.9829545454545454,
      "grad_norm": 0.4937269389629364,
      "learning_rate": 0.00010762446860203563,
      "loss": 0.5882,
      "step": 15570
    },
    {
      "epoch": 0.9835858585858586,
      "grad_norm": 0.533433198928833,
      "learning_rate": 0.00010752252679444755,
      "loss": 0.5087,
      "step": 15580
    },
    {
      "epoch": 0.9842171717171717,
      "grad_norm": 0.5859047174453735,
      "learning_rate": 0.00010742057712425199,
      "loss": 0.4464,
      "step": 15590
    },
    {
      "epoch": 0.9848484848484849,
      "grad_norm": 0.895573079586029,
      "learning_rate": 0.00010731861969800758,
      "loss": 0.451,
      "step": 15600
    },
    {
      "epoch": 0.985479797979798,
      "grad_norm": 0.499716579914093,
      "learning_rate": 0.00010721665462228105,
      "loss": 0.6959,
      "step": 15610
    },
    {
      "epoch": 0.9861111111111112,
      "grad_norm": 0.5909522771835327,
      "learning_rate": 0.00010711468200364718,
      "loss": 0.5886,
      "step": 15620
    },
    {
      "epoch": 0.9867424242424242,
      "grad_norm": 0.49402785301208496,
      "learning_rate": 0.00010701270194868856,
      "loss": 0.51,
      "step": 15630
    },
    {
      "epoch": 0.9873737373737373,
      "grad_norm": 0.7005777359008789,
      "learning_rate": 0.00010691071456399561,
      "loss": 0.459,
      "step": 15640
    },
    {
      "epoch": 0.9880050505050505,
      "grad_norm": 0.7656469345092773,
      "learning_rate": 0.00010680871995616639,
      "loss": 0.486,
      "step": 15650
    },
    {
      "epoch": 0.9886363636363636,
      "grad_norm": 0.5390546321868896,
      "learning_rate": 0.00010670671823180651,
      "loss": 0.7166,
      "step": 15660
    },
    {
      "epoch": 0.9892676767676768,
      "grad_norm": 0.6001823544502258,
      "learning_rate": 0.00010660470949752903,
      "loss": 0.5847,
      "step": 15670
    },
    {
      "epoch": 0.98989898989899,
      "grad_norm": 0.5716586112976074,
      "learning_rate": 0.00010650269385995433,
      "loss": 0.525,
      "step": 15680
    },
    {
      "epoch": 0.990530303030303,
      "grad_norm": 0.5916579961776733,
      "learning_rate": 0.00010640067142570995,
      "loss": 0.4679,
      "step": 15690
    },
    {
      "epoch": 0.9911616161616161,
      "grad_norm": 1.0259119272232056,
      "learning_rate": 0.00010629864230143067,
      "loss": 0.4842,
      "step": 15700
    },
    {
      "epoch": 0.9917929292929293,
      "grad_norm": 0.4954005181789398,
      "learning_rate": 0.00010619660659375814,
      "loss": 0.6682,
      "step": 15710
    },
    {
      "epoch": 0.9924242424242424,
      "grad_norm": 0.5898115634918213,
      "learning_rate": 0.0001060945644093409,
      "loss": 0.5738,
      "step": 15720
    },
    {
      "epoch": 0.9930555555555556,
      "grad_norm": 0.5380865931510925,
      "learning_rate": 0.0001059925158548343,
      "loss": 0.5204,
      "step": 15730
    },
    {
      "epoch": 0.9936868686868687,
      "grad_norm": 0.6176660060882568,
      "learning_rate": 0.00010589046103690035,
      "loss": 0.4423,
      "step": 15740
    },
    {
      "epoch": 0.9943181818181818,
      "grad_norm": 0.7423949241638184,
      "learning_rate": 0.00010578840006220763,
      "loss": 0.478,
      "step": 15750
    },
    {
      "epoch": 0.9949494949494949,
      "grad_norm": 0.49208900332450867,
      "learning_rate": 0.00010568633303743102,
      "loss": 0.7208,
      "step": 15760
    },
    {
      "epoch": 0.9955808080808081,
      "grad_norm": 0.5474214553833008,
      "learning_rate": 0.00010558426006925195,
      "loss": 0.5623,
      "step": 15770
    },
    {
      "epoch": 0.9962121212121212,
      "grad_norm": 0.5788156390190125,
      "learning_rate": 0.0001054821812643578,
      "loss": 0.4927,
      "step": 15780
    },
    {
      "epoch": 0.9968434343434344,
      "grad_norm": 0.6256375908851624,
      "learning_rate": 0.00010538009672944231,
      "loss": 0.4573,
      "step": 15790
    },
    {
      "epoch": 0.9974747474747475,
      "grad_norm": 0.8009647727012634,
      "learning_rate": 0.000105278006571205,
      "loss": 0.4427,
      "step": 15800
    },
    {
      "epoch": 0.9981060606060606,
      "grad_norm": 0.5171905755996704,
      "learning_rate": 0.0001051759108963514,
      "loss": 0.6458,
      "step": 15810
    },
    {
      "epoch": 0.9987373737373737,
      "grad_norm": 0.5787621736526489,
      "learning_rate": 0.00010507380981159272,
      "loss": 0.5628,
      "step": 15820
    },
    {
      "epoch": 0.9993686868686869,
      "grad_norm": 0.6118173003196716,
      "learning_rate": 0.00010497170342364586,
      "loss": 0.4619,
      "step": 15830
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7081486582756042,
      "learning_rate": 0.0001048695918392333,
      "loss": 0.4778,
      "step": 15840
    },
    {
      "epoch": 1.0006313131313131,
      "grad_norm": 0.524154007434845,
      "learning_rate": 0.0001047674751650829,
      "loss": 0.6492,
      "step": 15850
    },
    {
      "epoch": 1.0012626262626263,
      "grad_norm": 0.5427746772766113,
      "learning_rate": 0.00010466535350792786,
      "loss": 0.5706,
      "step": 15860
    },
    {
      "epoch": 1.0018939393939394,
      "grad_norm": 0.5079688429832458,
      "learning_rate": 0.00010456322697450654,
      "loss": 0.5123,
      "step": 15870
    },
    {
      "epoch": 1.0025252525252526,
      "grad_norm": 0.5890278816223145,
      "learning_rate": 0.00010446109567156252,
      "loss": 0.4427,
      "step": 15880
    },
    {
      "epoch": 1.0031565656565657,
      "grad_norm": 0.9656274318695068,
      "learning_rate": 0.00010435895970584422,
      "loss": 0.4595,
      "step": 15890
    },
    {
      "epoch": 1.003787878787879,
      "grad_norm": 0.5378276109695435,
      "learning_rate": 0.00010425681918410506,
      "loss": 0.6281,
      "step": 15900
    },
    {
      "epoch": 1.0044191919191918,
      "grad_norm": 0.5836766362190247,
      "learning_rate": 0.00010415467421310308,
      "loss": 0.5781,
      "step": 15910
    },
    {
      "epoch": 1.005050505050505,
      "grad_norm": 0.6407049298286438,
      "learning_rate": 0.00010405252489960113,
      "loss": 0.5162,
      "step": 15920
    },
    {
      "epoch": 1.0056818181818181,
      "grad_norm": 0.6097946166992188,
      "learning_rate": 0.00010395037135036649,
      "loss": 0.4385,
      "step": 15930
    },
    {
      "epoch": 1.0063131313131313,
      "grad_norm": 0.730926513671875,
      "learning_rate": 0.0001038482136721709,
      "loss": 0.4923,
      "step": 15940
    },
    {
      "epoch": 1.0069444444444444,
      "grad_norm": 0.5502397418022156,
      "learning_rate": 0.00010374605197179044,
      "loss": 0.6687,
      "step": 15950
    },
    {
      "epoch": 1.0075757575757576,
      "grad_norm": 0.541259765625,
      "learning_rate": 0.00010364388635600529,
      "loss": 0.541,
      "step": 15960
    },
    {
      "epoch": 1.0082070707070707,
      "grad_norm": 0.5438472032546997,
      "learning_rate": 0.00010354171693159989,
      "loss": 0.4921,
      "step": 15970
    },
    {
      "epoch": 1.0088383838383839,
      "grad_norm": 0.5502214431762695,
      "learning_rate": 0.0001034395438053625,
      "loss": 0.453,
      "step": 15980
    },
    {
      "epoch": 1.009469696969697,
      "grad_norm": 0.8026833534240723,
      "learning_rate": 0.00010333736708408537,
      "loss": 0.4876,
      "step": 15990
    },
    {
      "epoch": 1.0101010101010102,
      "grad_norm": 0.4484994113445282,
      "learning_rate": 0.00010323518687456442,
      "loss": 0.6558,
      "step": 16000
    },
    {
      "epoch": 1.0101010101010102,
      "eval_loss": 0.5320479273796082,
      "eval_runtime": 34.7374,
      "eval_samples_per_second": 73.696,
      "eval_steps_per_second": 9.212,
      "step": 16000
    },
    {
      "epoch": 1.0107323232323233,
      "grad_norm": 0.5209174752235413,
      "learning_rate": 0.0001031330032835993,
      "loss": 0.5297,
      "step": 16010
    },
    {
      "epoch": 1.0113636363636365,
      "grad_norm": 0.5444884300231934,
      "learning_rate": 0.0001030308164179931,
      "loss": 0.497,
      "step": 16020
    },
    {
      "epoch": 1.0119949494949494,
      "grad_norm": 0.721052885055542,
      "learning_rate": 0.00010292862638455242,
      "loss": 0.4322,
      "step": 16030
    },
    {
      "epoch": 1.0126262626262625,
      "grad_norm": 0.7435801029205322,
      "learning_rate": 0.0001028264332900871,
      "loss": 0.4786,
      "step": 16040
    },
    {
      "epoch": 1.0132575757575757,
      "grad_norm": 0.4850148856639862,
      "learning_rate": 0.00010272423724141025,
      "loss": 0.6692,
      "step": 16050
    },
    {
      "epoch": 1.0138888888888888,
      "grad_norm": 0.5057579874992371,
      "learning_rate": 0.00010262203834533801,
      "loss": 0.5479,
      "step": 16060
    },
    {
      "epoch": 1.014520202020202,
      "grad_norm": 0.6087522506713867,
      "learning_rate": 0.00010251983670868947,
      "loss": 0.4917,
      "step": 16070
    },
    {
      "epoch": 1.0151515151515151,
      "grad_norm": 0.5943182706832886,
      "learning_rate": 0.00010241763243828671,
      "loss": 0.4496,
      "step": 16080
    },
    {
      "epoch": 1.0157828282828283,
      "grad_norm": 0.7245150208473206,
      "learning_rate": 0.00010231542564095442,
      "loss": 0.4579,
      "step": 16090
    },
    {
      "epoch": 1.0164141414141414,
      "grad_norm": 0.474331796169281,
      "learning_rate": 0.00010221321642352002,
      "loss": 0.6619,
      "step": 16100
    },
    {
      "epoch": 1.0170454545454546,
      "grad_norm": 0.5791044235229492,
      "learning_rate": 0.00010211100489281342,
      "loss": 0.5783,
      "step": 16110
    },
    {
      "epoch": 1.0176767676767677,
      "grad_norm": 0.599807858467102,
      "learning_rate": 0.000102008791155667,
      "loss": 0.5007,
      "step": 16120
    },
    {
      "epoch": 1.0183080808080809,
      "grad_norm": 0.5312196016311646,
      "learning_rate": 0.00010190657531891535,
      "loss": 0.4401,
      "step": 16130
    },
    {
      "epoch": 1.018939393939394,
      "grad_norm": 0.839154839515686,
      "learning_rate": 0.00010180435748939533,
      "loss": 0.4851,
      "step": 16140
    },
    {
      "epoch": 1.0195707070707072,
      "grad_norm": 0.5295944809913635,
      "learning_rate": 0.00010170213777394591,
      "loss": 0.6254,
      "step": 16150
    },
    {
      "epoch": 1.02020202020202,
      "grad_norm": 0.5221174955368042,
      "learning_rate": 0.00010159991627940793,
      "loss": 0.5656,
      "step": 16160
    },
    {
      "epoch": 1.0208333333333333,
      "grad_norm": 0.585257887840271,
      "learning_rate": 0.00010149769311262413,
      "loss": 0.4913,
      "step": 16170
    },
    {
      "epoch": 1.0214646464646464,
      "grad_norm": 0.6038619875907898,
      "learning_rate": 0.00010139546838043909,
      "loss": 0.4524,
      "step": 16180
    },
    {
      "epoch": 1.0220959595959596,
      "grad_norm": 0.8553668856620789,
      "learning_rate": 0.00010129324218969894,
      "loss": 0.4396,
      "step": 16190
    },
    {
      "epoch": 1.0227272727272727,
      "grad_norm": 0.5342764854431152,
      "learning_rate": 0.00010119101464725125,
      "loss": 0.6722,
      "step": 16200
    },
    {
      "epoch": 1.0233585858585859,
      "grad_norm": 0.5965899229049683,
      "learning_rate": 0.00010108878585994517,
      "loss": 0.5584,
      "step": 16210
    },
    {
      "epoch": 1.023989898989899,
      "grad_norm": 0.5197022557258606,
      "learning_rate": 0.00010098655593463107,
      "loss": 0.4954,
      "step": 16220
    },
    {
      "epoch": 1.0246212121212122,
      "grad_norm": 0.5633710026741028,
      "learning_rate": 0.00010088432497816052,
      "loss": 0.4292,
      "step": 16230
    },
    {
      "epoch": 1.0252525252525253,
      "grad_norm": 0.7924883365631104,
      "learning_rate": 0.00010078209309738614,
      "loss": 0.4624,
      "step": 16240
    },
    {
      "epoch": 1.0258838383838385,
      "grad_norm": 0.4959127902984619,
      "learning_rate": 0.00010067986039916157,
      "loss": 0.6726,
      "step": 16250
    },
    {
      "epoch": 1.0265151515151516,
      "grad_norm": 0.5158752799034119,
      "learning_rate": 0.00010057762699034123,
      "loss": 0.5635,
      "step": 16260
    },
    {
      "epoch": 1.0271464646464648,
      "grad_norm": 0.6387549042701721,
      "learning_rate": 0.00010047539297778042,
      "loss": 0.4747,
      "step": 16270
    },
    {
      "epoch": 1.0277777777777777,
      "grad_norm": 0.5745179057121277,
      "learning_rate": 0.00010037315846833489,
      "loss": 0.464,
      "step": 16280
    },
    {
      "epoch": 1.0284090909090908,
      "grad_norm": 0.7287876009941101,
      "learning_rate": 0.00010027092356886102,
      "loss": 0.4536,
      "step": 16290
    },
    {
      "epoch": 1.029040404040404,
      "grad_norm": 0.5411181449890137,
      "learning_rate": 0.0001001686883862156,
      "loss": 0.6854,
      "step": 16300
    },
    {
      "epoch": 1.0296717171717171,
      "grad_norm": 0.5277042984962463,
      "learning_rate": 0.00010006645302725568,
      "loss": 0.5869,
      "step": 16310
    },
    {
      "epoch": 1.0303030303030303,
      "grad_norm": 0.5270055532455444,
      "learning_rate": 9.996421759883849e-05,
      "loss": 0.4899,
      "step": 16320
    },
    {
      "epoch": 1.0309343434343434,
      "grad_norm": 0.5066084861755371,
      "learning_rate": 9.986198220782135e-05,
      "loss": 0.433,
      "step": 16330
    },
    {
      "epoch": 1.0315656565656566,
      "grad_norm": 0.7880100011825562,
      "learning_rate": 9.97597469610616e-05,
      "loss": 0.4601,
      "step": 16340
    },
    {
      "epoch": 1.0321969696969697,
      "grad_norm": 0.49286118149757385,
      "learning_rate": 9.965751196541628e-05,
      "loss": 0.6599,
      "step": 16350
    },
    {
      "epoch": 1.0328282828282829,
      "grad_norm": 0.537778377532959,
      "learning_rate": 9.955527732774232e-05,
      "loss": 0.5718,
      "step": 16360
    },
    {
      "epoch": 1.033459595959596,
      "grad_norm": 0.4922434985637665,
      "learning_rate": 9.945304315489617e-05,
      "loss": 0.4928,
      "step": 16370
    },
    {
      "epoch": 1.0340909090909092,
      "grad_norm": 0.5365387797355652,
      "learning_rate": 9.93508095537339e-05,
      "loss": 0.4283,
      "step": 16380
    },
    {
      "epoch": 1.0347222222222223,
      "grad_norm": 0.9712038040161133,
      "learning_rate": 9.924857663111083e-05,
      "loss": 0.4812,
      "step": 16390
    },
    {
      "epoch": 1.0353535353535352,
      "grad_norm": 0.47451311349868774,
      "learning_rate": 9.914634449388173e-05,
      "loss": 0.6765,
      "step": 16400
    },
    {
      "epoch": 1.0359848484848484,
      "grad_norm": 0.49145764112472534,
      "learning_rate": 9.904411324890047e-05,
      "loss": 0.5627,
      "step": 16410
    },
    {
      "epoch": 1.0366161616161615,
      "grad_norm": 0.5579835176467896,
      "learning_rate": 9.894188300301996e-05,
      "loss": 0.4796,
      "step": 16420
    },
    {
      "epoch": 1.0372474747474747,
      "grad_norm": 0.53536456823349,
      "learning_rate": 9.883965386309214e-05,
      "loss": 0.4478,
      "step": 16430
    },
    {
      "epoch": 1.0378787878787878,
      "grad_norm": 0.7814289331436157,
      "learning_rate": 9.87374259359677e-05,
      "loss": 0.4917,
      "step": 16440
    },
    {
      "epoch": 1.038510101010101,
      "grad_norm": 0.4666963815689087,
      "learning_rate": 9.863519932849622e-05,
      "loss": 0.6403,
      "step": 16450
    },
    {
      "epoch": 1.0391414141414141,
      "grad_norm": 0.47612008452415466,
      "learning_rate": 9.853297414752569e-05,
      "loss": 0.5387,
      "step": 16460
    },
    {
      "epoch": 1.0397727272727273,
      "grad_norm": 0.5433072447776794,
      "learning_rate": 9.84307504999028e-05,
      "loss": 0.4818,
      "step": 16470
    },
    {
      "epoch": 1.0404040404040404,
      "grad_norm": 0.5782619118690491,
      "learning_rate": 9.83285284924725e-05,
      "loss": 0.45,
      "step": 16480
    },
    {
      "epoch": 1.0410353535353536,
      "grad_norm": 0.7566031217575073,
      "learning_rate": 9.822630823207813e-05,
      "loss": 0.4808,
      "step": 16490
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 0.41841715574264526,
      "learning_rate": 9.81240898255611e-05,
      "loss": 0.6287,
      "step": 16500
    },
    {
      "epoch": 1.04229797979798,
      "grad_norm": 0.5495050549507141,
      "learning_rate": 9.8021873379761e-05,
      "loss": 0.5777,
      "step": 16510
    },
    {
      "epoch": 1.0429292929292928,
      "grad_norm": 0.5126795768737793,
      "learning_rate": 9.791965900151525e-05,
      "loss": 0.5019,
      "step": 16520
    },
    {
      "epoch": 1.043560606060606,
      "grad_norm": 0.5592412948608398,
      "learning_rate": 9.78174467976592e-05,
      "loss": 0.4366,
      "step": 16530
    },
    {
      "epoch": 1.0441919191919191,
      "grad_norm": 0.6961165070533752,
      "learning_rate": 9.771523687502591e-05,
      "loss": 0.4673,
      "step": 16540
    },
    {
      "epoch": 1.0448232323232323,
      "grad_norm": 0.5230078101158142,
      "learning_rate": 9.761302934044598e-05,
      "loss": 0.6255,
      "step": 16550
    },
    {
      "epoch": 1.0454545454545454,
      "grad_norm": 0.5702651143074036,
      "learning_rate": 9.751082430074765e-05,
      "loss": 0.5567,
      "step": 16560
    },
    {
      "epoch": 1.0460858585858586,
      "grad_norm": 0.6453103423118591,
      "learning_rate": 9.74086218627564e-05,
      "loss": 0.5237,
      "step": 16570
    },
    {
      "epoch": 1.0467171717171717,
      "grad_norm": 0.4849470555782318,
      "learning_rate": 9.730642213329514e-05,
      "loss": 0.4559,
      "step": 16580
    },
    {
      "epoch": 1.0473484848484849,
      "grad_norm": 0.8288688063621521,
      "learning_rate": 9.720422521918381e-05,
      "loss": 0.4743,
      "step": 16590
    },
    {
      "epoch": 1.047979797979798,
      "grad_norm": 0.4851199686527252,
      "learning_rate": 9.710203122723954e-05,
      "loss": 0.6609,
      "step": 16600
    },
    {
      "epoch": 1.0486111111111112,
      "grad_norm": 0.46516045928001404,
      "learning_rate": 9.699984026427626e-05,
      "loss": 0.5758,
      "step": 16610
    },
    {
      "epoch": 1.0492424242424243,
      "grad_norm": 0.5310516953468323,
      "learning_rate": 9.689765243710489e-05,
      "loss": 0.5124,
      "step": 16620
    },
    {
      "epoch": 1.0498737373737375,
      "grad_norm": 0.5914387106895447,
      "learning_rate": 9.679546785253294e-05,
      "loss": 0.4438,
      "step": 16630
    },
    {
      "epoch": 1.0505050505050506,
      "grad_norm": 0.8239650726318359,
      "learning_rate": 9.669328661736464e-05,
      "loss": 0.4841,
      "step": 16640
    },
    {
      "epoch": 1.0511363636363635,
      "grad_norm": 0.5629608035087585,
      "learning_rate": 9.659110883840064e-05,
      "loss": 0.6689,
      "step": 16650
    },
    {
      "epoch": 1.0517676767676767,
      "grad_norm": 0.4741332232952118,
      "learning_rate": 9.648893462243797e-05,
      "loss": 0.5571,
      "step": 16660
    },
    {
      "epoch": 1.0523989898989898,
      "grad_norm": 0.5632776618003845,
      "learning_rate": 9.638676407627002e-05,
      "loss": 0.4934,
      "step": 16670
    },
    {
      "epoch": 1.053030303030303,
      "grad_norm": 0.6711219549179077,
      "learning_rate": 9.628459730668625e-05,
      "loss": 0.4488,
      "step": 16680
    },
    {
      "epoch": 1.0536616161616161,
      "grad_norm": 0.9275533556938171,
      "learning_rate": 9.618243442047227e-05,
      "loss": 0.4553,
      "step": 16690
    },
    {
      "epoch": 1.0542929292929293,
      "grad_norm": 0.5107008218765259,
      "learning_rate": 9.608027552440951e-05,
      "loss": 0.6921,
      "step": 16700
    },
    {
      "epoch": 1.0549242424242424,
      "grad_norm": 0.48894524574279785,
      "learning_rate": 9.597812072527535e-05,
      "loss": 0.5353,
      "step": 16710
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 0.5316181778907776,
      "learning_rate": 9.587597012984282e-05,
      "loss": 0.5001,
      "step": 16720
    },
    {
      "epoch": 1.0561868686868687,
      "grad_norm": 0.5444760918617249,
      "learning_rate": 9.577382384488057e-05,
      "loss": 0.449,
      "step": 16730
    },
    {
      "epoch": 1.0568181818181819,
      "grad_norm": 0.7648149728775024,
      "learning_rate": 9.567168197715273e-05,
      "loss": 0.4538,
      "step": 16740
    },
    {
      "epoch": 1.057449494949495,
      "grad_norm": 0.5076440572738647,
      "learning_rate": 9.556954463341886e-05,
      "loss": 0.6528,
      "step": 16750
    },
    {
      "epoch": 1.0580808080808082,
      "grad_norm": 0.46122071146965027,
      "learning_rate": 9.546741192043373e-05,
      "loss": 0.5516,
      "step": 16760
    },
    {
      "epoch": 1.058712121212121,
      "grad_norm": 0.5460119247436523,
      "learning_rate": 9.536528394494733e-05,
      "loss": 0.4994,
      "step": 16770
    },
    {
      "epoch": 1.0593434343434343,
      "grad_norm": 0.6389074921607971,
      "learning_rate": 9.526316081370465e-05,
      "loss": 0.4577,
      "step": 16780
    },
    {
      "epoch": 1.0599747474747474,
      "grad_norm": 0.7505120038986206,
      "learning_rate": 9.516104263344562e-05,
      "loss": 0.4949,
      "step": 16790
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 0.4791771471500397,
      "learning_rate": 9.505892951090504e-05,
      "loss": 0.6357,
      "step": 16800
    },
    {
      "epoch": 1.0612373737373737,
      "grad_norm": 0.4964117109775543,
      "learning_rate": 9.495682155281236e-05,
      "loss": 0.5324,
      "step": 16810
    },
    {
      "epoch": 1.0618686868686869,
      "grad_norm": 0.4948064088821411,
      "learning_rate": 9.485471886589171e-05,
      "loss": 0.4838,
      "step": 16820
    },
    {
      "epoch": 1.0625,
      "grad_norm": 0.5069225430488586,
      "learning_rate": 9.475262155686162e-05,
      "loss": 0.445,
      "step": 16830
    },
    {
      "epoch": 1.0631313131313131,
      "grad_norm": 0.7910954356193542,
      "learning_rate": 9.465052973243509e-05,
      "loss": 0.451,
      "step": 16840
    },
    {
      "epoch": 1.0637626262626263,
      "grad_norm": 0.5061965584754944,
      "learning_rate": 9.45484434993193e-05,
      "loss": 0.674,
      "step": 16850
    },
    {
      "epoch": 1.0643939393939394,
      "grad_norm": 0.516194224357605,
      "learning_rate": 9.444636296421567e-05,
      "loss": 0.5456,
      "step": 16860
    },
    {
      "epoch": 1.0650252525252526,
      "grad_norm": 0.57015061378479,
      "learning_rate": 9.434428823381959e-05,
      "loss": 0.4986,
      "step": 16870
    },
    {
      "epoch": 1.0656565656565657,
      "grad_norm": 0.5860256552696228,
      "learning_rate": 9.424221941482044e-05,
      "loss": 0.4625,
      "step": 16880
    },
    {
      "epoch": 1.066287878787879,
      "grad_norm": 1.0616750717163086,
      "learning_rate": 9.41401566139014e-05,
      "loss": 0.4505,
      "step": 16890
    },
    {
      "epoch": 1.0669191919191918,
      "grad_norm": 0.4561691880226135,
      "learning_rate": 9.403809993773931e-05,
      "loss": 0.6814,
      "step": 16900
    },
    {
      "epoch": 1.067550505050505,
      "grad_norm": 0.5624040365219116,
      "learning_rate": 9.393604949300472e-05,
      "loss": 0.557,
      "step": 16910
    },
    {
      "epoch": 1.0681818181818181,
      "grad_norm": 0.5173816084861755,
      "learning_rate": 9.383400538636155e-05,
      "loss": 0.4806,
      "step": 16920
    },
    {
      "epoch": 1.0688131313131313,
      "grad_norm": 0.5015897750854492,
      "learning_rate": 9.37319677244672e-05,
      "loss": 0.4406,
      "step": 16930
    },
    {
      "epoch": 1.0694444444444444,
      "grad_norm": 0.8604454398155212,
      "learning_rate": 9.362993661397222e-05,
      "loss": 0.4406,
      "step": 16940
    },
    {
      "epoch": 1.0700757575757576,
      "grad_norm": 0.5586884021759033,
      "learning_rate": 9.352791216152043e-05,
      "loss": 0.6947,
      "step": 16950
    },
    {
      "epoch": 1.0707070707070707,
      "grad_norm": 0.5237382650375366,
      "learning_rate": 9.342589447374859e-05,
      "loss": 0.5571,
      "step": 16960
    },
    {
      "epoch": 1.0713383838383839,
      "grad_norm": 0.5232495665550232,
      "learning_rate": 9.332388365728648e-05,
      "loss": 0.5107,
      "step": 16970
    },
    {
      "epoch": 1.071969696969697,
      "grad_norm": 0.5281140804290771,
      "learning_rate": 9.322187981875661e-05,
      "loss": 0.446,
      "step": 16980
    },
    {
      "epoch": 1.0726010101010102,
      "grad_norm": 0.7845908403396606,
      "learning_rate": 9.311988306477427e-05,
      "loss": 0.4876,
      "step": 16990
    },
    {
      "epoch": 1.0732323232323233,
      "grad_norm": 0.47692570090293884,
      "learning_rate": 9.301789350194732e-05,
      "loss": 0.6606,
      "step": 17000
    },
    {
      "epoch": 1.0732323232323233,
      "eval_loss": 0.5285595655441284,
      "eval_runtime": 27.4681,
      "eval_samples_per_second": 93.199,
      "eval_steps_per_second": 11.65,
      "step": 17000
    },
    {
      "epoch": 1.0738636363636365,
      "grad_norm": 0.5161764025688171,
      "learning_rate": 9.291591123687604e-05,
      "loss": 0.5571,
      "step": 17010
    },
    {
      "epoch": 1.0744949494949494,
      "grad_norm": 0.5069671869277954,
      "learning_rate": 9.281393637615322e-05,
      "loss": 0.4803,
      "step": 17020
    },
    {
      "epoch": 1.0751262626262625,
      "grad_norm": 0.5231711864471436,
      "learning_rate": 9.271196902636376e-05,
      "loss": 0.43,
      "step": 17030
    },
    {
      "epoch": 1.0757575757575757,
      "grad_norm": 0.8829436302185059,
      "learning_rate": 9.261000929408485e-05,
      "loss": 0.4637,
      "step": 17040
    },
    {
      "epoch": 1.0763888888888888,
      "grad_norm": 0.5509874820709229,
      "learning_rate": 9.250805728588559e-05,
      "loss": 0.6165,
      "step": 17050
    },
    {
      "epoch": 1.077020202020202,
      "grad_norm": 0.450594961643219,
      "learning_rate": 9.24061131083271e-05,
      "loss": 0.5477,
      "step": 17060
    },
    {
      "epoch": 1.0776515151515151,
      "grad_norm": 0.5617102980613708,
      "learning_rate": 9.230417686796227e-05,
      "loss": 0.4693,
      "step": 17070
    },
    {
      "epoch": 1.0782828282828283,
      "grad_norm": 0.5217280983924866,
      "learning_rate": 9.220224867133572e-05,
      "loss": 0.4361,
      "step": 17080
    },
    {
      "epoch": 1.0789141414141414,
      "grad_norm": 0.7813169956207275,
      "learning_rate": 9.21003286249836e-05,
      "loss": 0.4695,
      "step": 17090
    },
    {
      "epoch": 1.0795454545454546,
      "grad_norm": 0.50001060962677,
      "learning_rate": 9.199841683543365e-05,
      "loss": 0.6669,
      "step": 17100
    },
    {
      "epoch": 1.0801767676767677,
      "grad_norm": 0.5155724287033081,
      "learning_rate": 9.189651340920488e-05,
      "loss": 0.5287,
      "step": 17110
    },
    {
      "epoch": 1.0808080808080809,
      "grad_norm": 0.5209864974021912,
      "learning_rate": 9.179461845280762e-05,
      "loss": 0.4652,
      "step": 17120
    },
    {
      "epoch": 1.081439393939394,
      "grad_norm": 0.6313825845718384,
      "learning_rate": 9.169273207274333e-05,
      "loss": 0.4843,
      "step": 17130
    },
    {
      "epoch": 1.0820707070707072,
      "grad_norm": 0.6853444576263428,
      "learning_rate": 9.159085437550445e-05,
      "loss": 0.4765,
      "step": 17140
    },
    {
      "epoch": 1.08270202020202,
      "grad_norm": 0.5039776563644409,
      "learning_rate": 9.148898546757445e-05,
      "loss": 0.675,
      "step": 17150
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.4620496928691864,
      "learning_rate": 9.138712545542754e-05,
      "loss": 0.5512,
      "step": 17160
    },
    {
      "epoch": 1.0839646464646464,
      "grad_norm": 0.49504441022872925,
      "learning_rate": 9.128527444552864e-05,
      "loss": 0.4851,
      "step": 17170
    },
    {
      "epoch": 1.0845959595959596,
      "grad_norm": 0.5593615174293518,
      "learning_rate": 9.118343254433329e-05,
      "loss": 0.4323,
      "step": 17180
    },
    {
      "epoch": 1.0852272727272727,
      "grad_norm": 0.753055214881897,
      "learning_rate": 9.108159985828749e-05,
      "loss": 0.4774,
      "step": 17190
    },
    {
      "epoch": 1.0858585858585859,
      "grad_norm": 0.5396095514297485,
      "learning_rate": 9.097977649382758e-05,
      "loss": 0.6743,
      "step": 17200
    },
    {
      "epoch": 1.086489898989899,
      "grad_norm": 0.5961353182792664,
      "learning_rate": 9.087796255738024e-05,
      "loss": 0.5432,
      "step": 17210
    },
    {
      "epoch": 1.0871212121212122,
      "grad_norm": 0.5399110913276672,
      "learning_rate": 9.077615815536219e-05,
      "loss": 0.4952,
      "step": 17220
    },
    {
      "epoch": 1.0877525252525253,
      "grad_norm": 0.6189905405044556,
      "learning_rate": 9.067436339418027e-05,
      "loss": 0.4491,
      "step": 17230
    },
    {
      "epoch": 1.0883838383838385,
      "grad_norm": 0.6571228504180908,
      "learning_rate": 9.05725783802312e-05,
      "loss": 0.4633,
      "step": 17240
    },
    {
      "epoch": 1.0890151515151516,
      "grad_norm": 0.5123175382614136,
      "learning_rate": 9.04708032199015e-05,
      "loss": 0.6293,
      "step": 17250
    },
    {
      "epoch": 1.0896464646464645,
      "grad_norm": 0.4783455431461334,
      "learning_rate": 9.036903801956746e-05,
      "loss": 0.5555,
      "step": 17260
    },
    {
      "epoch": 1.0902777777777777,
      "grad_norm": 0.5398286581039429,
      "learning_rate": 9.026728288559487e-05,
      "loss": 0.498,
      "step": 17270
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.6067071557044983,
      "learning_rate": 9.016553792433907e-05,
      "loss": 0.4706,
      "step": 17280
    },
    {
      "epoch": 1.091540404040404,
      "grad_norm": 0.6810243129730225,
      "learning_rate": 9.00638032421447e-05,
      "loss": 0.4418,
      "step": 17290
    },
    {
      "epoch": 1.0921717171717171,
      "grad_norm": 0.460597962141037,
      "learning_rate": 8.996207894534573e-05,
      "loss": 0.6701,
      "step": 17300
    },
    {
      "epoch": 1.0928030303030303,
      "grad_norm": 0.5435577034950256,
      "learning_rate": 8.986036514026523e-05,
      "loss": 0.5752,
      "step": 17310
    },
    {
      "epoch": 1.0934343434343434,
      "grad_norm": 0.5049775242805481,
      "learning_rate": 8.975866193321533e-05,
      "loss": 0.4834,
      "step": 17320
    },
    {
      "epoch": 1.0940656565656566,
      "grad_norm": 0.5516049265861511,
      "learning_rate": 8.965696943049703e-05,
      "loss": 0.427,
      "step": 17330
    },
    {
      "epoch": 1.0946969696969697,
      "grad_norm": 0.7647221088409424,
      "learning_rate": 8.955528773840022e-05,
      "loss": 0.4607,
      "step": 17340
    },
    {
      "epoch": 1.0953282828282829,
      "grad_norm": 0.4620892107486725,
      "learning_rate": 8.945361696320341e-05,
      "loss": 0.6902,
      "step": 17350
    },
    {
      "epoch": 1.095959595959596,
      "grad_norm": 0.550468921661377,
      "learning_rate": 8.935195721117379e-05,
      "loss": 0.5718,
      "step": 17360
    },
    {
      "epoch": 1.0965909090909092,
      "grad_norm": 0.58933424949646,
      "learning_rate": 8.925030858856693e-05,
      "loss": 0.4541,
      "step": 17370
    },
    {
      "epoch": 1.0972222222222223,
      "grad_norm": 0.6594313979148865,
      "learning_rate": 8.914867120162681e-05,
      "loss": 0.4236,
      "step": 17380
    },
    {
      "epoch": 1.0978535353535355,
      "grad_norm": 0.864001452922821,
      "learning_rate": 8.904704515658572e-05,
      "loss": 0.4429,
      "step": 17390
    },
    {
      "epoch": 1.0984848484848484,
      "grad_norm": 0.5141990780830383,
      "learning_rate": 8.8945430559664e-05,
      "loss": 0.6824,
      "step": 17400
    },
    {
      "epoch": 1.0991161616161615,
      "grad_norm": 0.47007688879966736,
      "learning_rate": 8.88438275170701e-05,
      "loss": 0.5659,
      "step": 17410
    },
    {
      "epoch": 1.0997474747474747,
      "grad_norm": 0.6006884574890137,
      "learning_rate": 8.874223613500033e-05,
      "loss": 0.4813,
      "step": 17420
    },
    {
      "epoch": 1.1003787878787878,
      "grad_norm": 0.5883241891860962,
      "learning_rate": 8.86406565196389e-05,
      "loss": 0.4412,
      "step": 17430
    },
    {
      "epoch": 1.101010101010101,
      "grad_norm": 0.8758259415626526,
      "learning_rate": 8.853908877715762e-05,
      "loss": 0.4748,
      "step": 17440
    },
    {
      "epoch": 1.1016414141414141,
      "grad_norm": 0.47014492750167847,
      "learning_rate": 8.843753301371596e-05,
      "loss": 0.6675,
      "step": 17450
    },
    {
      "epoch": 1.1022727272727273,
      "grad_norm": 0.5432495474815369,
      "learning_rate": 8.833598933546083e-05,
      "loss": 0.5562,
      "step": 17460
    },
    {
      "epoch": 1.1029040404040404,
      "grad_norm": 0.6188808679580688,
      "learning_rate": 8.823445784852657e-05,
      "loss": 0.505,
      "step": 17470
    },
    {
      "epoch": 1.1035353535353536,
      "grad_norm": 0.5063956379890442,
      "learning_rate": 8.81329386590347e-05,
      "loss": 0.4239,
      "step": 17480
    },
    {
      "epoch": 1.1041666666666667,
      "grad_norm": 0.8028571605682373,
      "learning_rate": 8.803143187309388e-05,
      "loss": 0.4626,
      "step": 17490
    },
    {
      "epoch": 1.10479797979798,
      "grad_norm": 0.546577513217926,
      "learning_rate": 8.792993759679993e-05,
      "loss": 0.6406,
      "step": 17500
    },
    {
      "epoch": 1.1054292929292928,
      "grad_norm": 0.469923734664917,
      "learning_rate": 8.782845593623544e-05,
      "loss": 0.514,
      "step": 17510
    },
    {
      "epoch": 1.106060606060606,
      "grad_norm": 0.4967038929462433,
      "learning_rate": 8.772698699746993e-05,
      "loss": 0.4969,
      "step": 17520
    },
    {
      "epoch": 1.1066919191919191,
      "grad_norm": 0.6178613305091858,
      "learning_rate": 8.762553088655954e-05,
      "loss": 0.4351,
      "step": 17530
    },
    {
      "epoch": 1.1073232323232323,
      "grad_norm": 0.6884841918945312,
      "learning_rate": 8.75240877095471e-05,
      "loss": 0.4784,
      "step": 17540
    },
    {
      "epoch": 1.1079545454545454,
      "grad_norm": 0.43727409839630127,
      "learning_rate": 8.742265757246179e-05,
      "loss": 0.6497,
      "step": 17550
    },
    {
      "epoch": 1.1085858585858586,
      "grad_norm": 0.5113731026649475,
      "learning_rate": 8.732124058131928e-05,
      "loss": 0.5601,
      "step": 17560
    },
    {
      "epoch": 1.1092171717171717,
      "grad_norm": 0.502832293510437,
      "learning_rate": 8.721983684212143e-05,
      "loss": 0.4886,
      "step": 17570
    },
    {
      "epoch": 1.1098484848484849,
      "grad_norm": 0.5226873159408569,
      "learning_rate": 8.71184464608563e-05,
      "loss": 0.4588,
      "step": 17580
    },
    {
      "epoch": 1.110479797979798,
      "grad_norm": 0.8416422009468079,
      "learning_rate": 8.701706954349791e-05,
      "loss": 0.4752,
      "step": 17590
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.4795874357223511,
      "learning_rate": 8.691570619600629e-05,
      "loss": 0.6833,
      "step": 17600
    },
    {
      "epoch": 1.1117424242424243,
      "grad_norm": 0.48682600259780884,
      "learning_rate": 8.681435652432724e-05,
      "loss": 0.5677,
      "step": 17610
    },
    {
      "epoch": 1.1123737373737375,
      "grad_norm": 0.5446732640266418,
      "learning_rate": 8.671302063439227e-05,
      "loss": 0.5033,
      "step": 17620
    },
    {
      "epoch": 1.1130050505050506,
      "grad_norm": 0.6336302757263184,
      "learning_rate": 8.661169863211853e-05,
      "loss": 0.4458,
      "step": 17630
    },
    {
      "epoch": 1.1136363636363635,
      "grad_norm": 0.8577134013175964,
      "learning_rate": 8.651039062340858e-05,
      "loss": 0.4403,
      "step": 17640
    },
    {
      "epoch": 1.1142676767676767,
      "grad_norm": 0.4539087414741516,
      "learning_rate": 8.640909671415043e-05,
      "loss": 0.7011,
      "step": 17650
    },
    {
      "epoch": 1.1148989898989898,
      "grad_norm": 0.5096144080162048,
      "learning_rate": 8.630781701021726e-05,
      "loss": 0.5292,
      "step": 17660
    },
    {
      "epoch": 1.115530303030303,
      "grad_norm": 0.5313740372657776,
      "learning_rate": 8.620655161746752e-05,
      "loss": 0.5171,
      "step": 17670
    },
    {
      "epoch": 1.1161616161616161,
      "grad_norm": 0.6119078397750854,
      "learning_rate": 8.610530064174459e-05,
      "loss": 0.4512,
      "step": 17680
    },
    {
      "epoch": 1.1167929292929293,
      "grad_norm": 0.9361298680305481,
      "learning_rate": 8.600406418887688e-05,
      "loss": 0.4718,
      "step": 17690
    },
    {
      "epoch": 1.1174242424242424,
      "grad_norm": 0.4670369029045105,
      "learning_rate": 8.590284236467751e-05,
      "loss": 0.6659,
      "step": 17700
    },
    {
      "epoch": 1.1180555555555556,
      "grad_norm": 0.4811541438102722,
      "learning_rate": 8.580163527494443e-05,
      "loss": 0.5833,
      "step": 17710
    },
    {
      "epoch": 1.1186868686868687,
      "grad_norm": 0.59810870885849,
      "learning_rate": 8.57004430254601e-05,
      "loss": 0.4995,
      "step": 17720
    },
    {
      "epoch": 1.1193181818181819,
      "grad_norm": 0.5674394965171814,
      "learning_rate": 8.55992657219915e-05,
      "loss": 0.451,
      "step": 17730
    },
    {
      "epoch": 1.119949494949495,
      "grad_norm": 0.7885088920593262,
      "learning_rate": 8.549810347029001e-05,
      "loss": 0.4502,
      "step": 17740
    },
    {
      "epoch": 1.1205808080808082,
      "grad_norm": 0.5180090665817261,
      "learning_rate": 8.53969563760912e-05,
      "loss": 0.6479,
      "step": 17750
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 0.4340236783027649,
      "learning_rate": 8.529582454511493e-05,
      "loss": 0.5513,
      "step": 17760
    },
    {
      "epoch": 1.1218434343434343,
      "grad_norm": 0.5985410809516907,
      "learning_rate": 8.519470808306497e-05,
      "loss": 0.4891,
      "step": 17770
    },
    {
      "epoch": 1.1224747474747474,
      "grad_norm": 0.5328133702278137,
      "learning_rate": 8.509360709562912e-05,
      "loss": 0.4612,
      "step": 17780
    },
    {
      "epoch": 1.1231060606060606,
      "grad_norm": 0.8327333927154541,
      "learning_rate": 8.499252168847896e-05,
      "loss": 0.4722,
      "step": 17790
    },
    {
      "epoch": 1.1237373737373737,
      "grad_norm": 0.48439306020736694,
      "learning_rate": 8.48914519672698e-05,
      "loss": 0.6434,
      "step": 17800
    },
    {
      "epoch": 1.1243686868686869,
      "grad_norm": 0.4865387976169586,
      "learning_rate": 8.479039803764053e-05,
      "loss": 0.5438,
      "step": 17810
    },
    {
      "epoch": 1.125,
      "grad_norm": 0.5928113460540771,
      "learning_rate": 8.46893600052136e-05,
      "loss": 0.4698,
      "step": 17820
    },
    {
      "epoch": 1.1256313131313131,
      "grad_norm": 0.4809982180595398,
      "learning_rate": 8.458833797559477e-05,
      "loss": 0.4552,
      "step": 17830
    },
    {
      "epoch": 1.1262626262626263,
      "grad_norm": 0.7976271510124207,
      "learning_rate": 8.44873320543731e-05,
      "loss": 0.4792,
      "step": 17840
    },
    {
      "epoch": 1.1268939393939394,
      "grad_norm": 0.4961656332015991,
      "learning_rate": 8.438634234712085e-05,
      "loss": 0.6794,
      "step": 17850
    },
    {
      "epoch": 1.1275252525252526,
      "grad_norm": 0.5168903470039368,
      "learning_rate": 8.428536895939327e-05,
      "loss": 0.5242,
      "step": 17860
    },
    {
      "epoch": 1.1281565656565657,
      "grad_norm": 0.5541771054267883,
      "learning_rate": 8.41844119967286e-05,
      "loss": 0.4911,
      "step": 17870
    },
    {
      "epoch": 1.128787878787879,
      "grad_norm": 0.6875489950180054,
      "learning_rate": 8.408347156464787e-05,
      "loss": 0.4511,
      "step": 17880
    },
    {
      "epoch": 1.1294191919191918,
      "grad_norm": 0.7212429642677307,
      "learning_rate": 8.398254776865491e-05,
      "loss": 0.4755,
      "step": 17890
    },
    {
      "epoch": 1.130050505050505,
      "grad_norm": 0.5057896971702576,
      "learning_rate": 8.388164071423604e-05,
      "loss": 0.6628,
      "step": 17900
    },
    {
      "epoch": 1.1306818181818181,
      "grad_norm": 0.45573484897613525,
      "learning_rate": 8.378075050686023e-05,
      "loss": 0.5412,
      "step": 17910
    },
    {
      "epoch": 1.1313131313131313,
      "grad_norm": 0.5374221801757812,
      "learning_rate": 8.367987725197868e-05,
      "loss": 0.4999,
      "step": 17920
    },
    {
      "epoch": 1.1319444444444444,
      "grad_norm": 0.4883977472782135,
      "learning_rate": 8.357902105502504e-05,
      "loss": 0.4259,
      "step": 17930
    },
    {
      "epoch": 1.1325757575757576,
      "grad_norm": 0.6887547373771667,
      "learning_rate": 8.347818202141497e-05,
      "loss": 0.4602,
      "step": 17940
    },
    {
      "epoch": 1.1332070707070707,
      "grad_norm": 0.4789484143257141,
      "learning_rate": 8.337736025654633e-05,
      "loss": 0.6602,
      "step": 17950
    },
    {
      "epoch": 1.1338383838383839,
      "grad_norm": 0.5255754590034485,
      "learning_rate": 8.32765558657988e-05,
      "loss": 0.5694,
      "step": 17960
    },
    {
      "epoch": 1.134469696969697,
      "grad_norm": 0.526261568069458,
      "learning_rate": 8.3175768954534e-05,
      "loss": 0.4771,
      "step": 17970
    },
    {
      "epoch": 1.1351010101010102,
      "grad_norm": 0.5165016651153564,
      "learning_rate": 8.307499962809528e-05,
      "loss": 0.4515,
      "step": 17980
    },
    {
      "epoch": 1.1357323232323233,
      "grad_norm": 0.995589017868042,
      "learning_rate": 8.29742479918075e-05,
      "loss": 0.4615,
      "step": 17990
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 0.42680126428604126,
      "learning_rate": 8.28735141509772e-05,
      "loss": 0.6257,
      "step": 18000
    },
    {
      "epoch": 1.1363636363636362,
      "eval_loss": 0.5253483057022095,
      "eval_runtime": 27.3809,
      "eval_samples_per_second": 93.496,
      "eval_steps_per_second": 11.687,
      "step": 18000
    },
    {
      "epoch": 1.1369949494949494,
      "grad_norm": 0.529390811920166,
      "learning_rate": 8.277279821089213e-05,
      "loss": 0.5546,
      "step": 18010
    },
    {
      "epoch": 1.1376262626262625,
      "grad_norm": 0.5317897796630859,
      "learning_rate": 8.267210027682151e-05,
      "loss": 0.4995,
      "step": 18020
    },
    {
      "epoch": 1.1382575757575757,
      "grad_norm": 0.5987676382064819,
      "learning_rate": 8.257142045401559e-05,
      "loss": 0.4492,
      "step": 18030
    },
    {
      "epoch": 1.1388888888888888,
      "grad_norm": 0.8689323663711548,
      "learning_rate": 8.247075884770583e-05,
      "loss": 0.4639,
      "step": 18040
    },
    {
      "epoch": 1.139520202020202,
      "grad_norm": 0.4805876314640045,
      "learning_rate": 8.23701155631045e-05,
      "loss": 0.6539,
      "step": 18050
    },
    {
      "epoch": 1.1401515151515151,
      "grad_norm": 0.47371044754981995,
      "learning_rate": 8.226949070540487e-05,
      "loss": 0.5413,
      "step": 18060
    },
    {
      "epoch": 1.1407828282828283,
      "grad_norm": 0.5044762492179871,
      "learning_rate": 8.216888437978082e-05,
      "loss": 0.4971,
      "step": 18070
    },
    {
      "epoch": 1.1414141414141414,
      "grad_norm": 0.6505971550941467,
      "learning_rate": 8.206829669138693e-05,
      "loss": 0.4364,
      "step": 18080
    },
    {
      "epoch": 1.1420454545454546,
      "grad_norm": 0.6962739825248718,
      "learning_rate": 8.196772774535832e-05,
      "loss": 0.4564,
      "step": 18090
    },
    {
      "epoch": 1.1426767676767677,
      "grad_norm": 0.4560368061065674,
      "learning_rate": 8.186717764681042e-05,
      "loss": 0.6462,
      "step": 18100
    },
    {
      "epoch": 1.1433080808080809,
      "grad_norm": 0.5195703506469727,
      "learning_rate": 8.176664650083911e-05,
      "loss": 0.5212,
      "step": 18110
    },
    {
      "epoch": 1.143939393939394,
      "grad_norm": 0.5155909657478333,
      "learning_rate": 8.166613441252034e-05,
      "loss": 0.4893,
      "step": 18120
    },
    {
      "epoch": 1.1445707070707072,
      "grad_norm": 0.5430394411087036,
      "learning_rate": 8.156564148691018e-05,
      "loss": 0.4217,
      "step": 18130
    },
    {
      "epoch": 1.14520202020202,
      "grad_norm": 0.787203848361969,
      "learning_rate": 8.146516782904464e-05,
      "loss": 0.4723,
      "step": 18140
    },
    {
      "epoch": 1.1458333333333333,
      "grad_norm": 0.4363991916179657,
      "learning_rate": 8.136471354393969e-05,
      "loss": 0.6998,
      "step": 18150
    },
    {
      "epoch": 1.1464646464646464,
      "grad_norm": 0.49360090494155884,
      "learning_rate": 8.126427873659091e-05,
      "loss": 0.5673,
      "step": 18160
    },
    {
      "epoch": 1.1470959595959596,
      "grad_norm": 0.58037930727005,
      "learning_rate": 8.116386351197367e-05,
      "loss": 0.4551,
      "step": 18170
    },
    {
      "epoch": 1.1477272727272727,
      "grad_norm": 0.5553324222564697,
      "learning_rate": 8.106346797504276e-05,
      "loss": 0.4378,
      "step": 18180
    },
    {
      "epoch": 1.1483585858585859,
      "grad_norm": 0.869662344455719,
      "learning_rate": 8.096309223073239e-05,
      "loss": 0.4461,
      "step": 18190
    },
    {
      "epoch": 1.148989898989899,
      "grad_norm": 0.4729017913341522,
      "learning_rate": 8.086273638395619e-05,
      "loss": 0.6658,
      "step": 18200
    },
    {
      "epoch": 1.1496212121212122,
      "grad_norm": 0.529049813747406,
      "learning_rate": 8.076240053960686e-05,
      "loss": 0.5457,
      "step": 18210
    },
    {
      "epoch": 1.1502525252525253,
      "grad_norm": 0.5512956380844116,
      "learning_rate": 8.066208480255633e-05,
      "loss": 0.4825,
      "step": 18220
    },
    {
      "epoch": 1.1508838383838385,
      "grad_norm": 0.589235782623291,
      "learning_rate": 8.056178927765536e-05,
      "loss": 0.4431,
      "step": 18230
    },
    {
      "epoch": 1.1515151515151516,
      "grad_norm": 0.8152034282684326,
      "learning_rate": 8.046151406973376e-05,
      "loss": 0.4606,
      "step": 18240
    },
    {
      "epoch": 1.1521464646464645,
      "grad_norm": 0.44198018312454224,
      "learning_rate": 8.036125928359992e-05,
      "loss": 0.6271,
      "step": 18250
    },
    {
      "epoch": 1.1527777777777777,
      "grad_norm": 0.4428420960903168,
      "learning_rate": 8.026102502404104e-05,
      "loss": 0.5762,
      "step": 18260
    },
    {
      "epoch": 1.1534090909090908,
      "grad_norm": 0.50733482837677,
      "learning_rate": 8.016081139582276e-05,
      "loss": 0.5027,
      "step": 18270
    },
    {
      "epoch": 1.154040404040404,
      "grad_norm": 0.5446891784667969,
      "learning_rate": 8.006061850368923e-05,
      "loss": 0.4378,
      "step": 18280
    },
    {
      "epoch": 1.1546717171717171,
      "grad_norm": 0.7666568160057068,
      "learning_rate": 7.996044645236283e-05,
      "loss": 0.4698,
      "step": 18290
    },
    {
      "epoch": 1.1553030303030303,
      "grad_norm": 0.5225488543510437,
      "learning_rate": 7.986029534654431e-05,
      "loss": 0.6293,
      "step": 18300
    },
    {
      "epoch": 1.1559343434343434,
      "grad_norm": 0.4946850538253784,
      "learning_rate": 7.97601652909124e-05,
      "loss": 0.5657,
      "step": 18310
    },
    {
      "epoch": 1.1565656565656566,
      "grad_norm": 0.5096394419670105,
      "learning_rate": 7.966005639012383e-05,
      "loss": 0.4916,
      "step": 18320
    },
    {
      "epoch": 1.1571969696969697,
      "grad_norm": 0.4750211238861084,
      "learning_rate": 7.95599687488133e-05,
      "loss": 0.449,
      "step": 18330
    },
    {
      "epoch": 1.1578282828282829,
      "grad_norm": 0.7353947758674622,
      "learning_rate": 7.945990247159321e-05,
      "loss": 0.4768,
      "step": 18340
    },
    {
      "epoch": 1.158459595959596,
      "grad_norm": 0.4892811179161072,
      "learning_rate": 7.93598576630537e-05,
      "loss": 0.6374,
      "step": 18350
    },
    {
      "epoch": 1.1590909090909092,
      "grad_norm": 0.5272536873817444,
      "learning_rate": 7.925983442776241e-05,
      "loss": 0.5865,
      "step": 18360
    },
    {
      "epoch": 1.1597222222222223,
      "grad_norm": 0.5196574330329895,
      "learning_rate": 7.915983287026447e-05,
      "loss": 0.4865,
      "step": 18370
    },
    {
      "epoch": 1.1603535353535355,
      "grad_norm": 0.5909883379936218,
      "learning_rate": 7.905985309508232e-05,
      "loss": 0.4394,
      "step": 18380
    },
    {
      "epoch": 1.1609848484848484,
      "grad_norm": 0.7921611070632935,
      "learning_rate": 7.895989520671567e-05,
      "loss": 0.4593,
      "step": 18390
    },
    {
      "epoch": 1.1616161616161615,
      "grad_norm": 0.46757879853248596,
      "learning_rate": 7.885995930964131e-05,
      "loss": 0.6613,
      "step": 18400
    },
    {
      "epoch": 1.1622474747474747,
      "grad_norm": 0.5359594225883484,
      "learning_rate": 7.876004550831313e-05,
      "loss": 0.5366,
      "step": 18410
    },
    {
      "epoch": 1.1628787878787878,
      "grad_norm": 0.5272716879844666,
      "learning_rate": 7.866015390716182e-05,
      "loss": 0.4768,
      "step": 18420
    },
    {
      "epoch": 1.163510101010101,
      "grad_norm": 0.4700778126716614,
      "learning_rate": 7.856028461059488e-05,
      "loss": 0.4251,
      "step": 18430
    },
    {
      "epoch": 1.1641414141414141,
      "grad_norm": 0.8750786781311035,
      "learning_rate": 7.846043772299658e-05,
      "loss": 0.4533,
      "step": 18440
    },
    {
      "epoch": 1.1647727272727273,
      "grad_norm": 0.43443918228149414,
      "learning_rate": 7.83606133487277e-05,
      "loss": 0.6611,
      "step": 18450
    },
    {
      "epoch": 1.1654040404040404,
      "grad_norm": 0.5551744699478149,
      "learning_rate": 7.826081159212551e-05,
      "loss": 0.5483,
      "step": 18460
    },
    {
      "epoch": 1.1660353535353536,
      "grad_norm": 0.5870027542114258,
      "learning_rate": 7.816103255750361e-05,
      "loss": 0.4812,
      "step": 18470
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.6272733807563782,
      "learning_rate": 7.806127634915192e-05,
      "loss": 0.4616,
      "step": 18480
    },
    {
      "epoch": 1.16729797979798,
      "grad_norm": 0.7479315400123596,
      "learning_rate": 7.796154307133642e-05,
      "loss": 0.4645,
      "step": 18490
    },
    {
      "epoch": 1.1679292929292928,
      "grad_norm": 0.4741468131542206,
      "learning_rate": 7.786183282829918e-05,
      "loss": 0.6363,
      "step": 18500
    },
    {
      "epoch": 1.168560606060606,
      "grad_norm": 0.45523712038993835,
      "learning_rate": 7.776214572425816e-05,
      "loss": 0.5745,
      "step": 18510
    },
    {
      "epoch": 1.1691919191919191,
      "grad_norm": 0.5333874821662903,
      "learning_rate": 7.766248186340718e-05,
      "loss": 0.4985,
      "step": 18520
    },
    {
      "epoch": 1.1698232323232323,
      "grad_norm": 0.48828446865081787,
      "learning_rate": 7.756284134991568e-05,
      "loss": 0.435,
      "step": 18530
    },
    {
      "epoch": 1.1704545454545454,
      "grad_norm": 0.6885200142860413,
      "learning_rate": 7.746322428792882e-05,
      "loss": 0.455,
      "step": 18540
    },
    {
      "epoch": 1.1710858585858586,
      "grad_norm": 0.44531694054603577,
      "learning_rate": 7.736363078156714e-05,
      "loss": 0.6296,
      "step": 18550
    },
    {
      "epoch": 1.1717171717171717,
      "grad_norm": 0.5073379874229431,
      "learning_rate": 7.726406093492659e-05,
      "loss": 0.5452,
      "step": 18560
    },
    {
      "epoch": 1.1723484848484849,
      "grad_norm": 0.5039548277854919,
      "learning_rate": 7.716451485207843e-05,
      "loss": 0.4942,
      "step": 18570
    },
    {
      "epoch": 1.172979797979798,
      "grad_norm": 0.5002532005310059,
      "learning_rate": 7.706499263706901e-05,
      "loss": 0.4344,
      "step": 18580
    },
    {
      "epoch": 1.1736111111111112,
      "grad_norm": 0.7150200009346008,
      "learning_rate": 7.696549439391985e-05,
      "loss": 0.4524,
      "step": 18590
    },
    {
      "epoch": 1.1742424242424243,
      "grad_norm": 0.4652675688266754,
      "learning_rate": 7.686602022662725e-05,
      "loss": 0.6616,
      "step": 18600
    },
    {
      "epoch": 1.1748737373737375,
      "grad_norm": 0.4678129553794861,
      "learning_rate": 7.676657023916252e-05,
      "loss": 0.522,
      "step": 18610
    },
    {
      "epoch": 1.1755050505050506,
      "grad_norm": 0.4403233230113983,
      "learning_rate": 7.666714453547155e-05,
      "loss": 0.4883,
      "step": 18620
    },
    {
      "epoch": 1.1761363636363638,
      "grad_norm": 0.5112884640693665,
      "learning_rate": 7.656774321947495e-05,
      "loss": 0.4415,
      "step": 18630
    },
    {
      "epoch": 1.1767676767676767,
      "grad_norm": 0.8504379391670227,
      "learning_rate": 7.646836639506778e-05,
      "loss": 0.4583,
      "step": 18640
    },
    {
      "epoch": 1.1773989898989898,
      "grad_norm": 0.4823513925075531,
      "learning_rate": 7.636901416611954e-05,
      "loss": 0.6718,
      "step": 18650
    },
    {
      "epoch": 1.178030303030303,
      "grad_norm": 0.4756855368614197,
      "learning_rate": 7.626968663647401e-05,
      "loss": 0.5568,
      "step": 18660
    },
    {
      "epoch": 1.1786616161616161,
      "grad_norm": 0.4695776700973511,
      "learning_rate": 7.617038390994909e-05,
      "loss": 0.4864,
      "step": 18670
    },
    {
      "epoch": 1.1792929292929293,
      "grad_norm": 0.5455960631370544,
      "learning_rate": 7.60711060903369e-05,
      "loss": 0.4269,
      "step": 18680
    },
    {
      "epoch": 1.1799242424242424,
      "grad_norm": 0.6798410415649414,
      "learning_rate": 7.597185328140337e-05,
      "loss": 0.4662,
      "step": 18690
    },
    {
      "epoch": 1.1805555555555556,
      "grad_norm": 0.5450704097747803,
      "learning_rate": 7.587262558688841e-05,
      "loss": 0.6537,
      "step": 18700
    },
    {
      "epoch": 1.1811868686868687,
      "grad_norm": 0.4593014121055603,
      "learning_rate": 7.57734231105056e-05,
      "loss": 0.544,
      "step": 18710
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 0.45190489292144775,
      "learning_rate": 7.567424595594221e-05,
      "loss": 0.4978,
      "step": 18720
    },
    {
      "epoch": 1.182449494949495,
      "grad_norm": 0.6046778559684753,
      "learning_rate": 7.5575094226859e-05,
      "loss": 0.4368,
      "step": 18730
    },
    {
      "epoch": 1.1830808080808082,
      "grad_norm": 0.8185080289840698,
      "learning_rate": 7.547596802689023e-05,
      "loss": 0.4828,
      "step": 18740
    },
    {
      "epoch": 1.183712121212121,
      "grad_norm": 0.472097784280777,
      "learning_rate": 7.537686745964339e-05,
      "loss": 0.6358,
      "step": 18750
    },
    {
      "epoch": 1.1843434343434343,
      "grad_norm": 0.4375781714916229,
      "learning_rate": 7.527779262869921e-05,
      "loss": 0.5518,
      "step": 18760
    },
    {
      "epoch": 1.1849747474747474,
      "grad_norm": 0.48529329895973206,
      "learning_rate": 7.517874363761158e-05,
      "loss": 0.4728,
      "step": 18770
    },
    {
      "epoch": 1.1856060606060606,
      "grad_norm": 0.49113327264785767,
      "learning_rate": 7.507972058990725e-05,
      "loss": 0.4406,
      "step": 18780
    },
    {
      "epoch": 1.1862373737373737,
      "grad_norm": 0.8806625604629517,
      "learning_rate": 7.498072358908598e-05,
      "loss": 0.4528,
      "step": 18790
    },
    {
      "epoch": 1.1868686868686869,
      "grad_norm": 0.4602676331996918,
      "learning_rate": 7.488175273862023e-05,
      "loss": 0.6576,
      "step": 18800
    },
    {
      "epoch": 1.1875,
      "grad_norm": 0.48899713158607483,
      "learning_rate": 7.478280814195521e-05,
      "loss": 0.5497,
      "step": 18810
    },
    {
      "epoch": 1.1881313131313131,
      "grad_norm": 0.47932446002960205,
      "learning_rate": 7.468388990250857e-05,
      "loss": 0.4802,
      "step": 18820
    },
    {
      "epoch": 1.1887626262626263,
      "grad_norm": 0.5774960517883301,
      "learning_rate": 7.458499812367052e-05,
      "loss": 0.4343,
      "step": 18830
    },
    {
      "epoch": 1.1893939393939394,
      "grad_norm": 0.6550742983818054,
      "learning_rate": 7.448613290880353e-05,
      "loss": 0.4552,
      "step": 18840
    },
    {
      "epoch": 1.1900252525252526,
      "grad_norm": 0.4644404947757721,
      "learning_rate": 7.438729436124239e-05,
      "loss": 0.6497,
      "step": 18850
    },
    {
      "epoch": 1.1906565656565657,
      "grad_norm": 0.488452285528183,
      "learning_rate": 7.428848258429393e-05,
      "loss": 0.5437,
      "step": 18860
    },
    {
      "epoch": 1.191287878787879,
      "grad_norm": 0.48181673884391785,
      "learning_rate": 7.418969768123707e-05,
      "loss": 0.4967,
      "step": 18870
    },
    {
      "epoch": 1.1919191919191918,
      "grad_norm": 0.5398593544960022,
      "learning_rate": 7.409093975532258e-05,
      "loss": 0.4304,
      "step": 18880
    },
    {
      "epoch": 1.192550505050505,
      "grad_norm": 0.7669048309326172,
      "learning_rate": 7.399220890977313e-05,
      "loss": 0.4376,
      "step": 18890
    },
    {
      "epoch": 1.1931818181818181,
      "grad_norm": 0.4751874506473541,
      "learning_rate": 7.389350524778293e-05,
      "loss": 0.6488,
      "step": 18900
    },
    {
      "epoch": 1.1938131313131313,
      "grad_norm": 0.45342063903808594,
      "learning_rate": 7.379482887251792e-05,
      "loss": 0.5501,
      "step": 18910
    },
    {
      "epoch": 1.1944444444444444,
      "grad_norm": 0.48424434661865234,
      "learning_rate": 7.369617988711545e-05,
      "loss": 0.5033,
      "step": 18920
    },
    {
      "epoch": 1.1950757575757576,
      "grad_norm": 0.6554927825927734,
      "learning_rate": 7.359755839468427e-05,
      "loss": 0.4426,
      "step": 18930
    },
    {
      "epoch": 1.1957070707070707,
      "grad_norm": 1.1488062143325806,
      "learning_rate": 7.34989644983044e-05,
      "loss": 0.4877,
      "step": 18940
    },
    {
      "epoch": 1.1963383838383839,
      "grad_norm": 0.4572375416755676,
      "learning_rate": 7.340039830102693e-05,
      "loss": 0.6343,
      "step": 18950
    },
    {
      "epoch": 1.196969696969697,
      "grad_norm": 0.49416792392730713,
      "learning_rate": 7.330185990587418e-05,
      "loss": 0.5429,
      "step": 18960
    },
    {
      "epoch": 1.1976010101010102,
      "grad_norm": 0.49780532717704773,
      "learning_rate": 7.32033494158392e-05,
      "loss": 0.4731,
      "step": 18970
    },
    {
      "epoch": 1.1982323232323233,
      "grad_norm": 0.4408602714538574,
      "learning_rate": 7.3104866933886e-05,
      "loss": 0.4563,
      "step": 18980
    },
    {
      "epoch": 1.1988636363636362,
      "grad_norm": 0.746766209602356,
      "learning_rate": 7.300641256294931e-05,
      "loss": 0.4665,
      "step": 18990
    },
    {
      "epoch": 1.1994949494949494,
      "grad_norm": 0.46935126185417175,
      "learning_rate": 7.290798640593446e-05,
      "loss": 0.6514,
      "step": 19000
    },
    {
      "epoch": 1.1994949494949494,
      "eval_loss": 0.5221877098083496,
      "eval_runtime": 27.6897,
      "eval_samples_per_second": 92.453,
      "eval_steps_per_second": 11.557,
      "step": 19000
    },
    {
      "epoch": 1.2001262626262625,
      "grad_norm": 0.45242318511009216,
      "learning_rate": 7.280958856571727e-05,
      "loss": 0.551,
      "step": 19010
    },
    {
      "epoch": 1.2007575757575757,
      "grad_norm": 0.5324847102165222,
      "learning_rate": 7.271121914514396e-05,
      "loss": 0.5096,
      "step": 19020
    },
    {
      "epoch": 1.2013888888888888,
      "grad_norm": 0.5291740298271179,
      "learning_rate": 7.261287824703109e-05,
      "loss": 0.4384,
      "step": 19030
    },
    {
      "epoch": 1.202020202020202,
      "grad_norm": 0.9663389921188354,
      "learning_rate": 7.251456597416537e-05,
      "loss": 0.4779,
      "step": 19040
    },
    {
      "epoch": 1.2026515151515151,
      "grad_norm": 0.4497404396533966,
      "learning_rate": 7.241628242930362e-05,
      "loss": 0.6351,
      "step": 19050
    },
    {
      "epoch": 1.2032828282828283,
      "grad_norm": 0.5446988940238953,
      "learning_rate": 7.231802771517263e-05,
      "loss": 0.5314,
      "step": 19060
    },
    {
      "epoch": 1.2039141414141414,
      "grad_norm": 0.4733375012874603,
      "learning_rate": 7.2219801934469e-05,
      "loss": 0.4712,
      "step": 19070
    },
    {
      "epoch": 1.2045454545454546,
      "grad_norm": 0.6679375171661377,
      "learning_rate": 7.212160518985916e-05,
      "loss": 0.4859,
      "step": 19080
    },
    {
      "epoch": 1.2051767676767677,
      "grad_norm": 0.7175835967063904,
      "learning_rate": 7.202343758397918e-05,
      "loss": 0.4614,
      "step": 19090
    },
    {
      "epoch": 1.2058080808080809,
      "grad_norm": 0.45016443729400635,
      "learning_rate": 7.192529921943462e-05,
      "loss": 0.6495,
      "step": 19100
    },
    {
      "epoch": 1.206439393939394,
      "grad_norm": 0.4571230411529541,
      "learning_rate": 7.182719019880055e-05,
      "loss": 0.5352,
      "step": 19110
    },
    {
      "epoch": 1.2070707070707072,
      "grad_norm": 0.48315146565437317,
      "learning_rate": 7.172911062462127e-05,
      "loss": 0.4548,
      "step": 19120
    },
    {
      "epoch": 1.20770202020202,
      "grad_norm": 0.5598030686378479,
      "learning_rate": 7.163106059941046e-05,
      "loss": 0.4324,
      "step": 19130
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 0.7534079551696777,
      "learning_rate": 7.153304022565077e-05,
      "loss": 0.4608,
      "step": 19140
    },
    {
      "epoch": 1.2089646464646464,
      "grad_norm": 0.4329894185066223,
      "learning_rate": 7.143504960579386e-05,
      "loss": 0.6354,
      "step": 19150
    },
    {
      "epoch": 1.2095959595959596,
      "grad_norm": 0.5393070578575134,
      "learning_rate": 7.133708884226044e-05,
      "loss": 0.5413,
      "step": 19160
    },
    {
      "epoch": 1.2102272727272727,
      "grad_norm": 0.5109167098999023,
      "learning_rate": 7.123915803743982e-05,
      "loss": 0.4796,
      "step": 19170
    },
    {
      "epoch": 1.2108585858585859,
      "grad_norm": 0.5153391361236572,
      "learning_rate": 7.114125729369017e-05,
      "loss": 0.4373,
      "step": 19180
    },
    {
      "epoch": 1.211489898989899,
      "grad_norm": 0.8093287348747253,
      "learning_rate": 7.10433867133381e-05,
      "loss": 0.4636,
      "step": 19190
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 0.48193514347076416,
      "learning_rate": 7.09455463986788e-05,
      "loss": 0.6493,
      "step": 19200
    },
    {
      "epoch": 1.2127525252525253,
      "grad_norm": 0.4756128787994385,
      "learning_rate": 7.084773645197573e-05,
      "loss": 0.5426,
      "step": 19210
    },
    {
      "epoch": 1.2133838383838385,
      "grad_norm": 0.5096138715744019,
      "learning_rate": 7.074995697546072e-05,
      "loss": 0.4726,
      "step": 19220
    },
    {
      "epoch": 1.2140151515151516,
      "grad_norm": 0.5783759355545044,
      "learning_rate": 7.065220807133364e-05,
      "loss": 0.4517,
      "step": 19230
    },
    {
      "epoch": 1.2146464646464645,
      "grad_norm": 0.8214303255081177,
      "learning_rate": 7.055448984176248e-05,
      "loss": 0.4708,
      "step": 19240
    },
    {
      "epoch": 1.2152777777777777,
      "grad_norm": 0.4840848743915558,
      "learning_rate": 7.045680238888315e-05,
      "loss": 0.6675,
      "step": 19250
    },
    {
      "epoch": 1.2159090909090908,
      "grad_norm": 0.495682954788208,
      "learning_rate": 7.035914581479934e-05,
      "loss": 0.5631,
      "step": 19260
    },
    {
      "epoch": 1.216540404040404,
      "grad_norm": 0.5533711910247803,
      "learning_rate": 7.026152022158259e-05,
      "loss": 0.5002,
      "step": 19270
    },
    {
      "epoch": 1.2171717171717171,
      "grad_norm": 0.49384984374046326,
      "learning_rate": 7.01639257112719e-05,
      "loss": 0.4438,
      "step": 19280
    },
    {
      "epoch": 1.2178030303030303,
      "grad_norm": 0.7933764457702637,
      "learning_rate": 7.006636238587394e-05,
      "loss": 0.4892,
      "step": 19290
    },
    {
      "epoch": 1.2184343434343434,
      "grad_norm": 0.457105427980423,
      "learning_rate": 6.996883034736265e-05,
      "loss": 0.6499,
      "step": 19300
    },
    {
      "epoch": 1.2190656565656566,
      "grad_norm": 0.5594663023948669,
      "learning_rate": 6.987132969767935e-05,
      "loss": 0.5768,
      "step": 19310
    },
    {
      "epoch": 1.2196969696969697,
      "grad_norm": 0.4636578857898712,
      "learning_rate": 6.977386053873253e-05,
      "loss": 0.471,
      "step": 19320
    },
    {
      "epoch": 1.2203282828282829,
      "grad_norm": 0.4700497090816498,
      "learning_rate": 6.967642297239778e-05,
      "loss": 0.4247,
      "step": 19330
    },
    {
      "epoch": 1.220959595959596,
      "grad_norm": 0.7234543561935425,
      "learning_rate": 6.95790171005176e-05,
      "loss": 0.4599,
      "step": 19340
    },
    {
      "epoch": 1.2215909090909092,
      "grad_norm": 0.4767664074897766,
      "learning_rate": 6.948164302490148e-05,
      "loss": 0.5782,
      "step": 19350
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.4925253093242645,
      "learning_rate": 6.938430084732559e-05,
      "loss": 0.5382,
      "step": 19360
    },
    {
      "epoch": 1.2228535353535355,
      "grad_norm": 0.5171133875846863,
      "learning_rate": 6.928699066953275e-05,
      "loss": 0.4994,
      "step": 19370
    },
    {
      "epoch": 1.2234848484848484,
      "grad_norm": 0.5824904441833496,
      "learning_rate": 6.918971259323242e-05,
      "loss": 0.4484,
      "step": 19380
    },
    {
      "epoch": 1.2241161616161615,
      "grad_norm": 0.6376275420188904,
      "learning_rate": 6.909246672010039e-05,
      "loss": 0.4655,
      "step": 19390
    },
    {
      "epoch": 1.2247474747474747,
      "grad_norm": 0.46290260553359985,
      "learning_rate": 6.899525315177893e-05,
      "loss": 0.6315,
      "step": 19400
    },
    {
      "epoch": 1.2253787878787878,
      "grad_norm": 0.5054980516433716,
      "learning_rate": 6.889807198987641e-05,
      "loss": 0.5682,
      "step": 19410
    },
    {
      "epoch": 1.226010101010101,
      "grad_norm": 0.5157968997955322,
      "learning_rate": 6.880092333596741e-05,
      "loss": 0.4797,
      "step": 19420
    },
    {
      "epoch": 1.2266414141414141,
      "grad_norm": 0.4960765838623047,
      "learning_rate": 6.870380729159251e-05,
      "loss": 0.4296,
      "step": 19430
    },
    {
      "epoch": 1.2272727272727273,
      "grad_norm": 0.7788719534873962,
      "learning_rate": 6.860672395825822e-05,
      "loss": 0.4562,
      "step": 19440
    },
    {
      "epoch": 1.2279040404040404,
      "grad_norm": 0.48432376980781555,
      "learning_rate": 6.850967343743679e-05,
      "loss": 0.6455,
      "step": 19450
    },
    {
      "epoch": 1.2285353535353536,
      "grad_norm": 0.49752989411354065,
      "learning_rate": 6.841265583056632e-05,
      "loss": 0.5506,
      "step": 19460
    },
    {
      "epoch": 1.2291666666666667,
      "grad_norm": 0.5107890963554382,
      "learning_rate": 6.831567123905034e-05,
      "loss": 0.495,
      "step": 19470
    },
    {
      "epoch": 1.22979797979798,
      "grad_norm": 0.5609291791915894,
      "learning_rate": 6.8218719764258e-05,
      "loss": 0.4481,
      "step": 19480
    },
    {
      "epoch": 1.2304292929292928,
      "grad_norm": 0.6911831498146057,
      "learning_rate": 6.812180150752377e-05,
      "loss": 0.4427,
      "step": 19490
    },
    {
      "epoch": 1.231060606060606,
      "grad_norm": 0.42399993538856506,
      "learning_rate": 6.802491657014739e-05,
      "loss": 0.6355,
      "step": 19500
    },
    {
      "epoch": 1.2316919191919191,
      "grad_norm": 0.48136258125305176,
      "learning_rate": 6.792806505339384e-05,
      "loss": 0.5548,
      "step": 19510
    },
    {
      "epoch": 1.2323232323232323,
      "grad_norm": 0.4815998375415802,
      "learning_rate": 6.783124705849311e-05,
      "loss": 0.4912,
      "step": 19520
    },
    {
      "epoch": 1.2329545454545454,
      "grad_norm": 0.5290104746818542,
      "learning_rate": 6.77344626866402e-05,
      "loss": 0.4344,
      "step": 19530
    },
    {
      "epoch": 1.2335858585858586,
      "grad_norm": 0.7896630167961121,
      "learning_rate": 6.763771203899488e-05,
      "loss": 0.4843,
      "step": 19540
    },
    {
      "epoch": 1.2342171717171717,
      "grad_norm": 0.5004517436027527,
      "learning_rate": 6.75409952166818e-05,
      "loss": 0.6488,
      "step": 19550
    },
    {
      "epoch": 1.2348484848484849,
      "grad_norm": 0.5146282911300659,
      "learning_rate": 6.744431232079013e-05,
      "loss": 0.5754,
      "step": 19560
    },
    {
      "epoch": 1.235479797979798,
      "grad_norm": 0.5259457230567932,
      "learning_rate": 6.734766345237369e-05,
      "loss": 0.4804,
      "step": 19570
    },
    {
      "epoch": 1.2361111111111112,
      "grad_norm": 0.5813038349151611,
      "learning_rate": 6.725104871245063e-05,
      "loss": 0.4209,
      "step": 19580
    },
    {
      "epoch": 1.2367424242424243,
      "grad_norm": 0.697964608669281,
      "learning_rate": 6.715446820200351e-05,
      "loss": 0.4714,
      "step": 19590
    },
    {
      "epoch": 1.2373737373737375,
      "grad_norm": 0.4538722634315491,
      "learning_rate": 6.705792202197908e-05,
      "loss": 0.6119,
      "step": 19600
    },
    {
      "epoch": 1.2380050505050506,
      "grad_norm": 0.4695068597793579,
      "learning_rate": 6.696141027328817e-05,
      "loss": 0.5533,
      "step": 19610
    },
    {
      "epoch": 1.2386363636363638,
      "grad_norm": 0.48709988594055176,
      "learning_rate": 6.686493305680572e-05,
      "loss": 0.4949,
      "step": 19620
    },
    {
      "epoch": 1.2392676767676767,
      "grad_norm": 0.5488510727882385,
      "learning_rate": 6.676849047337048e-05,
      "loss": 0.4338,
      "step": 19630
    },
    {
      "epoch": 1.2398989898989898,
      "grad_norm": 0.8206371665000916,
      "learning_rate": 6.667208262378505e-05,
      "loss": 0.4733,
      "step": 19640
    },
    {
      "epoch": 1.240530303030303,
      "grad_norm": 0.4891751706600189,
      "learning_rate": 6.65757096088157e-05,
      "loss": 0.6369,
      "step": 19650
    },
    {
      "epoch": 1.2411616161616161,
      "grad_norm": 0.4659107029438019,
      "learning_rate": 6.647937152919235e-05,
      "loss": 0.5398,
      "step": 19660
    },
    {
      "epoch": 1.2417929292929293,
      "grad_norm": 0.48341241478919983,
      "learning_rate": 6.63830684856083e-05,
      "loss": 0.4984,
      "step": 19670
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 0.5395740270614624,
      "learning_rate": 6.628680057872034e-05,
      "loss": 0.4689,
      "step": 19680
    },
    {
      "epoch": 1.2430555555555556,
      "grad_norm": 0.7336998581886292,
      "learning_rate": 6.619056790914846e-05,
      "loss": 0.4337,
      "step": 19690
    },
    {
      "epoch": 1.2436868686868687,
      "grad_norm": 0.4955081641674042,
      "learning_rate": 6.609437057747587e-05,
      "loss": 0.6214,
      "step": 19700
    },
    {
      "epoch": 1.2443181818181819,
      "grad_norm": 0.4285346567630768,
      "learning_rate": 6.599820868424882e-05,
      "loss": 0.5696,
      "step": 19710
    },
    {
      "epoch": 1.244949494949495,
      "grad_norm": 0.4890213906764984,
      "learning_rate": 6.590208232997646e-05,
      "loss": 0.482,
      "step": 19720
    },
    {
      "epoch": 1.2455808080808082,
      "grad_norm": 0.5427502393722534,
      "learning_rate": 6.580599161513093e-05,
      "loss": 0.4309,
      "step": 19730
    },
    {
      "epoch": 1.246212121212121,
      "grad_norm": 0.7179339528083801,
      "learning_rate": 6.5709936640147e-05,
      "loss": 0.4429,
      "step": 19740
    },
    {
      "epoch": 1.2468434343434343,
      "grad_norm": 0.4716965854167938,
      "learning_rate": 6.561391750542214e-05,
      "loss": 0.6401,
      "step": 19750
    },
    {
      "epoch": 1.2474747474747474,
      "grad_norm": 0.5203694105148315,
      "learning_rate": 6.551793431131635e-05,
      "loss": 0.5457,
      "step": 19760
    },
    {
      "epoch": 1.2481060606060606,
      "grad_norm": 0.48590222001075745,
      "learning_rate": 6.542198715815205e-05,
      "loss": 0.4846,
      "step": 19770
    },
    {
      "epoch": 1.2487373737373737,
      "grad_norm": 0.5421984195709229,
      "learning_rate": 6.532607614621401e-05,
      "loss": 0.4626,
      "step": 19780
    },
    {
      "epoch": 1.2493686868686869,
      "grad_norm": 0.7683762311935425,
      "learning_rate": 6.523020137574923e-05,
      "loss": 0.4677,
      "step": 19790
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.4764084815979004,
      "learning_rate": 6.513436294696678e-05,
      "loss": 0.6579,
      "step": 19800
    },
    {
      "epoch": 1.2506313131313131,
      "grad_norm": 0.43963056802749634,
      "learning_rate": 6.503856096003783e-05,
      "loss": 0.5593,
      "step": 19810
    },
    {
      "epoch": 1.2512626262626263,
      "grad_norm": 0.46825310587882996,
      "learning_rate": 6.494279551509537e-05,
      "loss": 0.4832,
      "step": 19820
    },
    {
      "epoch": 1.2518939393939394,
      "grad_norm": 0.6663048267364502,
      "learning_rate": 6.484706671223427e-05,
      "loss": 0.4445,
      "step": 19830
    },
    {
      "epoch": 1.2525252525252526,
      "grad_norm": 0.8505446910858154,
      "learning_rate": 6.475137465151107e-05,
      "loss": 0.4517,
      "step": 19840
    },
    {
      "epoch": 1.2531565656565657,
      "grad_norm": 0.4367096424102783,
      "learning_rate": 6.465571943294385e-05,
      "loss": 0.6528,
      "step": 19850
    },
    {
      "epoch": 1.253787878787879,
      "grad_norm": 0.4428832530975342,
      "learning_rate": 6.45601011565123e-05,
      "loss": 0.5566,
      "step": 19860
    },
    {
      "epoch": 1.254419191919192,
      "grad_norm": 0.47530704736709595,
      "learning_rate": 6.446451992215744e-05,
      "loss": 0.4906,
      "step": 19870
    },
    {
      "epoch": 1.255050505050505,
      "grad_norm": 0.5173557996749878,
      "learning_rate": 6.436897582978151e-05,
      "loss": 0.4404,
      "step": 19880
    },
    {
      "epoch": 1.2556818181818181,
      "grad_norm": 0.7568047642707825,
      "learning_rate": 6.427346897924804e-05,
      "loss": 0.486,
      "step": 19890
    },
    {
      "epoch": 1.2563131313131313,
      "grad_norm": 0.4380193054676056,
      "learning_rate": 6.417799947038159e-05,
      "loss": 0.655,
      "step": 19900
    },
    {
      "epoch": 1.2569444444444444,
      "grad_norm": 0.5062862634658813,
      "learning_rate": 6.408256740296766e-05,
      "loss": 0.5243,
      "step": 19910
    },
    {
      "epoch": 1.2575757575757576,
      "grad_norm": 0.5449812412261963,
      "learning_rate": 6.398717287675265e-05,
      "loss": 0.4923,
      "step": 19920
    },
    {
      "epoch": 1.2582070707070707,
      "grad_norm": 0.49120938777923584,
      "learning_rate": 6.389181599144367e-05,
      "loss": 0.4369,
      "step": 19930
    },
    {
      "epoch": 1.2588383838383839,
      "grad_norm": 0.757641077041626,
      "learning_rate": 6.37964968467086e-05,
      "loss": 0.4684,
      "step": 19940
    },
    {
      "epoch": 1.259469696969697,
      "grad_norm": 0.4683571457862854,
      "learning_rate": 6.370121554217576e-05,
      "loss": 0.6158,
      "step": 19950
    },
    {
      "epoch": 1.2601010101010102,
      "grad_norm": 0.45599365234375,
      "learning_rate": 6.360597217743396e-05,
      "loss": 0.553,
      "step": 19960
    },
    {
      "epoch": 1.2607323232323233,
      "grad_norm": 0.4503135681152344,
      "learning_rate": 6.351076685203236e-05,
      "loss": 0.4885,
      "step": 19970
    },
    {
      "epoch": 1.2613636363636362,
      "grad_norm": 0.5521941781044006,
      "learning_rate": 6.341559966548036e-05,
      "loss": 0.4385,
      "step": 19980
    },
    {
      "epoch": 1.2619949494949494,
      "grad_norm": 0.6340996623039246,
      "learning_rate": 6.332047071724751e-05,
      "loss": 0.4354,
      "step": 19990
    },
    {
      "epoch": 1.2626262626262625,
      "grad_norm": 0.4666442275047302,
      "learning_rate": 6.322538010676334e-05,
      "loss": 0.6382,
      "step": 20000
    },
    {
      "epoch": 1.2626262626262625,
      "eval_loss": 0.5190016627311707,
      "eval_runtime": 28.1702,
      "eval_samples_per_second": 90.876,
      "eval_steps_per_second": 11.36,
      "step": 20000
    },
    {
      "epoch": 1.2632575757575757,
      "grad_norm": 0.4414881467819214,
      "learning_rate": 6.313032793341739e-05,
      "loss": 0.5473,
      "step": 20010
    },
    {
      "epoch": 1.2638888888888888,
      "grad_norm": 0.5707439184188843,
      "learning_rate": 6.303531429655893e-05,
      "loss": 0.4855,
      "step": 20020
    },
    {
      "epoch": 1.264520202020202,
      "grad_norm": 0.44746696949005127,
      "learning_rate": 6.294033929549707e-05,
      "loss": 0.4427,
      "step": 20030
    },
    {
      "epoch": 1.2651515151515151,
      "grad_norm": 0.7376410961151123,
      "learning_rate": 6.28454030295004e-05,
      "loss": 0.4821,
      "step": 20040
    },
    {
      "epoch": 1.2657828282828283,
      "grad_norm": 0.4672841429710388,
      "learning_rate": 6.275050559779714e-05,
      "loss": 0.6482,
      "step": 20050
    },
    {
      "epoch": 1.2664141414141414,
      "grad_norm": 0.5418099164962769,
      "learning_rate": 6.265564709957483e-05,
      "loss": 0.5333,
      "step": 20060
    },
    {
      "epoch": 1.2670454545454546,
      "grad_norm": 0.5425534844398499,
      "learning_rate": 6.256082763398042e-05,
      "loss": 0.4803,
      "step": 20070
    },
    {
      "epoch": 1.2676767676767677,
      "grad_norm": 0.5541210770606995,
      "learning_rate": 6.246604730011998e-05,
      "loss": 0.4272,
      "step": 20080
    },
    {
      "epoch": 1.2683080808080809,
      "grad_norm": 0.7219862341880798,
      "learning_rate": 6.237130619705866e-05,
      "loss": 0.4572,
      "step": 20090
    },
    {
      "epoch": 1.268939393939394,
      "grad_norm": 0.4995500147342682,
      "learning_rate": 6.227660442382071e-05,
      "loss": 0.6444,
      "step": 20100
    },
    {
      "epoch": 1.2695707070707072,
      "grad_norm": 0.5389330983161926,
      "learning_rate": 6.218194207938917e-05,
      "loss": 0.5439,
      "step": 20110
    },
    {
      "epoch": 1.2702020202020203,
      "grad_norm": 0.5086691379547119,
      "learning_rate": 6.208731926270592e-05,
      "loss": 0.4652,
      "step": 20120
    },
    {
      "epoch": 1.2708333333333333,
      "grad_norm": 0.5444271564483643,
      "learning_rate": 6.199273607267151e-05,
      "loss": 0.4498,
      "step": 20130
    },
    {
      "epoch": 1.2714646464646464,
      "grad_norm": 0.6745392680168152,
      "learning_rate": 6.18981926081451e-05,
      "loss": 0.4936,
      "step": 20140
    },
    {
      "epoch": 1.2720959595959596,
      "grad_norm": 0.4642731249332428,
      "learning_rate": 6.180368896794426e-05,
      "loss": 0.6332,
      "step": 20150
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 0.4208710491657257,
      "learning_rate": 6.170922525084504e-05,
      "loss": 0.5335,
      "step": 20160
    },
    {
      "epoch": 1.2733585858585859,
      "grad_norm": 0.4570563733577728,
      "learning_rate": 6.161480155558163e-05,
      "loss": 0.4694,
      "step": 20170
    },
    {
      "epoch": 1.273989898989899,
      "grad_norm": 0.5115138292312622,
      "learning_rate": 6.152041798084653e-05,
      "loss": 0.4351,
      "step": 20180
    },
    {
      "epoch": 1.2746212121212122,
      "grad_norm": 0.663151204586029,
      "learning_rate": 6.142607462529019e-05,
      "loss": 0.4626,
      "step": 20190
    },
    {
      "epoch": 1.2752525252525253,
      "grad_norm": 0.43917232751846313,
      "learning_rate": 6.133177158752108e-05,
      "loss": 0.6452,
      "step": 20200
    },
    {
      "epoch": 1.2758838383838385,
      "grad_norm": 0.425523966550827,
      "learning_rate": 6.123750896610555e-05,
      "loss": 0.5566,
      "step": 20210
    },
    {
      "epoch": 1.2765151515151514,
      "grad_norm": 0.49686992168426514,
      "learning_rate": 6.114328685956759e-05,
      "loss": 0.5019,
      "step": 20220
    },
    {
      "epoch": 1.2771464646464645,
      "grad_norm": 0.5938845872879028,
      "learning_rate": 6.104910536638903e-05,
      "loss": 0.4461,
      "step": 20230
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 0.7381828427314758,
      "learning_rate": 6.095496458500907e-05,
      "loss": 0.4581,
      "step": 20240
    },
    {
      "epoch": 1.2784090909090908,
      "grad_norm": 0.45920538902282715,
      "learning_rate": 6.0860864613824496e-05,
      "loss": 0.634,
      "step": 20250
    },
    {
      "epoch": 1.279040404040404,
      "grad_norm": 0.4789677858352661,
      "learning_rate": 6.0766805551189346e-05,
      "loss": 0.5309,
      "step": 20260
    },
    {
      "epoch": 1.2796717171717171,
      "grad_norm": 0.45103296637535095,
      "learning_rate": 6.067278749541495e-05,
      "loss": 0.46,
      "step": 20270
    },
    {
      "epoch": 1.2803030303030303,
      "grad_norm": 0.5412425994873047,
      "learning_rate": 6.057881054476974e-05,
      "loss": 0.4564,
      "step": 20280
    },
    {
      "epoch": 1.2809343434343434,
      "grad_norm": 0.7075912356376648,
      "learning_rate": 6.048487479747924e-05,
      "loss": 0.4563,
      "step": 20290
    },
    {
      "epoch": 1.2815656565656566,
      "grad_norm": 0.4233781397342682,
      "learning_rate": 6.0390980351725834e-05,
      "loss": 0.6443,
      "step": 20300
    },
    {
      "epoch": 1.2821969696969697,
      "grad_norm": 0.5166439414024353,
      "learning_rate": 6.029712730564879e-05,
      "loss": 0.5471,
      "step": 20310
    },
    {
      "epoch": 1.2828282828282829,
      "grad_norm": 0.4844439625740051,
      "learning_rate": 6.020331575734411e-05,
      "loss": 0.4719,
      "step": 20320
    },
    {
      "epoch": 1.283459595959596,
      "grad_norm": 0.46585404872894287,
      "learning_rate": 6.010954580486439e-05,
      "loss": 0.4295,
      "step": 20330
    },
    {
      "epoch": 1.2840909090909092,
      "grad_norm": 0.6814891695976257,
      "learning_rate": 6.001581754621875e-05,
      "loss": 0.4554,
      "step": 20340
    },
    {
      "epoch": 1.2847222222222223,
      "grad_norm": 0.4379822313785553,
      "learning_rate": 5.992213107937276e-05,
      "loss": 0.6161,
      "step": 20350
    },
    {
      "epoch": 1.2853535353535355,
      "grad_norm": 0.45136788487434387,
      "learning_rate": 5.98284865022483e-05,
      "loss": 0.5594,
      "step": 20360
    },
    {
      "epoch": 1.2859848484848486,
      "grad_norm": 0.5029224753379822,
      "learning_rate": 5.9734883912723425e-05,
      "loss": 0.4955,
      "step": 20370
    },
    {
      "epoch": 1.2866161616161615,
      "grad_norm": 0.5394028425216675,
      "learning_rate": 5.96413234086324e-05,
      "loss": 0.4394,
      "step": 20380
    },
    {
      "epoch": 1.2872474747474747,
      "grad_norm": 0.6789172887802124,
      "learning_rate": 5.954780508776537e-05,
      "loss": 0.4445,
      "step": 20390
    },
    {
      "epoch": 1.2878787878787878,
      "grad_norm": 0.40210697054862976,
      "learning_rate": 5.9454329047868526e-05,
      "loss": 0.7105,
      "step": 20400
    },
    {
      "epoch": 1.288510101010101,
      "grad_norm": 0.5056177377700806,
      "learning_rate": 5.9360895386643756e-05,
      "loss": 0.5558,
      "step": 20410
    },
    {
      "epoch": 1.2891414141414141,
      "grad_norm": 0.4858035147190094,
      "learning_rate": 5.9267504201748725e-05,
      "loss": 0.4728,
      "step": 20420
    },
    {
      "epoch": 1.2897727272727273,
      "grad_norm": 0.4977230727672577,
      "learning_rate": 5.917415559079666e-05,
      "loss": 0.4338,
      "step": 20430
    },
    {
      "epoch": 1.2904040404040404,
      "grad_norm": 0.6562464833259583,
      "learning_rate": 5.9080849651356294e-05,
      "loss": 0.4452,
      "step": 20440
    },
    {
      "epoch": 1.2910353535353536,
      "grad_norm": 0.4898117482662201,
      "learning_rate": 5.898758648095182e-05,
      "loss": 0.6098,
      "step": 20450
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 0.45555680990219116,
      "learning_rate": 5.889436617706261e-05,
      "loss": 0.5258,
      "step": 20460
    },
    {
      "epoch": 1.2922979797979797,
      "grad_norm": 0.47538965940475464,
      "learning_rate": 5.8801188837123356e-05,
      "loss": 0.4926,
      "step": 20470
    },
    {
      "epoch": 1.2929292929292928,
      "grad_norm": 0.47778239846229553,
      "learning_rate": 5.870805455852375e-05,
      "loss": 0.4087,
      "step": 20480
    },
    {
      "epoch": 1.293560606060606,
      "grad_norm": 0.7609968781471252,
      "learning_rate": 5.861496343860855e-05,
      "loss": 0.4936,
      "step": 20490
    },
    {
      "epoch": 1.2941919191919191,
      "grad_norm": 0.4300961494445801,
      "learning_rate": 5.8521915574677323e-05,
      "loss": 0.6471,
      "step": 20500
    },
    {
      "epoch": 1.2948232323232323,
      "grad_norm": 0.4249681234359741,
      "learning_rate": 5.842891106398451e-05,
      "loss": 0.5246,
      "step": 20510
    },
    {
      "epoch": 1.2954545454545454,
      "grad_norm": 0.4657164514064789,
      "learning_rate": 5.833595000373917e-05,
      "loss": 0.4696,
      "step": 20520
    },
    {
      "epoch": 1.2960858585858586,
      "grad_norm": 0.5060919523239136,
      "learning_rate": 5.8243032491105e-05,
      "loss": 0.4653,
      "step": 20530
    },
    {
      "epoch": 1.2967171717171717,
      "grad_norm": 0.6991802453994751,
      "learning_rate": 5.8150158623200143e-05,
      "loss": 0.4697,
      "step": 20540
    },
    {
      "epoch": 1.2973484848484849,
      "grad_norm": 0.4482983946800232,
      "learning_rate": 5.805732849709714e-05,
      "loss": 0.6235,
      "step": 20550
    },
    {
      "epoch": 1.297979797979798,
      "grad_norm": 0.42940041422843933,
      "learning_rate": 5.796454220982277e-05,
      "loss": 0.5635,
      "step": 20560
    },
    {
      "epoch": 1.2986111111111112,
      "grad_norm": 0.5086824297904968,
      "learning_rate": 5.787179985835811e-05,
      "loss": 0.4722,
      "step": 20570
    },
    {
      "epoch": 1.2992424242424243,
      "grad_norm": 0.5506146550178528,
      "learning_rate": 5.777910153963819e-05,
      "loss": 0.4466,
      "step": 20580
    },
    {
      "epoch": 1.2998737373737375,
      "grad_norm": 0.907427966594696,
      "learning_rate": 5.7686447350552065e-05,
      "loss": 0.4459,
      "step": 20590
    },
    {
      "epoch": 1.3005050505050506,
      "grad_norm": 0.4666186273097992,
      "learning_rate": 5.759383738794263e-05,
      "loss": 0.6482,
      "step": 20600
    },
    {
      "epoch": 1.3011363636363638,
      "grad_norm": 0.5053258538246155,
      "learning_rate": 5.750127174860667e-05,
      "loss": 0.5408,
      "step": 20610
    },
    {
      "epoch": 1.3017676767676767,
      "grad_norm": 0.5149907469749451,
      "learning_rate": 5.74087505292945e-05,
      "loss": 0.4873,
      "step": 20620
    },
    {
      "epoch": 1.3023989898989898,
      "grad_norm": 0.5483953952789307,
      "learning_rate": 5.731627382671008e-05,
      "loss": 0.4452,
      "step": 20630
    },
    {
      "epoch": 1.303030303030303,
      "grad_norm": 0.6864135265350342,
      "learning_rate": 5.7223841737510786e-05,
      "loss": 0.4604,
      "step": 20640
    },
    {
      "epoch": 1.3036616161616161,
      "grad_norm": 0.4468350112438202,
      "learning_rate": 5.713145435830749e-05,
      "loss": 0.6308,
      "step": 20650
    },
    {
      "epoch": 1.3042929292929293,
      "grad_norm": 0.47400206327438354,
      "learning_rate": 5.7039111785664215e-05,
      "loss": 0.5491,
      "step": 20660
    },
    {
      "epoch": 1.3049242424242424,
      "grad_norm": 0.46368589997291565,
      "learning_rate": 5.694681411609817e-05,
      "loss": 0.4973,
      "step": 20670
    },
    {
      "epoch": 1.3055555555555556,
      "grad_norm": 0.4960976541042328,
      "learning_rate": 5.685456144607966e-05,
      "loss": 0.4426,
      "step": 20680
    },
    {
      "epoch": 1.3061868686868687,
      "grad_norm": 0.6926572322845459,
      "learning_rate": 5.676235387203188e-05,
      "loss": 0.4581,
      "step": 20690
    },
    {
      "epoch": 1.3068181818181819,
      "grad_norm": 0.44552838802337646,
      "learning_rate": 5.6670191490331036e-05,
      "loss": 0.6166,
      "step": 20700
    },
    {
      "epoch": 1.307449494949495,
      "grad_norm": 0.4406771659851074,
      "learning_rate": 5.6578074397306e-05,
      "loss": 0.5415,
      "step": 20710
    },
    {
      "epoch": 1.308080808080808,
      "grad_norm": 0.49766069650650024,
      "learning_rate": 5.648600268923832e-05,
      "loss": 0.5005,
      "step": 20720
    },
    {
      "epoch": 1.308712121212121,
      "grad_norm": 0.519093930721283,
      "learning_rate": 5.639397646236204e-05,
      "loss": 0.4508,
      "step": 20730
    },
    {
      "epoch": 1.3093434343434343,
      "grad_norm": 0.6618961691856384,
      "learning_rate": 5.630199581286384e-05,
      "loss": 0.4572,
      "step": 20740
    },
    {
      "epoch": 1.3099747474747474,
      "grad_norm": 0.43184202909469604,
      "learning_rate": 5.62100608368826e-05,
      "loss": 0.6504,
      "step": 20750
    },
    {
      "epoch": 1.3106060606060606,
      "grad_norm": 0.4547349214553833,
      "learning_rate": 5.611817163050954e-05,
      "loss": 0.5416,
      "step": 20760
    },
    {
      "epoch": 1.3112373737373737,
      "grad_norm": 0.46161070466041565,
      "learning_rate": 5.602632828978799e-05,
      "loss": 0.491,
      "step": 20770
    },
    {
      "epoch": 1.3118686868686869,
      "grad_norm": 0.4892999827861786,
      "learning_rate": 5.5934530910713426e-05,
      "loss": 0.4381,
      "step": 20780
    },
    {
      "epoch": 1.3125,
      "grad_norm": 0.7340584993362427,
      "learning_rate": 5.584277958923321e-05,
      "loss": 0.455,
      "step": 20790
    },
    {
      "epoch": 1.3131313131313131,
      "grad_norm": 0.38653048872947693,
      "learning_rate": 5.5751074421246565e-05,
      "loss": 0.6517,
      "step": 20800
    },
    {
      "epoch": 1.3137626262626263,
      "grad_norm": 0.4502006769180298,
      "learning_rate": 5.5659415502604515e-05,
      "loss": 0.5381,
      "step": 20810
    },
    {
      "epoch": 1.3143939393939394,
      "grad_norm": 0.43679487705230713,
      "learning_rate": 5.556780292910968e-05,
      "loss": 0.5012,
      "step": 20820
    },
    {
      "epoch": 1.3150252525252526,
      "grad_norm": 0.5819414258003235,
      "learning_rate": 5.547623679651637e-05,
      "loss": 0.4362,
      "step": 20830
    },
    {
      "epoch": 1.3156565656565657,
      "grad_norm": 0.8579784035682678,
      "learning_rate": 5.5384717200530204e-05,
      "loss": 0.4701,
      "step": 20840
    },
    {
      "epoch": 1.316287878787879,
      "grad_norm": 0.4551079273223877,
      "learning_rate": 5.529324423680824e-05,
      "loss": 0.6015,
      "step": 20850
    },
    {
      "epoch": 1.316919191919192,
      "grad_norm": 0.5102733969688416,
      "learning_rate": 5.520181800095874e-05,
      "loss": 0.5498,
      "step": 20860
    },
    {
      "epoch": 1.317550505050505,
      "grad_norm": 0.48779749870300293,
      "learning_rate": 5.511043858854124e-05,
      "loss": 0.4695,
      "step": 20870
    },
    {
      "epoch": 1.3181818181818181,
      "grad_norm": 0.46082445979118347,
      "learning_rate": 5.501910609506621e-05,
      "loss": 0.4399,
      "step": 20880
    },
    {
      "epoch": 1.3188131313131313,
      "grad_norm": 0.7137327194213867,
      "learning_rate": 5.492782061599515e-05,
      "loss": 0.44,
      "step": 20890
    },
    {
      "epoch": 1.3194444444444444,
      "grad_norm": 0.44216838479042053,
      "learning_rate": 5.48365822467404e-05,
      "loss": 0.6311,
      "step": 20900
    },
    {
      "epoch": 1.3200757575757576,
      "grad_norm": 0.49105289578437805,
      "learning_rate": 5.474539108266501e-05,
      "loss": 0.5665,
      "step": 20910
    },
    {
      "epoch": 1.3207070707070707,
      "grad_norm": 0.44830822944641113,
      "learning_rate": 5.4654247219082833e-05,
      "loss": 0.4942,
      "step": 20920
    },
    {
      "epoch": 1.3213383838383839,
      "grad_norm": 0.5417020916938782,
      "learning_rate": 5.456315075125814e-05,
      "loss": 0.3931,
      "step": 20930
    },
    {
      "epoch": 1.321969696969697,
      "grad_norm": 0.7355385422706604,
      "learning_rate": 5.4472101774405726e-05,
      "loss": 0.4561,
      "step": 20940
    },
    {
      "epoch": 1.3226010101010102,
      "grad_norm": 0.4357534348964691,
      "learning_rate": 5.43811003836907e-05,
      "loss": 0.6534,
      "step": 20950
    },
    {
      "epoch": 1.3232323232323233,
      "grad_norm": 0.45392417907714844,
      "learning_rate": 5.429014667422854e-05,
      "loss": 0.5876,
      "step": 20960
    },
    {
      "epoch": 1.3238636363636362,
      "grad_norm": 0.46078217029571533,
      "learning_rate": 5.419924074108479e-05,
      "loss": 0.4861,
      "step": 20970
    },
    {
      "epoch": 1.3244949494949494,
      "grad_norm": 0.5232935547828674,
      "learning_rate": 5.410838267927507e-05,
      "loss": 0.4297,
      "step": 20980
    },
    {
      "epoch": 1.3251262626262625,
      "grad_norm": 0.7405436038970947,
      "learning_rate": 5.401757258376496e-05,
      "loss": 0.4434,
      "step": 20990
    },
    {
      "epoch": 1.3257575757575757,
      "grad_norm": 0.4317428767681122,
      "learning_rate": 5.3926810549470016e-05,
      "loss": 0.6181,
      "step": 21000
    },
    {
      "epoch": 1.3257575757575757,
      "eval_loss": 0.5173641443252563,
      "eval_runtime": 27.3495,
      "eval_samples_per_second": 93.603,
      "eval_steps_per_second": 11.7,
      "step": 21000
    },
    {
      "epoch": 1.3263888888888888,
      "grad_norm": 0.4617176353931427,
      "learning_rate": 5.3836096671255396e-05,
      "loss": 0.532,
      "step": 21010
    },
    {
      "epoch": 1.327020202020202,
      "grad_norm": 0.514692485332489,
      "learning_rate": 5.374543104393601e-05,
      "loss": 0.4404,
      "step": 21020
    },
    {
      "epoch": 1.3276515151515151,
      "grad_norm": 0.5293654203414917,
      "learning_rate": 5.3654813762276346e-05,
      "loss": 0.4372,
      "step": 21030
    },
    {
      "epoch": 1.3282828282828283,
      "grad_norm": 0.6653153300285339,
      "learning_rate": 5.356424492099028e-05,
      "loss": 0.4534,
      "step": 21040
    },
    {
      "epoch": 1.3289141414141414,
      "grad_norm": 0.4219818413257599,
      "learning_rate": 5.3473724614741204e-05,
      "loss": 0.6477,
      "step": 21050
    },
    {
      "epoch": 1.3295454545454546,
      "grad_norm": 0.4551902413368225,
      "learning_rate": 5.338325293814165e-05,
      "loss": 0.5374,
      "step": 21060
    },
    {
      "epoch": 1.3301767676767677,
      "grad_norm": 0.4801942706108093,
      "learning_rate": 5.3292829985753354e-05,
      "loss": 0.492,
      "step": 21070
    },
    {
      "epoch": 1.3308080808080809,
      "grad_norm": 0.49966374039649963,
      "learning_rate": 5.320245585208713e-05,
      "loss": 0.4593,
      "step": 21080
    },
    {
      "epoch": 1.331439393939394,
      "grad_norm": 0.7282687425613403,
      "learning_rate": 5.311213063160283e-05,
      "loss": 0.455,
      "step": 21090
    },
    {
      "epoch": 1.3320707070707072,
      "grad_norm": 0.45082786679267883,
      "learning_rate": 5.302185441870908e-05,
      "loss": 0.6235,
      "step": 21100
    },
    {
      "epoch": 1.3327020202020203,
      "grad_norm": 0.42022401094436646,
      "learning_rate": 5.293162730776336e-05,
      "loss": 0.5478,
      "step": 21110
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.4752803146839142,
      "learning_rate": 5.2841449393071696e-05,
      "loss": 0.4785,
      "step": 21120
    },
    {
      "epoch": 1.3339646464646464,
      "grad_norm": 0.4902014136314392,
      "learning_rate": 5.275132076888893e-05,
      "loss": 0.443,
      "step": 21130
    },
    {
      "epoch": 1.3345959595959596,
      "grad_norm": 0.6750965118408203,
      "learning_rate": 5.266124152941819e-05,
      "loss": 0.4637,
      "step": 21140
    },
    {
      "epoch": 1.3352272727272727,
      "grad_norm": 0.4007698893547058,
      "learning_rate": 5.257121176881103e-05,
      "loss": 0.6992,
      "step": 21150
    },
    {
      "epoch": 1.3358585858585859,
      "grad_norm": 0.45784276723861694,
      "learning_rate": 5.2481231581167336e-05,
      "loss": 0.5404,
      "step": 21160
    },
    {
      "epoch": 1.336489898989899,
      "grad_norm": 0.45798975229263306,
      "learning_rate": 5.239130106053509e-05,
      "loss": 0.4855,
      "step": 21170
    },
    {
      "epoch": 1.3371212121212122,
      "grad_norm": 0.8127303719520569,
      "learning_rate": 5.230142030091049e-05,
      "loss": 0.4286,
      "step": 21180
    },
    {
      "epoch": 1.3377525252525253,
      "grad_norm": 0.6886858940124512,
      "learning_rate": 5.221158939623761e-05,
      "loss": 0.468,
      "step": 21190
    },
    {
      "epoch": 1.3383838383838385,
      "grad_norm": 0.4558931887149811,
      "learning_rate": 5.212180844040848e-05,
      "loss": 0.65,
      "step": 21200
    },
    {
      "epoch": 1.3390151515151514,
      "grad_norm": 0.41850024461746216,
      "learning_rate": 5.2032077527262854e-05,
      "loss": 0.5383,
      "step": 21210
    },
    {
      "epoch": 1.3396464646464645,
      "grad_norm": 0.4657609164714813,
      "learning_rate": 5.1942396750588294e-05,
      "loss": 0.4734,
      "step": 21220
    },
    {
      "epoch": 1.3402777777777777,
      "grad_norm": 0.5403146743774414,
      "learning_rate": 5.1852766204119876e-05,
      "loss": 0.4285,
      "step": 21230
    },
    {
      "epoch": 1.3409090909090908,
      "grad_norm": 0.6852820515632629,
      "learning_rate": 5.176318598154017e-05,
      "loss": 0.4211,
      "step": 21240
    },
    {
      "epoch": 1.341540404040404,
      "grad_norm": 0.4335922300815582,
      "learning_rate": 5.167365617647917e-05,
      "loss": 0.631,
      "step": 21250
    },
    {
      "epoch": 1.3421717171717171,
      "grad_norm": 0.46229445934295654,
      "learning_rate": 5.158417688251415e-05,
      "loss": 0.5474,
      "step": 21260
    },
    {
      "epoch": 1.3428030303030303,
      "grad_norm": 0.674629271030426,
      "learning_rate": 5.149474819316966e-05,
      "loss": 0.4746,
      "step": 21270
    },
    {
      "epoch": 1.3434343434343434,
      "grad_norm": 0.4452902674674988,
      "learning_rate": 5.140537020191731e-05,
      "loss": 0.4378,
      "step": 21280
    },
    {
      "epoch": 1.3440656565656566,
      "grad_norm": 0.7994735836982727,
      "learning_rate": 5.1316043002175695e-05,
      "loss": 0.4605,
      "step": 21290
    },
    {
      "epoch": 1.3446969696969697,
      "grad_norm": 0.46155163645744324,
      "learning_rate": 5.12267666873103e-05,
      "loss": 0.6508,
      "step": 21300
    },
    {
      "epoch": 1.3453282828282829,
      "grad_norm": 0.42523056268692017,
      "learning_rate": 5.1137541350633566e-05,
      "loss": 0.5289,
      "step": 21310
    },
    {
      "epoch": 1.345959595959596,
      "grad_norm": 0.4760738015174866,
      "learning_rate": 5.10483670854045e-05,
      "loss": 0.4868,
      "step": 21320
    },
    {
      "epoch": 1.3465909090909092,
      "grad_norm": 0.6022870540618896,
      "learning_rate": 5.0959243984828805e-05,
      "loss": 0.4548,
      "step": 21330
    },
    {
      "epoch": 1.3472222222222223,
      "grad_norm": 0.8091849684715271,
      "learning_rate": 5.087017214205865e-05,
      "loss": 0.4504,
      "step": 21340
    },
    {
      "epoch": 1.3478535353535355,
      "grad_norm": 0.4387182891368866,
      "learning_rate": 5.07811516501927e-05,
      "loss": 0.6368,
      "step": 21350
    },
    {
      "epoch": 1.3484848484848486,
      "grad_norm": 0.4882941246032715,
      "learning_rate": 5.0692182602275926e-05,
      "loss": 0.5226,
      "step": 21360
    },
    {
      "epoch": 1.3491161616161615,
      "grad_norm": 0.5363661050796509,
      "learning_rate": 5.060326509129949e-05,
      "loss": 0.4866,
      "step": 21370
    },
    {
      "epoch": 1.3497474747474747,
      "grad_norm": 0.42393821477890015,
      "learning_rate": 5.0514399210200715e-05,
      "loss": 0.4126,
      "step": 21380
    },
    {
      "epoch": 1.3503787878787878,
      "grad_norm": 0.7000961899757385,
      "learning_rate": 5.0425585051862926e-05,
      "loss": 0.4569,
      "step": 21390
    },
    {
      "epoch": 1.351010101010101,
      "grad_norm": 0.4222504496574402,
      "learning_rate": 5.0336822709115474e-05,
      "loss": 0.6124,
      "step": 21400
    },
    {
      "epoch": 1.3516414141414141,
      "grad_norm": 0.4944135844707489,
      "learning_rate": 5.024811227473348e-05,
      "loss": 0.523,
      "step": 21410
    },
    {
      "epoch": 1.3522727272727273,
      "grad_norm": 0.4668051302433014,
      "learning_rate": 5.015945384143783e-05,
      "loss": 0.4814,
      "step": 21420
    },
    {
      "epoch": 1.3529040404040404,
      "grad_norm": 0.44435033202171326,
      "learning_rate": 5.007084750189501e-05,
      "loss": 0.4459,
      "step": 21430
    },
    {
      "epoch": 1.3535353535353536,
      "grad_norm": 0.6274774670600891,
      "learning_rate": 4.998229334871718e-05,
      "loss": 0.4561,
      "step": 21440
    },
    {
      "epoch": 1.3541666666666667,
      "grad_norm": 0.41578224301338196,
      "learning_rate": 4.989379147446183e-05,
      "loss": 0.6401,
      "step": 21450
    },
    {
      "epoch": 1.3547979797979797,
      "grad_norm": 0.44388896226882935,
      "learning_rate": 4.980534197163186e-05,
      "loss": 0.5694,
      "step": 21460
    },
    {
      "epoch": 1.3554292929292928,
      "grad_norm": 0.4203701317310333,
      "learning_rate": 4.971694493267539e-05,
      "loss": 0.4849,
      "step": 21470
    },
    {
      "epoch": 1.356060606060606,
      "grad_norm": 0.4643706679344177,
      "learning_rate": 4.962860044998582e-05,
      "loss": 0.4314,
      "step": 21480
    },
    {
      "epoch": 1.3566919191919191,
      "grad_norm": 0.7142492532730103,
      "learning_rate": 4.9540308615901474e-05,
      "loss": 0.4548,
      "step": 21490
    },
    {
      "epoch": 1.3573232323232323,
      "grad_norm": 0.44119957089424133,
      "learning_rate": 4.945206952270575e-05,
      "loss": 0.6206,
      "step": 21500
    },
    {
      "epoch": 1.3579545454545454,
      "grad_norm": 0.4871567189693451,
      "learning_rate": 4.9363883262626845e-05,
      "loss": 0.5471,
      "step": 21510
    },
    {
      "epoch": 1.3585858585858586,
      "grad_norm": 0.44828560948371887,
      "learning_rate": 4.927574992783774e-05,
      "loss": 0.4829,
      "step": 21520
    },
    {
      "epoch": 1.3592171717171717,
      "grad_norm": 0.4987519085407257,
      "learning_rate": 4.918766961045623e-05,
      "loss": 0.4346,
      "step": 21530
    },
    {
      "epoch": 1.3598484848484849,
      "grad_norm": 0.6764976382255554,
      "learning_rate": 4.909964240254453e-05,
      "loss": 0.4484,
      "step": 21540
    },
    {
      "epoch": 1.360479797979798,
      "grad_norm": 0.4455175995826721,
      "learning_rate": 4.9011668396109424e-05,
      "loss": 0.6328,
      "step": 21550
    },
    {
      "epoch": 1.3611111111111112,
      "grad_norm": 0.41613808274269104,
      "learning_rate": 4.892374768310203e-05,
      "loss": 0.5492,
      "step": 21560
    },
    {
      "epoch": 1.3617424242424243,
      "grad_norm": 0.43143635988235474,
      "learning_rate": 4.8835880355417874e-05,
      "loss": 0.473,
      "step": 21570
    },
    {
      "epoch": 1.3623737373737375,
      "grad_norm": 0.572159469127655,
      "learning_rate": 4.874806650489658e-05,
      "loss": 0.4447,
      "step": 21580
    },
    {
      "epoch": 1.3630050505050506,
      "grad_norm": 0.8295307755470276,
      "learning_rate": 4.866030622332194e-05,
      "loss": 0.4547,
      "step": 21590
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.46078312397003174,
      "learning_rate": 4.857259960242171e-05,
      "loss": 0.6793,
      "step": 21600
    },
    {
      "epoch": 1.3642676767676767,
      "grad_norm": 0.41178274154663086,
      "learning_rate": 4.848494673386753e-05,
      "loss": 0.5361,
      "step": 21610
    },
    {
      "epoch": 1.3648989898989898,
      "grad_norm": 0.48625344038009644,
      "learning_rate": 4.839734770927501e-05,
      "loss": 0.4826,
      "step": 21620
    },
    {
      "epoch": 1.365530303030303,
      "grad_norm": 0.6048316955566406,
      "learning_rate": 4.830980262020331e-05,
      "loss": 0.4207,
      "step": 21630
    },
    {
      "epoch": 1.3661616161616161,
      "grad_norm": 0.6643826961517334,
      "learning_rate": 4.8222311558155306e-05,
      "loss": 0.4329,
      "step": 21640
    },
    {
      "epoch": 1.3667929292929293,
      "grad_norm": 0.42011335492134094,
      "learning_rate": 4.813487461457734e-05,
      "loss": 0.698,
      "step": 21650
    },
    {
      "epoch": 1.3674242424242424,
      "grad_norm": 0.46360349655151367,
      "learning_rate": 4.804749188085931e-05,
      "loss": 0.5704,
      "step": 21660
    },
    {
      "epoch": 1.3680555555555556,
      "grad_norm": 0.46925660967826843,
      "learning_rate": 4.7960163448334316e-05,
      "loss": 0.4691,
      "step": 21670
    },
    {
      "epoch": 1.3686868686868687,
      "grad_norm": 0.47553446888923645,
      "learning_rate": 4.787288940827879e-05,
      "loss": 0.4473,
      "step": 21680
    },
    {
      "epoch": 1.3693181818181819,
      "grad_norm": 0.6944635510444641,
      "learning_rate": 4.778566985191223e-05,
      "loss": 0.4496,
      "step": 21690
    },
    {
      "epoch": 1.369949494949495,
      "grad_norm": 0.4606513977050781,
      "learning_rate": 4.769850487039732e-05,
      "loss": 0.6611,
      "step": 21700
    },
    {
      "epoch": 1.370580808080808,
      "grad_norm": 0.43288660049438477,
      "learning_rate": 4.7611394554839594e-05,
      "loss": 0.569,
      "step": 21710
    },
    {
      "epoch": 1.371212121212121,
      "grad_norm": 0.45480942726135254,
      "learning_rate": 4.7524338996287456e-05,
      "loss": 0.4945,
      "step": 21720
    },
    {
      "epoch": 1.3718434343434343,
      "grad_norm": 0.5464022755622864,
      "learning_rate": 4.7437338285732116e-05,
      "loss": 0.4351,
      "step": 21730
    },
    {
      "epoch": 1.3724747474747474,
      "grad_norm": 0.7756557464599609,
      "learning_rate": 4.73503925141074e-05,
      "loss": 0.4479,
      "step": 21740
    },
    {
      "epoch": 1.3731060606060606,
      "grad_norm": 0.44913992285728455,
      "learning_rate": 4.726350177228982e-05,
      "loss": 0.6583,
      "step": 21750
    },
    {
      "epoch": 1.3737373737373737,
      "grad_norm": 0.47316861152648926,
      "learning_rate": 4.717666615109827e-05,
      "loss": 0.5534,
      "step": 21760
    },
    {
      "epoch": 1.3743686868686869,
      "grad_norm": 0.4342898428440094,
      "learning_rate": 4.708988574129406e-05,
      "loss": 0.4966,
      "step": 21770
    },
    {
      "epoch": 1.375,
      "grad_norm": 0.49957695603370667,
      "learning_rate": 4.7003160633580754e-05,
      "loss": 0.4297,
      "step": 21780
    },
    {
      "epoch": 1.3756313131313131,
      "grad_norm": 0.7190739512443542,
      "learning_rate": 4.6916490918604226e-05,
      "loss": 0.4582,
      "step": 21790
    },
    {
      "epoch": 1.3762626262626263,
      "grad_norm": 0.4397237002849579,
      "learning_rate": 4.6829876686952365e-05,
      "loss": 0.6085,
      "step": 21800
    },
    {
      "epoch": 1.3768939393939394,
      "grad_norm": 0.4551575779914856,
      "learning_rate": 4.674331802915507e-05,
      "loss": 0.5492,
      "step": 21810
    },
    {
      "epoch": 1.3775252525252526,
      "grad_norm": 0.4817552864551544,
      "learning_rate": 4.665681503568412e-05,
      "loss": 0.4584,
      "step": 21820
    },
    {
      "epoch": 1.3781565656565657,
      "grad_norm": 0.46759918332099915,
      "learning_rate": 4.657036779695326e-05,
      "loss": 0.4266,
      "step": 21830
    },
    {
      "epoch": 1.378787878787879,
      "grad_norm": 0.7140228152275085,
      "learning_rate": 4.648397640331782e-05,
      "loss": 0.4576,
      "step": 21840
    },
    {
      "epoch": 1.379419191919192,
      "grad_norm": 0.3977717161178589,
      "learning_rate": 4.6397640945074796e-05,
      "loss": 0.6567,
      "step": 21850
    },
    {
      "epoch": 1.380050505050505,
      "grad_norm": 0.42709559202194214,
      "learning_rate": 4.6311361512462724e-05,
      "loss": 0.5361,
      "step": 21860
    },
    {
      "epoch": 1.3806818181818181,
      "grad_norm": 0.4809894561767578,
      "learning_rate": 4.622513819566156e-05,
      "loss": 0.4676,
      "step": 21870
    },
    {
      "epoch": 1.3813131313131313,
      "grad_norm": 0.5507734417915344,
      "learning_rate": 4.613897108479268e-05,
      "loss": 0.4547,
      "step": 21880
    },
    {
      "epoch": 1.3819444444444444,
      "grad_norm": 0.6481397151947021,
      "learning_rate": 4.605286026991866e-05,
      "loss": 0.4571,
      "step": 21890
    },
    {
      "epoch": 1.3825757575757576,
      "grad_norm": 0.43512266874313354,
      "learning_rate": 4.5966805841043213e-05,
      "loss": 0.6485,
      "step": 21900
    },
    {
      "epoch": 1.3832070707070707,
      "grad_norm": 0.4350760877132416,
      "learning_rate": 4.588080788811111e-05,
      "loss": 0.5352,
      "step": 21910
    },
    {
      "epoch": 1.3838383838383839,
      "grad_norm": 0.4586569368839264,
      "learning_rate": 4.5794866501008216e-05,
      "loss": 0.4933,
      "step": 21920
    },
    {
      "epoch": 1.384469696969697,
      "grad_norm": 0.47508668899536133,
      "learning_rate": 4.570898176956112e-05,
      "loss": 0.4537,
      "step": 21930
    },
    {
      "epoch": 1.3851010101010102,
      "grad_norm": 0.6852167248725891,
      "learning_rate": 4.562315378353729e-05,
      "loss": 0.4599,
      "step": 21940
    },
    {
      "epoch": 1.3857323232323233,
      "grad_norm": 0.41284987330436707,
      "learning_rate": 4.5537382632644774e-05,
      "loss": 0.648,
      "step": 21950
    },
    {
      "epoch": 1.3863636363636362,
      "grad_norm": 0.4891720712184906,
      "learning_rate": 4.54516684065324e-05,
      "loss": 0.5561,
      "step": 21960
    },
    {
      "epoch": 1.3869949494949494,
      "grad_norm": 0.47960254549980164,
      "learning_rate": 4.536601119478932e-05,
      "loss": 0.4742,
      "step": 21970
    },
    {
      "epoch": 1.3876262626262625,
      "grad_norm": 0.502896785736084,
      "learning_rate": 4.52804110869452e-05,
      "loss": 0.4049,
      "step": 21980
    },
    {
      "epoch": 1.3882575757575757,
      "grad_norm": 0.7962213754653931,
      "learning_rate": 4.5194868172469964e-05,
      "loss": 0.4648,
      "step": 21990
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 0.40548649430274963,
      "learning_rate": 4.510938254077373e-05,
      "loss": 0.6259,
      "step": 22000
    },
    {
      "epoch": 1.3888888888888888,
      "eval_loss": 0.5142101049423218,
      "eval_runtime": 32.0877,
      "eval_samples_per_second": 79.781,
      "eval_steps_per_second": 9.973,
      "step": 22000
    },
    {
      "epoch": 1.389520202020202,
      "grad_norm": 0.4889169931411743,
      "learning_rate": 4.502395428120687e-05,
      "loss": 0.5383,
      "step": 22010
    },
    {
      "epoch": 1.3901515151515151,
      "grad_norm": 0.4371633529663086,
      "learning_rate": 4.4938583483059695e-05,
      "loss": 0.4945,
      "step": 22020
    },
    {
      "epoch": 1.3907828282828283,
      "grad_norm": 0.43686604499816895,
      "learning_rate": 4.485327023556244e-05,
      "loss": 0.4277,
      "step": 22030
    },
    {
      "epoch": 1.3914141414141414,
      "grad_norm": 0.7110704779624939,
      "learning_rate": 4.47680146278852e-05,
      "loss": 0.4329,
      "step": 22040
    },
    {
      "epoch": 1.3920454545454546,
      "grad_norm": 0.4439482092857361,
      "learning_rate": 4.468281674913794e-05,
      "loss": 0.6751,
      "step": 22050
    },
    {
      "epoch": 1.3926767676767677,
      "grad_norm": 0.45274025201797485,
      "learning_rate": 4.4597676688370106e-05,
      "loss": 0.5556,
      "step": 22060
    },
    {
      "epoch": 1.3933080808080809,
      "grad_norm": 0.45186176896095276,
      "learning_rate": 4.451259453457083e-05,
      "loss": 0.4829,
      "step": 22070
    },
    {
      "epoch": 1.393939393939394,
      "grad_norm": 0.48243263363838196,
      "learning_rate": 4.442757037666867e-05,
      "loss": 0.4401,
      "step": 22080
    },
    {
      "epoch": 1.3945707070707072,
      "grad_norm": 0.7195322513580322,
      "learning_rate": 4.4342604303531555e-05,
      "loss": 0.4554,
      "step": 22090
    },
    {
      "epoch": 1.3952020202020203,
      "grad_norm": 0.3861812353134155,
      "learning_rate": 4.425769640396681e-05,
      "loss": 0.6223,
      "step": 22100
    },
    {
      "epoch": 1.3958333333333333,
      "grad_norm": 0.4180351197719574,
      "learning_rate": 4.417284676672082e-05,
      "loss": 0.5479,
      "step": 22110
    },
    {
      "epoch": 1.3964646464646464,
      "grad_norm": 0.46080926060676575,
      "learning_rate": 4.4088055480479154e-05,
      "loss": 0.4823,
      "step": 22120
    },
    {
      "epoch": 1.3970959595959596,
      "grad_norm": 0.4789007008075714,
      "learning_rate": 4.400332263386632e-05,
      "loss": 0.4552,
      "step": 22130
    },
    {
      "epoch": 1.3977272727272727,
      "grad_norm": 0.7304136157035828,
      "learning_rate": 4.391864831544585e-05,
      "loss": 0.4502,
      "step": 22140
    },
    {
      "epoch": 1.3983585858585859,
      "grad_norm": 0.3948152959346771,
      "learning_rate": 4.383403261372004e-05,
      "loss": 0.6323,
      "step": 22150
    },
    {
      "epoch": 1.398989898989899,
      "grad_norm": 0.48340553045272827,
      "learning_rate": 4.374947561712992e-05,
      "loss": 0.5588,
      "step": 22160
    },
    {
      "epoch": 1.3996212121212122,
      "grad_norm": 0.47220754623413086,
      "learning_rate": 4.3664977414055095e-05,
      "loss": 0.475,
      "step": 22170
    },
    {
      "epoch": 1.4002525252525253,
      "grad_norm": 0.4931844472885132,
      "learning_rate": 4.3580538092813904e-05,
      "loss": 0.4339,
      "step": 22180
    },
    {
      "epoch": 1.4008838383838385,
      "grad_norm": 0.7295424938201904,
      "learning_rate": 4.349615774166297e-05,
      "loss": 0.4502,
      "step": 22190
    },
    {
      "epoch": 1.4015151515151514,
      "grad_norm": 0.4430760145187378,
      "learning_rate": 4.3411836448797326e-05,
      "loss": 0.6,
      "step": 22200
    },
    {
      "epoch": 1.4021464646464645,
      "grad_norm": 0.39368292689323425,
      "learning_rate": 4.3327574302350305e-05,
      "loss": 0.547,
      "step": 22210
    },
    {
      "epoch": 1.4027777777777777,
      "grad_norm": 0.4961864650249481,
      "learning_rate": 4.324337139039337e-05,
      "loss": 0.4709,
      "step": 22220
    },
    {
      "epoch": 1.4034090909090908,
      "grad_norm": 0.47728708386421204,
      "learning_rate": 4.3159227800936166e-05,
      "loss": 0.4512,
      "step": 22230
    },
    {
      "epoch": 1.404040404040404,
      "grad_norm": 0.8540046811103821,
      "learning_rate": 4.307514362192623e-05,
      "loss": 0.4834,
      "step": 22240
    },
    {
      "epoch": 1.4046717171717171,
      "grad_norm": 0.4074122905731201,
      "learning_rate": 4.299111894124905e-05,
      "loss": 0.6599,
      "step": 22250
    },
    {
      "epoch": 1.4053030303030303,
      "grad_norm": 0.47160962224006653,
      "learning_rate": 4.29071538467279e-05,
      "loss": 0.5308,
      "step": 22260
    },
    {
      "epoch": 1.4059343434343434,
      "grad_norm": 0.44065073132514954,
      "learning_rate": 4.282324842612384e-05,
      "loss": 0.4753,
      "step": 22270
    },
    {
      "epoch": 1.4065656565656566,
      "grad_norm": 0.5044077038764954,
      "learning_rate": 4.273940276713549e-05,
      "loss": 0.4243,
      "step": 22280
    },
    {
      "epoch": 1.4071969696969697,
      "grad_norm": 0.6507319808006287,
      "learning_rate": 4.265561695739904e-05,
      "loss": 0.4702,
      "step": 22290
    },
    {
      "epoch": 1.4078282828282829,
      "grad_norm": 0.41343626379966736,
      "learning_rate": 4.257189108448806e-05,
      "loss": 0.6949,
      "step": 22300
    },
    {
      "epoch": 1.408459595959596,
      "grad_norm": 0.4214041829109192,
      "learning_rate": 4.24882252359136e-05,
      "loss": 0.542,
      "step": 22310
    },
    {
      "epoch": 1.4090909090909092,
      "grad_norm": 0.4060629904270172,
      "learning_rate": 4.240461949912388e-05,
      "loss": 0.4742,
      "step": 22320
    },
    {
      "epoch": 1.4097222222222223,
      "grad_norm": 0.5124459862709045,
      "learning_rate": 4.2321073961504297e-05,
      "loss": 0.4487,
      "step": 22330
    },
    {
      "epoch": 1.4103535353535355,
      "grad_norm": 0.7039934396743774,
      "learning_rate": 4.223758871037734e-05,
      "loss": 0.4751,
      "step": 22340
    },
    {
      "epoch": 1.4109848484848486,
      "grad_norm": 0.38797029852867126,
      "learning_rate": 4.215416383300247e-05,
      "loss": 0.6115,
      "step": 22350
    },
    {
      "epoch": 1.4116161616161615,
      "grad_norm": 0.461701363325119,
      "learning_rate": 4.2070799416576104e-05,
      "loss": 0.5601,
      "step": 22360
    },
    {
      "epoch": 1.4122474747474747,
      "grad_norm": 0.45004716515541077,
      "learning_rate": 4.198749554823141e-05,
      "loss": 0.4589,
      "step": 22370
    },
    {
      "epoch": 1.4128787878787878,
      "grad_norm": 0.5623342990875244,
      "learning_rate": 4.190425231503827e-05,
      "loss": 0.4362,
      "step": 22380
    },
    {
      "epoch": 1.413510101010101,
      "grad_norm": 0.7069045305252075,
      "learning_rate": 4.1821069804003186e-05,
      "loss": 0.4496,
      "step": 22390
    },
    {
      "epoch": 1.4141414141414141,
      "grad_norm": 0.3948723375797272,
      "learning_rate": 4.1737948102069256e-05,
      "loss": 0.643,
      "step": 22400
    },
    {
      "epoch": 1.4147727272727273,
      "grad_norm": 0.37743210792541504,
      "learning_rate": 4.165488729611596e-05,
      "loss": 0.5595,
      "step": 22410
    },
    {
      "epoch": 1.4154040404040404,
      "grad_norm": 0.45533087849617004,
      "learning_rate": 4.157188747295913e-05,
      "loss": 0.5025,
      "step": 22420
    },
    {
      "epoch": 1.4160353535353536,
      "grad_norm": 0.4501805305480957,
      "learning_rate": 4.148894871935086e-05,
      "loss": 0.4307,
      "step": 22430
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.7150195240974426,
      "learning_rate": 4.140607112197941e-05,
      "loss": 0.4367,
      "step": 22440
    },
    {
      "epoch": 1.4172979797979797,
      "grad_norm": 0.40726447105407715,
      "learning_rate": 4.132325476746919e-05,
      "loss": 0.6482,
      "step": 22450
    },
    {
      "epoch": 1.4179292929292928,
      "grad_norm": 0.40703338384628296,
      "learning_rate": 4.1240499742380526e-05,
      "loss": 0.5456,
      "step": 22460
    },
    {
      "epoch": 1.418560606060606,
      "grad_norm": 0.4586225152015686,
      "learning_rate": 4.115780613320962e-05,
      "loss": 0.4807,
      "step": 22470
    },
    {
      "epoch": 1.4191919191919191,
      "grad_norm": 0.5123053193092346,
      "learning_rate": 4.107517402638853e-05,
      "loss": 0.4524,
      "step": 22480
    },
    {
      "epoch": 1.4198232323232323,
      "grad_norm": 0.7069257497787476,
      "learning_rate": 4.099260350828506e-05,
      "loss": 0.4386,
      "step": 22490
    },
    {
      "epoch": 1.4204545454545454,
      "grad_norm": 0.41220808029174805,
      "learning_rate": 4.091009466520257e-05,
      "loss": 0.6135,
      "step": 22500
    },
    {
      "epoch": 1.4210858585858586,
      "grad_norm": 0.5172429084777832,
      "learning_rate": 4.0827647583379994e-05,
      "loss": 0.5447,
      "step": 22510
    },
    {
      "epoch": 1.4217171717171717,
      "grad_norm": 0.44193997979164124,
      "learning_rate": 4.074526234899166e-05,
      "loss": 0.5075,
      "step": 22520
    },
    {
      "epoch": 1.4223484848484849,
      "grad_norm": 0.5412805676460266,
      "learning_rate": 4.06629390481474e-05,
      "loss": 0.4346,
      "step": 22530
    },
    {
      "epoch": 1.422979797979798,
      "grad_norm": 0.6127790808677673,
      "learning_rate": 4.058067776689214e-05,
      "loss": 0.4614,
      "step": 22540
    },
    {
      "epoch": 1.4236111111111112,
      "grad_norm": 0.45513907074928284,
      "learning_rate": 4.049847859120608e-05,
      "loss": 0.632,
      "step": 22550
    },
    {
      "epoch": 1.4242424242424243,
      "grad_norm": 0.43210721015930176,
      "learning_rate": 4.041634160700447e-05,
      "loss": 0.5146,
      "step": 22560
    },
    {
      "epoch": 1.4248737373737375,
      "grad_norm": 0.4677096903324127,
      "learning_rate": 4.033426690013754e-05,
      "loss": 0.4536,
      "step": 22570
    },
    {
      "epoch": 1.4255050505050506,
      "grad_norm": 0.4830765724182129,
      "learning_rate": 4.0252254556390513e-05,
      "loss": 0.4244,
      "step": 22580
    },
    {
      "epoch": 1.4261363636363638,
      "grad_norm": 0.6494544744491577,
      "learning_rate": 4.017030466148335e-05,
      "loss": 0.4335,
      "step": 22590
    },
    {
      "epoch": 1.4267676767676767,
      "grad_norm": 0.40486451983451843,
      "learning_rate": 4.008841730107076e-05,
      "loss": 0.6219,
      "step": 22600
    },
    {
      "epoch": 1.4273989898989898,
      "grad_norm": 0.470364511013031,
      "learning_rate": 4.0006592560742054e-05,
      "loss": 0.5653,
      "step": 22610
    },
    {
      "epoch": 1.428030303030303,
      "grad_norm": 0.4197147786617279,
      "learning_rate": 3.992483052602119e-05,
      "loss": 0.4605,
      "step": 22620
    },
    {
      "epoch": 1.4286616161616161,
      "grad_norm": 0.453535795211792,
      "learning_rate": 3.9843131282366506e-05,
      "loss": 0.4411,
      "step": 22630
    },
    {
      "epoch": 1.4292929292929293,
      "grad_norm": 0.6795719265937805,
      "learning_rate": 3.976149491517073e-05,
      "loss": 0.4586,
      "step": 22640
    },
    {
      "epoch": 1.4299242424242424,
      "grad_norm": 0.43916550278663635,
      "learning_rate": 3.967992150976083e-05,
      "loss": 0.6307,
      "step": 22650
    },
    {
      "epoch": 1.4305555555555556,
      "grad_norm": 0.45555877685546875,
      "learning_rate": 3.959841115139807e-05,
      "loss": 0.5551,
      "step": 22660
    },
    {
      "epoch": 1.4311868686868687,
      "grad_norm": 0.4151912033557892,
      "learning_rate": 3.951696392527774e-05,
      "loss": 0.4861,
      "step": 22670
    },
    {
      "epoch": 1.4318181818181819,
      "grad_norm": 0.4281947612762451,
      "learning_rate": 3.943557991652914e-05,
      "loss": 0.4302,
      "step": 22680
    },
    {
      "epoch": 1.432449494949495,
      "grad_norm": 0.7647156119346619,
      "learning_rate": 3.935425921021552e-05,
      "loss": 0.4525,
      "step": 22690
    },
    {
      "epoch": 1.433080808080808,
      "grad_norm": 0.41949763894081116,
      "learning_rate": 3.9273001891333906e-05,
      "loss": 0.6609,
      "step": 22700
    },
    {
      "epoch": 1.433712121212121,
      "grad_norm": 0.3992539942264557,
      "learning_rate": 3.9191808044815214e-05,
      "loss": 0.5383,
      "step": 22710
    },
    {
      "epoch": 1.4343434343434343,
      "grad_norm": 0.5037947297096252,
      "learning_rate": 3.9110677755523894e-05,
      "loss": 0.4773,
      "step": 22720
    },
    {
      "epoch": 1.4349747474747474,
      "grad_norm": 0.44373685121536255,
      "learning_rate": 3.902961110825798e-05,
      "loss": 0.4498,
      "step": 22730
    },
    {
      "epoch": 1.4356060606060606,
      "grad_norm": 0.7671396136283875,
      "learning_rate": 3.894860818774897e-05,
      "loss": 0.4652,
      "step": 22740
    },
    {
      "epoch": 1.4362373737373737,
      "grad_norm": 0.4330361485481262,
      "learning_rate": 3.886766907866187e-05,
      "loss": 0.6102,
      "step": 22750
    },
    {
      "epoch": 1.4368686868686869,
      "grad_norm": 0.4658084213733673,
      "learning_rate": 3.878679386559488e-05,
      "loss": 0.5404,
      "step": 22760
    },
    {
      "epoch": 1.4375,
      "grad_norm": 0.45355191826820374,
      "learning_rate": 3.870598263307944e-05,
      "loss": 0.4881,
      "step": 22770
    },
    {
      "epoch": 1.4381313131313131,
      "grad_norm": 0.4516304135322571,
      "learning_rate": 3.862523546558011e-05,
      "loss": 0.4185,
      "step": 22780
    },
    {
      "epoch": 1.4387626262626263,
      "grad_norm": 0.6892069578170776,
      "learning_rate": 3.854455244749448e-05,
      "loss": 0.4601,
      "step": 22790
    },
    {
      "epoch": 1.4393939393939394,
      "grad_norm": 0.46936318278312683,
      "learning_rate": 3.846393366315316e-05,
      "loss": 0.6443,
      "step": 22800
    },
    {
      "epoch": 1.4400252525252526,
      "grad_norm": 0.46650877594947815,
      "learning_rate": 3.8383379196819556e-05,
      "loss": 0.5444,
      "step": 22810
    },
    {
      "epoch": 1.4406565656565657,
      "grad_norm": 0.47103145718574524,
      "learning_rate": 3.8302889132689865e-05,
      "loss": 0.4722,
      "step": 22820
    },
    {
      "epoch": 1.441287878787879,
      "grad_norm": 0.48222556710243225,
      "learning_rate": 3.822246355489294e-05,
      "loss": 0.4202,
      "step": 22830
    },
    {
      "epoch": 1.441919191919192,
      "grad_norm": 0.748563289642334,
      "learning_rate": 3.8142102547490324e-05,
      "loss": 0.4334,
      "step": 22840
    },
    {
      "epoch": 1.442550505050505,
      "grad_norm": 0.40147361159324646,
      "learning_rate": 3.806180619447596e-05,
      "loss": 0.6028,
      "step": 22850
    },
    {
      "epoch": 1.4431818181818181,
      "grad_norm": 0.42914047837257385,
      "learning_rate": 3.798157457977629e-05,
      "loss": 0.5212,
      "step": 22860
    },
    {
      "epoch": 1.4438131313131313,
      "grad_norm": 0.5142832398414612,
      "learning_rate": 3.790140778725002e-05,
      "loss": 0.4704,
      "step": 22870
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.42200008034706116,
      "learning_rate": 3.7821305900688205e-05,
      "loss": 0.4197,
      "step": 22880
    },
    {
      "epoch": 1.4450757575757576,
      "grad_norm": 0.835269033908844,
      "learning_rate": 3.7741269003813985e-05,
      "loss": 0.4748,
      "step": 22890
    },
    {
      "epoch": 1.4457070707070707,
      "grad_norm": 0.4477579891681671,
      "learning_rate": 3.7661297180282584e-05,
      "loss": 0.6732,
      "step": 22900
    },
    {
      "epoch": 1.4463383838383839,
      "grad_norm": 0.4410998523235321,
      "learning_rate": 3.7581390513681205e-05,
      "loss": 0.5444,
      "step": 22910
    },
    {
      "epoch": 1.446969696969697,
      "grad_norm": 0.466524213552475,
      "learning_rate": 3.750154908752894e-05,
      "loss": 0.4624,
      "step": 22920
    },
    {
      "epoch": 1.4476010101010102,
      "grad_norm": 0.47731730341911316,
      "learning_rate": 3.742177298527678e-05,
      "loss": 0.4103,
      "step": 22930
    },
    {
      "epoch": 1.4482323232323233,
      "grad_norm": 0.6826946139335632,
      "learning_rate": 3.734206229030732e-05,
      "loss": 0.4699,
      "step": 22940
    },
    {
      "epoch": 1.4488636363636362,
      "grad_norm": 0.40048977732658386,
      "learning_rate": 3.726241708593485e-05,
      "loss": 0.6254,
      "step": 22950
    },
    {
      "epoch": 1.4494949494949494,
      "grad_norm": 0.4129115045070648,
      "learning_rate": 3.718283745540517e-05,
      "loss": 0.5397,
      "step": 22960
    },
    {
      "epoch": 1.4501262626262625,
      "grad_norm": 0.4224603772163391,
      "learning_rate": 3.710332348189565e-05,
      "loss": 0.479,
      "step": 22970
    },
    {
      "epoch": 1.4507575757575757,
      "grad_norm": 0.5400387048721313,
      "learning_rate": 3.702387524851488e-05,
      "loss": 0.4428,
      "step": 22980
    },
    {
      "epoch": 1.4513888888888888,
      "grad_norm": 0.6470718383789062,
      "learning_rate": 3.6944492838302846e-05,
      "loss": 0.4323,
      "step": 22990
    },
    {
      "epoch": 1.452020202020202,
      "grad_norm": 0.4096415340900421,
      "learning_rate": 3.6865176334230665e-05,
      "loss": 0.6329,
      "step": 23000
    },
    {
      "epoch": 1.452020202020202,
      "eval_loss": 0.5124123692512512,
      "eval_runtime": 34.3867,
      "eval_samples_per_second": 74.447,
      "eval_steps_per_second": 9.306,
      "step": 23000
    },
    {
      "epoch": 1.4526515151515151,
      "grad_norm": 0.4434930980205536,
      "learning_rate": 3.6785925819200676e-05,
      "loss": 0.5717,
      "step": 23010
    },
    {
      "epoch": 1.4532828282828283,
      "grad_norm": 0.44434112310409546,
      "learning_rate": 3.6706741376046136e-05,
      "loss": 0.4789,
      "step": 23020
    },
    {
      "epoch": 1.4539141414141414,
      "grad_norm": 0.5096269845962524,
      "learning_rate": 3.6627623087531285e-05,
      "loss": 0.4543,
      "step": 23030
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.6615340113639832,
      "learning_rate": 3.6548571036351197e-05,
      "loss": 0.4437,
      "step": 23040
    },
    {
      "epoch": 1.4551767676767677,
      "grad_norm": 0.41827982664108276,
      "learning_rate": 3.6469585305131726e-05,
      "loss": 0.6302,
      "step": 23050
    },
    {
      "epoch": 1.4558080808080809,
      "grad_norm": 0.44636648893356323,
      "learning_rate": 3.639066597642948e-05,
      "loss": 0.5495,
      "step": 23060
    },
    {
      "epoch": 1.456439393939394,
      "grad_norm": 0.4779300391674042,
      "learning_rate": 3.631181313273154e-05,
      "loss": 0.4798,
      "step": 23070
    },
    {
      "epoch": 1.4570707070707072,
      "grad_norm": 0.4217225909233093,
      "learning_rate": 3.6233026856455573e-05,
      "loss": 0.3976,
      "step": 23080
    },
    {
      "epoch": 1.4577020202020203,
      "grad_norm": 0.675504207611084,
      "learning_rate": 3.6154307229949624e-05,
      "loss": 0.481,
      "step": 23090
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 0.3957456946372986,
      "learning_rate": 3.607565433549214e-05,
      "loss": 0.627,
      "step": 23100
    },
    {
      "epoch": 1.4589646464646464,
      "grad_norm": 0.4123198986053467,
      "learning_rate": 3.599706825529177e-05,
      "loss": 0.5194,
      "step": 23110
    },
    {
      "epoch": 1.4595959595959596,
      "grad_norm": 0.43556350469589233,
      "learning_rate": 3.5918549071487317e-05,
      "loss": 0.4628,
      "step": 23120
    },
    {
      "epoch": 1.4602272727272727,
      "grad_norm": 0.4966956079006195,
      "learning_rate": 3.58400968661477e-05,
      "loss": 0.4519,
      "step": 23130
    },
    {
      "epoch": 1.4608585858585859,
      "grad_norm": 0.7323412299156189,
      "learning_rate": 3.5761711721271774e-05,
      "loss": 0.4254,
      "step": 23140
    },
    {
      "epoch": 1.461489898989899,
      "grad_norm": 0.42329713702201843,
      "learning_rate": 3.5683393718788424e-05,
      "loss": 0.6141,
      "step": 23150
    },
    {
      "epoch": 1.4621212121212122,
      "grad_norm": 0.4279284179210663,
      "learning_rate": 3.5605142940556224e-05,
      "loss": 0.533,
      "step": 23160
    },
    {
      "epoch": 1.4627525252525253,
      "grad_norm": 0.4263475239276886,
      "learning_rate": 3.552695946836355e-05,
      "loss": 0.4893,
      "step": 23170
    },
    {
      "epoch": 1.4633838383838385,
      "grad_norm": 0.5375111103057861,
      "learning_rate": 3.5448843383928386e-05,
      "loss": 0.4239,
      "step": 23180
    },
    {
      "epoch": 1.4640151515151514,
      "grad_norm": 0.7864006161689758,
      "learning_rate": 3.537079476889837e-05,
      "loss": 0.4577,
      "step": 23190
    },
    {
      "epoch": 1.4646464646464645,
      "grad_norm": 0.38424617052078247,
      "learning_rate": 3.5292813704850536e-05,
      "loss": 0.6099,
      "step": 23200
    },
    {
      "epoch": 1.4652777777777777,
      "grad_norm": 0.4216543138027191,
      "learning_rate": 3.521490027329133e-05,
      "loss": 0.551,
      "step": 23210
    },
    {
      "epoch": 1.4659090909090908,
      "grad_norm": 0.3905814588069916,
      "learning_rate": 3.513705455565649e-05,
      "loss": 0.4779,
      "step": 23220
    },
    {
      "epoch": 1.466540404040404,
      "grad_norm": 0.41452109813690186,
      "learning_rate": 3.505927663331108e-05,
      "loss": 0.3967,
      "step": 23230
    },
    {
      "epoch": 1.4671717171717171,
      "grad_norm": 0.6387276649475098,
      "learning_rate": 3.49815665875492e-05,
      "loss": 0.4458,
      "step": 23240
    },
    {
      "epoch": 1.4678030303030303,
      "grad_norm": 0.4345443844795227,
      "learning_rate": 3.490392449959401e-05,
      "loss": 0.5821,
      "step": 23250
    },
    {
      "epoch": 1.4684343434343434,
      "grad_norm": 0.43953004479408264,
      "learning_rate": 3.482635045059769e-05,
      "loss": 0.5252,
      "step": 23260
    },
    {
      "epoch": 1.4690656565656566,
      "grad_norm": 0.47436630725860596,
      "learning_rate": 3.4748844521641245e-05,
      "loss": 0.449,
      "step": 23270
    },
    {
      "epoch": 1.4696969696969697,
      "grad_norm": 0.4337383806705475,
      "learning_rate": 3.467140679373456e-05,
      "loss": 0.4192,
      "step": 23280
    },
    {
      "epoch": 1.4703282828282829,
      "grad_norm": 0.5925551056861877,
      "learning_rate": 3.45940373478162e-05,
      "loss": 0.4445,
      "step": 23290
    },
    {
      "epoch": 1.470959595959596,
      "grad_norm": 0.40976157784461975,
      "learning_rate": 3.4516736264753313e-05,
      "loss": 0.6496,
      "step": 23300
    },
    {
      "epoch": 1.4715909090909092,
      "grad_norm": 0.42066678404808044,
      "learning_rate": 3.4439503625341605e-05,
      "loss": 0.535,
      "step": 23310
    },
    {
      "epoch": 1.4722222222222223,
      "grad_norm": 0.5422985553741455,
      "learning_rate": 3.436233951030536e-05,
      "loss": 0.4758,
      "step": 23320
    },
    {
      "epoch": 1.4728535353535355,
      "grad_norm": 0.5491816997528076,
      "learning_rate": 3.4285244000297114e-05,
      "loss": 0.4382,
      "step": 23330
    },
    {
      "epoch": 1.4734848484848486,
      "grad_norm": 0.7618215084075928,
      "learning_rate": 3.420821717589773e-05,
      "loss": 0.45,
      "step": 23340
    },
    {
      "epoch": 1.4741161616161615,
      "grad_norm": 0.397566556930542,
      "learning_rate": 3.4131259117616235e-05,
      "loss": 0.649,
      "step": 23350
    },
    {
      "epoch": 1.4747474747474747,
      "grad_norm": 0.42379170656204224,
      "learning_rate": 3.405436990588992e-05,
      "loss": 0.5343,
      "step": 23360
    },
    {
      "epoch": 1.4753787878787878,
      "grad_norm": 0.43068814277648926,
      "learning_rate": 3.397754962108398e-05,
      "loss": 0.5174,
      "step": 23370
    },
    {
      "epoch": 1.476010101010101,
      "grad_norm": 0.5001674890518188,
      "learning_rate": 3.390079834349163e-05,
      "loss": 0.422,
      "step": 23380
    },
    {
      "epoch": 1.4766414141414141,
      "grad_norm": 0.7200459241867065,
      "learning_rate": 3.382411615333392e-05,
      "loss": 0.4409,
      "step": 23390
    },
    {
      "epoch": 1.4772727272727273,
      "grad_norm": 0.4616473615169525,
      "learning_rate": 3.374750313075969e-05,
      "loss": 0.6282,
      "step": 23400
    },
    {
      "epoch": 1.4779040404040404,
      "grad_norm": 0.44215303659439087,
      "learning_rate": 3.36709593558456e-05,
      "loss": 0.5502,
      "step": 23410
    },
    {
      "epoch": 1.4785353535353536,
      "grad_norm": 0.44227439165115356,
      "learning_rate": 3.359448490859577e-05,
      "loss": 0.4517,
      "step": 23420
    },
    {
      "epoch": 1.4791666666666667,
      "grad_norm": 0.44097137451171875,
      "learning_rate": 3.351807986894193e-05,
      "loss": 0.4307,
      "step": 23430
    },
    {
      "epoch": 1.4797979797979797,
      "grad_norm": 0.6628656983375549,
      "learning_rate": 3.3441744316743264e-05,
      "loss": 0.4479,
      "step": 23440
    },
    {
      "epoch": 1.4804292929292928,
      "grad_norm": 0.46477290987968445,
      "learning_rate": 3.336547833178635e-05,
      "loss": 0.6322,
      "step": 23450
    },
    {
      "epoch": 1.481060606060606,
      "grad_norm": 0.5068609714508057,
      "learning_rate": 3.328928199378502e-05,
      "loss": 0.5296,
      "step": 23460
    },
    {
      "epoch": 1.4816919191919191,
      "grad_norm": 0.5191097259521484,
      "learning_rate": 3.3213155382380304e-05,
      "loss": 0.4725,
      "step": 23470
    },
    {
      "epoch": 1.4823232323232323,
      "grad_norm": 0.44763490557670593,
      "learning_rate": 3.3137098577140345e-05,
      "loss": 0.425,
      "step": 23480
    },
    {
      "epoch": 1.4829545454545454,
      "grad_norm": 0.6689375042915344,
      "learning_rate": 3.3061111657560416e-05,
      "loss": 0.4261,
      "step": 23490
    },
    {
      "epoch": 1.4835858585858586,
      "grad_norm": 0.44575023651123047,
      "learning_rate": 3.2985194703062636e-05,
      "loss": 0.6576,
      "step": 23500
    },
    {
      "epoch": 1.4842171717171717,
      "grad_norm": 0.45930781960487366,
      "learning_rate": 3.290934779299605e-05,
      "loss": 0.5484,
      "step": 23510
    },
    {
      "epoch": 1.4848484848484849,
      "grad_norm": 0.4064377248287201,
      "learning_rate": 3.283357100663647e-05,
      "loss": 0.4691,
      "step": 23520
    },
    {
      "epoch": 1.485479797979798,
      "grad_norm": 0.5352358818054199,
      "learning_rate": 3.2757864423186376e-05,
      "loss": 0.4484,
      "step": 23530
    },
    {
      "epoch": 1.4861111111111112,
      "grad_norm": 0.8213146924972534,
      "learning_rate": 3.2682228121775006e-05,
      "loss": 0.4325,
      "step": 23540
    },
    {
      "epoch": 1.4867424242424243,
      "grad_norm": 0.41424793004989624,
      "learning_rate": 3.2606662181458e-05,
      "loss": 0.6263,
      "step": 23550
    },
    {
      "epoch": 1.4873737373737375,
      "grad_norm": 0.46574291586875916,
      "learning_rate": 3.253116668121752e-05,
      "loss": 0.5516,
      "step": 23560
    },
    {
      "epoch": 1.4880050505050506,
      "grad_norm": 0.42555955052375793,
      "learning_rate": 3.245574169996205e-05,
      "loss": 0.4664,
      "step": 23570
    },
    {
      "epoch": 1.4886363636363638,
      "grad_norm": 0.45102453231811523,
      "learning_rate": 3.2380387316526485e-05,
      "loss": 0.4365,
      "step": 23580
    },
    {
      "epoch": 1.4892676767676767,
      "grad_norm": 0.685621440410614,
      "learning_rate": 3.2305103609671796e-05,
      "loss": 0.4721,
      "step": 23590
    },
    {
      "epoch": 1.4898989898989898,
      "grad_norm": 0.43717265129089355,
      "learning_rate": 3.222989065808516e-05,
      "loss": 0.5938,
      "step": 23600
    },
    {
      "epoch": 1.490530303030303,
      "grad_norm": 0.43397143483161926,
      "learning_rate": 3.215474854037976e-05,
      "loss": 0.5438,
      "step": 23610
    },
    {
      "epoch": 1.4911616161616161,
      "grad_norm": 0.4769268035888672,
      "learning_rate": 3.2079677335094746e-05,
      "loss": 0.4679,
      "step": 23620
    },
    {
      "epoch": 1.4917929292929293,
      "grad_norm": 0.4461749792098999,
      "learning_rate": 3.200467712069523e-05,
      "loss": 0.4287,
      "step": 23630
    },
    {
      "epoch": 1.4924242424242424,
      "grad_norm": 0.7335833311080933,
      "learning_rate": 3.192974797557201e-05,
      "loss": 0.4497,
      "step": 23640
    },
    {
      "epoch": 1.4930555555555556,
      "grad_norm": 0.4114065170288086,
      "learning_rate": 3.185488997804167e-05,
      "loss": 0.6163,
      "step": 23650
    },
    {
      "epoch": 1.4936868686868687,
      "grad_norm": 0.41317352652549744,
      "learning_rate": 3.1780103206346366e-05,
      "loss": 0.5076,
      "step": 23660
    },
    {
      "epoch": 1.4943181818181819,
      "grad_norm": 0.4491090178489685,
      "learning_rate": 3.1705387738653924e-05,
      "loss": 0.4497,
      "step": 23670
    },
    {
      "epoch": 1.494949494949495,
      "grad_norm": 0.5260043144226074,
      "learning_rate": 3.1630743653057536e-05,
      "loss": 0.4349,
      "step": 23680
    },
    {
      "epoch": 1.495580808080808,
      "grad_norm": 0.6597623825073242,
      "learning_rate": 3.155617102757583e-05,
      "loss": 0.4409,
      "step": 23690
    },
    {
      "epoch": 1.496212121212121,
      "grad_norm": 0.4213215112686157,
      "learning_rate": 3.148166994015268e-05,
      "loss": 0.6526,
      "step": 23700
    },
    {
      "epoch": 1.4968434343434343,
      "grad_norm": 0.45250797271728516,
      "learning_rate": 3.140724046865733e-05,
      "loss": 0.531,
      "step": 23710
    },
    {
      "epoch": 1.4974747474747474,
      "grad_norm": 0.456194132566452,
      "learning_rate": 3.1332882690884026e-05,
      "loss": 0.5057,
      "step": 23720
    },
    {
      "epoch": 1.4981060606060606,
      "grad_norm": 0.46152356266975403,
      "learning_rate": 3.1258596684552155e-05,
      "loss": 0.4454,
      "step": 23730
    },
    {
      "epoch": 1.4987373737373737,
      "grad_norm": 0.6388798952102661,
      "learning_rate": 3.118438252730607e-05,
      "loss": 0.4407,
      "step": 23740
    },
    {
      "epoch": 1.4993686868686869,
      "grad_norm": 0.38024693727493286,
      "learning_rate": 3.1110240296714966e-05,
      "loss": 0.6141,
      "step": 23750
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.4760723412036896,
      "learning_rate": 3.103617007027301e-05,
      "loss": 0.5575,
      "step": 23760
    },
    {
      "epoch": 1.5006313131313131,
      "grad_norm": 0.44481369853019714,
      "learning_rate": 3.0962171925398966e-05,
      "loss": 0.4629,
      "step": 23770
    },
    {
      "epoch": 1.5012626262626263,
      "grad_norm": 0.5052418112754822,
      "learning_rate": 3.0888245939436324e-05,
      "loss": 0.4307,
      "step": 23780
    },
    {
      "epoch": 1.5018939393939394,
      "grad_norm": 0.721168577671051,
      "learning_rate": 3.0814392189653095e-05,
      "loss": 0.4512,
      "step": 23790
    },
    {
      "epoch": 1.5025252525252526,
      "grad_norm": 0.4131733179092407,
      "learning_rate": 3.074061075324191e-05,
      "loss": 0.6324,
      "step": 23800
    },
    {
      "epoch": 1.5031565656565657,
      "grad_norm": 0.45086121559143066,
      "learning_rate": 3.0666901707319706e-05,
      "loss": 0.5401,
      "step": 23810
    },
    {
      "epoch": 1.503787878787879,
      "grad_norm": 0.41651010513305664,
      "learning_rate": 3.0593265128927774e-05,
      "loss": 0.4751,
      "step": 23820
    },
    {
      "epoch": 1.504419191919192,
      "grad_norm": 0.5192198157310486,
      "learning_rate": 3.0519701095031663e-05,
      "loss": 0.436,
      "step": 23830
    },
    {
      "epoch": 1.5050505050505052,
      "grad_norm": 0.7069835066795349,
      "learning_rate": 3.0446209682521152e-05,
      "loss": 0.4552,
      "step": 23840
    },
    {
      "epoch": 1.5056818181818183,
      "grad_norm": 0.3942624628543854,
      "learning_rate": 3.0372790968210075e-05,
      "loss": 0.633,
      "step": 23850
    },
    {
      "epoch": 1.5063131313131313,
      "grad_norm": 0.42054757475852966,
      "learning_rate": 3.029944502883626e-05,
      "loss": 0.5285,
      "step": 23860
    },
    {
      "epoch": 1.5069444444444444,
      "grad_norm": 0.47835367918014526,
      "learning_rate": 3.0226171941061508e-05,
      "loss": 0.4855,
      "step": 23870
    },
    {
      "epoch": 1.5075757575757576,
      "grad_norm": 0.4445173740386963,
      "learning_rate": 3.0152971781471416e-05,
      "loss": 0.4453,
      "step": 23880
    },
    {
      "epoch": 1.5082070707070707,
      "grad_norm": 0.6753395199775696,
      "learning_rate": 3.007984462657546e-05,
      "loss": 0.4195,
      "step": 23890
    },
    {
      "epoch": 1.5088383838383839,
      "grad_norm": 0.45503541827201843,
      "learning_rate": 3.000679055280673e-05,
      "loss": 0.6277,
      "step": 23900
    },
    {
      "epoch": 1.509469696969697,
      "grad_norm": 0.4483555257320404,
      "learning_rate": 2.9933809636521935e-05,
      "loss": 0.5252,
      "step": 23910
    },
    {
      "epoch": 1.51010101010101,
      "grad_norm": 0.49561506509780884,
      "learning_rate": 2.986090195400132e-05,
      "loss": 0.4828,
      "step": 23920
    },
    {
      "epoch": 1.510732323232323,
      "grad_norm": 0.4733634293079376,
      "learning_rate": 2.9788067581448653e-05,
      "loss": 0.4206,
      "step": 23930
    },
    {
      "epoch": 1.5113636363636362,
      "grad_norm": 0.6834756135940552,
      "learning_rate": 2.9715306594991e-05,
      "loss": 0.4521,
      "step": 23940
    },
    {
      "epoch": 1.5119949494949494,
      "grad_norm": 0.3832593262195587,
      "learning_rate": 2.9642619070678735e-05,
      "loss": 0.5994,
      "step": 23950
    },
    {
      "epoch": 1.5126262626262625,
      "grad_norm": 0.450655460357666,
      "learning_rate": 2.9570005084485485e-05,
      "loss": 0.5327,
      "step": 23960
    },
    {
      "epoch": 1.5132575757575757,
      "grad_norm": 0.46692657470703125,
      "learning_rate": 2.9497464712307944e-05,
      "loss": 0.4621,
      "step": 23970
    },
    {
      "epoch": 1.5138888888888888,
      "grad_norm": 0.5472939014434814,
      "learning_rate": 2.9424998029965967e-05,
      "loss": 0.4354,
      "step": 23980
    },
    {
      "epoch": 1.514520202020202,
      "grad_norm": 0.683337926864624,
      "learning_rate": 2.9352605113202326e-05,
      "loss": 0.4535,
      "step": 23990
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 0.4217137396335602,
      "learning_rate": 2.928028603768268e-05,
      "loss": 0.6307,
      "step": 24000
    },
    {
      "epoch": 1.5151515151515151,
      "eval_loss": 0.511528730392456,
      "eval_runtime": 27.0403,
      "eval_samples_per_second": 94.674,
      "eval_steps_per_second": 11.834,
      "step": 24000
    },
    {
      "epoch": 1.5157828282828283,
      "grad_norm": 0.3988342881202698,
      "learning_rate": 2.920804087899549e-05,
      "loss": 0.4833,
      "step": 24010
    },
    {
      "epoch": 1.5164141414141414,
      "grad_norm": 0.42308172583580017,
      "learning_rate": 2.9135869712652087e-05,
      "loss": 0.4677,
      "step": 24020
    },
    {
      "epoch": 1.5170454545454546,
      "grad_norm": 0.4585864245891571,
      "learning_rate": 2.9063772614086315e-05,
      "loss": 0.4086,
      "step": 24030
    },
    {
      "epoch": 1.5176767676767677,
      "grad_norm": 0.6273657083511353,
      "learning_rate": 2.8991749658654676e-05,
      "loss": 0.456,
      "step": 24040
    },
    {
      "epoch": 1.5183080808080809,
      "grad_norm": 0.4332241714000702,
      "learning_rate": 2.891980092163612e-05,
      "loss": 0.6494,
      "step": 24050
    },
    {
      "epoch": 1.518939393939394,
      "grad_norm": 0.3976767957210541,
      "learning_rate": 2.8847926478232122e-05,
      "loss": 0.5218,
      "step": 24060
    },
    {
      "epoch": 1.5195707070707072,
      "grad_norm": 0.4639469385147095,
      "learning_rate": 2.877612640356644e-05,
      "loss": 0.4688,
      "step": 24070
    },
    {
      "epoch": 1.5202020202020203,
      "grad_norm": 0.5924412608146667,
      "learning_rate": 2.870440077268509e-05,
      "loss": 0.4173,
      "step": 24080
    },
    {
      "epoch": 1.5208333333333335,
      "grad_norm": 0.6320622563362122,
      "learning_rate": 2.86327496605563e-05,
      "loss": 0.4353,
      "step": 24090
    },
    {
      "epoch": 1.5214646464646466,
      "grad_norm": 0.39651018381118774,
      "learning_rate": 2.8561173142070373e-05,
      "loss": 0.6495,
      "step": 24100
    },
    {
      "epoch": 1.5220959595959596,
      "grad_norm": 0.4167880415916443,
      "learning_rate": 2.8489671292039755e-05,
      "loss": 0.5586,
      "step": 24110
    },
    {
      "epoch": 1.5227272727272727,
      "grad_norm": 0.4010135531425476,
      "learning_rate": 2.8418244185198728e-05,
      "loss": 0.483,
      "step": 24120
    },
    {
      "epoch": 1.5233585858585859,
      "grad_norm": 0.5364156365394592,
      "learning_rate": 2.8346891896203508e-05,
      "loss": 0.4177,
      "step": 24130
    },
    {
      "epoch": 1.523989898989899,
      "grad_norm": 0.6645243167877197,
      "learning_rate": 2.8275614499632063e-05,
      "loss": 0.4595,
      "step": 24140
    },
    {
      "epoch": 1.5246212121212122,
      "grad_norm": 0.38341420888900757,
      "learning_rate": 2.8204412069984186e-05,
      "loss": 0.6372,
      "step": 24150
    },
    {
      "epoch": 1.5252525252525253,
      "grad_norm": 0.44138216972351074,
      "learning_rate": 2.8133284681681216e-05,
      "loss": 0.5382,
      "step": 24160
    },
    {
      "epoch": 1.5258838383838382,
      "grad_norm": 0.4701867997646332,
      "learning_rate": 2.8062232409066093e-05,
      "loss": 0.4791,
      "step": 24170
    },
    {
      "epoch": 1.5265151515151514,
      "grad_norm": 0.4699716866016388,
      "learning_rate": 2.79912553264032e-05,
      "loss": 0.417,
      "step": 24180
    },
    {
      "epoch": 1.5271464646464645,
      "grad_norm": 0.9493788480758667,
      "learning_rate": 2.792035350787845e-05,
      "loss": 0.4462,
      "step": 24190
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 0.4431594908237457,
      "learning_rate": 2.7849527027598975e-05,
      "loss": 0.6461,
      "step": 24200
    },
    {
      "epoch": 1.5284090909090908,
      "grad_norm": 0.42183253169059753,
      "learning_rate": 2.7778775959593195e-05,
      "loss": 0.5271,
      "step": 24210
    },
    {
      "epoch": 1.529040404040404,
      "grad_norm": 0.43354126811027527,
      "learning_rate": 2.770810037781073e-05,
      "loss": 0.4794,
      "step": 24220
    },
    {
      "epoch": 1.5296717171717171,
      "grad_norm": 0.5019744038581848,
      "learning_rate": 2.7637500356122237e-05,
      "loss": 0.4323,
      "step": 24230
    },
    {
      "epoch": 1.5303030303030303,
      "grad_norm": 0.6395037770271301,
      "learning_rate": 2.7566975968319507e-05,
      "loss": 0.4283,
      "step": 24240
    },
    {
      "epoch": 1.5309343434343434,
      "grad_norm": 0.39014363288879395,
      "learning_rate": 2.7496527288115205e-05,
      "loss": 0.6149,
      "step": 24250
    },
    {
      "epoch": 1.5315656565656566,
      "grad_norm": 0.4306115508079529,
      "learning_rate": 2.7426154389142856e-05,
      "loss": 0.5219,
      "step": 24260
    },
    {
      "epoch": 1.5321969696969697,
      "grad_norm": 0.4333879351615906,
      "learning_rate": 2.7355857344956783e-05,
      "loss": 0.4543,
      "step": 24270
    },
    {
      "epoch": 1.5328282828282829,
      "grad_norm": 0.4503730833530426,
      "learning_rate": 2.7285636229032085e-05,
      "loss": 0.4174,
      "step": 24280
    },
    {
      "epoch": 1.533459595959596,
      "grad_norm": 0.6348937153816223,
      "learning_rate": 2.7215491114764446e-05,
      "loss": 0.4364,
      "step": 24290
    },
    {
      "epoch": 1.5340909090909092,
      "grad_norm": 0.3701755106449127,
      "learning_rate": 2.7145422075470096e-05,
      "loss": 0.6074,
      "step": 24300
    },
    {
      "epoch": 1.5347222222222223,
      "grad_norm": 0.4204676151275635,
      "learning_rate": 2.7075429184385802e-05,
      "loss": 0.5385,
      "step": 24310
    },
    {
      "epoch": 1.5353535353535355,
      "grad_norm": 0.44205111265182495,
      "learning_rate": 2.7005512514668674e-05,
      "loss": 0.4716,
      "step": 24320
    },
    {
      "epoch": 1.5359848484848486,
      "grad_norm": 0.5337725281715393,
      "learning_rate": 2.693567213939625e-05,
      "loss": 0.4065,
      "step": 24330
    },
    {
      "epoch": 1.5366161616161618,
      "grad_norm": 0.7036146521568298,
      "learning_rate": 2.6865908131566255e-05,
      "loss": 0.4397,
      "step": 24340
    },
    {
      "epoch": 1.5372474747474747,
      "grad_norm": 0.38467273116111755,
      "learning_rate": 2.6796220564096608e-05,
      "loss": 0.6643,
      "step": 24350
    },
    {
      "epoch": 1.5378787878787878,
      "grad_norm": 0.4182779788970947,
      "learning_rate": 2.6726609509825286e-05,
      "loss": 0.5424,
      "step": 24360
    },
    {
      "epoch": 1.538510101010101,
      "grad_norm": 0.45578303933143616,
      "learning_rate": 2.6657075041510416e-05,
      "loss": 0.4645,
      "step": 24370
    },
    {
      "epoch": 1.5391414141414141,
      "grad_norm": 0.48374542593955994,
      "learning_rate": 2.6587617231829966e-05,
      "loss": 0.431,
      "step": 24380
    },
    {
      "epoch": 1.5397727272727273,
      "grad_norm": 0.6918668746948242,
      "learning_rate": 2.6518236153381815e-05,
      "loss": 0.4316,
      "step": 24390
    },
    {
      "epoch": 1.5404040404040404,
      "grad_norm": 0.4042019248008728,
      "learning_rate": 2.6448931878683626e-05,
      "loss": 0.6324,
      "step": 24400
    },
    {
      "epoch": 1.5410353535353534,
      "grad_norm": 0.4546421766281128,
      "learning_rate": 2.637970448017284e-05,
      "loss": 0.5229,
      "step": 24410
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 0.43692779541015625,
      "learning_rate": 2.6310554030206502e-05,
      "loss": 0.4578,
      "step": 24420
    },
    {
      "epoch": 1.5422979797979797,
      "grad_norm": 0.49451881647109985,
      "learning_rate": 2.6241480601061218e-05,
      "loss": 0.4317,
      "step": 24430
    },
    {
      "epoch": 1.5429292929292928,
      "grad_norm": 0.6686294674873352,
      "learning_rate": 2.6172484264933115e-05,
      "loss": 0.4474,
      "step": 24440
    },
    {
      "epoch": 1.543560606060606,
      "grad_norm": 0.41673359274864197,
      "learning_rate": 2.6103565093937722e-05,
      "loss": 0.6528,
      "step": 24450
    },
    {
      "epoch": 1.5441919191919191,
      "grad_norm": 0.4347488582134247,
      "learning_rate": 2.6034723160109965e-05,
      "loss": 0.5606,
      "step": 24460
    },
    {
      "epoch": 1.5448232323232323,
      "grad_norm": 0.4896625578403473,
      "learning_rate": 2.596595853540399e-05,
      "loss": 0.4538,
      "step": 24470
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 0.4652078449726105,
      "learning_rate": 2.5897271291693147e-05,
      "loss": 0.4011,
      "step": 24480
    },
    {
      "epoch": 1.5460858585858586,
      "grad_norm": 0.6653112769126892,
      "learning_rate": 2.5828661500769902e-05,
      "loss": 0.4284,
      "step": 24490
    },
    {
      "epoch": 1.5467171717171717,
      "grad_norm": 0.4090026319026947,
      "learning_rate": 2.576012923434581e-05,
      "loss": 0.6104,
      "step": 24500
    },
    {
      "epoch": 1.5473484848484849,
      "grad_norm": 0.4637334644794464,
      "learning_rate": 2.5691674564051348e-05,
      "loss": 0.5434,
      "step": 24510
    },
    {
      "epoch": 1.547979797979798,
      "grad_norm": 0.47577524185180664,
      "learning_rate": 2.5623297561435912e-05,
      "loss": 0.4589,
      "step": 24520
    },
    {
      "epoch": 1.5486111111111112,
      "grad_norm": 0.45928680896759033,
      "learning_rate": 2.555499829796768e-05,
      "loss": 0.3913,
      "step": 24530
    },
    {
      "epoch": 1.5492424242424243,
      "grad_norm": 0.6967994570732117,
      "learning_rate": 2.5486776845033654e-05,
      "loss": 0.4396,
      "step": 24540
    },
    {
      "epoch": 1.5498737373737375,
      "grad_norm": 0.39628565311431885,
      "learning_rate": 2.5418633273939462e-05,
      "loss": 0.6238,
      "step": 24550
    },
    {
      "epoch": 1.5505050505050506,
      "grad_norm": 0.4142070710659027,
      "learning_rate": 2.5350567655909308e-05,
      "loss": 0.5386,
      "step": 24560
    },
    {
      "epoch": 1.5511363636363638,
      "grad_norm": 0.4845874309539795,
      "learning_rate": 2.528258006208596e-05,
      "loss": 0.4714,
      "step": 24570
    },
    {
      "epoch": 1.551767676767677,
      "grad_norm": 0.5182803273200989,
      "learning_rate": 2.521467056353056e-05,
      "loss": 0.4064,
      "step": 24580
    },
    {
      "epoch": 1.55239898989899,
      "grad_norm": 0.6302624344825745,
      "learning_rate": 2.5146839231222764e-05,
      "loss": 0.4389,
      "step": 24590
    },
    {
      "epoch": 1.553030303030303,
      "grad_norm": 0.40915870666503906,
      "learning_rate": 2.5079086136060393e-05,
      "loss": 0.6167,
      "step": 24600
    },
    {
      "epoch": 1.5536616161616161,
      "grad_norm": 0.4403175413608551,
      "learning_rate": 2.501141134885957e-05,
      "loss": 0.5178,
      "step": 24610
    },
    {
      "epoch": 1.5542929292929293,
      "grad_norm": 0.41683289408683777,
      "learning_rate": 2.4943814940354492e-05,
      "loss": 0.4869,
      "step": 24620
    },
    {
      "epoch": 1.5549242424242424,
      "grad_norm": 0.45329755544662476,
      "learning_rate": 2.4876296981197555e-05,
      "loss": 0.3889,
      "step": 24630
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.6700581908226013,
      "learning_rate": 2.4808857541959074e-05,
      "loss": 0.4426,
      "step": 24640
    },
    {
      "epoch": 1.5561868686868687,
      "grad_norm": 0.4210430085659027,
      "learning_rate": 2.4741496693127287e-05,
      "loss": 0.6186,
      "step": 24650
    },
    {
      "epoch": 1.5568181818181817,
      "grad_norm": 0.406118780374527,
      "learning_rate": 2.4674214505108294e-05,
      "loss": 0.5229,
      "step": 24660
    },
    {
      "epoch": 1.5574494949494948,
      "grad_norm": 0.4504006505012512,
      "learning_rate": 2.460701104822607e-05,
      "loss": 0.4697,
      "step": 24670
    },
    {
      "epoch": 1.558080808080808,
      "grad_norm": 0.5487775802612305,
      "learning_rate": 2.4539886392722178e-05,
      "loss": 0.4128,
      "step": 24680
    },
    {
      "epoch": 1.558712121212121,
      "grad_norm": 0.6622227430343628,
      "learning_rate": 2.4472840608755864e-05,
      "loss": 0.4479,
      "step": 24690
    },
    {
      "epoch": 1.5593434343434343,
      "grad_norm": 0.4121619462966919,
      "learning_rate": 2.440587376640395e-05,
      "loss": 0.6265,
      "step": 24700
    },
    {
      "epoch": 1.5599747474747474,
      "grad_norm": 0.40283408761024475,
      "learning_rate": 2.433898593566071e-05,
      "loss": 0.5425,
      "step": 24710
    },
    {
      "epoch": 1.5606060606060606,
      "grad_norm": 0.42872554063796997,
      "learning_rate": 2.4272177186437896e-05,
      "loss": 0.4684,
      "step": 24720
    },
    {
      "epoch": 1.5612373737373737,
      "grad_norm": 0.5003705024719238,
      "learning_rate": 2.4205447588564566e-05,
      "loss": 0.4446,
      "step": 24730
    },
    {
      "epoch": 1.5618686868686869,
      "grad_norm": 0.6567077040672302,
      "learning_rate": 2.4138797211787034e-05,
      "loss": 0.455,
      "step": 24740
    },
    {
      "epoch": 1.5625,
      "grad_norm": 0.40153664350509644,
      "learning_rate": 2.4072226125768795e-05,
      "loss": 0.6411,
      "step": 24750
    },
    {
      "epoch": 1.5631313131313131,
      "grad_norm": 0.45308056473731995,
      "learning_rate": 2.4005734400090574e-05,
      "loss": 0.5461,
      "step": 24760
    },
    {
      "epoch": 1.5637626262626263,
      "grad_norm": 0.44626131653785706,
      "learning_rate": 2.393932210425004e-05,
      "loss": 0.4763,
      "step": 24770
    },
    {
      "epoch": 1.5643939393939394,
      "grad_norm": 0.4909355640411377,
      "learning_rate": 2.3872989307661865e-05,
      "loss": 0.4041,
      "step": 24780
    },
    {
      "epoch": 1.5650252525252526,
      "grad_norm": 0.7274428009986877,
      "learning_rate": 2.3806736079657654e-05,
      "loss": 0.4511,
      "step": 24790
    },
    {
      "epoch": 1.5656565656565657,
      "grad_norm": 0.4027017056941986,
      "learning_rate": 2.3740562489485806e-05,
      "loss": 0.6266,
      "step": 24800
    },
    {
      "epoch": 1.566287878787879,
      "grad_norm": 0.4314608573913574,
      "learning_rate": 2.367446860631154e-05,
      "loss": 0.5432,
      "step": 24810
    },
    {
      "epoch": 1.566919191919192,
      "grad_norm": 0.4664916694164276,
      "learning_rate": 2.3608454499216727e-05,
      "loss": 0.4688,
      "step": 24820
    },
    {
      "epoch": 1.5675505050505052,
      "grad_norm": 0.45476245880126953,
      "learning_rate": 2.354252023719985e-05,
      "loss": 0.4375,
      "step": 24830
    },
    {
      "epoch": 1.5681818181818183,
      "grad_norm": 0.6203468441963196,
      "learning_rate": 2.347666588917592e-05,
      "loss": 0.4174,
      "step": 24840
    },
    {
      "epoch": 1.5688131313131313,
      "grad_norm": 0.3561077117919922,
      "learning_rate": 2.3410891523976507e-05,
      "loss": 0.6311,
      "step": 24850
    },
    {
      "epoch": 1.5694444444444444,
      "grad_norm": 0.4214557409286499,
      "learning_rate": 2.3345197210349502e-05,
      "loss": 0.5318,
      "step": 24860
    },
    {
      "epoch": 1.5700757575757576,
      "grad_norm": 0.42629972100257874,
      "learning_rate": 2.3279583016959128e-05,
      "loss": 0.4809,
      "step": 24870
    },
    {
      "epoch": 1.5707070707070707,
      "grad_norm": 0.49112439155578613,
      "learning_rate": 2.3214049012385874e-05,
      "loss": 0.4146,
      "step": 24880
    },
    {
      "epoch": 1.5713383838383839,
      "grad_norm": 0.7346591353416443,
      "learning_rate": 2.3148595265126483e-05,
      "loss": 0.46,
      "step": 24890
    },
    {
      "epoch": 1.571969696969697,
      "grad_norm": 0.39347371459007263,
      "learning_rate": 2.3083221843593716e-05,
      "loss": 0.6262,
      "step": 24900
    },
    {
      "epoch": 1.57260101010101,
      "grad_norm": 0.4198032319545746,
      "learning_rate": 2.301792881611642e-05,
      "loss": 0.5456,
      "step": 24910
    },
    {
      "epoch": 1.573232323232323,
      "grad_norm": 0.43079736828804016,
      "learning_rate": 2.295271625093941e-05,
      "loss": 0.4706,
      "step": 24920
    },
    {
      "epoch": 1.5738636363636362,
      "grad_norm": 0.4573279619216919,
      "learning_rate": 2.2887584216223378e-05,
      "loss": 0.3888,
      "step": 24930
    },
    {
      "epoch": 1.5744949494949494,
      "grad_norm": 0.7964051365852356,
      "learning_rate": 2.2822532780044914e-05,
      "loss": 0.4687,
      "step": 24940
    },
    {
      "epoch": 1.5751262626262625,
      "grad_norm": 0.40763187408447266,
      "learning_rate": 2.2757562010396306e-05,
      "loss": 0.6137,
      "step": 24950
    },
    {
      "epoch": 1.5757575757575757,
      "grad_norm": 0.4497435986995697,
      "learning_rate": 2.269267197518552e-05,
      "loss": 0.5247,
      "step": 24960
    },
    {
      "epoch": 1.5763888888888888,
      "grad_norm": 0.42881089448928833,
      "learning_rate": 2.262786274223616e-05,
      "loss": 0.4465,
      "step": 24970
    },
    {
      "epoch": 1.577020202020202,
      "grad_norm": 0.4543597102165222,
      "learning_rate": 2.256313437928741e-05,
      "loss": 0.418,
      "step": 24980
    },
    {
      "epoch": 1.5776515151515151,
      "grad_norm": 0.6986626982688904,
      "learning_rate": 2.249848695399387e-05,
      "loss": 0.449,
      "step": 24990
    },
    {
      "epoch": 1.5782828282828283,
      "grad_norm": 0.3799048066139221,
      "learning_rate": 2.243392053392557e-05,
      "loss": 0.608,
      "step": 25000
    },
    {
      "epoch": 1.5782828282828283,
      "eval_loss": 0.5097788572311401,
      "eval_runtime": 28.8023,
      "eval_samples_per_second": 88.882,
      "eval_steps_per_second": 11.11,
      "step": 25000
    },
    {
      "epoch": 1.5789141414141414,
      "grad_norm": 0.39762189984321594,
      "learning_rate": 2.236943518656782e-05,
      "loss": 0.5283,
      "step": 25010
    },
    {
      "epoch": 1.5795454545454546,
      "grad_norm": 0.48207202553749084,
      "learning_rate": 2.230503097932134e-05,
      "loss": 0.4624,
      "step": 25020
    },
    {
      "epoch": 1.5801767676767677,
      "grad_norm": 0.43633148074150085,
      "learning_rate": 2.2240707979501874e-05,
      "loss": 0.4019,
      "step": 25030
    },
    {
      "epoch": 1.5808080808080809,
      "grad_norm": 0.6424391865730286,
      "learning_rate": 2.2176466254340366e-05,
      "loss": 0.4411,
      "step": 25040
    },
    {
      "epoch": 1.581439393939394,
      "grad_norm": 0.3877005875110626,
      "learning_rate": 2.211230587098281e-05,
      "loss": 0.6344,
      "step": 25050
    },
    {
      "epoch": 1.5820707070707072,
      "grad_norm": 0.3898267149925232,
      "learning_rate": 2.2048226896490142e-05,
      "loss": 0.5275,
      "step": 25060
    },
    {
      "epoch": 1.5827020202020203,
      "grad_norm": 0.4334281086921692,
      "learning_rate": 2.1984229397838285e-05,
      "loss": 0.489,
      "step": 25070
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.49070441722869873,
      "learning_rate": 2.192031344191794e-05,
      "loss": 0.4216,
      "step": 25080
    },
    {
      "epoch": 1.5839646464646466,
      "grad_norm": 0.6638519167900085,
      "learning_rate": 2.185647909553459e-05,
      "loss": 0.4353,
      "step": 25090
    },
    {
      "epoch": 1.5845959595959596,
      "grad_norm": 0.3895009160041809,
      "learning_rate": 2.1792726425408383e-05,
      "loss": 0.7096,
      "step": 25100
    },
    {
      "epoch": 1.5852272727272727,
      "grad_norm": 0.4452191889286041,
      "learning_rate": 2.1729055498174224e-05,
      "loss": 0.5397,
      "step": 25110
    },
    {
      "epoch": 1.5858585858585859,
      "grad_norm": 0.439225435256958,
      "learning_rate": 2.1665466380381438e-05,
      "loss": 0.4577,
      "step": 25120
    },
    {
      "epoch": 1.586489898989899,
      "grad_norm": 0.44191429018974304,
      "learning_rate": 2.160195913849392e-05,
      "loss": 0.4271,
      "step": 25130
    },
    {
      "epoch": 1.5871212121212122,
      "grad_norm": 0.6952357292175293,
      "learning_rate": 2.1538533838889953e-05,
      "loss": 0.433,
      "step": 25140
    },
    {
      "epoch": 1.5877525252525253,
      "grad_norm": 0.3966204524040222,
      "learning_rate": 2.1475190547862167e-05,
      "loss": 0.6199,
      "step": 25150
    },
    {
      "epoch": 1.5883838383838382,
      "grad_norm": 0.44066184759140015,
      "learning_rate": 2.1411929331617553e-05,
      "loss": 0.5051,
      "step": 25160
    },
    {
      "epoch": 1.5890151515151514,
      "grad_norm": 0.41693517565727234,
      "learning_rate": 2.1348750256277216e-05,
      "loss": 0.4604,
      "step": 25170
    },
    {
      "epoch": 1.5896464646464645,
      "grad_norm": 0.45809251070022583,
      "learning_rate": 2.128565338787648e-05,
      "loss": 0.4232,
      "step": 25180
    },
    {
      "epoch": 1.5902777777777777,
      "grad_norm": 0.6579938530921936,
      "learning_rate": 2.1222638792364658e-05,
      "loss": 0.4433,
      "step": 25190
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 0.38766852021217346,
      "learning_rate": 2.1159706535605207e-05,
      "loss": 0.6,
      "step": 25200
    },
    {
      "epoch": 1.591540404040404,
      "grad_norm": 0.41955262422561646,
      "learning_rate": 2.1096856683375398e-05,
      "loss": 0.5529,
      "step": 25210
    },
    {
      "epoch": 1.5921717171717171,
      "grad_norm": 0.4400937557220459,
      "learning_rate": 2.1034089301366434e-05,
      "loss": 0.4514,
      "step": 25220
    },
    {
      "epoch": 1.5928030303030303,
      "grad_norm": 0.47106051445007324,
      "learning_rate": 2.0971404455183263e-05,
      "loss": 0.3886,
      "step": 25230
    },
    {
      "epoch": 1.5934343434343434,
      "grad_norm": 0.6813675761222839,
      "learning_rate": 2.090880221034467e-05,
      "loss": 0.4363,
      "step": 25240
    },
    {
      "epoch": 1.5940656565656566,
      "grad_norm": 0.3853814899921417,
      "learning_rate": 2.0846282632283022e-05,
      "loss": 0.6251,
      "step": 25250
    },
    {
      "epoch": 1.5946969696969697,
      "grad_norm": 0.42189517617225647,
      "learning_rate": 2.078384578634428e-05,
      "loss": 0.5333,
      "step": 25260
    },
    {
      "epoch": 1.5953282828282829,
      "grad_norm": 0.4167974293231964,
      "learning_rate": 2.0721491737787978e-05,
      "loss": 0.4293,
      "step": 25270
    },
    {
      "epoch": 1.595959595959596,
      "grad_norm": 0.46880409121513367,
      "learning_rate": 2.065922055178704e-05,
      "loss": 0.4007,
      "step": 25280
    },
    {
      "epoch": 1.5965909090909092,
      "grad_norm": 0.7204930186271667,
      "learning_rate": 2.0597032293427887e-05,
      "loss": 0.435,
      "step": 25290
    },
    {
      "epoch": 1.5972222222222223,
      "grad_norm": 0.38388505578041077,
      "learning_rate": 2.053492702771018e-05,
      "loss": 0.6149,
      "step": 25300
    },
    {
      "epoch": 1.5978535353535355,
      "grad_norm": 0.44125598669052124,
      "learning_rate": 2.0472904819546856e-05,
      "loss": 0.5528,
      "step": 25310
    },
    {
      "epoch": 1.5984848484848486,
      "grad_norm": 0.46104615926742554,
      "learning_rate": 2.041096573376402e-05,
      "loss": 0.4788,
      "step": 25320
    },
    {
      "epoch": 1.5991161616161618,
      "grad_norm": 0.4604393243789673,
      "learning_rate": 2.0349109835100956e-05,
      "loss": 0.3965,
      "step": 25330
    },
    {
      "epoch": 1.5997474747474747,
      "grad_norm": 0.7032745480537415,
      "learning_rate": 2.0287337188209954e-05,
      "loss": 0.4397,
      "step": 25340
    },
    {
      "epoch": 1.6003787878787878,
      "grad_norm": 0.38548409938812256,
      "learning_rate": 2.0225647857656292e-05,
      "loss": 0.6174,
      "step": 25350
    },
    {
      "epoch": 1.601010101010101,
      "grad_norm": 0.441383957862854,
      "learning_rate": 2.016404190791814e-05,
      "loss": 0.5413,
      "step": 25360
    },
    {
      "epoch": 1.6016414141414141,
      "grad_norm": 0.47227907180786133,
      "learning_rate": 2.010251940338661e-05,
      "loss": 0.4631,
      "step": 25370
    },
    {
      "epoch": 1.6022727272727273,
      "grad_norm": 0.44490575790405273,
      "learning_rate": 2.004108040836551e-05,
      "loss": 0.4136,
      "step": 25380
    },
    {
      "epoch": 1.6029040404040404,
      "grad_norm": 0.6630451679229736,
      "learning_rate": 1.997972498707137e-05,
      "loss": 0.4503,
      "step": 25390
    },
    {
      "epoch": 1.6035353535353534,
      "grad_norm": 0.4467330276966095,
      "learning_rate": 1.9918453203633402e-05,
      "loss": 0.6366,
      "step": 25400
    },
    {
      "epoch": 1.6041666666666665,
      "grad_norm": 0.43621811270713806,
      "learning_rate": 1.985726512209335e-05,
      "loss": 0.5281,
      "step": 25410
    },
    {
      "epoch": 1.6047979797979797,
      "grad_norm": 0.4219101667404175,
      "learning_rate": 1.979616080640556e-05,
      "loss": 0.4679,
      "step": 25420
    },
    {
      "epoch": 1.6054292929292928,
      "grad_norm": 0.4925893545150757,
      "learning_rate": 1.973514032043674e-05,
      "loss": 0.4215,
      "step": 25430
    },
    {
      "epoch": 1.606060606060606,
      "grad_norm": 0.7317449450492859,
      "learning_rate": 1.9674203727966024e-05,
      "loss": 0.4248,
      "step": 25440
    },
    {
      "epoch": 1.6066919191919191,
      "grad_norm": 0.44519752264022827,
      "learning_rate": 1.9613351092684795e-05,
      "loss": 0.6265,
      "step": 25450
    },
    {
      "epoch": 1.6073232323232323,
      "grad_norm": 0.4156478941440582,
      "learning_rate": 1.955258247819681e-05,
      "loss": 0.5355,
      "step": 25460
    },
    {
      "epoch": 1.6079545454545454,
      "grad_norm": 0.43089646100997925,
      "learning_rate": 1.949189794801788e-05,
      "loss": 0.4611,
      "step": 25470
    },
    {
      "epoch": 1.6085858585858586,
      "grad_norm": 0.46781137585639954,
      "learning_rate": 1.9431297565576e-05,
      "loss": 0.392,
      "step": 25480
    },
    {
      "epoch": 1.6092171717171717,
      "grad_norm": 0.6788351535797119,
      "learning_rate": 1.937078139421118e-05,
      "loss": 0.4605,
      "step": 25490
    },
    {
      "epoch": 1.6098484848484849,
      "grad_norm": 0.3996943235397339,
      "learning_rate": 1.9310349497175405e-05,
      "loss": 0.6559,
      "step": 25500
    },
    {
      "epoch": 1.610479797979798,
      "grad_norm": 0.41638240218162537,
      "learning_rate": 1.9250001937632645e-05,
      "loss": 0.5269,
      "step": 25510
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 0.4280068278312683,
      "learning_rate": 1.918973877865864e-05,
      "loss": 0.4712,
      "step": 25520
    },
    {
      "epoch": 1.6117424242424243,
      "grad_norm": 0.4548400342464447,
      "learning_rate": 1.912956008324096e-05,
      "loss": 0.4245,
      "step": 25530
    },
    {
      "epoch": 1.6123737373737375,
      "grad_norm": 0.6461348533630371,
      "learning_rate": 1.9069465914278827e-05,
      "loss": 0.4474,
      "step": 25540
    },
    {
      "epoch": 1.6130050505050506,
      "grad_norm": 0.394814670085907,
      "learning_rate": 1.9009456334583243e-05,
      "loss": 0.6023,
      "step": 25550
    },
    {
      "epoch": 1.6136363636363638,
      "grad_norm": 0.4479523003101349,
      "learning_rate": 1.8949531406876684e-05,
      "loss": 0.5308,
      "step": 25560
    },
    {
      "epoch": 1.614267676767677,
      "grad_norm": 0.4570036232471466,
      "learning_rate": 1.888969119379318e-05,
      "loss": 0.4624,
      "step": 25570
    },
    {
      "epoch": 1.61489898989899,
      "grad_norm": 0.5415740013122559,
      "learning_rate": 1.8829935757878183e-05,
      "loss": 0.4261,
      "step": 25580
    },
    {
      "epoch": 1.615530303030303,
      "grad_norm": 0.7445491552352905,
      "learning_rate": 1.877026516158864e-05,
      "loss": 0.4686,
      "step": 25590
    },
    {
      "epoch": 1.6161616161616161,
      "grad_norm": 0.40850692987442017,
      "learning_rate": 1.8710679467292725e-05,
      "loss": 0.6377,
      "step": 25600
    },
    {
      "epoch": 1.6167929292929293,
      "grad_norm": 0.43257880210876465,
      "learning_rate": 1.8651178737269893e-05,
      "loss": 0.5316,
      "step": 25610
    },
    {
      "epoch": 1.6174242424242424,
      "grad_norm": 0.47887954115867615,
      "learning_rate": 1.8591763033710796e-05,
      "loss": 0.4594,
      "step": 25620
    },
    {
      "epoch": 1.6180555555555556,
      "grad_norm": 0.47956016659736633,
      "learning_rate": 1.8532432418717215e-05,
      "loss": 0.4287,
      "step": 25630
    },
    {
      "epoch": 1.6186868686868687,
      "grad_norm": 0.7030234932899475,
      "learning_rate": 1.8473186954302036e-05,
      "loss": 0.4743,
      "step": 25640
    },
    {
      "epoch": 1.6193181818181817,
      "grad_norm": 0.3859759569168091,
      "learning_rate": 1.8414026702389096e-05,
      "loss": 0.5933,
      "step": 25650
    },
    {
      "epoch": 1.6199494949494948,
      "grad_norm": 0.4088000953197479,
      "learning_rate": 1.8354951724813173e-05,
      "loss": 0.4957,
      "step": 25660
    },
    {
      "epoch": 1.620580808080808,
      "grad_norm": 0.4961065948009491,
      "learning_rate": 1.8295962083319905e-05,
      "loss": 0.4891,
      "step": 25670
    },
    {
      "epoch": 1.621212121212121,
      "grad_norm": 0.48720887303352356,
      "learning_rate": 1.8237057839565807e-05,
      "loss": 0.4437,
      "step": 25680
    },
    {
      "epoch": 1.6218434343434343,
      "grad_norm": 0.68683922290802,
      "learning_rate": 1.817823905511805e-05,
      "loss": 0.4616,
      "step": 25690
    },
    {
      "epoch": 1.6224747474747474,
      "grad_norm": 0.4039595425128937,
      "learning_rate": 1.8119505791454527e-05,
      "loss": 0.5983,
      "step": 25700
    },
    {
      "epoch": 1.6231060606060606,
      "grad_norm": 0.46107208728790283,
      "learning_rate": 1.8060858109963698e-05,
      "loss": 0.5267,
      "step": 25710
    },
    {
      "epoch": 1.6237373737373737,
      "grad_norm": 0.4927956461906433,
      "learning_rate": 1.800229607194467e-05,
      "loss": 0.4656,
      "step": 25720
    },
    {
      "epoch": 1.6243686868686869,
      "grad_norm": 0.4437328279018402,
      "learning_rate": 1.7943819738606927e-05,
      "loss": 0.3963,
      "step": 25730
    },
    {
      "epoch": 1.625,
      "grad_norm": 0.6264160871505737,
      "learning_rate": 1.7885429171070432e-05,
      "loss": 0.4543,
      "step": 25740
    },
    {
      "epoch": 1.6256313131313131,
      "grad_norm": 0.4209568202495575,
      "learning_rate": 1.7827124430365494e-05,
      "loss": 0.6297,
      "step": 25750
    },
    {
      "epoch": 1.6262626262626263,
      "grad_norm": 0.39118003845214844,
      "learning_rate": 1.776890557743267e-05,
      "loss": 0.5134,
      "step": 25760
    },
    {
      "epoch": 1.6268939393939394,
      "grad_norm": 0.4690782129764557,
      "learning_rate": 1.7710772673122867e-05,
      "loss": 0.4575,
      "step": 25770
    },
    {
      "epoch": 1.6275252525252526,
      "grad_norm": 0.5309972167015076,
      "learning_rate": 1.7652725778197022e-05,
      "loss": 0.4101,
      "step": 25780
    },
    {
      "epoch": 1.6281565656565657,
      "grad_norm": 0.6849254369735718,
      "learning_rate": 1.759476495332626e-05,
      "loss": 0.4344,
      "step": 25790
    },
    {
      "epoch": 1.628787878787879,
      "grad_norm": 0.36650320887565613,
      "learning_rate": 1.7536890259091675e-05,
      "loss": 0.5986,
      "step": 25800
    },
    {
      "epoch": 1.629419191919192,
      "grad_norm": 0.4410986304283142,
      "learning_rate": 1.747910175598443e-05,
      "loss": 0.5374,
      "step": 25810
    },
    {
      "epoch": 1.6300505050505052,
      "grad_norm": 0.42976081371307373,
      "learning_rate": 1.7421399504405534e-05,
      "loss": 0.4579,
      "step": 25820
    },
    {
      "epoch": 1.6306818181818183,
      "grad_norm": 0.482328861951828,
      "learning_rate": 1.736378356466585e-05,
      "loss": 0.4189,
      "step": 25830
    },
    {
      "epoch": 1.6313131313131313,
      "grad_norm": 0.6487354636192322,
      "learning_rate": 1.7306253996986e-05,
      "loss": 0.4401,
      "step": 25840
    },
    {
      "epoch": 1.6319444444444444,
      "grad_norm": 0.42800217866897583,
      "learning_rate": 1.7248810861496445e-05,
      "loss": 0.6349,
      "step": 25850
    },
    {
      "epoch": 1.6325757575757576,
      "grad_norm": 0.41585561633110046,
      "learning_rate": 1.719145421823718e-05,
      "loss": 0.5592,
      "step": 25860
    },
    {
      "epoch": 1.6332070707070707,
      "grad_norm": 0.3961845338344574,
      "learning_rate": 1.713418412715786e-05,
      "loss": 0.4481,
      "step": 25870
    },
    {
      "epoch": 1.6338383838383839,
      "grad_norm": 0.4431505799293518,
      "learning_rate": 1.7077000648117637e-05,
      "loss": 0.4232,
      "step": 25880
    },
    {
      "epoch": 1.634469696969697,
      "grad_norm": 0.6712501049041748,
      "learning_rate": 1.7019903840885166e-05,
      "loss": 0.4469,
      "step": 25890
    },
    {
      "epoch": 1.63510101010101,
      "grad_norm": 0.3783828914165497,
      "learning_rate": 1.696289376513852e-05,
      "loss": 0.612,
      "step": 25900
    },
    {
      "epoch": 1.635732323232323,
      "grad_norm": 0.43801870942115784,
      "learning_rate": 1.6905970480465106e-05,
      "loss": 0.5346,
      "step": 25910
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 0.5084664821624756,
      "learning_rate": 1.6849134046361604e-05,
      "loss": 0.4569,
      "step": 25920
    },
    {
      "epoch": 1.6369949494949494,
      "grad_norm": 0.4687870740890503,
      "learning_rate": 1.6792384522233896e-05,
      "loss": 0.4312,
      "step": 25930
    },
    {
      "epoch": 1.6376262626262625,
      "grad_norm": 0.6375806331634521,
      "learning_rate": 1.6735721967397112e-05,
      "loss": 0.4281,
      "step": 25940
    },
    {
      "epoch": 1.6382575757575757,
      "grad_norm": 0.41795942187309265,
      "learning_rate": 1.66791464410754e-05,
      "loss": 0.6133,
      "step": 25950
    },
    {
      "epoch": 1.6388888888888888,
      "grad_norm": 0.448137491941452,
      "learning_rate": 1.662265800240197e-05,
      "loss": 0.5487,
      "step": 25960
    },
    {
      "epoch": 1.639520202020202,
      "grad_norm": 0.3928079605102539,
      "learning_rate": 1.6566256710419015e-05,
      "loss": 0.4571,
      "step": 25970
    },
    {
      "epoch": 1.6401515151515151,
      "grad_norm": 0.5174705982208252,
      "learning_rate": 1.650994262407759e-05,
      "loss": 0.3948,
      "step": 25980
    },
    {
      "epoch": 1.6407828282828283,
      "grad_norm": 0.6959177851676941,
      "learning_rate": 1.6453715802237714e-05,
      "loss": 0.456,
      "step": 25990
    },
    {
      "epoch": 1.6414141414141414,
      "grad_norm": 0.3987191915512085,
      "learning_rate": 1.6397576303668093e-05,
      "loss": 0.6226,
      "step": 26000
    },
    {
      "epoch": 1.6414141414141414,
      "eval_loss": 0.509061336517334,
      "eval_runtime": 27.0074,
      "eval_samples_per_second": 94.789,
      "eval_steps_per_second": 11.849,
      "step": 26000
    },
    {
      "epoch": 1.6420454545454546,
      "grad_norm": 0.43296536803245544,
      "learning_rate": 1.6341524187046198e-05,
      "loss": 0.5307,
      "step": 26010
    },
    {
      "epoch": 1.6426767676767677,
      "grad_norm": 0.49283885955810547,
      "learning_rate": 1.6285559510958148e-05,
      "loss": 0.4772,
      "step": 26020
    },
    {
      "epoch": 1.6433080808080809,
      "grad_norm": 0.43414080142974854,
      "learning_rate": 1.622968233389872e-05,
      "loss": 0.4488,
      "step": 26030
    },
    {
      "epoch": 1.643939393939394,
      "grad_norm": 0.6317444443702698,
      "learning_rate": 1.617389271427121e-05,
      "loss": 0.4414,
      "step": 26040
    },
    {
      "epoch": 1.6445707070707072,
      "grad_norm": 0.4147894084453583,
      "learning_rate": 1.611819071038736e-05,
      "loss": 0.5865,
      "step": 26050
    },
    {
      "epoch": 1.6452020202020203,
      "grad_norm": 0.4171673357486725,
      "learning_rate": 1.6062576380467364e-05,
      "loss": 0.5174,
      "step": 26060
    },
    {
      "epoch": 1.6458333333333335,
      "grad_norm": 0.45333749055862427,
      "learning_rate": 1.6007049782639827e-05,
      "loss": 0.4505,
      "step": 26070
    },
    {
      "epoch": 1.6464646464646466,
      "grad_norm": 0.4431546628475189,
      "learning_rate": 1.595161097494158e-05,
      "loss": 0.4177,
      "step": 26080
    },
    {
      "epoch": 1.6470959595959596,
      "grad_norm": 0.7078986763954163,
      "learning_rate": 1.589626001531772e-05,
      "loss": 0.4298,
      "step": 26090
    },
    {
      "epoch": 1.6477272727272727,
      "grad_norm": 0.39004507660865784,
      "learning_rate": 1.5840996961621545e-05,
      "loss": 0.6348,
      "step": 26100
    },
    {
      "epoch": 1.6483585858585859,
      "grad_norm": 0.43536511063575745,
      "learning_rate": 1.5785821871614414e-05,
      "loss": 0.5507,
      "step": 26110
    },
    {
      "epoch": 1.648989898989899,
      "grad_norm": 0.4174380898475647,
      "learning_rate": 1.5730734802965863e-05,
      "loss": 0.4513,
      "step": 26120
    },
    {
      "epoch": 1.6496212121212122,
      "grad_norm": 0.4409605860710144,
      "learning_rate": 1.5675735813253313e-05,
      "loss": 0.4216,
      "step": 26130
    },
    {
      "epoch": 1.6502525252525253,
      "grad_norm": 0.7676545977592468,
      "learning_rate": 1.5620824959962165e-05,
      "loss": 0.4461,
      "step": 26140
    },
    {
      "epoch": 1.6508838383838382,
      "grad_norm": 0.38422659039497375,
      "learning_rate": 1.5566002300485672e-05,
      "loss": 0.6434,
      "step": 26150
    },
    {
      "epoch": 1.6515151515151514,
      "grad_norm": 0.4052896797657013,
      "learning_rate": 1.551126789212499e-05,
      "loss": 0.5355,
      "step": 26160
    },
    {
      "epoch": 1.6521464646464645,
      "grad_norm": 0.44290855526924133,
      "learning_rate": 1.5456621792088967e-05,
      "loss": 0.4945,
      "step": 26170
    },
    {
      "epoch": 1.6527777777777777,
      "grad_norm": 0.48631617426872253,
      "learning_rate": 1.5402064057494124e-05,
      "loss": 0.422,
      "step": 26180
    },
    {
      "epoch": 1.6534090909090908,
      "grad_norm": 0.6958306431770325,
      "learning_rate": 1.534759474536467e-05,
      "loss": 0.4495,
      "step": 26190
    },
    {
      "epoch": 1.654040404040404,
      "grad_norm": 0.38636526465415955,
      "learning_rate": 1.5293213912632397e-05,
      "loss": 0.5771,
      "step": 26200
    },
    {
      "epoch": 1.6546717171717171,
      "grad_norm": 0.4276208281517029,
      "learning_rate": 1.5238921616136614e-05,
      "loss": 0.5254,
      "step": 26210
    },
    {
      "epoch": 1.6553030303030303,
      "grad_norm": 0.4711621105670929,
      "learning_rate": 1.5184717912624059e-05,
      "loss": 0.4636,
      "step": 26220
    },
    {
      "epoch": 1.6559343434343434,
      "grad_norm": 0.5275881290435791,
      "learning_rate": 1.5130602858748888e-05,
      "loss": 0.4142,
      "step": 26230
    },
    {
      "epoch": 1.6565656565656566,
      "grad_norm": 0.6510305404663086,
      "learning_rate": 1.5076576511072594e-05,
      "loss": 0.4643,
      "step": 26240
    },
    {
      "epoch": 1.6571969696969697,
      "grad_norm": 0.4095313251018524,
      "learning_rate": 1.5022638926064014e-05,
      "loss": 0.6013,
      "step": 26250
    },
    {
      "epoch": 1.6578282828282829,
      "grad_norm": 0.46075233817100525,
      "learning_rate": 1.4968790160099133e-05,
      "loss": 0.5304,
      "step": 26260
    },
    {
      "epoch": 1.658459595959596,
      "grad_norm": 0.4828908145427704,
      "learning_rate": 1.4915030269461117e-05,
      "loss": 0.4581,
      "step": 26270
    },
    {
      "epoch": 1.6590909090909092,
      "grad_norm": 0.4615575671195984,
      "learning_rate": 1.486135931034024e-05,
      "loss": 0.4082,
      "step": 26280
    },
    {
      "epoch": 1.6597222222222223,
      "grad_norm": 0.6225960850715637,
      "learning_rate": 1.4807777338833883e-05,
      "loss": 0.4357,
      "step": 26290
    },
    {
      "epoch": 1.6603535353535355,
      "grad_norm": 0.3977207541465759,
      "learning_rate": 1.4754284410946328e-05,
      "loss": 0.6567,
      "step": 26300
    },
    {
      "epoch": 1.6609848484848486,
      "grad_norm": 0.4314989745616913,
      "learning_rate": 1.470088058258885e-05,
      "loss": 0.5323,
      "step": 26310
    },
    {
      "epoch": 1.6616161616161618,
      "grad_norm": 0.5232790112495422,
      "learning_rate": 1.4647565909579552e-05,
      "loss": 0.4664,
      "step": 26320
    },
    {
      "epoch": 1.6622474747474747,
      "grad_norm": 0.4557105302810669,
      "learning_rate": 1.4594340447643373e-05,
      "loss": 0.4316,
      "step": 26330
    },
    {
      "epoch": 1.6628787878787878,
      "grad_norm": 0.685209333896637,
      "learning_rate": 1.4541204252412034e-05,
      "loss": 0.4418,
      "step": 26340
    },
    {
      "epoch": 1.663510101010101,
      "grad_norm": 0.3821668028831482,
      "learning_rate": 1.4488157379423906e-05,
      "loss": 0.5965,
      "step": 26350
    },
    {
      "epoch": 1.6641414141414141,
      "grad_norm": 0.4523756504058838,
      "learning_rate": 1.4435199884124028e-05,
      "loss": 0.5467,
      "step": 26360
    },
    {
      "epoch": 1.6647727272727273,
      "grad_norm": 0.40662387013435364,
      "learning_rate": 1.4382331821863993e-05,
      "loss": 0.4527,
      "step": 26370
    },
    {
      "epoch": 1.6654040404040404,
      "grad_norm": 0.4685943126678467,
      "learning_rate": 1.4329553247901984e-05,
      "loss": 0.4302,
      "step": 26380
    },
    {
      "epoch": 1.6660353535353534,
      "grad_norm": 0.6857625842094421,
      "learning_rate": 1.4276864217402585e-05,
      "loss": 0.4263,
      "step": 26390
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.3943825364112854,
      "learning_rate": 1.4224264785436791e-05,
      "loss": 0.6027,
      "step": 26400
    },
    {
      "epoch": 1.6672979797979797,
      "grad_norm": 0.42507311701774597,
      "learning_rate": 1.4171755006981958e-05,
      "loss": 0.5575,
      "step": 26410
    },
    {
      "epoch": 1.6679292929292928,
      "grad_norm": 0.4582642912864685,
      "learning_rate": 1.4119334936921791e-05,
      "loss": 0.4695,
      "step": 26420
    },
    {
      "epoch": 1.668560606060606,
      "grad_norm": 0.420850932598114,
      "learning_rate": 1.4067004630046154e-05,
      "loss": 0.4296,
      "step": 26430
    },
    {
      "epoch": 1.6691919191919191,
      "grad_norm": 0.7103458642959595,
      "learning_rate": 1.4014764141051118e-05,
      "loss": 0.4429,
      "step": 26440
    },
    {
      "epoch": 1.6698232323232323,
      "grad_norm": 0.43056339025497437,
      "learning_rate": 1.396261352453887e-05,
      "loss": 0.6069,
      "step": 26450
    },
    {
      "epoch": 1.6704545454545454,
      "grad_norm": 0.4376552999019623,
      "learning_rate": 1.3910552835017653e-05,
      "loss": 0.5554,
      "step": 26460
    },
    {
      "epoch": 1.6710858585858586,
      "grad_norm": 0.4632534682750702,
      "learning_rate": 1.3858582126901775e-05,
      "loss": 0.4575,
      "step": 26470
    },
    {
      "epoch": 1.6717171717171717,
      "grad_norm": 0.44298237562179565,
      "learning_rate": 1.380670145451144e-05,
      "loss": 0.4473,
      "step": 26480
    },
    {
      "epoch": 1.6723484848484849,
      "grad_norm": 0.6274639368057251,
      "learning_rate": 1.3754910872072735e-05,
      "loss": 0.4498,
      "step": 26490
    },
    {
      "epoch": 1.672979797979798,
      "grad_norm": 0.4010200500488281,
      "learning_rate": 1.3703210433717607e-05,
      "loss": 0.6426,
      "step": 26500
    },
    {
      "epoch": 1.6736111111111112,
      "grad_norm": 0.4232848882675171,
      "learning_rate": 1.3651600193483816e-05,
      "loss": 0.5293,
      "step": 26510
    },
    {
      "epoch": 1.6742424242424243,
      "grad_norm": 0.42848455905914307,
      "learning_rate": 1.3600080205314792e-05,
      "loss": 0.4667,
      "step": 26520
    },
    {
      "epoch": 1.6748737373737375,
      "grad_norm": 0.5571542382240295,
      "learning_rate": 1.3548650523059648e-05,
      "loss": 0.4036,
      "step": 26530
    },
    {
      "epoch": 1.6755050505050506,
      "grad_norm": 0.6765036582946777,
      "learning_rate": 1.3497311200473117e-05,
      "loss": 0.4358,
      "step": 26540
    },
    {
      "epoch": 1.6761363636363638,
      "grad_norm": 0.3733026087284088,
      "learning_rate": 1.344606229121551e-05,
      "loss": 0.6044,
      "step": 26550
    },
    {
      "epoch": 1.676767676767677,
      "grad_norm": 0.5072200298309326,
      "learning_rate": 1.3394903848852592e-05,
      "loss": 0.5292,
      "step": 26560
    },
    {
      "epoch": 1.67739898989899,
      "grad_norm": 0.43106913566589355,
      "learning_rate": 1.3343835926855597e-05,
      "loss": 0.4561,
      "step": 26570
    },
    {
      "epoch": 1.678030303030303,
      "grad_norm": 0.46331462264060974,
      "learning_rate": 1.3292858578601131e-05,
      "loss": 0.4033,
      "step": 26580
    },
    {
      "epoch": 1.6786616161616161,
      "grad_norm": 0.6870613694190979,
      "learning_rate": 1.3241971857371139e-05,
      "loss": 0.4387,
      "step": 26590
    },
    {
      "epoch": 1.6792929292929293,
      "grad_norm": 0.4312451183795929,
      "learning_rate": 1.3191175816352873e-05,
      "loss": 0.6337,
      "step": 26600
    },
    {
      "epoch": 1.6799242424242424,
      "grad_norm": 0.41747525334358215,
      "learning_rate": 1.3140470508638769e-05,
      "loss": 0.5251,
      "step": 26610
    },
    {
      "epoch": 1.6805555555555556,
      "grad_norm": 0.43072810769081116,
      "learning_rate": 1.3089855987226417e-05,
      "loss": 0.4436,
      "step": 26620
    },
    {
      "epoch": 1.6811868686868687,
      "grad_norm": 0.4819819927215576,
      "learning_rate": 1.3039332305018526e-05,
      "loss": 0.4227,
      "step": 26630
    },
    {
      "epoch": 1.6818181818181817,
      "grad_norm": 0.68693608045578,
      "learning_rate": 1.2988899514822906e-05,
      "loss": 0.4176,
      "step": 26640
    },
    {
      "epoch": 1.6824494949494948,
      "grad_norm": 0.36890801787376404,
      "learning_rate": 1.2938557669352314e-05,
      "loss": 0.6011,
      "step": 26650
    },
    {
      "epoch": 1.683080808080808,
      "grad_norm": 0.43755072355270386,
      "learning_rate": 1.288830682122445e-05,
      "loss": 0.5022,
      "step": 26660
    },
    {
      "epoch": 1.683712121212121,
      "grad_norm": 0.4376354217529297,
      "learning_rate": 1.2838147022961944e-05,
      "loss": 0.4764,
      "step": 26670
    },
    {
      "epoch": 1.6843434343434343,
      "grad_norm": 0.45586535334587097,
      "learning_rate": 1.278807832699218e-05,
      "loss": 0.4217,
      "step": 26680
    },
    {
      "epoch": 1.6849747474747474,
      "grad_norm": 0.7166849374771118,
      "learning_rate": 1.2738100785647456e-05,
      "loss": 0.436,
      "step": 26690
    },
    {
      "epoch": 1.6856060606060606,
      "grad_norm": 0.3696635663509369,
      "learning_rate": 1.2688214451164649e-05,
      "loss": 0.589,
      "step": 26700
    },
    {
      "epoch": 1.6862373737373737,
      "grad_norm": 0.41480526328086853,
      "learning_rate": 1.2638419375685407e-05,
      "loss": 0.525,
      "step": 26710
    },
    {
      "epoch": 1.6868686868686869,
      "grad_norm": 0.4272504150867462,
      "learning_rate": 1.2588715611255919e-05,
      "loss": 0.4588,
      "step": 26720
    },
    {
      "epoch": 1.6875,
      "grad_norm": 0.4523774981498718,
      "learning_rate": 1.2539103209827008e-05,
      "loss": 0.4305,
      "step": 26730
    },
    {
      "epoch": 1.6881313131313131,
      "grad_norm": 0.6769853234291077,
      "learning_rate": 1.248958222325396e-05,
      "loss": 0.4533,
      "step": 26740
    },
    {
      "epoch": 1.6887626262626263,
      "grad_norm": 0.40434718132019043,
      "learning_rate": 1.2440152703296515e-05,
      "loss": 0.6508,
      "step": 26750
    },
    {
      "epoch": 1.6893939393939394,
      "grad_norm": 0.400602787733078,
      "learning_rate": 1.2390814701618791e-05,
      "loss": 0.5196,
      "step": 26760
    },
    {
      "epoch": 1.6900252525252526,
      "grad_norm": 0.4641979932785034,
      "learning_rate": 1.2341568269789338e-05,
      "loss": 0.4692,
      "step": 26770
    },
    {
      "epoch": 1.6906565656565657,
      "grad_norm": 0.4323160648345947,
      "learning_rate": 1.2292413459280905e-05,
      "loss": 0.4097,
      "step": 26780
    },
    {
      "epoch": 1.691287878787879,
      "grad_norm": 0.7160953879356384,
      "learning_rate": 1.2243350321470493e-05,
      "loss": 0.4488,
      "step": 26790
    },
    {
      "epoch": 1.691919191919192,
      "grad_norm": 0.3546707332134247,
      "learning_rate": 1.2194378907639326e-05,
      "loss": 0.6407,
      "step": 26800
    },
    {
      "epoch": 1.6925505050505052,
      "grad_norm": 0.40618082880973816,
      "learning_rate": 1.2145499268972693e-05,
      "loss": 0.5269,
      "step": 26810
    },
    {
      "epoch": 1.6931818181818183,
      "grad_norm": 0.39257216453552246,
      "learning_rate": 1.209671145656005e-05,
      "loss": 0.4409,
      "step": 26820
    },
    {
      "epoch": 1.6938131313131313,
      "grad_norm": 0.4610605537891388,
      "learning_rate": 1.2048015521394807e-05,
      "loss": 0.4311,
      "step": 26830
    },
    {
      "epoch": 1.6944444444444444,
      "grad_norm": 0.5984483361244202,
      "learning_rate": 1.1999411514374348e-05,
      "loss": 0.4474,
      "step": 26840
    },
    {
      "epoch": 1.6950757575757576,
      "grad_norm": 0.37755730748176575,
      "learning_rate": 1.1950899486299972e-05,
      "loss": 0.6667,
      "step": 26850
    },
    {
      "epoch": 1.6957070707070707,
      "grad_norm": 0.4127722680568695,
      "learning_rate": 1.1902479487876884e-05,
      "loss": 0.5207,
      "step": 26860
    },
    {
      "epoch": 1.6963383838383839,
      "grad_norm": 0.4038284718990326,
      "learning_rate": 1.1854151569714067e-05,
      "loss": 0.4666,
      "step": 26870
    },
    {
      "epoch": 1.696969696969697,
      "grad_norm": 0.44196605682373047,
      "learning_rate": 1.1805915782324239e-05,
      "loss": 0.4126,
      "step": 26880
    },
    {
      "epoch": 1.69760101010101,
      "grad_norm": 0.6894423365592957,
      "learning_rate": 1.1757772176123848e-05,
      "loss": 0.4396,
      "step": 26890
    },
    {
      "epoch": 1.698232323232323,
      "grad_norm": 0.3741565942764282,
      "learning_rate": 1.1709720801433022e-05,
      "loss": 0.623,
      "step": 26900
    },
    {
      "epoch": 1.6988636363636362,
      "grad_norm": 0.41941627860069275,
      "learning_rate": 1.1661761708475439e-05,
      "loss": 0.5493,
      "step": 26910
    },
    {
      "epoch": 1.6994949494949494,
      "grad_norm": 0.44238531589508057,
      "learning_rate": 1.1613894947378334e-05,
      "loss": 0.4655,
      "step": 26920
    },
    {
      "epoch": 1.7001262626262625,
      "grad_norm": 0.45728087425231934,
      "learning_rate": 1.1566120568172456e-05,
      "loss": 0.4269,
      "step": 26930
    },
    {
      "epoch": 1.7007575757575757,
      "grad_norm": 0.6213828921318054,
      "learning_rate": 1.151843862079195e-05,
      "loss": 0.4615,
      "step": 26940
    },
    {
      "epoch": 1.7013888888888888,
      "grad_norm": 0.39322319626808167,
      "learning_rate": 1.1470849155074425e-05,
      "loss": 0.6304,
      "step": 26950
    },
    {
      "epoch": 1.702020202020202,
      "grad_norm": 0.4449021518230438,
      "learning_rate": 1.1423352220760774e-05,
      "loss": 0.56,
      "step": 26960
    },
    {
      "epoch": 1.7026515151515151,
      "grad_norm": 0.44644904136657715,
      "learning_rate": 1.1375947867495186e-05,
      "loss": 0.4625,
      "step": 26970
    },
    {
      "epoch": 1.7032828282828283,
      "grad_norm": 0.4388924539089203,
      "learning_rate": 1.1328636144825055e-05,
      "loss": 0.4186,
      "step": 26980
    },
    {
      "epoch": 1.7039141414141414,
      "grad_norm": 0.6112365126609802,
      "learning_rate": 1.1281417102201042e-05,
      "loss": 0.4222,
      "step": 26990
    },
    {
      "epoch": 1.7045454545454546,
      "grad_norm": 0.4127504825592041,
      "learning_rate": 1.1234290788976843e-05,
      "loss": 0.5973,
      "step": 27000
    },
    {
      "epoch": 1.7045454545454546,
      "eval_loss": 0.5080034136772156,
      "eval_runtime": 27.0104,
      "eval_samples_per_second": 94.778,
      "eval_steps_per_second": 11.847,
      "step": 27000
    },
    {
      "epoch": 1.7051767676767677,
      "grad_norm": 0.41241076588630676,
      "learning_rate": 1.118725725440929e-05,
      "loss": 0.5128,
      "step": 27010
    },
    {
      "epoch": 1.7058080808080809,
      "grad_norm": 0.45528507232666016,
      "learning_rate": 1.1140316547658203e-05,
      "loss": 0.477,
      "step": 27020
    },
    {
      "epoch": 1.706439393939394,
      "grad_norm": 0.42525845766067505,
      "learning_rate": 1.1093468717786427e-05,
      "loss": 0.4062,
      "step": 27030
    },
    {
      "epoch": 1.7070707070707072,
      "grad_norm": 0.7073312401771545,
      "learning_rate": 1.104671381375968e-05,
      "loss": 0.4473,
      "step": 27040
    },
    {
      "epoch": 1.7077020202020203,
      "grad_norm": 0.39191934466362,
      "learning_rate": 1.1000051884446593e-05,
      "loss": 0.6317,
      "step": 27050
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 0.42459115386009216,
      "learning_rate": 1.0953482978618601e-05,
      "loss": 0.5232,
      "step": 27060
    },
    {
      "epoch": 1.7089646464646466,
      "grad_norm": 0.521478533744812,
      "learning_rate": 1.0907007144949877e-05,
      "loss": 0.4723,
      "step": 27070
    },
    {
      "epoch": 1.7095959595959596,
      "grad_norm": 0.4662797451019287,
      "learning_rate": 1.0860624432017396e-05,
      "loss": 0.4102,
      "step": 27080
    },
    {
      "epoch": 1.7102272727272727,
      "grad_norm": 0.6875424981117249,
      "learning_rate": 1.0814334888300737e-05,
      "loss": 0.4587,
      "step": 27090
    },
    {
      "epoch": 1.7108585858585859,
      "grad_norm": 0.39388108253479004,
      "learning_rate": 1.0768138562182106e-05,
      "loss": 0.658,
      "step": 27100
    },
    {
      "epoch": 1.711489898989899,
      "grad_norm": 0.4671901762485504,
      "learning_rate": 1.0722035501946282e-05,
      "loss": 0.5597,
      "step": 27110
    },
    {
      "epoch": 1.7121212121212122,
      "grad_norm": 0.44498902559280396,
      "learning_rate": 1.0676025755780595e-05,
      "loss": 0.4508,
      "step": 27120
    },
    {
      "epoch": 1.7127525252525253,
      "grad_norm": 0.47709572315216064,
      "learning_rate": 1.0630109371774798e-05,
      "loss": 0.4247,
      "step": 27130
    },
    {
      "epoch": 1.7133838383838382,
      "grad_norm": 0.6531805992126465,
      "learning_rate": 1.0584286397921084e-05,
      "loss": 0.4207,
      "step": 27140
    },
    {
      "epoch": 1.7140151515151514,
      "grad_norm": 0.43079859018325806,
      "learning_rate": 1.0538556882113981e-05,
      "loss": 0.6366,
      "step": 27150
    },
    {
      "epoch": 1.7146464646464645,
      "grad_norm": 0.3802516758441925,
      "learning_rate": 1.049292087215037e-05,
      "loss": 0.5179,
      "step": 27160
    },
    {
      "epoch": 1.7152777777777777,
      "grad_norm": 0.44202375411987305,
      "learning_rate": 1.0447378415729414e-05,
      "loss": 0.4605,
      "step": 27170
    },
    {
      "epoch": 1.7159090909090908,
      "grad_norm": 0.42103806138038635,
      "learning_rate": 1.0401929560452438e-05,
      "loss": 0.4061,
      "step": 27180
    },
    {
      "epoch": 1.716540404040404,
      "grad_norm": 0.7452353239059448,
      "learning_rate": 1.035657435382298e-05,
      "loss": 0.453,
      "step": 27190
    },
    {
      "epoch": 1.7171717171717171,
      "grad_norm": 0.363778680562973,
      "learning_rate": 1.0311312843246646e-05,
      "loss": 0.6409,
      "step": 27200
    },
    {
      "epoch": 1.7178030303030303,
      "grad_norm": 0.4214743971824646,
      "learning_rate": 1.0266145076031186e-05,
      "loss": 0.548,
      "step": 27210
    },
    {
      "epoch": 1.7184343434343434,
      "grad_norm": 0.414027601480484,
      "learning_rate": 1.0221071099386303e-05,
      "loss": 0.4567,
      "step": 27220
    },
    {
      "epoch": 1.7190656565656566,
      "grad_norm": 0.4672662019729614,
      "learning_rate": 1.0176090960423701e-05,
      "loss": 0.4287,
      "step": 27230
    },
    {
      "epoch": 1.7196969696969697,
      "grad_norm": 0.6627621054649353,
      "learning_rate": 1.0131204706156972e-05,
      "loss": 0.4277,
      "step": 27240
    },
    {
      "epoch": 1.7203282828282829,
      "grad_norm": 0.41942164301872253,
      "learning_rate": 1.0086412383501643e-05,
      "loss": 0.6437,
      "step": 27250
    },
    {
      "epoch": 1.720959595959596,
      "grad_norm": 0.39773592352867126,
      "learning_rate": 1.0041714039274986e-05,
      "loss": 0.5447,
      "step": 27260
    },
    {
      "epoch": 1.7215909090909092,
      "grad_norm": 0.4620871841907501,
      "learning_rate": 9.997109720196097e-06,
      "loss": 0.4914,
      "step": 27270
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 0.4407508969306946,
      "learning_rate": 9.952599472885781e-06,
      "loss": 0.4129,
      "step": 27280
    },
    {
      "epoch": 1.7228535353535355,
      "grad_norm": 0.6385383009910583,
      "learning_rate": 9.908183343866496e-06,
      "loss": 0.4481,
      "step": 27290
    },
    {
      "epoch": 1.7234848484848486,
      "grad_norm": 0.4019645154476166,
      "learning_rate": 9.86386137956239e-06,
      "loss": 0.6004,
      "step": 27300
    },
    {
      "epoch": 1.7241161616161618,
      "grad_norm": 0.3936876058578491,
      "learning_rate": 9.819633626299118e-06,
      "loss": 0.5252,
      "step": 27310
    },
    {
      "epoch": 1.7247474747474747,
      "grad_norm": 0.43007829785346985,
      "learning_rate": 9.775500130303905e-06,
      "loss": 0.4679,
      "step": 27320
    },
    {
      "epoch": 1.7253787878787878,
      "grad_norm": 0.4537234604358673,
      "learning_rate": 9.731460937705428e-06,
      "loss": 0.422,
      "step": 27330
    },
    {
      "epoch": 1.726010101010101,
      "grad_norm": 0.6721708178520203,
      "learning_rate": 9.687516094533855e-06,
      "loss": 0.4358,
      "step": 27340
    },
    {
      "epoch": 1.7266414141414141,
      "grad_norm": 0.39443856477737427,
      "learning_rate": 9.643665646720678e-06,
      "loss": 0.6369,
      "step": 27350
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.41347044706344604,
      "learning_rate": 9.599909640098748e-06,
      "loss": 0.5264,
      "step": 27360
    },
    {
      "epoch": 1.7279040404040404,
      "grad_norm": 0.42487892508506775,
      "learning_rate": 9.556248120402201e-06,
      "loss": 0.4635,
      "step": 27370
    },
    {
      "epoch": 1.7285353535353534,
      "grad_norm": 0.43486925959587097,
      "learning_rate": 9.512681133266444e-06,
      "loss": 0.4339,
      "step": 27380
    },
    {
      "epoch": 1.7291666666666665,
      "grad_norm": 0.6719384789466858,
      "learning_rate": 9.46920872422804e-06,
      "loss": 0.4249,
      "step": 27390
    },
    {
      "epoch": 1.7297979797979797,
      "grad_norm": 0.42312857508659363,
      "learning_rate": 9.425830938724712e-06,
      "loss": 0.6435,
      "step": 27400
    },
    {
      "epoch": 1.7304292929292928,
      "grad_norm": 0.39697521924972534,
      "learning_rate": 9.382547822095278e-06,
      "loss": 0.5386,
      "step": 27410
    },
    {
      "epoch": 1.731060606060606,
      "grad_norm": 0.4386359453201294,
      "learning_rate": 9.339359419579607e-06,
      "loss": 0.4789,
      "step": 27420
    },
    {
      "epoch": 1.7316919191919191,
      "grad_norm": 0.44774898886680603,
      "learning_rate": 9.296265776318592e-06,
      "loss": 0.4288,
      "step": 27430
    },
    {
      "epoch": 1.7323232323232323,
      "grad_norm": 0.6963292956352234,
      "learning_rate": 9.253266937354066e-06,
      "loss": 0.4568,
      "step": 27440
    },
    {
      "epoch": 1.7329545454545454,
      "grad_norm": 0.3737088739871979,
      "learning_rate": 9.210362947628759e-06,
      "loss": 0.6457,
      "step": 27450
    },
    {
      "epoch": 1.7335858585858586,
      "grad_norm": 0.4268574118614197,
      "learning_rate": 9.167553851986265e-06,
      "loss": 0.5446,
      "step": 27460
    },
    {
      "epoch": 1.7342171717171717,
      "grad_norm": 0.4064997732639313,
      "learning_rate": 9.124839695171039e-06,
      "loss": 0.4545,
      "step": 27470
    },
    {
      "epoch": 1.7348484848484849,
      "grad_norm": 0.48901504278182983,
      "learning_rate": 9.082220521828266e-06,
      "loss": 0.3971,
      "step": 27480
    },
    {
      "epoch": 1.735479797979798,
      "grad_norm": 0.6898402571678162,
      "learning_rate": 9.03969637650386e-06,
      "loss": 0.4331,
      "step": 27490
    },
    {
      "epoch": 1.7361111111111112,
      "grad_norm": 0.3791167438030243,
      "learning_rate": 8.997267303644385e-06,
      "loss": 0.6352,
      "step": 27500
    },
    {
      "epoch": 1.7367424242424243,
      "grad_norm": 0.44029945135116577,
      "learning_rate": 8.954933347597071e-06,
      "loss": 0.523,
      "step": 27510
    },
    {
      "epoch": 1.7373737373737375,
      "grad_norm": 0.43430545926094055,
      "learning_rate": 8.912694552609747e-06,
      "loss": 0.48,
      "step": 27520
    },
    {
      "epoch": 1.7380050505050506,
      "grad_norm": 0.4805610179901123,
      "learning_rate": 8.870550962830738e-06,
      "loss": 0.3988,
      "step": 27530
    },
    {
      "epoch": 1.7386363636363638,
      "grad_norm": 0.6732788681983948,
      "learning_rate": 8.828502622308864e-06,
      "loss": 0.429,
      "step": 27540
    },
    {
      "epoch": 1.739267676767677,
      "grad_norm": 0.38399314880371094,
      "learning_rate": 8.786549574993385e-06,
      "loss": 0.6485,
      "step": 27550
    },
    {
      "epoch": 1.73989898989899,
      "grad_norm": 0.4111500680446625,
      "learning_rate": 8.744691864734023e-06,
      "loss": 0.5229,
      "step": 27560
    },
    {
      "epoch": 1.740530303030303,
      "grad_norm": 0.41498667001724243,
      "learning_rate": 8.702929535280769e-06,
      "loss": 0.4819,
      "step": 27570
    },
    {
      "epoch": 1.7411616161616161,
      "grad_norm": 0.44993364810943604,
      "learning_rate": 8.661262630283962e-06,
      "loss": 0.4353,
      "step": 27580
    },
    {
      "epoch": 1.7417929292929293,
      "grad_norm": 0.6720306277275085,
      "learning_rate": 8.619691193294188e-06,
      "loss": 0.4197,
      "step": 27590
    },
    {
      "epoch": 1.7424242424242424,
      "grad_norm": 0.38782310485839844,
      "learning_rate": 8.578215267762291e-06,
      "loss": 0.6365,
      "step": 27600
    },
    {
      "epoch": 1.7430555555555556,
      "grad_norm": 0.4087623655796051,
      "learning_rate": 8.536834897039225e-06,
      "loss": 0.5259,
      "step": 27610
    },
    {
      "epoch": 1.7436868686868687,
      "grad_norm": 0.41990265250205994,
      "learning_rate": 8.495550124376106e-06,
      "loss": 0.4461,
      "step": 27620
    },
    {
      "epoch": 1.7443181818181817,
      "grad_norm": 0.4073118269443512,
      "learning_rate": 8.454360992924115e-06,
      "loss": 0.4248,
      "step": 27630
    },
    {
      "epoch": 1.7449494949494948,
      "grad_norm": 0.6742762923240662,
      "learning_rate": 8.41326754573447e-06,
      "loss": 0.4754,
      "step": 27640
    },
    {
      "epoch": 1.745580808080808,
      "grad_norm": 0.40440207719802856,
      "learning_rate": 8.372269825758405e-06,
      "loss": 0.6359,
      "step": 27650
    },
    {
      "epoch": 1.746212121212121,
      "grad_norm": 0.4247008264064789,
      "learning_rate": 8.331367875847063e-06,
      "loss": 0.5291,
      "step": 27660
    },
    {
      "epoch": 1.7468434343434343,
      "grad_norm": 0.4461291432380676,
      "learning_rate": 8.290561738751501e-06,
      "loss": 0.4784,
      "step": 27670
    },
    {
      "epoch": 1.7474747474747474,
      "grad_norm": 0.4549032747745514,
      "learning_rate": 8.249851457122626e-06,
      "loss": 0.4218,
      "step": 27680
    },
    {
      "epoch": 1.7481060606060606,
      "grad_norm": 0.6600461006164551,
      "learning_rate": 8.209237073511178e-06,
      "loss": 0.4455,
      "step": 27690
    },
    {
      "epoch": 1.7487373737373737,
      "grad_norm": 0.3873669505119324,
      "learning_rate": 8.168718630367655e-06,
      "loss": 0.642,
      "step": 27700
    },
    {
      "epoch": 1.7493686868686869,
      "grad_norm": 0.4332106113433838,
      "learning_rate": 8.12829617004225e-06,
      "loss": 0.5472,
      "step": 27710
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.4315853714942932,
      "learning_rate": 8.087969734784839e-06,
      "loss": 0.4782,
      "step": 27720
    },
    {
      "epoch": 1.7506313131313131,
      "grad_norm": 0.4350881278514862,
      "learning_rate": 8.047739366744989e-06,
      "loss": 0.4165,
      "step": 27730
    },
    {
      "epoch": 1.7512626262626263,
      "grad_norm": 0.7229750752449036,
      "learning_rate": 8.007605107971804e-06,
      "loss": 0.4483,
      "step": 27740
    },
    {
      "epoch": 1.7518939393939394,
      "grad_norm": 0.3949470520019531,
      "learning_rate": 7.967567000413922e-06,
      "loss": 0.6263,
      "step": 27750
    },
    {
      "epoch": 1.7525252525252526,
      "grad_norm": 0.4197571575641632,
      "learning_rate": 7.927625085919533e-06,
      "loss": 0.51,
      "step": 27760
    },
    {
      "epoch": 1.7531565656565657,
      "grad_norm": 0.3887924253940582,
      "learning_rate": 7.887779406236217e-06,
      "loss": 0.4419,
      "step": 27770
    },
    {
      "epoch": 1.753787878787879,
      "grad_norm": 0.4353720545768738,
      "learning_rate": 7.848030003011064e-06,
      "loss": 0.4267,
      "step": 27780
    },
    {
      "epoch": 1.754419191919192,
      "grad_norm": 0.6810263991355896,
      "learning_rate": 7.80837691779045e-06,
      "loss": 0.4481,
      "step": 27790
    },
    {
      "epoch": 1.7550505050505052,
      "grad_norm": 0.4080912470817566,
      "learning_rate": 7.76882019202011e-06,
      "loss": 0.6036,
      "step": 27800
    },
    {
      "epoch": 1.7556818181818183,
      "grad_norm": 0.39756399393081665,
      "learning_rate": 7.729359867045038e-06,
      "loss": 0.5279,
      "step": 27810
    },
    {
      "epoch": 1.7563131313131313,
      "grad_norm": 0.4077172577381134,
      "learning_rate": 7.689995984109555e-06,
      "loss": 0.4911,
      "step": 27820
    },
    {
      "epoch": 1.7569444444444444,
      "grad_norm": 0.42099085450172424,
      "learning_rate": 7.650728584357069e-06,
      "loss": 0.4092,
      "step": 27830
    },
    {
      "epoch": 1.7575757575757576,
      "grad_norm": 0.7809528112411499,
      "learning_rate": 7.611557708830208e-06,
      "loss": 0.4403,
      "step": 27840
    },
    {
      "epoch": 1.7582070707070707,
      "grad_norm": 0.40139755606651306,
      "learning_rate": 7.572483398470709e-06,
      "loss": 0.6562,
      "step": 27850
    },
    {
      "epoch": 1.7588383838383839,
      "grad_norm": 0.42048242688179016,
      "learning_rate": 7.5335056941193246e-06,
      "loss": 0.5306,
      "step": 27860
    },
    {
      "epoch": 1.759469696969697,
      "grad_norm": 0.42488566040992737,
      "learning_rate": 7.494624636515924e-06,
      "loss": 0.4565,
      "step": 27870
    },
    {
      "epoch": 1.76010101010101,
      "grad_norm": 0.41689229011535645,
      "learning_rate": 7.4558402662992855e-06,
      "loss": 0.4119,
      "step": 27880
    },
    {
      "epoch": 1.760732323232323,
      "grad_norm": 0.62203449010849,
      "learning_rate": 7.417152624007162e-06,
      "loss": 0.4588,
      "step": 27890
    },
    {
      "epoch": 1.7613636363636362,
      "grad_norm": 0.3827669024467468,
      "learning_rate": 7.378561750076174e-06,
      "loss": 0.6209,
      "step": 27900
    },
    {
      "epoch": 1.7619949494949494,
      "grad_norm": 0.4140413701534271,
      "learning_rate": 7.340067684841878e-06,
      "loss": 0.5531,
      "step": 27910
    },
    {
      "epoch": 1.7626262626262625,
      "grad_norm": 0.44085991382598877,
      "learning_rate": 7.301670468538546e-06,
      "loss": 0.4695,
      "step": 27920
    },
    {
      "epoch": 1.7632575757575757,
      "grad_norm": 0.47442445158958435,
      "learning_rate": 7.263370141299286e-06,
      "loss": 0.4056,
      "step": 27930
    },
    {
      "epoch": 1.7638888888888888,
      "grad_norm": 0.6516683101654053,
      "learning_rate": 7.225166743155886e-06,
      "loss": 0.4305,
      "step": 27940
    },
    {
      "epoch": 1.764520202020202,
      "grad_norm": 0.38749340176582336,
      "learning_rate": 7.187060314038907e-06,
      "loss": 0.6042,
      "step": 27950
    },
    {
      "epoch": 1.7651515151515151,
      "grad_norm": 0.3974403440952301,
      "learning_rate": 7.149050893777476e-06,
      "loss": 0.5445,
      "step": 27960
    },
    {
      "epoch": 1.7657828282828283,
      "grad_norm": 0.45322978496551514,
      "learning_rate": 7.11113852209937e-06,
      "loss": 0.4666,
      "step": 27970
    },
    {
      "epoch": 1.7664141414141414,
      "grad_norm": 0.43115583062171936,
      "learning_rate": 7.073323238630891e-06,
      "loss": 0.452,
      "step": 27980
    },
    {
      "epoch": 1.7670454545454546,
      "grad_norm": 0.6510546803474426,
      "learning_rate": 7.03560508289689e-06,
      "loss": 0.4538,
      "step": 27990
    },
    {
      "epoch": 1.7676767676767677,
      "grad_norm": 0.387446790933609,
      "learning_rate": 6.9979840943207196e-06,
      "loss": 0.6083,
      "step": 28000
    },
    {
      "epoch": 1.7676767676767677,
      "eval_loss": 0.5071060061454773,
      "eval_runtime": 27.0304,
      "eval_samples_per_second": 94.708,
      "eval_steps_per_second": 11.839,
      "step": 28000
    },
    {
      "epoch": 1.7683080808080809,
      "grad_norm": 0.4072693884372711,
      "learning_rate": 6.960460312224126e-06,
      "loss": 0.532,
      "step": 28010
    },
    {
      "epoch": 1.768939393939394,
      "grad_norm": 0.45786526799201965,
      "learning_rate": 6.923033775827304e-06,
      "loss": 0.485,
      "step": 28020
    },
    {
      "epoch": 1.7695707070707072,
      "grad_norm": 0.502916157245636,
      "learning_rate": 6.885704524248737e-06,
      "loss": 0.4184,
      "step": 28030
    },
    {
      "epoch": 1.7702020202020203,
      "grad_norm": 0.6643739342689514,
      "learning_rate": 6.848472596505329e-06,
      "loss": 0.4171,
      "step": 28040
    },
    {
      "epoch": 1.7708333333333335,
      "grad_norm": 0.37872055172920227,
      "learning_rate": 6.811338031512149e-06,
      "loss": 0.5839,
      "step": 28050
    },
    {
      "epoch": 1.7714646464646466,
      "grad_norm": 0.40221700072288513,
      "learning_rate": 6.774300868082584e-06,
      "loss": 0.513,
      "step": 28060
    },
    {
      "epoch": 1.7720959595959596,
      "grad_norm": 0.43679237365722656,
      "learning_rate": 6.7373611449281556e-06,
      "loss": 0.4474,
      "step": 28070
    },
    {
      "epoch": 1.7727272727272727,
      "grad_norm": 0.4421585500240326,
      "learning_rate": 6.700518900658592e-06,
      "loss": 0.4097,
      "step": 28080
    },
    {
      "epoch": 1.7733585858585859,
      "grad_norm": 0.6928813457489014,
      "learning_rate": 6.663774173781723e-06,
      "loss": 0.4284,
      "step": 28090
    },
    {
      "epoch": 1.773989898989899,
      "grad_norm": 0.3861277103424072,
      "learning_rate": 6.627127002703415e-06,
      "loss": 0.6212,
      "step": 28100
    },
    {
      "epoch": 1.7746212121212122,
      "grad_norm": 0.4134763777256012,
      "learning_rate": 6.590577425727606e-06,
      "loss": 0.5365,
      "step": 28110
    },
    {
      "epoch": 1.7752525252525253,
      "grad_norm": 0.41844528913497925,
      "learning_rate": 6.554125481056206e-06,
      "loss": 0.4735,
      "step": 28120
    },
    {
      "epoch": 1.7758838383838382,
      "grad_norm": 0.4382152557373047,
      "learning_rate": 6.517771206789125e-06,
      "loss": 0.4136,
      "step": 28130
    },
    {
      "epoch": 1.7765151515151514,
      "grad_norm": 0.7379689812660217,
      "learning_rate": 6.481514640924124e-06,
      "loss": 0.4281,
      "step": 28140
    },
    {
      "epoch": 1.7771464646464645,
      "grad_norm": 0.3714727759361267,
      "learning_rate": 6.445355821356868e-06,
      "loss": 0.6429,
      "step": 28150
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.39278125762939453,
      "learning_rate": 6.409294785880848e-06,
      "loss": 0.5161,
      "step": 28160
    },
    {
      "epoch": 1.7784090909090908,
      "grad_norm": 0.4346485435962677,
      "learning_rate": 6.373331572187391e-06,
      "loss": 0.4751,
      "step": 28170
    },
    {
      "epoch": 1.779040404040404,
      "grad_norm": 0.4524182975292206,
      "learning_rate": 6.337466217865528e-06,
      "loss": 0.4014,
      "step": 28180
    },
    {
      "epoch": 1.7796717171717171,
      "grad_norm": 0.6273704171180725,
      "learning_rate": 6.3016987604020195e-06,
      "loss": 0.446,
      "step": 28190
    },
    {
      "epoch": 1.7803030303030303,
      "grad_norm": 0.37421149015426636,
      "learning_rate": 6.266029237181315e-06,
      "loss": 0.6121,
      "step": 28200
    },
    {
      "epoch": 1.7809343434343434,
      "grad_norm": 0.39970293641090393,
      "learning_rate": 6.23045768548548e-06,
      "loss": 0.522,
      "step": 28210
    },
    {
      "epoch": 1.7815656565656566,
      "grad_norm": 0.40363845229148865,
      "learning_rate": 6.19498414249422e-06,
      "loss": 0.4783,
      "step": 28220
    },
    {
      "epoch": 1.7821969696969697,
      "grad_norm": 0.4351358413696289,
      "learning_rate": 6.159608645284776e-06,
      "loss": 0.4198,
      "step": 28230
    },
    {
      "epoch": 1.7828282828282829,
      "grad_norm": 0.6440562605857849,
      "learning_rate": 6.124331230831892e-06,
      "loss": 0.4122,
      "step": 28240
    },
    {
      "epoch": 1.783459595959596,
      "grad_norm": 0.38826167583465576,
      "learning_rate": 6.08915193600782e-06,
      "loss": 0.649,
      "step": 28250
    },
    {
      "epoch": 1.7840909090909092,
      "grad_norm": 0.4515387713909149,
      "learning_rate": 6.054070797582267e-06,
      "loss": 0.4896,
      "step": 28260
    },
    {
      "epoch": 1.7847222222222223,
      "grad_norm": 0.4540277123451233,
      "learning_rate": 6.019087852222316e-06,
      "loss": 0.4521,
      "step": 28270
    },
    {
      "epoch": 1.7853535353535355,
      "grad_norm": 0.4440763592720032,
      "learning_rate": 5.98420313649245e-06,
      "loss": 0.4049,
      "step": 28280
    },
    {
      "epoch": 1.7859848484848486,
      "grad_norm": 0.6742933988571167,
      "learning_rate": 5.949416686854426e-06,
      "loss": 0.443,
      "step": 28290
    },
    {
      "epoch": 1.7866161616161618,
      "grad_norm": 0.37878718972206116,
      "learning_rate": 5.914728539667369e-06,
      "loss": 0.6593,
      "step": 28300
    },
    {
      "epoch": 1.7872474747474747,
      "grad_norm": 0.39596596360206604,
      "learning_rate": 5.880138731187601e-06,
      "loss": 0.5316,
      "step": 28310
    },
    {
      "epoch": 1.7878787878787878,
      "grad_norm": 0.40239447355270386,
      "learning_rate": 5.84564729756868e-06,
      "loss": 0.4786,
      "step": 28320
    },
    {
      "epoch": 1.788510101010101,
      "grad_norm": 0.4722409248352051,
      "learning_rate": 5.811254274861344e-06,
      "loss": 0.4496,
      "step": 28330
    },
    {
      "epoch": 1.7891414141414141,
      "grad_norm": 0.7197638154029846,
      "learning_rate": 5.77695969901344e-06,
      "loss": 0.4403,
      "step": 28340
    },
    {
      "epoch": 1.7897727272727273,
      "grad_norm": 0.3756431043148041,
      "learning_rate": 5.742763605869983e-06,
      "loss": 0.6647,
      "step": 28350
    },
    {
      "epoch": 1.7904040404040404,
      "grad_norm": 0.3834754228591919,
      "learning_rate": 5.708666031172993e-06,
      "loss": 0.512,
      "step": 28360
    },
    {
      "epoch": 1.7910353535353534,
      "grad_norm": 0.4252083897590637,
      "learning_rate": 5.674667010561541e-06,
      "loss": 0.4557,
      "step": 28370
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 0.4601663053035736,
      "learning_rate": 5.640766579571665e-06,
      "loss": 0.4384,
      "step": 28380
    },
    {
      "epoch": 1.7922979797979797,
      "grad_norm": 0.6541887521743774,
      "learning_rate": 5.606964773636425e-06,
      "loss": 0.4434,
      "step": 28390
    },
    {
      "epoch": 1.7929292929292928,
      "grad_norm": 0.4100918769836426,
      "learning_rate": 5.573261628085702e-06,
      "loss": 0.6467,
      "step": 28400
    },
    {
      "epoch": 1.793560606060606,
      "grad_norm": 0.4118904769420624,
      "learning_rate": 5.53965717814634e-06,
      "loss": 0.506,
      "step": 28410
    },
    {
      "epoch": 1.7941919191919191,
      "grad_norm": 0.43645742535591125,
      "learning_rate": 5.506151458941955e-06,
      "loss": 0.4716,
      "step": 28420
    },
    {
      "epoch": 1.7948232323232323,
      "grad_norm": 0.4508480131626129,
      "learning_rate": 5.472744505493033e-06,
      "loss": 0.4138,
      "step": 28430
    },
    {
      "epoch": 1.7954545454545454,
      "grad_norm": 0.623839259147644,
      "learning_rate": 5.439436352716776e-06,
      "loss": 0.4357,
      "step": 28440
    },
    {
      "epoch": 1.7960858585858586,
      "grad_norm": 0.40596479177474976,
      "learning_rate": 5.40622703542717e-06,
      "loss": 0.5978,
      "step": 28450
    },
    {
      "epoch": 1.7967171717171717,
      "grad_norm": 0.4452552795410156,
      "learning_rate": 5.373116588334836e-06,
      "loss": 0.5281,
      "step": 28460
    },
    {
      "epoch": 1.7973484848484849,
      "grad_norm": 0.43639835715293884,
      "learning_rate": 5.340105046047095e-06,
      "loss": 0.4717,
      "step": 28470
    },
    {
      "epoch": 1.797979797979798,
      "grad_norm": 0.4729858338832855,
      "learning_rate": 5.30719244306791e-06,
      "loss": 0.412,
      "step": 28480
    },
    {
      "epoch": 1.7986111111111112,
      "grad_norm": 0.7158402800559998,
      "learning_rate": 5.274378813797798e-06,
      "loss": 0.4463,
      "step": 28490
    },
    {
      "epoch": 1.7992424242424243,
      "grad_norm": 0.38511526584625244,
      "learning_rate": 5.241664192533824e-06,
      "loss": 0.602,
      "step": 28500
    },
    {
      "epoch": 1.7998737373737375,
      "grad_norm": 0.43598541617393494,
      "learning_rate": 5.209048613469569e-06,
      "loss": 0.5351,
      "step": 28510
    },
    {
      "epoch": 1.8005050505050506,
      "grad_norm": 0.41504865884780884,
      "learning_rate": 5.176532110695153e-06,
      "loss": 0.4795,
      "step": 28520
    },
    {
      "epoch": 1.8011363636363638,
      "grad_norm": 0.4786302447319031,
      "learning_rate": 5.144114718197057e-06,
      "loss": 0.4229,
      "step": 28530
    },
    {
      "epoch": 1.801767676767677,
      "grad_norm": 0.6773526668548584,
      "learning_rate": 5.111796469858232e-06,
      "loss": 0.4196,
      "step": 28540
    },
    {
      "epoch": 1.80239898989899,
      "grad_norm": 0.3953631818294525,
      "learning_rate": 5.079577399457946e-06,
      "loss": 0.5994,
      "step": 28550
    },
    {
      "epoch": 1.803030303030303,
      "grad_norm": 0.5392120480537415,
      "learning_rate": 5.047457540671873e-06,
      "loss": 0.528,
      "step": 28560
    },
    {
      "epoch": 1.8036616161616161,
      "grad_norm": 0.42312416434288025,
      "learning_rate": 5.015436927071948e-06,
      "loss": 0.4798,
      "step": 28570
    },
    {
      "epoch": 1.8042929292929293,
      "grad_norm": 0.4176766872406006,
      "learning_rate": 4.983515592126375e-06,
      "loss": 0.4127,
      "step": 28580
    },
    {
      "epoch": 1.8049242424242424,
      "grad_norm": 0.6546382308006287,
      "learning_rate": 4.9516935691996e-06,
      "loss": 0.4431,
      "step": 28590
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 0.3516315817832947,
      "learning_rate": 4.919970891552261e-06,
      "loss": 0.6197,
      "step": 28600
    },
    {
      "epoch": 1.8061868686868687,
      "grad_norm": 0.40217167139053345,
      "learning_rate": 4.888347592341181e-06,
      "loss": 0.516,
      "step": 28610
    },
    {
      "epoch": 1.8068181818181817,
      "grad_norm": 0.406975120306015,
      "learning_rate": 4.856823704619285e-06,
      "loss": 0.4551,
      "step": 28620
    },
    {
      "epoch": 1.8074494949494948,
      "grad_norm": 0.49915918707847595,
      "learning_rate": 4.825399261335595e-06,
      "loss": 0.4404,
      "step": 28630
    },
    {
      "epoch": 1.808080808080808,
      "grad_norm": 0.6185495853424072,
      "learning_rate": 4.794074295335205e-06,
      "loss": 0.454,
      "step": 28640
    },
    {
      "epoch": 1.808712121212121,
      "grad_norm": 0.37139394879341125,
      "learning_rate": 4.762848839359235e-06,
      "loss": 0.61,
      "step": 28650
    },
    {
      "epoch": 1.8093434343434343,
      "grad_norm": 0.39512544870376587,
      "learning_rate": 4.7317229260447906e-06,
      "loss": 0.5323,
      "step": 28660
    },
    {
      "epoch": 1.8099747474747474,
      "grad_norm": 0.4140406847000122,
      "learning_rate": 4.700696587924936e-06,
      "loss": 0.4705,
      "step": 28670
    },
    {
      "epoch": 1.8106060606060606,
      "grad_norm": 0.4221775531768799,
      "learning_rate": 4.669769857428652e-06,
      "loss": 0.4095,
      "step": 28680
    },
    {
      "epoch": 1.8112373737373737,
      "grad_norm": 0.6621573567390442,
      "learning_rate": 4.638942766880805e-06,
      "loss": 0.4412,
      "step": 28690
    },
    {
      "epoch": 1.8118686868686869,
      "grad_norm": 0.38018810749053955,
      "learning_rate": 4.608215348502154e-06,
      "loss": 0.6184,
      "step": 28700
    },
    {
      "epoch": 1.8125,
      "grad_norm": 0.41721853613853455,
      "learning_rate": 4.577587634409219e-06,
      "loss": 0.5347,
      "step": 28710
    },
    {
      "epoch": 1.8131313131313131,
      "grad_norm": 0.4683428406715393,
      "learning_rate": 4.547059656614372e-06,
      "loss": 0.4588,
      "step": 28720
    },
    {
      "epoch": 1.8137626262626263,
      "grad_norm": 0.4231473207473755,
      "learning_rate": 4.5166314470256765e-06,
      "loss": 0.4144,
      "step": 28730
    },
    {
      "epoch": 1.8143939393939394,
      "grad_norm": 0.7377522587776184,
      "learning_rate": 4.4863030374469814e-06,
      "loss": 0.4348,
      "step": 28740
    },
    {
      "epoch": 1.8150252525252526,
      "grad_norm": 0.3708767592906952,
      "learning_rate": 4.456074459577775e-06,
      "loss": 0.5923,
      "step": 28750
    },
    {
      "epoch": 1.8156565656565657,
      "grad_norm": 0.40152350068092346,
      "learning_rate": 4.425945745013227e-06,
      "loss": 0.5522,
      "step": 28760
    },
    {
      "epoch": 1.816287878787879,
      "grad_norm": 0.44644445180892944,
      "learning_rate": 4.395916925244104e-06,
      "loss": 0.4586,
      "step": 28770
    },
    {
      "epoch": 1.816919191919192,
      "grad_norm": 0.46084463596343994,
      "learning_rate": 4.365988031656798e-06,
      "loss": 0.4222,
      "step": 28780
    },
    {
      "epoch": 1.8175505050505052,
      "grad_norm": 0.6517527103424072,
      "learning_rate": 4.336159095533221e-06,
      "loss": 0.4239,
      "step": 28790
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.41801607608795166,
      "learning_rate": 4.306430148050844e-06,
      "loss": 0.6273,
      "step": 28800
    },
    {
      "epoch": 1.8188131313131313,
      "grad_norm": 0.38014334440231323,
      "learning_rate": 4.276801220282589e-06,
      "loss": 0.5422,
      "step": 28810
    },
    {
      "epoch": 1.8194444444444444,
      "grad_norm": 0.43440675735473633,
      "learning_rate": 4.247272343196851e-06,
      "loss": 0.4476,
      "step": 28820
    },
    {
      "epoch": 1.8200757575757576,
      "grad_norm": 0.4556400179862976,
      "learning_rate": 4.217843547657496e-06,
      "loss": 0.4032,
      "step": 28830
    },
    {
      "epoch": 1.8207070707070707,
      "grad_norm": 0.6571252942085266,
      "learning_rate": 4.188514864423709e-06,
      "loss": 0.426,
      "step": 28840
    },
    {
      "epoch": 1.8213383838383839,
      "grad_norm": 0.40433910489082336,
      "learning_rate": 4.159286324150091e-06,
      "loss": 0.6006,
      "step": 28850
    },
    {
      "epoch": 1.821969696969697,
      "grad_norm": 0.3914039134979248,
      "learning_rate": 4.130157957386549e-06,
      "loss": 0.5134,
      "step": 28860
    },
    {
      "epoch": 1.82260101010101,
      "grad_norm": 0.41930314898490906,
      "learning_rate": 4.1011297945782955e-06,
      "loss": 0.4544,
      "step": 28870
    },
    {
      "epoch": 1.823232323232323,
      "grad_norm": 0.44386979937553406,
      "learning_rate": 4.0722018660658055e-06,
      "loss": 0.4048,
      "step": 28880
    },
    {
      "epoch": 1.8238636363636362,
      "grad_norm": 0.6996903419494629,
      "learning_rate": 4.0433742020848044e-06,
      "loss": 0.4297,
      "step": 28890
    },
    {
      "epoch": 1.8244949494949494,
      "grad_norm": 0.37537965178489685,
      "learning_rate": 4.014646832766167e-06,
      "loss": 0.6361,
      "step": 28900
    },
    {
      "epoch": 1.8251262626262625,
      "grad_norm": 0.47627222537994385,
      "learning_rate": 3.986019788136031e-06,
      "loss": 0.5154,
      "step": 28910
    },
    {
      "epoch": 1.8257575757575757,
      "grad_norm": 0.43284085392951965,
      "learning_rate": 3.9574930981155835e-06,
      "loss": 0.4716,
      "step": 28920
    },
    {
      "epoch": 1.8263888888888888,
      "grad_norm": 0.47500675916671753,
      "learning_rate": 3.929066792521174e-06,
      "loss": 0.4206,
      "step": 28930
    },
    {
      "epoch": 1.827020202020202,
      "grad_norm": 0.6179473400115967,
      "learning_rate": 3.900740901064215e-06,
      "loss": 0.4288,
      "step": 28940
    },
    {
      "epoch": 1.8276515151515151,
      "grad_norm": 0.3800835907459259,
      "learning_rate": 3.872515453351133e-06,
      "loss": 0.6247,
      "step": 28950
    },
    {
      "epoch": 1.8282828282828283,
      "grad_norm": 0.4156925082206726,
      "learning_rate": 3.844390478883453e-06,
      "loss": 0.5102,
      "step": 28960
    },
    {
      "epoch": 1.8289141414141414,
      "grad_norm": 0.46106380224227905,
      "learning_rate": 3.816366007057593e-06,
      "loss": 0.4519,
      "step": 28970
    },
    {
      "epoch": 1.8295454545454546,
      "grad_norm": 0.47998157143592834,
      "learning_rate": 3.788442067164977e-06,
      "loss": 0.4059,
      "step": 28980
    },
    {
      "epoch": 1.8301767676767677,
      "grad_norm": 0.6900421977043152,
      "learning_rate": 3.760618688391926e-06,
      "loss": 0.4382,
      "step": 28990
    },
    {
      "epoch": 1.8308080808080809,
      "grad_norm": 0.4089113771915436,
      "learning_rate": 3.732895899819677e-06,
      "loss": 0.629,
      "step": 29000
    },
    {
      "epoch": 1.8308080808080809,
      "eval_loss": 0.5071499943733215,
      "eval_runtime": 26.9382,
      "eval_samples_per_second": 95.032,
      "eval_steps_per_second": 11.879,
      "step": 29000
    },
    {
      "epoch": 1.831439393939394,
      "grad_norm": 0.43708720803260803,
      "learning_rate": 3.70527373042433e-06,
      "loss": 0.5124,
      "step": 29010
    },
    {
      "epoch": 1.8320707070707072,
      "grad_norm": 0.4035877287387848,
      "learning_rate": 3.6777522090768013e-06,
      "loss": 0.4589,
      "step": 29020
    },
    {
      "epoch": 1.8327020202020203,
      "grad_norm": 0.46937450766563416,
      "learning_rate": 3.6503313645427917e-06,
      "loss": 0.4276,
      "step": 29030
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.679122269153595,
      "learning_rate": 3.6230112254828086e-06,
      "loss": 0.4429,
      "step": 29040
    },
    {
      "epoch": 1.8339646464646466,
      "grad_norm": 0.38301411271095276,
      "learning_rate": 3.59579182045211e-06,
      "loss": 0.582,
      "step": 29050
    },
    {
      "epoch": 1.8345959595959596,
      "grad_norm": 0.4193822145462036,
      "learning_rate": 3.5686731779006165e-06,
      "loss": 0.5075,
      "step": 29060
    },
    {
      "epoch": 1.8352272727272727,
      "grad_norm": 0.4677910804748535,
      "learning_rate": 3.5416553261729657e-06,
      "loss": 0.4659,
      "step": 29070
    },
    {
      "epoch": 1.8358585858585859,
      "grad_norm": 0.4101646840572357,
      "learning_rate": 3.514738293508435e-06,
      "loss": 0.415,
      "step": 29080
    },
    {
      "epoch": 1.836489898989899,
      "grad_norm": 0.6549572944641113,
      "learning_rate": 3.487922108040942e-06,
      "loss": 0.4408,
      "step": 29090
    },
    {
      "epoch": 1.8371212121212122,
      "grad_norm": 0.3907179832458496,
      "learning_rate": 3.461206797798988e-06,
      "loss": 0.6028,
      "step": 29100
    },
    {
      "epoch": 1.8377525252525253,
      "grad_norm": 0.4258978068828583,
      "learning_rate": 3.4345923907056266e-06,
      "loss": 0.5325,
      "step": 29110
    },
    {
      "epoch": 1.8383838383838382,
      "grad_norm": 0.4629825949668884,
      "learning_rate": 3.408078914578461e-06,
      "loss": 0.449,
      "step": 29120
    },
    {
      "epoch": 1.8390151515151514,
      "grad_norm": 0.4329051971435547,
      "learning_rate": 3.3816663971296013e-06,
      "loss": 0.4145,
      "step": 29130
    },
    {
      "epoch": 1.8396464646464645,
      "grad_norm": 0.6830952763557434,
      "learning_rate": 3.3553548659656207e-06,
      "loss": 0.4265,
      "step": 29140
    },
    {
      "epoch": 1.8402777777777777,
      "grad_norm": 0.38268524408340454,
      "learning_rate": 3.329144348587565e-06,
      "loss": 0.6238,
      "step": 29150
    },
    {
      "epoch": 1.8409090909090908,
      "grad_norm": 0.4226663410663605,
      "learning_rate": 3.3030348723908755e-06,
      "loss": 0.5366,
      "step": 29160
    },
    {
      "epoch": 1.841540404040404,
      "grad_norm": 0.42966580390930176,
      "learning_rate": 3.2770264646653783e-06,
      "loss": 0.4607,
      "step": 29170
    },
    {
      "epoch": 1.8421717171717171,
      "grad_norm": 0.430320143699646,
      "learning_rate": 3.2511191525953055e-06,
      "loss": 0.4057,
      "step": 29180
    },
    {
      "epoch": 1.8428030303030303,
      "grad_norm": 0.6997997760772705,
      "learning_rate": 3.225312963259186e-06,
      "loss": 0.4583,
      "step": 29190
    },
    {
      "epoch": 1.8434343434343434,
      "grad_norm": 0.4019269049167633,
      "learning_rate": 3.199607923629855e-06,
      "loss": 0.6324,
      "step": 29200
    },
    {
      "epoch": 1.8440656565656566,
      "grad_norm": 0.4334375560283661,
      "learning_rate": 3.1740040605744204e-06,
      "loss": 0.5459,
      "step": 29210
    },
    {
      "epoch": 1.8446969696969697,
      "grad_norm": 0.41901129484176636,
      "learning_rate": 3.148501400854287e-06,
      "loss": 0.4244,
      "step": 29220
    },
    {
      "epoch": 1.8453282828282829,
      "grad_norm": 0.4624382555484772,
      "learning_rate": 3.123099971125032e-06,
      "loss": 0.4332,
      "step": 29230
    },
    {
      "epoch": 1.845959595959596,
      "grad_norm": 0.6417666673660278,
      "learning_rate": 3.0977997979364405e-06,
      "loss": 0.4482,
      "step": 29240
    },
    {
      "epoch": 1.8465909090909092,
      "grad_norm": 0.37254276871681213,
      "learning_rate": 3.072600907732448e-06,
      "loss": 0.6587,
      "step": 29250
    },
    {
      "epoch": 1.8472222222222223,
      "grad_norm": 0.4432487189769745,
      "learning_rate": 3.0475033268511644e-06,
      "loss": 0.5257,
      "step": 29260
    },
    {
      "epoch": 1.8478535353535355,
      "grad_norm": 0.37863579392433167,
      "learning_rate": 3.0225070815247835e-06,
      "loss": 0.4474,
      "step": 29270
    },
    {
      "epoch": 1.8484848484848486,
      "grad_norm": 0.4559776782989502,
      "learning_rate": 2.9976121978795844e-06,
      "loss": 0.4386,
      "step": 29280
    },
    {
      "epoch": 1.8491161616161618,
      "grad_norm": 0.7043043971061707,
      "learning_rate": 2.97281870193592e-06,
      "loss": 0.4286,
      "step": 29290
    },
    {
      "epoch": 1.8497474747474747,
      "grad_norm": 0.41244375705718994,
      "learning_rate": 2.9481266196081268e-06,
      "loss": 0.61,
      "step": 29300
    },
    {
      "epoch": 1.8503787878787878,
      "grad_norm": 0.40418678522109985,
      "learning_rate": 2.9235359767046167e-06,
      "loss": 0.5331,
      "step": 29310
    },
    {
      "epoch": 1.851010101010101,
      "grad_norm": 0.48707979917526245,
      "learning_rate": 2.899046798927707e-06,
      "loss": 0.4787,
      "step": 29320
    },
    {
      "epoch": 1.8516414141414141,
      "grad_norm": 0.43522974848747253,
      "learning_rate": 2.8746591118736897e-06,
      "loss": 0.427,
      "step": 29330
    },
    {
      "epoch": 1.8522727272727273,
      "grad_norm": 0.6410099864006042,
      "learning_rate": 2.850372941032775e-06,
      "loss": 0.4537,
      "step": 29340
    },
    {
      "epoch": 1.8529040404040404,
      "grad_norm": 0.3959592282772064,
      "learning_rate": 2.826188311789102e-06,
      "loss": 0.6332,
      "step": 29350
    },
    {
      "epoch": 1.8535353535353534,
      "grad_norm": 0.4359036386013031,
      "learning_rate": 2.8021052494206058e-06,
      "loss": 0.5272,
      "step": 29360
    },
    {
      "epoch": 1.8541666666666665,
      "grad_norm": 0.3898739516735077,
      "learning_rate": 2.778123779099129e-06,
      "loss": 0.4523,
      "step": 29370
    },
    {
      "epoch": 1.8547979797979797,
      "grad_norm": 0.4296514391899109,
      "learning_rate": 2.7542439258902984e-06,
      "loss": 0.4204,
      "step": 29380
    },
    {
      "epoch": 1.8554292929292928,
      "grad_norm": 0.7164686918258667,
      "learning_rate": 2.730465714753516e-06,
      "loss": 0.4433,
      "step": 29390
    },
    {
      "epoch": 1.856060606060606,
      "grad_norm": 0.38132157921791077,
      "learning_rate": 2.70678917054199e-06,
      "loss": 0.5763,
      "step": 29400
    },
    {
      "epoch": 1.8566919191919191,
      "grad_norm": 0.4482697546482086,
      "learning_rate": 2.683214318002636e-06,
      "loss": 0.5236,
      "step": 29410
    },
    {
      "epoch": 1.8573232323232323,
      "grad_norm": 0.41005441546440125,
      "learning_rate": 2.6597411817760765e-06,
      "loss": 0.4645,
      "step": 29420
    },
    {
      "epoch": 1.8579545454545454,
      "grad_norm": 0.46083012223243713,
      "learning_rate": 2.636369786396631e-06,
      "loss": 0.4126,
      "step": 29430
    },
    {
      "epoch": 1.8585858585858586,
      "grad_norm": 0.6308289170265198,
      "learning_rate": 2.6131001562922918e-06,
      "loss": 0.4081,
      "step": 29440
    },
    {
      "epoch": 1.8592171717171717,
      "grad_norm": 0.4211837351322174,
      "learning_rate": 2.589932315784649e-06,
      "loss": 0.6132,
      "step": 29450
    },
    {
      "epoch": 1.8598484848484849,
      "grad_norm": 0.4097226858139038,
      "learning_rate": 2.5668662890889316e-06,
      "loss": 0.5259,
      "step": 29460
    },
    {
      "epoch": 1.860479797979798,
      "grad_norm": 0.4053901731967926,
      "learning_rate": 2.5439021003139328e-06,
      "loss": 0.4616,
      "step": 29470
    },
    {
      "epoch": 1.8611111111111112,
      "grad_norm": 0.4626646935939789,
      "learning_rate": 2.5210397734620305e-06,
      "loss": 0.4207,
      "step": 29480
    },
    {
      "epoch": 1.8617424242424243,
      "grad_norm": 0.631087064743042,
      "learning_rate": 2.4982793324291097e-06,
      "loss": 0.4413,
      "step": 29490
    },
    {
      "epoch": 1.8623737373737375,
      "grad_norm": 0.37360623478889465,
      "learning_rate": 2.4756208010045634e-06,
      "loss": 0.6102,
      "step": 29500
    },
    {
      "epoch": 1.8630050505050506,
      "grad_norm": 0.425121009349823,
      "learning_rate": 2.4530642028712914e-06,
      "loss": 0.5392,
      "step": 29510
    },
    {
      "epoch": 1.8636363636363638,
      "grad_norm": 0.4639739394187927,
      "learning_rate": 2.4306095616056234e-06,
      "loss": 0.452,
      "step": 29520
    },
    {
      "epoch": 1.864267676767677,
      "grad_norm": 0.4481338560581207,
      "learning_rate": 2.4082569006773526e-06,
      "loss": 0.4098,
      "step": 29530
    },
    {
      "epoch": 1.86489898989899,
      "grad_norm": 0.6814507842063904,
      "learning_rate": 2.3860062434496678e-06,
      "loss": 0.4453,
      "step": 29540
    },
    {
      "epoch": 1.865530303030303,
      "grad_norm": 0.4012945294380188,
      "learning_rate": 2.3638576131791214e-06,
      "loss": 0.6388,
      "step": 29550
    },
    {
      "epoch": 1.8661616161616161,
      "grad_norm": 0.38844338059425354,
      "learning_rate": 2.3418110330156505e-06,
      "loss": 0.536,
      "step": 29560
    },
    {
      "epoch": 1.8667929292929293,
      "grad_norm": 0.4306163191795349,
      "learning_rate": 2.3198665260025342e-06,
      "loss": 0.4844,
      "step": 29570
    },
    {
      "epoch": 1.8674242424242424,
      "grad_norm": 0.4545679986476898,
      "learning_rate": 2.298024115076347e-06,
      "loss": 0.4237,
      "step": 29580
    },
    {
      "epoch": 1.8680555555555556,
      "grad_norm": 0.7062551975250244,
      "learning_rate": 2.2762838230669715e-06,
      "loss": 0.435,
      "step": 29590
    },
    {
      "epoch": 1.8686868686868687,
      "grad_norm": 0.39881110191345215,
      "learning_rate": 2.2546456726975084e-06,
      "loss": 0.6215,
      "step": 29600
    },
    {
      "epoch": 1.8693181818181817,
      "grad_norm": 0.406459242105484,
      "learning_rate": 2.2331096865843672e-06,
      "loss": 0.5236,
      "step": 29610
    },
    {
      "epoch": 1.8699494949494948,
      "grad_norm": 0.5363574624061584,
      "learning_rate": 2.21167588723713e-06,
      "loss": 0.446,
      "step": 29620
    },
    {
      "epoch": 1.870580808080808,
      "grad_norm": 0.4661993980407715,
      "learning_rate": 2.1903442970585665e-06,
      "loss": 0.4094,
      "step": 29630
    },
    {
      "epoch": 1.871212121212121,
      "grad_norm": 0.7518194913864136,
      "learning_rate": 2.1691149383446517e-06,
      "loss": 0.4232,
      "step": 29640
    },
    {
      "epoch": 1.8718434343434343,
      "grad_norm": 0.3789515495300293,
      "learning_rate": 2.1479878332844703e-06,
      "loss": 0.6106,
      "step": 29650
    },
    {
      "epoch": 1.8724747474747474,
      "grad_norm": 0.39376839995384216,
      "learning_rate": 2.1269630039602582e-06,
      "loss": 0.5036,
      "step": 29660
    },
    {
      "epoch": 1.8731060606060606,
      "grad_norm": 0.4553840756416321,
      "learning_rate": 2.106040472347348e-06,
      "loss": 0.4622,
      "step": 29670
    },
    {
      "epoch": 1.8737373737373737,
      "grad_norm": 0.40890827775001526,
      "learning_rate": 2.085220260314136e-06,
      "loss": 0.3846,
      "step": 29680
    },
    {
      "epoch": 1.8743686868686869,
      "grad_norm": 0.6524300575256348,
      "learning_rate": 2.0645023896220694e-06,
      "loss": 0.4459,
      "step": 29690
    },
    {
      "epoch": 1.875,
      "grad_norm": 0.37531423568725586,
      "learning_rate": 2.0438868819256607e-06,
      "loss": 0.6114,
      "step": 29700
    },
    {
      "epoch": 1.8756313131313131,
      "grad_norm": 0.4177490472793579,
      "learning_rate": 2.0233737587723957e-06,
      "loss": 0.5135,
      "step": 29710
    },
    {
      "epoch": 1.8762626262626263,
      "grad_norm": 0.40183329582214355,
      "learning_rate": 2.002963041602768e-06,
      "loss": 0.4669,
      "step": 29720
    },
    {
      "epoch": 1.8768939393939394,
      "grad_norm": 0.44533082842826843,
      "learning_rate": 1.9826547517502124e-06,
      "loss": 0.4149,
      "step": 29730
    },
    {
      "epoch": 1.8775252525252526,
      "grad_norm": 0.7138758897781372,
      "learning_rate": 1.9624489104411502e-06,
      "loss": 0.4349,
      "step": 29740
    },
    {
      "epoch": 1.8781565656565657,
      "grad_norm": 0.384761244058609,
      "learning_rate": 1.9423455387948653e-06,
      "loss": 0.6545,
      "step": 29750
    },
    {
      "epoch": 1.878787878787879,
      "grad_norm": 0.406581312417984,
      "learning_rate": 1.9223446578235937e-06,
      "loss": 0.5216,
      "step": 29760
    },
    {
      "epoch": 1.879419191919192,
      "grad_norm": 0.4645657241344452,
      "learning_rate": 1.9024462884324135e-06,
      "loss": 0.4499,
      "step": 29770
    },
    {
      "epoch": 1.8800505050505052,
      "grad_norm": 0.5528056025505066,
      "learning_rate": 1.8826504514192543e-06,
      "loss": 0.4262,
      "step": 29780
    },
    {
      "epoch": 1.8806818181818183,
      "grad_norm": 0.6512721180915833,
      "learning_rate": 1.8629571674748991e-06,
      "loss": 0.4441,
      "step": 29790
    },
    {
      "epoch": 1.8813131313131313,
      "grad_norm": 0.37701869010925293,
      "learning_rate": 1.843366457182938e-06,
      "loss": 0.6473,
      "step": 29800
    },
    {
      "epoch": 1.8819444444444444,
      "grad_norm": 0.3994258940219879,
      "learning_rate": 1.8238783410197358e-06,
      "loss": 0.5023,
      "step": 29810
    },
    {
      "epoch": 1.8825757575757576,
      "grad_norm": 0.42110511660575867,
      "learning_rate": 1.8044928393544325e-06,
      "loss": 0.4654,
      "step": 29820
    },
    {
      "epoch": 1.8832070707070707,
      "grad_norm": 0.4425188899040222,
      "learning_rate": 1.78520997244892e-06,
      "loss": 0.4259,
      "step": 29830
    },
    {
      "epoch": 1.8838383838383839,
      "grad_norm": 0.6783208250999451,
      "learning_rate": 1.7660297604578102e-06,
      "loss": 0.421,
      "step": 29840
    },
    {
      "epoch": 1.884469696969697,
      "grad_norm": 0.4096035063266754,
      "learning_rate": 1.7469522234284108e-06,
      "loss": 0.6032,
      "step": 29850
    },
    {
      "epoch": 1.88510101010101,
      "grad_norm": 0.4257996082305908,
      "learning_rate": 1.7279773813007382e-06,
      "loss": 0.4958,
      "step": 29860
    },
    {
      "epoch": 1.885732323232323,
      "grad_norm": 0.4170207977294922,
      "learning_rate": 1.7091052539074393e-06,
      "loss": 0.4473,
      "step": 29870
    },
    {
      "epoch": 1.8863636363636362,
      "grad_norm": 0.4623202681541443,
      "learning_rate": 1.690335860973835e-06,
      "loss": 0.43,
      "step": 29880
    },
    {
      "epoch": 1.8869949494949494,
      "grad_norm": 0.7011020183563232,
      "learning_rate": 1.6716692221178443e-06,
      "loss": 0.443,
      "step": 29890
    },
    {
      "epoch": 1.8876262626262625,
      "grad_norm": 0.39586350321769714,
      "learning_rate": 1.6531053568499822e-06,
      "loss": 0.6263,
      "step": 29900
    },
    {
      "epoch": 1.8882575757575757,
      "grad_norm": 0.40204334259033203,
      "learning_rate": 1.6346442845733612e-06,
      "loss": 0.5292,
      "step": 29910
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.4178355038166046,
      "learning_rate": 1.6162860245836576e-06,
      "loss": 0.4697,
      "step": 29920
    },
    {
      "epoch": 1.889520202020202,
      "grad_norm": 0.4789908230304718,
      "learning_rate": 1.5980305960690667e-06,
      "loss": 0.4325,
      "step": 29930
    },
    {
      "epoch": 1.8901515151515151,
      "grad_norm": 0.6859927773475647,
      "learning_rate": 1.5798780181103256e-06,
      "loss": 0.4536,
      "step": 29940
    },
    {
      "epoch": 1.8907828282828283,
      "grad_norm": 0.3584800362586975,
      "learning_rate": 1.5618283096806351e-06,
      "loss": 0.6601,
      "step": 29950
    },
    {
      "epoch": 1.8914141414141414,
      "grad_norm": 0.4383043348789215,
      "learning_rate": 1.5438814896457155e-06,
      "loss": 0.5123,
      "step": 29960
    },
    {
      "epoch": 1.8920454545454546,
      "grad_norm": 0.4497312903404236,
      "learning_rate": 1.5260375767637504e-06,
      "loss": 0.4662,
      "step": 29970
    },
    {
      "epoch": 1.8926767676767677,
      "grad_norm": 0.4855920076370239,
      "learning_rate": 1.5082965896853096e-06,
      "loss": 0.3955,
      "step": 29980
    },
    {
      "epoch": 1.8933080808080809,
      "grad_norm": 0.6658608913421631,
      "learning_rate": 1.4906585469534495e-06,
      "loss": 0.4292,
      "step": 29990
    },
    {
      "epoch": 1.893939393939394,
      "grad_norm": 0.39562585949897766,
      "learning_rate": 1.4731234670035787e-06,
      "loss": 0.5852,
      "step": 30000
    },
    {
      "epoch": 1.893939393939394,
      "eval_loss": 0.5064666867256165,
      "eval_runtime": 26.9058,
      "eval_samples_per_second": 95.147,
      "eval_steps_per_second": 11.893,
      "step": 30000
    },
    {
      "epoch": 1.8945707070707072,
      "grad_norm": 0.43086040019989014,
      "learning_rate": 1.4556913681635253e-06,
      "loss": 0.5242,
      "step": 30010
    },
    {
      "epoch": 1.8952020202020203,
      "grad_norm": 0.42520469427108765,
      "learning_rate": 1.4383622686534482e-06,
      "loss": 0.4606,
      "step": 30020
    },
    {
      "epoch": 1.8958333333333335,
      "grad_norm": 0.4288230836391449,
      "learning_rate": 1.4211361865858808e-06,
      "loss": 0.4322,
      "step": 30030
    },
    {
      "epoch": 1.8964646464646466,
      "grad_norm": 0.6540399789810181,
      "learning_rate": 1.4040131399656541e-06,
      "loss": 0.4468,
      "step": 30040
    },
    {
      "epoch": 1.8970959595959596,
      "grad_norm": 0.41137734055519104,
      "learning_rate": 1.3869931466899299e-06,
      "loss": 0.6271,
      "step": 30050
    },
    {
      "epoch": 1.8977272727272727,
      "grad_norm": 0.43777579069137573,
      "learning_rate": 1.3700762245481446e-06,
      "loss": 0.5318,
      "step": 30060
    },
    {
      "epoch": 1.8983585858585859,
      "grad_norm": 0.4440259337425232,
      "learning_rate": 1.3532623912219987e-06,
      "loss": 0.4643,
      "step": 30070
    },
    {
      "epoch": 1.898989898989899,
      "grad_norm": 0.4682289659976959,
      "learning_rate": 1.336551664285446e-06,
      "loss": 0.4148,
      "step": 30080
    },
    {
      "epoch": 1.8996212121212122,
      "grad_norm": 0.6334570646286011,
      "learning_rate": 1.3199440612047032e-06,
      "loss": 0.4493,
      "step": 30090
    },
    {
      "epoch": 1.9002525252525253,
      "grad_norm": 0.41128337383270264,
      "learning_rate": 1.3034395993381521e-06,
      "loss": 0.6435,
      "step": 30100
    },
    {
      "epoch": 1.9008838383838382,
      "grad_norm": 0.42423391342163086,
      "learning_rate": 1.2870382959363937e-06,
      "loss": 0.5439,
      "step": 30110
    },
    {
      "epoch": 1.9015151515151514,
      "grad_norm": 0.4165758490562439,
      "learning_rate": 1.270740168142215e-06,
      "loss": 0.473,
      "step": 30120
    },
    {
      "epoch": 1.9021464646464645,
      "grad_norm": 0.4741300046443939,
      "learning_rate": 1.2545452329905449e-06,
      "loss": 0.4475,
      "step": 30130
    },
    {
      "epoch": 1.9027777777777777,
      "grad_norm": 0.6712568402290344,
      "learning_rate": 1.2384535074084769e-06,
      "loss": 0.4447,
      "step": 30140
    },
    {
      "epoch": 1.9034090909090908,
      "grad_norm": 0.3542194068431854,
      "learning_rate": 1.222465008215201e-06,
      "loss": 0.6007,
      "step": 30150
    },
    {
      "epoch": 1.904040404040404,
      "grad_norm": 0.4053613543510437,
      "learning_rate": 1.20657975212205e-06,
      "loss": 0.5178,
      "step": 30160
    },
    {
      "epoch": 1.9046717171717171,
      "grad_norm": 0.39800265431404114,
      "learning_rate": 1.190797755732398e-06,
      "loss": 0.4767,
      "step": 30170
    },
    {
      "epoch": 1.9053030303030303,
      "grad_norm": 0.43740615248680115,
      "learning_rate": 1.1751190355417497e-06,
      "loss": 0.4242,
      "step": 30180
    },
    {
      "epoch": 1.9059343434343434,
      "grad_norm": 0.6775363087654114,
      "learning_rate": 1.1595436079376076e-06,
      "loss": 0.431,
      "step": 30190
    },
    {
      "epoch": 1.9065656565656566,
      "grad_norm": 0.3698512017726898,
      "learning_rate": 1.1440714891995608e-06,
      "loss": 0.6195,
      "step": 30200
    },
    {
      "epoch": 1.9071969696969697,
      "grad_norm": 0.4457211196422577,
      "learning_rate": 1.128702695499173e-06,
      "loss": 0.5413,
      "step": 30210
    },
    {
      "epoch": 1.9078282828282829,
      "grad_norm": 0.4219370484352112,
      "learning_rate": 1.1134372429000506e-06,
      "loss": 0.48,
      "step": 30220
    },
    {
      "epoch": 1.908459595959596,
      "grad_norm": 0.4498884379863739,
      "learning_rate": 1.0982751473577635e-06,
      "loss": 0.4053,
      "step": 30230
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 0.7374063730239868,
      "learning_rate": 1.0832164247198685e-06,
      "loss": 0.4134,
      "step": 30240
    },
    {
      "epoch": 1.9097222222222223,
      "grad_norm": 0.38928699493408203,
      "learning_rate": 1.068261090725864e-06,
      "loss": 0.6148,
      "step": 30250
    },
    {
      "epoch": 1.9103535353535355,
      "grad_norm": 0.4272971749305725,
      "learning_rate": 1.0534091610071795e-06,
      "loss": 0.5256,
      "step": 30260
    },
    {
      "epoch": 1.9109848484848486,
      "grad_norm": 0.43909114599227905,
      "learning_rate": 1.0386606510871976e-06,
      "loss": 0.4727,
      "step": 30270
    },
    {
      "epoch": 1.9116161616161618,
      "grad_norm": 0.4896784722805023,
      "learning_rate": 1.0240155763811655e-06,
      "loss": 0.4139,
      "step": 30280
    },
    {
      "epoch": 1.9122474747474747,
      "grad_norm": 0.6393734216690063,
      "learning_rate": 1.0094739521962383e-06,
      "loss": 0.4443,
      "step": 30290
    },
    {
      "epoch": 1.9128787878787878,
      "grad_norm": 0.3955063819885254,
      "learning_rate": 9.950357937314357e-07,
      "loss": 0.5897,
      "step": 30300
    },
    {
      "epoch": 1.913510101010101,
      "grad_norm": 0.4029874801635742,
      "learning_rate": 9.807011160776537e-07,
      "loss": 0.5242,
      "step": 30310
    },
    {
      "epoch": 1.9141414141414141,
      "grad_norm": 0.432944118976593,
      "learning_rate": 9.664699342176286e-07,
      "loss": 0.4442,
      "step": 30320
    },
    {
      "epoch": 1.9147727272727273,
      "grad_norm": 0.4457510709762573,
      "learning_rate": 9.523422630258849e-07,
      "loss": 0.403,
      "step": 30330
    },
    {
      "epoch": 1.9154040404040404,
      "grad_norm": 0.6986823081970215,
      "learning_rate": 9.383181172687883e-07,
      "loss": 0.4493,
      "step": 30340
    },
    {
      "epoch": 1.9160353535353534,
      "grad_norm": 0.36466842889785767,
      "learning_rate": 9.243975116044912e-07,
      "loss": 0.6147,
      "step": 30350
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.42294710874557495,
      "learning_rate": 9.105804605829327e-07,
      "loss": 0.5206,
      "step": 30360
    },
    {
      "epoch": 1.9172979797979797,
      "grad_norm": 0.40915346145629883,
      "learning_rate": 8.968669786458161e-07,
      "loss": 0.4616,
      "step": 30370
    },
    {
      "epoch": 1.9179292929292928,
      "grad_norm": 0.481277197599411,
      "learning_rate": 8.832570801265761e-07,
      "loss": 0.4152,
      "step": 30380
    },
    {
      "epoch": 1.918560606060606,
      "grad_norm": 0.6799905896186829,
      "learning_rate": 8.697507792503778e-07,
      "loss": 0.4491,
      "step": 30390
    },
    {
      "epoch": 1.9191919191919191,
      "grad_norm": 0.38340261578559875,
      "learning_rate": 8.563480901341514e-07,
      "loss": 0.6338,
      "step": 30400
    },
    {
      "epoch": 1.9198232323232323,
      "grad_norm": 0.4219994843006134,
      "learning_rate": 8.430490267864799e-07,
      "loss": 0.5207,
      "step": 30410
    },
    {
      "epoch": 1.9204545454545454,
      "grad_norm": 0.4647262990474701,
      "learning_rate": 8.298536031076554e-07,
      "loss": 0.4611,
      "step": 30420
    },
    {
      "epoch": 1.9210858585858586,
      "grad_norm": 0.45487117767333984,
      "learning_rate": 8.167618328896342e-07,
      "loss": 0.4097,
      "step": 30430
    },
    {
      "epoch": 1.9217171717171717,
      "grad_norm": 0.6779937148094177,
      "learning_rate": 8.037737298160707e-07,
      "loss": 0.4395,
      "step": 30440
    },
    {
      "epoch": 1.9223484848484849,
      "grad_norm": 0.4052455723285675,
      "learning_rate": 7.90889307462217e-07,
      "loss": 0.6269,
      "step": 30450
    },
    {
      "epoch": 1.922979797979798,
      "grad_norm": 0.42626264691352844,
      "learning_rate": 7.781085792949894e-07,
      "loss": 0.5174,
      "step": 30460
    },
    {
      "epoch": 1.9236111111111112,
      "grad_norm": 0.4461120665073395,
      "learning_rate": 7.654315586729022e-07,
      "loss": 0.4711,
      "step": 30470
    },
    {
      "epoch": 1.9242424242424243,
      "grad_norm": 0.44483235478401184,
      "learning_rate": 7.528582588460787e-07,
      "loss": 0.4065,
      "step": 30480
    },
    {
      "epoch": 1.9248737373737375,
      "grad_norm": 0.6563407778739929,
      "learning_rate": 7.403886929562398e-07,
      "loss": 0.432,
      "step": 30490
    },
    {
      "epoch": 1.9255050505050506,
      "grad_norm": 0.39352303743362427,
      "learning_rate": 7.280228740366935e-07,
      "loss": 0.6246,
      "step": 30500
    },
    {
      "epoch": 1.9261363636363638,
      "grad_norm": 0.3949766755104065,
      "learning_rate": 7.157608150122896e-07,
      "loss": 0.5476,
      "step": 30510
    },
    {
      "epoch": 1.926767676767677,
      "grad_norm": 0.4165651798248291,
      "learning_rate": 7.036025286994208e-07,
      "loss": 0.4481,
      "step": 30520
    },
    {
      "epoch": 1.92739898989899,
      "grad_norm": 0.44192102551460266,
      "learning_rate": 6.915480278060438e-07,
      "loss": 0.4201,
      "step": 30530
    },
    {
      "epoch": 1.928030303030303,
      "grad_norm": 0.6627871990203857,
      "learning_rate": 6.795973249316245e-07,
      "loss": 0.4346,
      "step": 30540
    },
    {
      "epoch": 1.9286616161616161,
      "grad_norm": 0.3909218907356262,
      "learning_rate": 6.677504325671157e-07,
      "loss": 0.6343,
      "step": 30550
    },
    {
      "epoch": 1.9292929292929293,
      "grad_norm": 0.3883088529109955,
      "learning_rate": 6.560073630950125e-07,
      "loss": 0.5274,
      "step": 30560
    },
    {
      "epoch": 1.9299242424242424,
      "grad_norm": 0.4328033924102783,
      "learning_rate": 6.443681287892522e-07,
      "loss": 0.4748,
      "step": 30570
    },
    {
      "epoch": 1.9305555555555556,
      "grad_norm": 0.4382910132408142,
      "learning_rate": 6.328327418152702e-07,
      "loss": 0.3943,
      "step": 30580
    },
    {
      "epoch": 1.9311868686868687,
      "grad_norm": 0.6540747880935669,
      "learning_rate": 6.214012142299441e-07,
      "loss": 0.4346,
      "step": 30590
    },
    {
      "epoch": 1.9318181818181817,
      "grad_norm": 0.38947129249572754,
      "learning_rate": 6.100735579816053e-07,
      "loss": 0.6362,
      "step": 30600
    },
    {
      "epoch": 1.9324494949494948,
      "grad_norm": 0.3988260328769684,
      "learning_rate": 5.988497849099939e-07,
      "loss": 0.531,
      "step": 30610
    },
    {
      "epoch": 1.933080808080808,
      "grad_norm": 0.4020186960697174,
      "learning_rate": 5.87729906746326e-07,
      "loss": 0.4873,
      "step": 30620
    },
    {
      "epoch": 1.933712121212121,
      "grad_norm": 0.4855620563030243,
      "learning_rate": 5.767139351131601e-07,
      "loss": 0.4144,
      "step": 30630
    },
    {
      "epoch": 1.9343434343434343,
      "grad_norm": 0.6507089138031006,
      "learning_rate": 5.65801881524497e-07,
      "loss": 0.4193,
      "step": 30640
    },
    {
      "epoch": 1.9349747474747474,
      "grad_norm": 0.3791436553001404,
      "learning_rate": 5.549937573857023e-07,
      "loss": 0.6026,
      "step": 30650
    },
    {
      "epoch": 1.9356060606060606,
      "grad_norm": 0.3906269371509552,
      "learning_rate": 5.442895739935172e-07,
      "loss": 0.5311,
      "step": 30660
    },
    {
      "epoch": 1.9362373737373737,
      "grad_norm": 0.4233091473579407,
      "learning_rate": 5.336893425360367e-07,
      "loss": 0.4733,
      "step": 30670
    },
    {
      "epoch": 1.9368686868686869,
      "grad_norm": 0.4519568979740143,
      "learning_rate": 5.231930740927316e-07,
      "loss": 0.4378,
      "step": 30680
    },
    {
      "epoch": 1.9375,
      "grad_norm": 0.6541242599487305,
      "learning_rate": 5.128007796343592e-07,
      "loss": 0.4673,
      "step": 30690
    },
    {
      "epoch": 1.9381313131313131,
      "grad_norm": 0.37468382716178894,
      "learning_rate": 5.025124700230533e-07,
      "loss": 0.6028,
      "step": 30700
    },
    {
      "epoch": 1.9387626262626263,
      "grad_norm": 0.38615044951438904,
      "learning_rate": 4.923281560122228e-07,
      "loss": 0.5253,
      "step": 30710
    },
    {
      "epoch": 1.9393939393939394,
      "grad_norm": 0.44578811526298523,
      "learning_rate": 4.822478482466197e-07,
      "loss": 0.4774,
      "step": 30720
    },
    {
      "epoch": 1.9400252525252526,
      "grad_norm": 0.4432288110256195,
      "learning_rate": 4.7227155726224936e-07,
      "loss": 0.4285,
      "step": 30730
    },
    {
      "epoch": 1.9406565656565657,
      "grad_norm": 0.6545559167861938,
      "learning_rate": 4.623992934864263e-07,
      "loss": 0.4203,
      "step": 30740
    },
    {
      "epoch": 1.941287878787879,
      "grad_norm": 0.38618743419647217,
      "learning_rate": 4.526310672377077e-07,
      "loss": 0.6407,
      "step": 30750
    },
    {
      "epoch": 1.941919191919192,
      "grad_norm": 0.408234566450119,
      "learning_rate": 4.429668887259375e-07,
      "loss": 0.527,
      "step": 30760
    },
    {
      "epoch": 1.9425505050505052,
      "grad_norm": 0.41710877418518066,
      "learning_rate": 4.334067680521803e-07,
      "loss": 0.448,
      "step": 30770
    },
    {
      "epoch": 1.9431818181818183,
      "grad_norm": 0.4013133943080902,
      "learning_rate": 4.239507152087652e-07,
      "loss": 0.4155,
      "step": 30780
    },
    {
      "epoch": 1.9438131313131313,
      "grad_norm": 0.7394953370094299,
      "learning_rate": 4.1459874007924173e-07,
      "loss": 0.4305,
      "step": 30790
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.36822739243507385,
      "learning_rate": 4.053508524383687e-07,
      "loss": 0.6272,
      "step": 30800
    },
    {
      "epoch": 1.9450757575757576,
      "grad_norm": 0.41704192757606506,
      "learning_rate": 3.96207061952103e-07,
      "loss": 0.543,
      "step": 30810
    },
    {
      "epoch": 1.9457070707070707,
      "grad_norm": 0.4535962641239166,
      "learning_rate": 3.871673781776219e-07,
      "loss": 0.4656,
      "step": 30820
    },
    {
      "epoch": 1.9463383838383839,
      "grad_norm": 0.43648087978363037,
      "learning_rate": 3.7823181056327873e-07,
      "loss": 0.4194,
      "step": 30830
    },
    {
      "epoch": 1.946969696969697,
      "grad_norm": 0.6897698044776917,
      "learning_rate": 3.694003684486025e-07,
      "loss": 0.4387,
      "step": 30840
    },
    {
      "epoch": 1.94760101010101,
      "grad_norm": 0.3837716281414032,
      "learning_rate": 3.6067306106427614e-07,
      "loss": 0.5776,
      "step": 30850
    },
    {
      "epoch": 1.948232323232323,
      "grad_norm": 0.41173505783081055,
      "learning_rate": 3.5204989753216955e-07,
      "loss": 0.5484,
      "step": 30860
    },
    {
      "epoch": 1.9488636363636362,
      "grad_norm": 0.44055473804473877,
      "learning_rate": 3.435308868652842e-07,
      "loss": 0.4737,
      "step": 30870
    },
    {
      "epoch": 1.9494949494949494,
      "grad_norm": 0.4370468258857727,
      "learning_rate": 3.3511603796775316e-07,
      "loss": 0.4248,
      "step": 30880
    },
    {
      "epoch": 1.9501262626262625,
      "grad_norm": 0.634183406829834,
      "learning_rate": 3.2680535963485193e-07,
      "loss": 0.4531,
      "step": 30890
    },
    {
      "epoch": 1.9507575757575757,
      "grad_norm": 0.3957810699939728,
      "learning_rate": 3.185988605529655e-07,
      "loss": 0.6568,
      "step": 30900
    },
    {
      "epoch": 1.9513888888888888,
      "grad_norm": 0.40749603509902954,
      "learning_rate": 3.1049654929959927e-07,
      "loss": 0.5384,
      "step": 30910
    },
    {
      "epoch": 1.952020202020202,
      "grad_norm": 0.4271171987056732,
      "learning_rate": 3.0249843434335677e-07,
      "loss": 0.4587,
      "step": 30920
    },
    {
      "epoch": 1.9526515151515151,
      "grad_norm": 0.4103407859802246,
      "learning_rate": 2.946045240439288e-07,
      "loss": 0.424,
      "step": 30930
    },
    {
      "epoch": 1.9532828282828283,
      "grad_norm": 0.6057783961296082,
      "learning_rate": 2.8681482665210424e-07,
      "loss": 0.4249,
      "step": 30940
    },
    {
      "epoch": 1.9539141414141414,
      "grad_norm": 0.42032676935195923,
      "learning_rate": 2.7912935030972587e-07,
      "loss": 0.6297,
      "step": 30950
    },
    {
      "epoch": 1.9545454545454546,
      "grad_norm": 0.4184949994087219,
      "learning_rate": 2.7154810304973467e-07,
      "loss": 0.5203,
      "step": 30960
    },
    {
      "epoch": 1.9551767676767677,
      "grad_norm": 0.432246595621109,
      "learning_rate": 2.640710927961032e-07,
      "loss": 0.4473,
      "step": 30970
    },
    {
      "epoch": 1.9558080808080809,
      "grad_norm": 0.41363558173179626,
      "learning_rate": 2.5669832736386903e-07,
      "loss": 0.4131,
      "step": 30980
    },
    {
      "epoch": 1.956439393939394,
      "grad_norm": 0.745895266532898,
      "learning_rate": 2.4942981445910117e-07,
      "loss": 0.4459,
      "step": 30990
    },
    {
      "epoch": 1.9570707070707072,
      "grad_norm": 0.37905916571617126,
      "learning_rate": 2.422655616789116e-07,
      "loss": 0.6563,
      "step": 31000
    },
    {
      "epoch": 1.9570707070707072,
      "eval_loss": 0.5063484907150269,
      "eval_runtime": 26.8368,
      "eval_samples_per_second": 95.392,
      "eval_steps_per_second": 11.924,
      "step": 31000
    }
  ],
  "logging_steps": 10,
  "max_steps": 31680,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4227347592909947e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}

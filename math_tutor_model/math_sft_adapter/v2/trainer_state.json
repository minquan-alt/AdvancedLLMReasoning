{
  "best_global_step": 29000,
  "best_metric": 0.6294780969619751,
  "best_model_checkpoint": "math_tutor_model/math_sft_adapter/v2/checkpoint-29000",
  "epoch": 2.0,
  "eval_steps": 1000,
  "global_step": 31680,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "eval_loss": 1.3709996938705444,
      "eval_runtime": 32.0161,
      "eval_samples_per_second": 79.96,
      "eval_steps_per_second": 9.995,
      "step": 0
    },
    {
      "epoch": 6.313131313131313e-05,
      "grad_norm": 0.490906298160553,
      "learning_rate": 0.0,
      "loss": 1.0841,
      "step": 1
    },
    {
      "epoch": 0.0006313131313131314,
      "grad_norm": 1.0018937587738037,
      "learning_rate": 1.8927444794952682e-06,
      "loss": 1.22,
      "step": 10
    },
    {
      "epoch": 0.0012626262626262627,
      "grad_norm": 1.0823438167572021,
      "learning_rate": 3.995793901156677e-06,
      "loss": 1.2537,
      "step": 20
    },
    {
      "epoch": 0.001893939393939394,
      "grad_norm": 1.422310709953308,
      "learning_rate": 6.098843322818087e-06,
      "loss": 1.276,
      "step": 30
    },
    {
      "epoch": 0.0025252525252525255,
      "grad_norm": 1.1952061653137207,
      "learning_rate": 8.201892744479495e-06,
      "loss": 1.2269,
      "step": 40
    },
    {
      "epoch": 0.0031565656565656565,
      "grad_norm": 1.6389073133468628,
      "learning_rate": 1.0304942166140905e-05,
      "loss": 1.2178,
      "step": 50
    },
    {
      "epoch": 0.003787878787878788,
      "grad_norm": 0.5534136891365051,
      "learning_rate": 1.2407991587802314e-05,
      "loss": 1.0681,
      "step": 60
    },
    {
      "epoch": 0.004419191919191919,
      "grad_norm": 0.6957975029945374,
      "learning_rate": 1.4511041009463724e-05,
      "loss": 0.9996,
      "step": 70
    },
    {
      "epoch": 0.005050505050505051,
      "grad_norm": 0.70899897813797,
      "learning_rate": 1.661409043112513e-05,
      "loss": 0.8709,
      "step": 80
    },
    {
      "epoch": 0.005681818181818182,
      "grad_norm": 0.8173007965087891,
      "learning_rate": 1.871713985278654e-05,
      "loss": 0.808,
      "step": 90
    },
    {
      "epoch": 0.006313131313131313,
      "grad_norm": 1.6896014213562012,
      "learning_rate": 2.0820189274447953e-05,
      "loss": 0.8526,
      "step": 100
    },
    {
      "epoch": 0.006944444444444444,
      "grad_norm": 0.5234919786453247,
      "learning_rate": 2.292323869610936e-05,
      "loss": 1.0149,
      "step": 110
    },
    {
      "epoch": 0.007575757575757576,
      "grad_norm": 0.5327485799789429,
      "learning_rate": 2.5026288117770768e-05,
      "loss": 0.8879,
      "step": 120
    },
    {
      "epoch": 0.008207070707070708,
      "grad_norm": 0.6737896203994751,
      "learning_rate": 2.7129337539432176e-05,
      "loss": 0.8363,
      "step": 130
    },
    {
      "epoch": 0.008838383838383838,
      "grad_norm": 0.7395294904708862,
      "learning_rate": 2.9232386961093587e-05,
      "loss": 0.7296,
      "step": 140
    },
    {
      "epoch": 0.00946969696969697,
      "grad_norm": 1.668662667274475,
      "learning_rate": 3.1335436382754995e-05,
      "loss": 0.7693,
      "step": 150
    },
    {
      "epoch": 0.010101010101010102,
      "grad_norm": 0.5535212159156799,
      "learning_rate": 3.34384858044164e-05,
      "loss": 0.9519,
      "step": 160
    },
    {
      "epoch": 0.010732323232323232,
      "grad_norm": 0.6307892799377441,
      "learning_rate": 3.554153522607782e-05,
      "loss": 0.8358,
      "step": 170
    },
    {
      "epoch": 0.011363636363636364,
      "grad_norm": 0.6438137888908386,
      "learning_rate": 3.7644584647739225e-05,
      "loss": 0.7745,
      "step": 180
    },
    {
      "epoch": 0.011994949494949494,
      "grad_norm": 0.7144246101379395,
      "learning_rate": 3.974763406940063e-05,
      "loss": 0.7007,
      "step": 190
    },
    {
      "epoch": 0.012626262626262626,
      "grad_norm": 1.9161252975463867,
      "learning_rate": 4.185068349106204e-05,
      "loss": 0.724,
      "step": 200
    },
    {
      "epoch": 0.013257575757575758,
      "grad_norm": 0.5266805291175842,
      "learning_rate": 4.395373291272345e-05,
      "loss": 0.9511,
      "step": 210
    },
    {
      "epoch": 0.013888888888888888,
      "grad_norm": 0.5986496806144714,
      "learning_rate": 4.6056782334384864e-05,
      "loss": 0.8257,
      "step": 220
    },
    {
      "epoch": 0.01452020202020202,
      "grad_norm": 0.6377519965171814,
      "learning_rate": 4.815983175604627e-05,
      "loss": 0.7553,
      "step": 230
    },
    {
      "epoch": 0.015151515151515152,
      "grad_norm": 0.6731777191162109,
      "learning_rate": 5.026288117770768e-05,
      "loss": 0.6692,
      "step": 240
    },
    {
      "epoch": 0.015782828282828284,
      "grad_norm": 1.3522768020629883,
      "learning_rate": 5.236593059936909e-05,
      "loss": 0.683,
      "step": 250
    },
    {
      "epoch": 0.016414141414141416,
      "grad_norm": 0.5405439734458923,
      "learning_rate": 5.44689800210305e-05,
      "loss": 0.9479,
      "step": 260
    },
    {
      "epoch": 0.017045454545454544,
      "grad_norm": 0.5220975279808044,
      "learning_rate": 5.657202944269191e-05,
      "loss": 0.7966,
      "step": 270
    },
    {
      "epoch": 0.017676767676767676,
      "grad_norm": 0.6665446758270264,
      "learning_rate": 5.867507886435332e-05,
      "loss": 0.7365,
      "step": 280
    },
    {
      "epoch": 0.018308080808080808,
      "grad_norm": 0.7211830019950867,
      "learning_rate": 6.0778128286014725e-05,
      "loss": 0.6801,
      "step": 290
    },
    {
      "epoch": 0.01893939393939394,
      "grad_norm": 1.2634302377700806,
      "learning_rate": 6.288117770767613e-05,
      "loss": 0.7054,
      "step": 300
    },
    {
      "epoch": 0.019570707070707072,
      "grad_norm": 0.5557650923728943,
      "learning_rate": 6.498422712933754e-05,
      "loss": 0.9082,
      "step": 310
    },
    {
      "epoch": 0.020202020202020204,
      "grad_norm": 0.5234262347221375,
      "learning_rate": 6.708727655099896e-05,
      "loss": 0.7947,
      "step": 320
    },
    {
      "epoch": 0.020833333333333332,
      "grad_norm": 0.6234301328659058,
      "learning_rate": 6.919032597266037e-05,
      "loss": 0.7455,
      "step": 330
    },
    {
      "epoch": 0.021464646464646464,
      "grad_norm": 0.6706495881080627,
      "learning_rate": 7.129337539432177e-05,
      "loss": 0.6608,
      "step": 340
    },
    {
      "epoch": 0.022095959595959596,
      "grad_norm": 1.2267265319824219,
      "learning_rate": 7.339642481598317e-05,
      "loss": 0.687,
      "step": 350
    },
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 0.4677242934703827,
      "learning_rate": 7.549947423764459e-05,
      "loss": 0.9127,
      "step": 360
    },
    {
      "epoch": 0.02335858585858586,
      "grad_norm": 0.5519731640815735,
      "learning_rate": 7.760252365930599e-05,
      "loss": 0.8002,
      "step": 370
    },
    {
      "epoch": 0.023989898989898988,
      "grad_norm": 0.56685870885849,
      "learning_rate": 7.970557308096742e-05,
      "loss": 0.7342,
      "step": 380
    },
    {
      "epoch": 0.02462121212121212,
      "grad_norm": 0.5993671417236328,
      "learning_rate": 8.180862250262882e-05,
      "loss": 0.6593,
      "step": 390
    },
    {
      "epoch": 0.025252525252525252,
      "grad_norm": 0.9631473422050476,
      "learning_rate": 8.391167192429022e-05,
      "loss": 0.7359,
      "step": 400
    },
    {
      "epoch": 0.025883838383838384,
      "grad_norm": 0.5693632364273071,
      "learning_rate": 8.601472134595163e-05,
      "loss": 0.9326,
      "step": 410
    },
    {
      "epoch": 0.026515151515151516,
      "grad_norm": 0.495771586894989,
      "learning_rate": 8.811777076761303e-05,
      "loss": 0.7682,
      "step": 420
    },
    {
      "epoch": 0.027146464646464648,
      "grad_norm": 0.5473682880401611,
      "learning_rate": 9.022082018927446e-05,
      "loss": 0.7115,
      "step": 430
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 0.5804258584976196,
      "learning_rate": 9.232386961093586e-05,
      "loss": 0.6358,
      "step": 440
    },
    {
      "epoch": 0.028409090909090908,
      "grad_norm": 0.9949278235435486,
      "learning_rate": 9.442691903259728e-05,
      "loss": 0.6745,
      "step": 450
    },
    {
      "epoch": 0.02904040404040404,
      "grad_norm": 0.4494951367378235,
      "learning_rate": 9.652996845425868e-05,
      "loss": 0.8493,
      "step": 460
    },
    {
      "epoch": 0.029671717171717172,
      "grad_norm": 0.49754762649536133,
      "learning_rate": 9.863301787592008e-05,
      "loss": 0.7822,
      "step": 470
    },
    {
      "epoch": 0.030303030303030304,
      "grad_norm": 0.5147463083267212,
      "learning_rate": 0.0001007360672975815,
      "loss": 0.6862,
      "step": 480
    },
    {
      "epoch": 0.030934343434343436,
      "grad_norm": 0.5949280261993408,
      "learning_rate": 0.00010283911671924291,
      "loss": 0.6305,
      "step": 490
    },
    {
      "epoch": 0.03156565656565657,
      "grad_norm": 1.1045747995376587,
      "learning_rate": 0.00010494216614090431,
      "loss": 0.6764,
      "step": 500
    },
    {
      "epoch": 0.032196969696969696,
      "grad_norm": 0.44871777296066284,
      "learning_rate": 0.00010704521556256572,
      "loss": 0.9279,
      "step": 510
    },
    {
      "epoch": 0.03282828282828283,
      "grad_norm": 0.4858054518699646,
      "learning_rate": 0.00010914826498422714,
      "loss": 0.8031,
      "step": 520
    },
    {
      "epoch": 0.03345959595959596,
      "grad_norm": 0.5162699222564697,
      "learning_rate": 0.00011125131440588854,
      "loss": 0.7231,
      "step": 530
    },
    {
      "epoch": 0.03409090909090909,
      "grad_norm": 0.5993492603302002,
      "learning_rate": 0.00011335436382754996,
      "loss": 0.6565,
      "step": 540
    },
    {
      "epoch": 0.034722222222222224,
      "grad_norm": 0.9255920648574829,
      "learning_rate": 0.00011545741324921136,
      "loss": 0.6531,
      "step": 550
    },
    {
      "epoch": 0.03535353535353535,
      "grad_norm": 0.42099088430404663,
      "learning_rate": 0.00011756046267087277,
      "loss": 0.9295,
      "step": 560
    },
    {
      "epoch": 0.03598484848484849,
      "grad_norm": 0.47725096344947815,
      "learning_rate": 0.00011966351209253419,
      "loss": 0.79,
      "step": 570
    },
    {
      "epoch": 0.036616161616161616,
      "grad_norm": 0.5381262302398682,
      "learning_rate": 0.00012176656151419559,
      "loss": 0.7164,
      "step": 580
    },
    {
      "epoch": 0.037247474747474744,
      "grad_norm": 0.594364583492279,
      "learning_rate": 0.000123869610935857,
      "loss": 0.6318,
      "step": 590
    },
    {
      "epoch": 0.03787878787878788,
      "grad_norm": 0.9910649657249451,
      "learning_rate": 0.0001259726603575184,
      "loss": 0.6702,
      "step": 600
    },
    {
      "epoch": 0.03851010101010101,
      "grad_norm": 0.43368422985076904,
      "learning_rate": 0.00012807570977917983,
      "loss": 0.8936,
      "step": 610
    },
    {
      "epoch": 0.039141414141414144,
      "grad_norm": 0.4895420968532562,
      "learning_rate": 0.00013017875920084122,
      "loss": 0.7977,
      "step": 620
    },
    {
      "epoch": 0.03977272727272727,
      "grad_norm": 0.4895530939102173,
      "learning_rate": 0.00013228180862250263,
      "loss": 0.7068,
      "step": 630
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": 0.5211889147758484,
      "learning_rate": 0.00013438485804416405,
      "loss": 0.6591,
      "step": 640
    },
    {
      "epoch": 0.041035353535353536,
      "grad_norm": 1.0674573183059692,
      "learning_rate": 0.00013648790746582546,
      "loss": 0.6482,
      "step": 650
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.4068051278591156,
      "learning_rate": 0.00013859095688748688,
      "loss": 0.921,
      "step": 660
    },
    {
      "epoch": 0.0422979797979798,
      "grad_norm": 0.4381791651248932,
      "learning_rate": 0.00014069400630914826,
      "loss": 0.7772,
      "step": 670
    },
    {
      "epoch": 0.04292929292929293,
      "grad_norm": 0.4767996668815613,
      "learning_rate": 0.00014279705573080968,
      "loss": 0.6954,
      "step": 680
    },
    {
      "epoch": 0.043560606060606064,
      "grad_norm": 0.47304007411003113,
      "learning_rate": 0.0001449001051524711,
      "loss": 0.6502,
      "step": 690
    },
    {
      "epoch": 0.04419191919191919,
      "grad_norm": 0.8629213571548462,
      "learning_rate": 0.0001470031545741325,
      "loss": 0.6614,
      "step": 700
    },
    {
      "epoch": 0.04482323232323232,
      "grad_norm": 0.3985675275325775,
      "learning_rate": 0.00014910620399579392,
      "loss": 0.8946,
      "step": 710
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.44894739985466003,
      "learning_rate": 0.0001512092534174553,
      "loss": 0.7952,
      "step": 720
    },
    {
      "epoch": 0.046085858585858584,
      "grad_norm": 0.4835611879825592,
      "learning_rate": 0.00015331230283911672,
      "loss": 0.718,
      "step": 730
    },
    {
      "epoch": 0.04671717171717172,
      "grad_norm": 0.49465349316596985,
      "learning_rate": 0.00015541535226077814,
      "loss": 0.6442,
      "step": 740
    },
    {
      "epoch": 0.04734848484848485,
      "grad_norm": 0.7368137836456299,
      "learning_rate": 0.00015751840168243955,
      "loss": 0.6396,
      "step": 750
    },
    {
      "epoch": 0.047979797979797977,
      "grad_norm": 0.38300102949142456,
      "learning_rate": 0.00015962145110410097,
      "loss": 0.8958,
      "step": 760
    },
    {
      "epoch": 0.04861111111111111,
      "grad_norm": 0.44355830550193787,
      "learning_rate": 0.00016172450052576236,
      "loss": 0.7673,
      "step": 770
    },
    {
      "epoch": 0.04924242424242424,
      "grad_norm": 0.47793594002723694,
      "learning_rate": 0.00016382754994742377,
      "loss": 0.7401,
      "step": 780
    },
    {
      "epoch": 0.049873737373737376,
      "grad_norm": 0.5164859294891357,
      "learning_rate": 0.00016593059936908516,
      "loss": 0.6427,
      "step": 790
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 0.8349427580833435,
      "learning_rate": 0.0001680336487907466,
      "loss": 0.6199,
      "step": 800
    },
    {
      "epoch": 0.05113636363636364,
      "grad_norm": 0.40242621302604675,
      "learning_rate": 0.00017013669821240801,
      "loss": 0.9335,
      "step": 810
    },
    {
      "epoch": 0.05176767676767677,
      "grad_norm": 0.4517170190811157,
      "learning_rate": 0.0001722397476340694,
      "loss": 0.7692,
      "step": 820
    },
    {
      "epoch": 0.052398989898989896,
      "grad_norm": 0.45013660192489624,
      "learning_rate": 0.00017434279705573082,
      "loss": 0.6921,
      "step": 830
    },
    {
      "epoch": 0.05303030303030303,
      "grad_norm": 0.4709964990615845,
      "learning_rate": 0.00017644584647739223,
      "loss": 0.645,
      "step": 840
    },
    {
      "epoch": 0.05366161616161616,
      "grad_norm": 0.7999598979949951,
      "learning_rate": 0.00017854889589905365,
      "loss": 0.6632,
      "step": 850
    },
    {
      "epoch": 0.054292929292929296,
      "grad_norm": 0.38375377655029297,
      "learning_rate": 0.00018065194532071506,
      "loss": 0.8537,
      "step": 860
    },
    {
      "epoch": 0.054924242424242424,
      "grad_norm": 0.44689035415649414,
      "learning_rate": 0.00018275499474237645,
      "loss": 0.7736,
      "step": 870
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 0.4442255198955536,
      "learning_rate": 0.00018485804416403786,
      "loss": 0.6989,
      "step": 880
    },
    {
      "epoch": 0.05618686868686869,
      "grad_norm": 0.489020973443985,
      "learning_rate": 0.00018696109358569928,
      "loss": 0.6224,
      "step": 890
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 0.7354667782783508,
      "learning_rate": 0.0001890641430073607,
      "loss": 0.6256,
      "step": 900
    },
    {
      "epoch": 0.05744949494949495,
      "grad_norm": 0.40955308079719543,
      "learning_rate": 0.0001911671924290221,
      "loss": 0.9214,
      "step": 910
    },
    {
      "epoch": 0.05808080808080808,
      "grad_norm": 0.44214847683906555,
      "learning_rate": 0.0001932702418506835,
      "loss": 0.7615,
      "step": 920
    },
    {
      "epoch": 0.058712121212121215,
      "grad_norm": 0.468892365694046,
      "learning_rate": 0.0001953732912723449,
      "loss": 0.695,
      "step": 930
    },
    {
      "epoch": 0.059343434343434344,
      "grad_norm": 0.4735174775123596,
      "learning_rate": 0.00019747634069400632,
      "loss": 0.6283,
      "step": 940
    },
    {
      "epoch": 0.05997474747474747,
      "grad_norm": 0.9234786033630371,
      "learning_rate": 0.00019957939011566774,
      "loss": 0.6823,
      "step": 950
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": 0.37491700053215027,
      "learning_rate": 0.00019999996655333316,
      "loss": 0.9323,
      "step": 960
    },
    {
      "epoch": 0.061237373737373736,
      "grad_norm": 0.42789751291275024,
      "learning_rate": 0.00019999983067628733,
      "loss": 0.7321,
      "step": 970
    },
    {
      "epoch": 0.06186868686868687,
      "grad_norm": 0.49659231305122375,
      "learning_rate": 0.0001999995902785878,
      "loss": 0.6708,
      "step": 980
    },
    {
      "epoch": 0.0625,
      "grad_norm": 0.487367182970047,
      "learning_rate": 0.0001999992453604858,
      "loss": 0.6143,
      "step": 990
    },
    {
      "epoch": 0.06313131313131314,
      "grad_norm": 0.8526012897491455,
      "learning_rate": 0.00019999879592234187,
      "loss": 0.6757,
      "step": 1000
    },
    {
      "epoch": 0.06313131313131314,
      "eval_loss": 0.7751284241676331,
      "eval_runtime": 31.7674,
      "eval_samples_per_second": 80.586,
      "eval_steps_per_second": 10.073,
      "step": 1000
    },
    {
      "epoch": 0.06376262626262626,
      "grad_norm": 0.4152131676673889,
      "learning_rate": 0.00019999824196462578,
      "loss": 0.9472,
      "step": 1010
    },
    {
      "epoch": 0.06439393939393939,
      "grad_norm": 0.4250233471393585,
      "learning_rate": 0.00019999758348791647,
      "loss": 0.7889,
      "step": 1020
    },
    {
      "epoch": 0.06502525252525253,
      "grad_norm": 0.4225434958934784,
      "learning_rate": 0.00019999682049290227,
      "loss": 0.7022,
      "step": 1030
    },
    {
      "epoch": 0.06565656565656566,
      "grad_norm": 0.5299443602561951,
      "learning_rate": 0.00019999595298038057,
      "loss": 0.6146,
      "step": 1040
    },
    {
      "epoch": 0.06628787878787878,
      "grad_norm": 0.8127761483192444,
      "learning_rate": 0.00019999498095125818,
      "loss": 0.6585,
      "step": 1050
    },
    {
      "epoch": 0.06691919191919192,
      "grad_norm": 0.3961228132247925,
      "learning_rate": 0.00019999390440655107,
      "loss": 0.9162,
      "step": 1060
    },
    {
      "epoch": 0.06755050505050506,
      "grad_norm": 0.4230523705482483,
      "learning_rate": 0.00019999272334738442,
      "loss": 0.7888,
      "step": 1070
    },
    {
      "epoch": 0.06818181818181818,
      "grad_norm": 0.48050034046173096,
      "learning_rate": 0.0001999914377749927,
      "loss": 0.6882,
      "step": 1080
    },
    {
      "epoch": 0.06881313131313131,
      "grad_norm": 0.4680651128292084,
      "learning_rate": 0.00019999004769071955,
      "loss": 0.6343,
      "step": 1090
    },
    {
      "epoch": 0.06944444444444445,
      "grad_norm": 0.7974424958229065,
      "learning_rate": 0.00019998855309601797,
      "loss": 0.663,
      "step": 1100
    },
    {
      "epoch": 0.07007575757575757,
      "grad_norm": 0.40376752614974976,
      "learning_rate": 0.00019998695399245012,
      "loss": 0.9206,
      "step": 1110
    },
    {
      "epoch": 0.0707070707070707,
      "grad_norm": 0.42499181628227234,
      "learning_rate": 0.00019998525038168735,
      "loss": 0.7368,
      "step": 1120
    },
    {
      "epoch": 0.07133838383838384,
      "grad_norm": 0.4832080900669098,
      "learning_rate": 0.00019998344226551028,
      "loss": 0.692,
      "step": 1130
    },
    {
      "epoch": 0.07196969696969698,
      "grad_norm": 0.4461613595485687,
      "learning_rate": 0.00019998152964580883,
      "loss": 0.6132,
      "step": 1140
    },
    {
      "epoch": 0.0726010101010101,
      "grad_norm": 0.7037252187728882,
      "learning_rate": 0.00019997951252458205,
      "loss": 0.6589,
      "step": 1150
    },
    {
      "epoch": 0.07323232323232323,
      "grad_norm": 0.40062662959098816,
      "learning_rate": 0.00019997739090393823,
      "loss": 0.8907,
      "step": 1160
    },
    {
      "epoch": 0.07386363636363637,
      "grad_norm": 0.4312976002693176,
      "learning_rate": 0.00019997516478609498,
      "loss": 0.732,
      "step": 1170
    },
    {
      "epoch": 0.07449494949494949,
      "grad_norm": 0.4743122458457947,
      "learning_rate": 0.00019997283417337895,
      "loss": 0.6657,
      "step": 1180
    },
    {
      "epoch": 0.07512626262626262,
      "grad_norm": 0.4862683415412903,
      "learning_rate": 0.00019997039906822624,
      "loss": 0.6093,
      "step": 1190
    },
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 0.7100052833557129,
      "learning_rate": 0.00019996785947318192,
      "loss": 0.6438,
      "step": 1200
    },
    {
      "epoch": 0.0763888888888889,
      "grad_norm": 0.3909468948841095,
      "learning_rate": 0.00019996521539090046,
      "loss": 0.905,
      "step": 1210
    },
    {
      "epoch": 0.07702020202020202,
      "grad_norm": 0.44086772203445435,
      "learning_rate": 0.00019996246682414548,
      "loss": 0.7663,
      "step": 1220
    },
    {
      "epoch": 0.07765151515151515,
      "grad_norm": 0.42741236090660095,
      "learning_rate": 0.0001999596137757898,
      "loss": 0.7173,
      "step": 1230
    },
    {
      "epoch": 0.07828282828282829,
      "grad_norm": 0.5240170955657959,
      "learning_rate": 0.0001999566562488154,
      "loss": 0.6172,
      "step": 1240
    },
    {
      "epoch": 0.07891414141414141,
      "grad_norm": 0.6592351198196411,
      "learning_rate": 0.0001999535942463136,
      "loss": 0.651,
      "step": 1250
    },
    {
      "epoch": 0.07954545454545454,
      "grad_norm": 0.372723788022995,
      "learning_rate": 0.00019995042777148477,
      "loss": 0.9317,
      "step": 1260
    },
    {
      "epoch": 0.08017676767676768,
      "grad_norm": 0.46218475699424744,
      "learning_rate": 0.00019994715682763854,
      "loss": 0.7681,
      "step": 1270
    },
    {
      "epoch": 0.08080808080808081,
      "grad_norm": 0.44269469380378723,
      "learning_rate": 0.00019994378141819378,
      "loss": 0.66,
      "step": 1280
    },
    {
      "epoch": 0.08143939393939394,
      "grad_norm": 0.4815066158771515,
      "learning_rate": 0.0001999403015466784,
      "loss": 0.6242,
      "step": 1290
    },
    {
      "epoch": 0.08207070707070707,
      "grad_norm": 0.7626144289970398,
      "learning_rate": 0.0001999367172167297,
      "loss": 0.619,
      "step": 1300
    },
    {
      "epoch": 0.08270202020202021,
      "grad_norm": 0.39788204431533813,
      "learning_rate": 0.00019993302843209393,
      "loss": 0.9056,
      "step": 1310
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.41249555349349976,
      "learning_rate": 0.00019992923519662674,
      "loss": 0.7569,
      "step": 1320
    },
    {
      "epoch": 0.08396464646464646,
      "grad_norm": 0.4086765944957733,
      "learning_rate": 0.00019992533751429282,
      "loss": 0.6393,
      "step": 1330
    },
    {
      "epoch": 0.0845959595959596,
      "grad_norm": 0.5065137147903442,
      "learning_rate": 0.00019992133538916608,
      "loss": 0.6025,
      "step": 1340
    },
    {
      "epoch": 0.08522727272727272,
      "grad_norm": 0.8079489469528198,
      "learning_rate": 0.0001999172288254295,
      "loss": 0.6634,
      "step": 1350
    },
    {
      "epoch": 0.08585858585858586,
      "grad_norm": 0.3665786385536194,
      "learning_rate": 0.00019991301782737537,
      "loss": 0.9533,
      "step": 1360
    },
    {
      "epoch": 0.08648989898989899,
      "grad_norm": 0.3954637348651886,
      "learning_rate": 0.00019990870239940502,
      "loss": 0.7459,
      "step": 1370
    },
    {
      "epoch": 0.08712121212121213,
      "grad_norm": 0.43833497166633606,
      "learning_rate": 0.000199904282546029,
      "loss": 0.7013,
      "step": 1380
    },
    {
      "epoch": 0.08775252525252525,
      "grad_norm": 0.46377429366111755,
      "learning_rate": 0.00019989975827186695,
      "loss": 0.6203,
      "step": 1390
    },
    {
      "epoch": 0.08838383838383838,
      "grad_norm": 0.6925950646400452,
      "learning_rate": 0.00019989512958164772,
      "loss": 0.686,
      "step": 1400
    },
    {
      "epoch": 0.08901515151515152,
      "grad_norm": 0.3730338215827942,
      "learning_rate": 0.0001998903964802092,
      "loss": 0.8619,
      "step": 1410
    },
    {
      "epoch": 0.08964646464646464,
      "grad_norm": 0.4593350291252136,
      "learning_rate": 0.0001998855589724985,
      "loss": 0.7617,
      "step": 1420
    },
    {
      "epoch": 0.09027777777777778,
      "grad_norm": 0.439101904630661,
      "learning_rate": 0.0001998806170635718,
      "loss": 0.6654,
      "step": 1430
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.5115461945533752,
      "learning_rate": 0.0001998755707585945,
      "loss": 0.6212,
      "step": 1440
    },
    {
      "epoch": 0.09154040404040405,
      "grad_norm": 0.733974814414978,
      "learning_rate": 0.00019987042006284095,
      "loss": 0.6359,
      "step": 1450
    },
    {
      "epoch": 0.09217171717171717,
      "grad_norm": 0.3736056387424469,
      "learning_rate": 0.00019986516498169473,
      "loss": 0.9278,
      "step": 1460
    },
    {
      "epoch": 0.0928030303030303,
      "grad_norm": 0.40236544609069824,
      "learning_rate": 0.0001998598055206485,
      "loss": 0.7304,
      "step": 1470
    },
    {
      "epoch": 0.09343434343434344,
      "grad_norm": 0.43914780020713806,
      "learning_rate": 0.00019985434168530398,
      "loss": 0.6722,
      "step": 1480
    },
    {
      "epoch": 0.09406565656565656,
      "grad_norm": 0.5030515193939209,
      "learning_rate": 0.0001998487734813721,
      "loss": 0.6272,
      "step": 1490
    },
    {
      "epoch": 0.0946969696969697,
      "grad_norm": 0.8057199716567993,
      "learning_rate": 0.0001998431009146727,
      "loss": 0.625,
      "step": 1500
    },
    {
      "epoch": 0.09532828282828283,
      "grad_norm": 0.40450990200042725,
      "learning_rate": 0.00019983732399113483,
      "loss": 0.8736,
      "step": 1510
    },
    {
      "epoch": 0.09595959595959595,
      "grad_norm": 0.42038848996162415,
      "learning_rate": 0.0001998314427167966,
      "loss": 0.7607,
      "step": 1520
    },
    {
      "epoch": 0.09659090909090909,
      "grad_norm": 0.43843209743499756,
      "learning_rate": 0.00019982545709780515,
      "loss": 0.6551,
      "step": 1530
    },
    {
      "epoch": 0.09722222222222222,
      "grad_norm": 0.41590139269828796,
      "learning_rate": 0.00019981936714041668,
      "loss": 0.6006,
      "step": 1540
    },
    {
      "epoch": 0.09785353535353536,
      "grad_norm": 0.7438271045684814,
      "learning_rate": 0.00019981317285099653,
      "loss": 0.6235,
      "step": 1550
    },
    {
      "epoch": 0.09848484848484848,
      "grad_norm": 0.41930055618286133,
      "learning_rate": 0.0001998068742360189,
      "loss": 0.9263,
      "step": 1560
    },
    {
      "epoch": 0.09911616161616162,
      "grad_norm": 0.4062010645866394,
      "learning_rate": 0.00019980047130206728,
      "loss": 0.7672,
      "step": 1570
    },
    {
      "epoch": 0.09974747474747475,
      "grad_norm": 0.4763449430465698,
      "learning_rate": 0.000199793964055834,
      "loss": 0.6591,
      "step": 1580
    },
    {
      "epoch": 0.10037878787878787,
      "grad_norm": 0.5426257252693176,
      "learning_rate": 0.0001997873525041205,
      "loss": 0.6287,
      "step": 1590
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 0.7129928469657898,
      "learning_rate": 0.00019978063665383725,
      "loss": 0.6364,
      "step": 1600
    },
    {
      "epoch": 0.10164141414141414,
      "grad_norm": 0.3966273069381714,
      "learning_rate": 0.0001997738165120037,
      "loss": 0.8429,
      "step": 1610
    },
    {
      "epoch": 0.10227272727272728,
      "grad_norm": 0.4119454324245453,
      "learning_rate": 0.00019976689208574827,
      "loss": 0.742,
      "step": 1620
    },
    {
      "epoch": 0.1029040404040404,
      "grad_norm": 0.4555959105491638,
      "learning_rate": 0.00019975986338230853,
      "loss": 0.6623,
      "step": 1630
    },
    {
      "epoch": 0.10353535353535354,
      "grad_norm": 0.43793985247612,
      "learning_rate": 0.00019975273040903087,
      "loss": 0.5639,
      "step": 1640
    },
    {
      "epoch": 0.10416666666666667,
      "grad_norm": 0.7531010508537292,
      "learning_rate": 0.00019974549317337075,
      "loss": 0.6215,
      "step": 1650
    },
    {
      "epoch": 0.10479797979797979,
      "grad_norm": 0.3845902979373932,
      "learning_rate": 0.00019973815168289257,
      "loss": 0.8593,
      "step": 1660
    },
    {
      "epoch": 0.10542929292929293,
      "grad_norm": 0.37512749433517456,
      "learning_rate": 0.00019973070594526973,
      "loss": 0.738,
      "step": 1670
    },
    {
      "epoch": 0.10606060606060606,
      "grad_norm": 0.44194260239601135,
      "learning_rate": 0.00019972315596828458,
      "loss": 0.6434,
      "step": 1680
    },
    {
      "epoch": 0.10669191919191919,
      "grad_norm": 0.49364691972732544,
      "learning_rate": 0.0001997155017598284,
      "loss": 0.5978,
      "step": 1690
    },
    {
      "epoch": 0.10732323232323232,
      "grad_norm": 0.7883038520812988,
      "learning_rate": 0.0001997077433279015,
      "loss": 0.6137,
      "step": 1700
    },
    {
      "epoch": 0.10795454545454546,
      "grad_norm": 0.3548344373703003,
      "learning_rate": 0.00019969988068061295,
      "loss": 0.877,
      "step": 1710
    },
    {
      "epoch": 0.10858585858585859,
      "grad_norm": 0.43218061327934265,
      "learning_rate": 0.00019969191382618095,
      "loss": 0.7494,
      "step": 1720
    },
    {
      "epoch": 0.10921717171717171,
      "grad_norm": 0.4236109256744385,
      "learning_rate": 0.00019968384277293247,
      "loss": 0.6712,
      "step": 1730
    },
    {
      "epoch": 0.10984848484848485,
      "grad_norm": 0.4770107865333557,
      "learning_rate": 0.00019967566752930347,
      "loss": 0.6119,
      "step": 1740
    },
    {
      "epoch": 0.11047979797979798,
      "grad_norm": 0.7175971865653992,
      "learning_rate": 0.00019966738810383875,
      "loss": 0.626,
      "step": 1750
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.391148179769516,
      "learning_rate": 0.00019965900450519207,
      "loss": 0.8631,
      "step": 1760
    },
    {
      "epoch": 0.11174242424242424,
      "grad_norm": 0.4148746430873871,
      "learning_rate": 0.000199650516742126,
      "loss": 0.7649,
      "step": 1770
    },
    {
      "epoch": 0.11237373737373738,
      "grad_norm": 0.45345914363861084,
      "learning_rate": 0.00019964192482351204,
      "loss": 0.662,
      "step": 1780
    },
    {
      "epoch": 0.11300505050505051,
      "grad_norm": 0.4668276607990265,
      "learning_rate": 0.00019963322875833056,
      "loss": 0.6154,
      "step": 1790
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 0.7180171012878418,
      "learning_rate": 0.0001996244285556707,
      "loss": 0.6158,
      "step": 1800
    },
    {
      "epoch": 0.11426767676767677,
      "grad_norm": 0.3847852945327759,
      "learning_rate": 0.00019961552422473057,
      "loss": 0.885,
      "step": 1810
    },
    {
      "epoch": 0.1148989898989899,
      "grad_norm": 0.4073054790496826,
      "learning_rate": 0.000199606515774817,
      "loss": 0.7053,
      "step": 1820
    },
    {
      "epoch": 0.11553030303030302,
      "grad_norm": 0.4230479300022125,
      "learning_rate": 0.00019959740321534572,
      "loss": 0.6618,
      "step": 1830
    },
    {
      "epoch": 0.11616161616161616,
      "grad_norm": 0.5169723629951477,
      "learning_rate": 0.00019958818655584125,
      "loss": 0.5646,
      "step": 1840
    },
    {
      "epoch": 0.1167929292929293,
      "grad_norm": 0.744612455368042,
      "learning_rate": 0.0001995788658059369,
      "loss": 0.6431,
      "step": 1850
    },
    {
      "epoch": 0.11742424242424243,
      "grad_norm": 0.3684014678001404,
      "learning_rate": 0.00019956944097537484,
      "loss": 0.9573,
      "step": 1860
    },
    {
      "epoch": 0.11805555555555555,
      "grad_norm": 0.38837382197380066,
      "learning_rate": 0.00019955991207400595,
      "loss": 0.7535,
      "step": 1870
    },
    {
      "epoch": 0.11868686868686869,
      "grad_norm": 0.4441465139389038,
      "learning_rate": 0.0001995502791117899,
      "loss": 0.6887,
      "step": 1880
    },
    {
      "epoch": 0.11931818181818182,
      "grad_norm": 0.5103716254234314,
      "learning_rate": 0.0001995405420987952,
      "loss": 0.6143,
      "step": 1890
    },
    {
      "epoch": 0.11994949494949494,
      "grad_norm": 0.684220016002655,
      "learning_rate": 0.000199530701045199,
      "loss": 0.6455,
      "step": 1900
    },
    {
      "epoch": 0.12058080808080808,
      "grad_norm": 0.37400054931640625,
      "learning_rate": 0.00019952075596128726,
      "loss": 0.9101,
      "step": 1910
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 0.4491346478462219,
      "learning_rate": 0.0001995107068574547,
      "loss": 0.7633,
      "step": 1920
    },
    {
      "epoch": 0.12184343434343434,
      "grad_norm": 0.41543179750442505,
      "learning_rate": 0.00019950055374420468,
      "loss": 0.6819,
      "step": 1930
    },
    {
      "epoch": 0.12247474747474747,
      "grad_norm": 0.45397669076919556,
      "learning_rate": 0.00019949029663214937,
      "loss": 0.6184,
      "step": 1940
    },
    {
      "epoch": 0.12310606060606061,
      "grad_norm": 0.6901859045028687,
      "learning_rate": 0.00019947993553200955,
      "loss": 0.6247,
      "step": 1950
    },
    {
      "epoch": 0.12373737373737374,
      "grad_norm": 0.3749717175960541,
      "learning_rate": 0.00019946947045461472,
      "loss": 0.9043,
      "step": 1960
    },
    {
      "epoch": 0.12436868686868686,
      "grad_norm": 0.40582504868507385,
      "learning_rate": 0.00019945890141090307,
      "loss": 0.7541,
      "step": 1970
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.41686418652534485,
      "learning_rate": 0.0001994482284119215,
      "loss": 0.6351,
      "step": 1980
    },
    {
      "epoch": 0.12563131313131312,
      "grad_norm": 0.47160783410072327,
      "learning_rate": 0.0001994374514688255,
      "loss": 0.5695,
      "step": 1990
    },
    {
      "epoch": 0.12626262626262627,
      "grad_norm": 0.8251745700836182,
      "learning_rate": 0.00019942657059287918,
      "loss": 0.6143,
      "step": 2000
    },
    {
      "epoch": 0.12626262626262627,
      "eval_loss": 0.7293572425842285,
      "eval_runtime": 31.7574,
      "eval_samples_per_second": 80.611,
      "eval_steps_per_second": 10.076,
      "step": 2000
    },
    {
      "epoch": 0.1268939393939394,
      "grad_norm": 0.3971514403820038,
      "learning_rate": 0.00019941558579545534,
      "loss": 0.8608,
      "step": 2010
    },
    {
      "epoch": 0.1275252525252525,
      "grad_norm": 0.4247588813304901,
      "learning_rate": 0.00019940449708803537,
      "loss": 0.7522,
      "step": 2020
    },
    {
      "epoch": 0.12815656565656566,
      "grad_norm": 0.44713646173477173,
      "learning_rate": 0.00019939330448220932,
      "loss": 0.651,
      "step": 2030
    },
    {
      "epoch": 0.12878787878787878,
      "grad_norm": 0.4978382885456085,
      "learning_rate": 0.00019938200798967577,
      "loss": 0.6167,
      "step": 2040
    },
    {
      "epoch": 0.1294191919191919,
      "grad_norm": 0.7312682867050171,
      "learning_rate": 0.00019937060762224192,
      "loss": 0.6165,
      "step": 2050
    },
    {
      "epoch": 0.13005050505050506,
      "grad_norm": 0.3869039714336395,
      "learning_rate": 0.00019935910339182348,
      "loss": 0.8716,
      "step": 2060
    },
    {
      "epoch": 0.13068181818181818,
      "grad_norm": 0.42783471941947937,
      "learning_rate": 0.00019934749531044484,
      "loss": 0.7428,
      "step": 2070
    },
    {
      "epoch": 0.13131313131313133,
      "grad_norm": 0.46140751242637634,
      "learning_rate": 0.0001993357833902388,
      "loss": 0.6615,
      "step": 2080
    },
    {
      "epoch": 0.13194444444444445,
      "grad_norm": 0.45559343695640564,
      "learning_rate": 0.0001993239676434468,
      "loss": 0.5739,
      "step": 2090
    },
    {
      "epoch": 0.13257575757575757,
      "grad_norm": 0.740159809589386,
      "learning_rate": 0.00019931204808241873,
      "loss": 0.6273,
      "step": 2100
    },
    {
      "epoch": 0.13320707070707072,
      "grad_norm": 0.38589224219322205,
      "learning_rate": 0.00019930002471961301,
      "loss": 0.8367,
      "step": 2110
    },
    {
      "epoch": 0.13383838383838384,
      "grad_norm": 0.4188304543495178,
      "learning_rate": 0.00019928789756759661,
      "loss": 0.7171,
      "step": 2120
    },
    {
      "epoch": 0.13446969696969696,
      "grad_norm": 0.4339233934879303,
      "learning_rate": 0.00019927566663904486,
      "loss": 0.6495,
      "step": 2130
    },
    {
      "epoch": 0.1351010101010101,
      "grad_norm": 0.4782721996307373,
      "learning_rate": 0.0001992633319467417,
      "loss": 0.6358,
      "step": 2140
    },
    {
      "epoch": 0.13573232323232323,
      "grad_norm": 0.6503652334213257,
      "learning_rate": 0.00019925089350357938,
      "loss": 0.6369,
      "step": 2150
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 0.36016663908958435,
      "learning_rate": 0.00019923835132255874,
      "loss": 0.8935,
      "step": 2160
    },
    {
      "epoch": 0.1369949494949495,
      "grad_norm": 0.41628092527389526,
      "learning_rate": 0.00019922570541678887,
      "loss": 0.7555,
      "step": 2170
    },
    {
      "epoch": 0.13762626262626262,
      "grad_norm": 0.45247697830200195,
      "learning_rate": 0.00019921295579948748,
      "loss": 0.671,
      "step": 2180
    },
    {
      "epoch": 0.13825757575757575,
      "grad_norm": 0.4895693063735962,
      "learning_rate": 0.00019920010248398052,
      "loss": 0.5865,
      "step": 2190
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 0.6813344955444336,
      "learning_rate": 0.0001991871454837024,
      "loss": 0.6134,
      "step": 2200
    },
    {
      "epoch": 0.13952020202020202,
      "grad_norm": 0.3848794400691986,
      "learning_rate": 0.00019917408481219585,
      "loss": 0.9019,
      "step": 2210
    },
    {
      "epoch": 0.14015151515151514,
      "grad_norm": 0.42293110489845276,
      "learning_rate": 0.00019916092048311205,
      "loss": 0.7356,
      "step": 2220
    },
    {
      "epoch": 0.1407828282828283,
      "grad_norm": 0.4458554685115814,
      "learning_rate": 0.0001991476525102104,
      "loss": 0.6819,
      "step": 2230
    },
    {
      "epoch": 0.1414141414141414,
      "grad_norm": 0.423052579164505,
      "learning_rate": 0.00019913428090735877,
      "loss": 0.5677,
      "step": 2240
    },
    {
      "epoch": 0.14204545454545456,
      "grad_norm": 0.7755367755889893,
      "learning_rate": 0.00019912080568853323,
      "loss": 0.6153,
      "step": 2250
    },
    {
      "epoch": 0.14267676767676768,
      "grad_norm": 0.3501570224761963,
      "learning_rate": 0.0001991072268678182,
      "loss": 0.8899,
      "step": 2260
    },
    {
      "epoch": 0.1433080808080808,
      "grad_norm": 0.4546564221382141,
      "learning_rate": 0.00019909354445940634,
      "loss": 0.7632,
      "step": 2270
    },
    {
      "epoch": 0.14393939393939395,
      "grad_norm": 0.399573415517807,
      "learning_rate": 0.00019907975847759866,
      "loss": 0.6508,
      "step": 2280
    },
    {
      "epoch": 0.14457070707070707,
      "grad_norm": 0.486510694026947,
      "learning_rate": 0.00019906586893680438,
      "loss": 0.5724,
      "step": 2290
    },
    {
      "epoch": 0.1452020202020202,
      "grad_norm": 0.7777507901191711,
      "learning_rate": 0.00019905187585154095,
      "loss": 0.6107,
      "step": 2300
    },
    {
      "epoch": 0.14583333333333334,
      "grad_norm": 0.3735269606113434,
      "learning_rate": 0.00019903777923643406,
      "loss": 0.8514,
      "step": 2310
    },
    {
      "epoch": 0.14646464646464646,
      "grad_norm": 0.41821542382240295,
      "learning_rate": 0.00019902357910621762,
      "loss": 0.736,
      "step": 2320
    },
    {
      "epoch": 0.14709595959595959,
      "grad_norm": 0.44384944438934326,
      "learning_rate": 0.00019900927547573366,
      "loss": 0.6867,
      "step": 2330
    },
    {
      "epoch": 0.14772727272727273,
      "grad_norm": 0.4860313832759857,
      "learning_rate": 0.00019899486835993257,
      "loss": 0.5835,
      "step": 2340
    },
    {
      "epoch": 0.14835858585858586,
      "grad_norm": 0.8632450699806213,
      "learning_rate": 0.0001989803577738727,
      "loss": 0.62,
      "step": 2350
    },
    {
      "epoch": 0.14898989898989898,
      "grad_norm": 0.3772371709346771,
      "learning_rate": 0.00019896574373272065,
      "loss": 0.9176,
      "step": 2360
    },
    {
      "epoch": 0.14962121212121213,
      "grad_norm": 0.4327748417854309,
      "learning_rate": 0.00019895102625175118,
      "loss": 0.7337,
      "step": 2370
    },
    {
      "epoch": 0.15025252525252525,
      "grad_norm": 0.4539027214050293,
      "learning_rate": 0.00019893620534634706,
      "loss": 0.6587,
      "step": 2380
    },
    {
      "epoch": 0.15088383838383837,
      "grad_norm": 0.5412132740020752,
      "learning_rate": 0.00019892128103199928,
      "loss": 0.5945,
      "step": 2390
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 0.8182401657104492,
      "learning_rate": 0.0001989062533243068,
      "loss": 0.6248,
      "step": 2400
    },
    {
      "epoch": 0.15214646464646464,
      "grad_norm": 0.3909232020378113,
      "learning_rate": 0.00019889112223897676,
      "loss": 0.9355,
      "step": 2410
    },
    {
      "epoch": 0.1527777777777778,
      "grad_norm": 0.3995417356491089,
      "learning_rate": 0.0001988758877918243,
      "loss": 0.7545,
      "step": 2420
    },
    {
      "epoch": 0.1534090909090909,
      "grad_norm": 0.4399908483028412,
      "learning_rate": 0.0001988605499987725,
      "loss": 0.6412,
      "step": 2430
    },
    {
      "epoch": 0.15404040404040403,
      "grad_norm": 0.45129871368408203,
      "learning_rate": 0.00019884510887585263,
      "loss": 0.5649,
      "step": 2440
    },
    {
      "epoch": 0.15467171717171718,
      "grad_norm": 0.6832021474838257,
      "learning_rate": 0.00019882956443920388,
      "loss": 0.6044,
      "step": 2450
    },
    {
      "epoch": 0.1553030303030303,
      "grad_norm": 0.4032594859600067,
      "learning_rate": 0.00019881391670507342,
      "loss": 0.879,
      "step": 2460
    },
    {
      "epoch": 0.15593434343434343,
      "grad_norm": 0.4328239858150482,
      "learning_rate": 0.00019879816568981636,
      "loss": 0.7573,
      "step": 2470
    },
    {
      "epoch": 0.15656565656565657,
      "grad_norm": 0.4587946832180023,
      "learning_rate": 0.0001987823114098958,
      "loss": 0.6734,
      "step": 2480
    },
    {
      "epoch": 0.1571969696969697,
      "grad_norm": 0.5142857432365417,
      "learning_rate": 0.0001987663538818828,
      "loss": 0.6368,
      "step": 2490
    },
    {
      "epoch": 0.15782828282828282,
      "grad_norm": 0.7458919882774353,
      "learning_rate": 0.00019875029312245625,
      "loss": 0.6155,
      "step": 2500
    },
    {
      "epoch": 0.15845959595959597,
      "grad_norm": 0.3724263906478882,
      "learning_rate": 0.00019873412914840304,
      "loss": 0.8817,
      "step": 2510
    },
    {
      "epoch": 0.1590909090909091,
      "grad_norm": 0.39034390449523926,
      "learning_rate": 0.00019871786197661785,
      "loss": 0.7343,
      "step": 2520
    },
    {
      "epoch": 0.1597222222222222,
      "grad_norm": 0.38830745220184326,
      "learning_rate": 0.00019870149162410326,
      "loss": 0.6591,
      "step": 2530
    },
    {
      "epoch": 0.16035353535353536,
      "grad_norm": 0.43227821588516235,
      "learning_rate": 0.00019868501810796975,
      "loss": 0.6136,
      "step": 2540
    },
    {
      "epoch": 0.16098484848484848,
      "grad_norm": 0.7463453412055969,
      "learning_rate": 0.00019866844144543553,
      "loss": 0.6098,
      "step": 2550
    },
    {
      "epoch": 0.16161616161616163,
      "grad_norm": 0.3686067759990692,
      "learning_rate": 0.0001986517616538267,
      "loss": 0.8639,
      "step": 2560
    },
    {
      "epoch": 0.16224747474747475,
      "grad_norm": 0.40347689390182495,
      "learning_rate": 0.00019863497875057705,
      "loss": 0.78,
      "step": 2570
    },
    {
      "epoch": 0.16287878787878787,
      "grad_norm": 0.43249377608299255,
      "learning_rate": 0.00019861809275322826,
      "loss": 0.6724,
      "step": 2580
    },
    {
      "epoch": 0.16351010101010102,
      "grad_norm": 0.4517967998981476,
      "learning_rate": 0.00019860110367942975,
      "loss": 0.5797,
      "step": 2590
    },
    {
      "epoch": 0.16414141414141414,
      "grad_norm": 0.7727293372154236,
      "learning_rate": 0.00019858401154693858,
      "loss": 0.6257,
      "step": 2600
    },
    {
      "epoch": 0.16477272727272727,
      "grad_norm": 0.3854585289955139,
      "learning_rate": 0.0001985668163736196,
      "loss": 0.8739,
      "step": 2610
    },
    {
      "epoch": 0.16540404040404041,
      "grad_norm": 0.41243264079093933,
      "learning_rate": 0.00019854951817744536,
      "loss": 0.7409,
      "step": 2620
    },
    {
      "epoch": 0.16603535353535354,
      "grad_norm": 0.4518876075744629,
      "learning_rate": 0.00019853211697649608,
      "loss": 0.6367,
      "step": 2630
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.5208210349082947,
      "learning_rate": 0.0001985146127889596,
      "loss": 0.6186,
      "step": 2640
    },
    {
      "epoch": 0.1672979797979798,
      "grad_norm": 0.7567474246025085,
      "learning_rate": 0.00019849700563313153,
      "loss": 0.6515,
      "step": 2650
    },
    {
      "epoch": 0.16792929292929293,
      "grad_norm": 0.35633984208106995,
      "learning_rate": 0.00019847929552741495,
      "loss": 0.8977,
      "step": 2660
    },
    {
      "epoch": 0.16856060606060605,
      "grad_norm": 0.4494902491569519,
      "learning_rate": 0.00019846148249032063,
      "loss": 0.7339,
      "step": 2670
    },
    {
      "epoch": 0.1691919191919192,
      "grad_norm": 0.4640219807624817,
      "learning_rate": 0.00019844356654046688,
      "loss": 0.6587,
      "step": 2680
    },
    {
      "epoch": 0.16982323232323232,
      "grad_norm": 0.4250389337539673,
      "learning_rate": 0.00019842554769657962,
      "loss": 0.5876,
      "step": 2690
    },
    {
      "epoch": 0.17045454545454544,
      "grad_norm": 0.6861960887908936,
      "learning_rate": 0.0001984074259774923,
      "loss": 0.6091,
      "step": 2700
    },
    {
      "epoch": 0.1710858585858586,
      "grad_norm": 0.37929242849349976,
      "learning_rate": 0.00019838920140214587,
      "loss": 0.8484,
      "step": 2710
    },
    {
      "epoch": 0.1717171717171717,
      "grad_norm": 0.41456496715545654,
      "learning_rate": 0.00019837087398958883,
      "loss": 0.7457,
      "step": 2720
    },
    {
      "epoch": 0.17234848484848486,
      "grad_norm": 0.44595852494239807,
      "learning_rate": 0.00019835244375897713,
      "loss": 0.6422,
      "step": 2730
    },
    {
      "epoch": 0.17297979797979798,
      "grad_norm": 0.4555014669895172,
      "learning_rate": 0.00019833391072957422,
      "loss": 0.5956,
      "step": 2740
    },
    {
      "epoch": 0.1736111111111111,
      "grad_norm": 0.7279156446456909,
      "learning_rate": 0.00019831527492075092,
      "loss": 0.6277,
      "step": 2750
    },
    {
      "epoch": 0.17424242424242425,
      "grad_norm": 0.3791496157646179,
      "learning_rate": 0.00019829653635198563,
      "loss": 0.8731,
      "step": 2760
    },
    {
      "epoch": 0.17487373737373738,
      "grad_norm": 0.39746934175491333,
      "learning_rate": 0.00019827769504286396,
      "loss": 0.7085,
      "step": 2770
    },
    {
      "epoch": 0.1755050505050505,
      "grad_norm": 0.446095734834671,
      "learning_rate": 0.00019825875101307905,
      "loss": 0.6749,
      "step": 2780
    },
    {
      "epoch": 0.17613636363636365,
      "grad_norm": 0.4769935607910156,
      "learning_rate": 0.00019823970428243135,
      "loss": 0.6057,
      "step": 2790
    },
    {
      "epoch": 0.17676767676767677,
      "grad_norm": 0.6308411955833435,
      "learning_rate": 0.00019822055487082866,
      "loss": 0.6145,
      "step": 2800
    },
    {
      "epoch": 0.1773989898989899,
      "grad_norm": 0.39507317543029785,
      "learning_rate": 0.00019820130279828613,
      "loss": 0.8697,
      "step": 2810
    },
    {
      "epoch": 0.17803030303030304,
      "grad_norm": 0.4061274230480194,
      "learning_rate": 0.00019818194808492615,
      "loss": 0.7446,
      "step": 2820
    },
    {
      "epoch": 0.17866161616161616,
      "grad_norm": 0.404574990272522,
      "learning_rate": 0.00019816249075097845,
      "loss": 0.647,
      "step": 2830
    },
    {
      "epoch": 0.17929292929292928,
      "grad_norm": 0.5524725317955017,
      "learning_rate": 0.00019814293081677994,
      "loss": 0.5774,
      "step": 2840
    },
    {
      "epoch": 0.17992424242424243,
      "grad_norm": 0.822968065738678,
      "learning_rate": 0.0001981232683027749,
      "loss": 0.6356,
      "step": 2850
    },
    {
      "epoch": 0.18055555555555555,
      "grad_norm": 0.372676283121109,
      "learning_rate": 0.00019810350322951474,
      "loss": 0.8591,
      "step": 2860
    },
    {
      "epoch": 0.18118686868686867,
      "grad_norm": 0.4423505663871765,
      "learning_rate": 0.00019808363561765806,
      "loss": 0.736,
      "step": 2870
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.4202714264392853,
      "learning_rate": 0.00019806366548797067,
      "loss": 0.637,
      "step": 2880
    },
    {
      "epoch": 0.18244949494949494,
      "grad_norm": 0.44987186789512634,
      "learning_rate": 0.00019804359286132548,
      "loss": 0.6007,
      "step": 2890
    },
    {
      "epoch": 0.1830808080808081,
      "grad_norm": 0.7314785718917847,
      "learning_rate": 0.0001980234177587026,
      "loss": 0.6087,
      "step": 2900
    },
    {
      "epoch": 0.18371212121212122,
      "grad_norm": 0.38028275966644287,
      "learning_rate": 0.00019800314020118918,
      "loss": 0.8777,
      "step": 2910
    },
    {
      "epoch": 0.18434343434343434,
      "grad_norm": 0.42788687348365784,
      "learning_rate": 0.00019798276020997952,
      "loss": 0.7576,
      "step": 2920
    },
    {
      "epoch": 0.1849747474747475,
      "grad_norm": 0.42276349663734436,
      "learning_rate": 0.00019796227780637498,
      "loss": 0.6487,
      "step": 2930
    },
    {
      "epoch": 0.1856060606060606,
      "grad_norm": 0.5084601640701294,
      "learning_rate": 0.0001979416930117839,
      "loss": 0.5939,
      "step": 2940
    },
    {
      "epoch": 0.18623737373737373,
      "grad_norm": 0.779350221157074,
      "learning_rate": 0.00019792100584772166,
      "loss": 0.6104,
      "step": 2950
    },
    {
      "epoch": 0.18686868686868688,
      "grad_norm": 0.3786032795906067,
      "learning_rate": 0.00019790021633581071,
      "loss": 0.8519,
      "step": 2960
    },
    {
      "epoch": 0.1875,
      "grad_norm": 0.46367624402046204,
      "learning_rate": 0.0001978793244977804,
      "loss": 0.7323,
      "step": 2970
    },
    {
      "epoch": 0.18813131313131312,
      "grad_norm": 0.441008061170578,
      "learning_rate": 0.00019785833035546702,
      "loss": 0.6688,
      "step": 2980
    },
    {
      "epoch": 0.18876262626262627,
      "grad_norm": 0.4679785966873169,
      "learning_rate": 0.00019783723393081387,
      "loss": 0.5823,
      "step": 2990
    },
    {
      "epoch": 0.1893939393939394,
      "grad_norm": 0.6764836311340332,
      "learning_rate": 0.00019781603524587107,
      "loss": 0.6004,
      "step": 3000
    },
    {
      "epoch": 0.1893939393939394,
      "eval_loss": 0.7263892292976379,
      "eval_runtime": 31.6856,
      "eval_samples_per_second": 80.794,
      "eval_steps_per_second": 10.099,
      "step": 3000
    },
    {
      "epoch": 0.1900252525252525,
      "grad_norm": 0.3845011591911316,
      "learning_rate": 0.00019779473432279568,
      "loss": 0.8952,
      "step": 3010
    },
    {
      "epoch": 0.19065656565656566,
      "grad_norm": 0.4471258521080017,
      "learning_rate": 0.00019777333118385163,
      "loss": 0.7322,
      "step": 3020
    },
    {
      "epoch": 0.19128787878787878,
      "grad_norm": 0.4127425253391266,
      "learning_rate": 0.00019775182585140958,
      "loss": 0.6633,
      "step": 3030
    },
    {
      "epoch": 0.1919191919191919,
      "grad_norm": 0.4388314485549927,
      "learning_rate": 0.0001977302183479472,
      "loss": 0.5564,
      "step": 3040
    },
    {
      "epoch": 0.19255050505050506,
      "grad_norm": 0.8173882365226746,
      "learning_rate": 0.00019770850869604872,
      "loss": 0.6157,
      "step": 3050
    },
    {
      "epoch": 0.19318181818181818,
      "grad_norm": 0.4082072973251343,
      "learning_rate": 0.0001976866969184053,
      "loss": 0.8619,
      "step": 3060
    },
    {
      "epoch": 0.19381313131313133,
      "grad_norm": 0.41982945799827576,
      "learning_rate": 0.00019766478303781475,
      "loss": 0.723,
      "step": 3070
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 0.4569641649723053,
      "learning_rate": 0.0001976427670771817,
      "loss": 0.6417,
      "step": 3080
    },
    {
      "epoch": 0.19507575757575757,
      "grad_norm": 0.43435773253440857,
      "learning_rate": 0.0001976206490595174,
      "loss": 0.5932,
      "step": 3090
    },
    {
      "epoch": 0.19570707070707072,
      "grad_norm": 0.6690974235534668,
      "learning_rate": 0.00019759842900793974,
      "loss": 0.618,
      "step": 3100
    },
    {
      "epoch": 0.19633838383838384,
      "grad_norm": 0.37854012846946716,
      "learning_rate": 0.00019757610694567338,
      "loss": 0.8516,
      "step": 3110
    },
    {
      "epoch": 0.19696969696969696,
      "grad_norm": 0.4181341826915741,
      "learning_rate": 0.00019755368289604944,
      "loss": 0.7588,
      "step": 3120
    },
    {
      "epoch": 0.1976010101010101,
      "grad_norm": 0.4465066194534302,
      "learning_rate": 0.0001975311568825058,
      "loss": 0.6422,
      "step": 3130
    },
    {
      "epoch": 0.19823232323232323,
      "grad_norm": 0.4442051947116852,
      "learning_rate": 0.00019750852892858677,
      "loss": 0.5629,
      "step": 3140
    },
    {
      "epoch": 0.19886363636363635,
      "grad_norm": 0.7465317249298096,
      "learning_rate": 0.00019748579905794333,
      "loss": 0.5969,
      "step": 3150
    },
    {
      "epoch": 0.1994949494949495,
      "grad_norm": 0.413459837436676,
      "learning_rate": 0.0001974629672943329,
      "loss": 0.8672,
      "step": 3160
    },
    {
      "epoch": 0.20012626262626262,
      "grad_norm": 0.41762325167655945,
      "learning_rate": 0.00019744003366161942,
      "loss": 0.7353,
      "step": 3170
    },
    {
      "epoch": 0.20075757575757575,
      "grad_norm": 0.42647886276245117,
      "learning_rate": 0.00019741699818377333,
      "loss": 0.6129,
      "step": 3180
    },
    {
      "epoch": 0.2013888888888889,
      "grad_norm": 0.4732725918292999,
      "learning_rate": 0.0001973938608848715,
      "loss": 0.5585,
      "step": 3190
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 0.7412172555923462,
      "learning_rate": 0.00019737062178909725,
      "loss": 0.6026,
      "step": 3200
    },
    {
      "epoch": 0.20265151515151514,
      "grad_norm": 0.38825249671936035,
      "learning_rate": 0.00019734728092074024,
      "loss": 0.8658,
      "step": 3210
    },
    {
      "epoch": 0.2032828282828283,
      "grad_norm": 0.458163857460022,
      "learning_rate": 0.00019732383830419658,
      "loss": 0.7367,
      "step": 3220
    },
    {
      "epoch": 0.2039141414141414,
      "grad_norm": 0.43253666162490845,
      "learning_rate": 0.00019730029396396862,
      "loss": 0.6311,
      "step": 3230
    },
    {
      "epoch": 0.20454545454545456,
      "grad_norm": 0.4963180720806122,
      "learning_rate": 0.00019727664792466515,
      "loss": 0.5829,
      "step": 3240
    },
    {
      "epoch": 0.20517676767676768,
      "grad_norm": 0.7818379402160645,
      "learning_rate": 0.0001972529002110012,
      "loss": 0.6372,
      "step": 3250
    },
    {
      "epoch": 0.2058080808080808,
      "grad_norm": 0.3763638138771057,
      "learning_rate": 0.00019722905084779808,
      "loss": 0.8868,
      "step": 3260
    },
    {
      "epoch": 0.20643939393939395,
      "grad_norm": 0.412898987531662,
      "learning_rate": 0.0001972050998599833,
      "loss": 0.7381,
      "step": 3270
    },
    {
      "epoch": 0.20707070707070707,
      "grad_norm": 0.4049209952354431,
      "learning_rate": 0.00019718104727259073,
      "loss": 0.6416,
      "step": 3280
    },
    {
      "epoch": 0.2077020202020202,
      "grad_norm": 0.47544535994529724,
      "learning_rate": 0.00019715689311076024,
      "loss": 0.5892,
      "step": 3290
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.6950272917747498,
      "learning_rate": 0.000197132637399738,
      "loss": 0.6003,
      "step": 3300
    },
    {
      "epoch": 0.20896464646464646,
      "grad_norm": 0.3939303755760193,
      "learning_rate": 0.00019710828016487628,
      "loss": 0.9305,
      "step": 3310
    },
    {
      "epoch": 0.20959595959595959,
      "grad_norm": 0.446317195892334,
      "learning_rate": 0.00019708382143163343,
      "loss": 0.7607,
      "step": 3320
    },
    {
      "epoch": 0.21022727272727273,
      "grad_norm": 0.4439278542995453,
      "learning_rate": 0.000197059261225574,
      "loss": 0.6659,
      "step": 3330
    },
    {
      "epoch": 0.21085858585858586,
      "grad_norm": 0.505388617515564,
      "learning_rate": 0.00019703459957236844,
      "loss": 0.5729,
      "step": 3340
    },
    {
      "epoch": 0.21148989898989898,
      "grad_norm": 0.6941298842430115,
      "learning_rate": 0.00019700983649779334,
      "loss": 0.5925,
      "step": 3350
    },
    {
      "epoch": 0.21212121212121213,
      "grad_norm": 0.38995271921157837,
      "learning_rate": 0.0001969849720277313,
      "loss": 0.8558,
      "step": 3360
    },
    {
      "epoch": 0.21275252525252525,
      "grad_norm": 0.4075118899345398,
      "learning_rate": 0.0001969600061881708,
      "loss": 0.6943,
      "step": 3370
    },
    {
      "epoch": 0.21338383838383837,
      "grad_norm": 0.44158869981765747,
      "learning_rate": 0.00019693493900520644,
      "loss": 0.6894,
      "step": 3380
    },
    {
      "epoch": 0.21401515151515152,
      "grad_norm": 0.521949052810669,
      "learning_rate": 0.0001969097705050386,
      "loss": 0.6074,
      "step": 3390
    },
    {
      "epoch": 0.21464646464646464,
      "grad_norm": 0.6393295526504517,
      "learning_rate": 0.00019688450071397357,
      "loss": 0.5758,
      "step": 3400
    },
    {
      "epoch": 0.2152777777777778,
      "grad_norm": 0.40309789776802063,
      "learning_rate": 0.00019685912965842359,
      "loss": 0.8518,
      "step": 3410
    },
    {
      "epoch": 0.2159090909090909,
      "grad_norm": 0.42819592356681824,
      "learning_rate": 0.00019683365736490672,
      "loss": 0.7465,
      "step": 3420
    },
    {
      "epoch": 0.21654040404040403,
      "grad_norm": 0.44560953974723816,
      "learning_rate": 0.00019680808386004673,
      "loss": 0.6606,
      "step": 3430
    },
    {
      "epoch": 0.21717171717171718,
      "grad_norm": 0.4993707537651062,
      "learning_rate": 0.00019678240917057335,
      "loss": 0.5934,
      "step": 3440
    },
    {
      "epoch": 0.2178030303030303,
      "grad_norm": 0.7279902696609497,
      "learning_rate": 0.0001967566333233219,
      "loss": 0.6034,
      "step": 3450
    },
    {
      "epoch": 0.21843434343434343,
      "grad_norm": 0.3884540796279907,
      "learning_rate": 0.0001967307563452336,
      "loss": 0.8374,
      "step": 3460
    },
    {
      "epoch": 0.21906565656565657,
      "grad_norm": 0.4163530170917511,
      "learning_rate": 0.00019670477826335517,
      "loss": 0.7388,
      "step": 3470
    },
    {
      "epoch": 0.2196969696969697,
      "grad_norm": 0.4720876216888428,
      "learning_rate": 0.00019667869910483922,
      "loss": 0.6706,
      "step": 3480
    },
    {
      "epoch": 0.22032828282828282,
      "grad_norm": 0.47071850299835205,
      "learning_rate": 0.00019665251889694387,
      "loss": 0.5888,
      "step": 3490
    },
    {
      "epoch": 0.22095959595959597,
      "grad_norm": 0.7972175478935242,
      "learning_rate": 0.00019662623766703285,
      "loss": 0.6277,
      "step": 3500
    },
    {
      "epoch": 0.2215909090909091,
      "grad_norm": 0.38138076663017273,
      "learning_rate": 0.00019659985544257557,
      "loss": 0.8848,
      "step": 3510
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.410422146320343,
      "learning_rate": 0.00019657337225114692,
      "loss": 0.705,
      "step": 3520
    },
    {
      "epoch": 0.22285353535353536,
      "grad_norm": 0.44009697437286377,
      "learning_rate": 0.0001965467881204274,
      "loss": 0.6448,
      "step": 3530
    },
    {
      "epoch": 0.22348484848484848,
      "grad_norm": 0.4647172689437866,
      "learning_rate": 0.00019652010307820287,
      "loss": 0.568,
      "step": 3540
    },
    {
      "epoch": 0.22411616161616163,
      "grad_norm": 0.8015565872192383,
      "learning_rate": 0.00019649331715236484,
      "loss": 0.6024,
      "step": 3550
    },
    {
      "epoch": 0.22474747474747475,
      "grad_norm": 0.3859432339668274,
      "learning_rate": 0.00019646643037091017,
      "loss": 0.8303,
      "step": 3560
    },
    {
      "epoch": 0.22537878787878787,
      "grad_norm": 0.43707528710365295,
      "learning_rate": 0.00019643944276194112,
      "loss": 0.7226,
      "step": 3570
    },
    {
      "epoch": 0.22601010101010102,
      "grad_norm": 0.4548543393611908,
      "learning_rate": 0.0001964123543536654,
      "loss": 0.6535,
      "step": 3580
    },
    {
      "epoch": 0.22664141414141414,
      "grad_norm": 0.4894196093082428,
      "learning_rate": 0.00019638516517439598,
      "loss": 0.576,
      "step": 3590
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 0.7038452625274658,
      "learning_rate": 0.0001963578752525513,
      "loss": 0.5854,
      "step": 3600
    },
    {
      "epoch": 0.22790404040404041,
      "grad_norm": 0.3919219672679901,
      "learning_rate": 0.00019633048461665492,
      "loss": 0.8632,
      "step": 3610
    },
    {
      "epoch": 0.22853535353535354,
      "grad_norm": 0.4514399468898773,
      "learning_rate": 0.00019630299329533583,
      "loss": 0.7328,
      "step": 3620
    },
    {
      "epoch": 0.22916666666666666,
      "grad_norm": 0.4152071177959442,
      "learning_rate": 0.00019627540131732815,
      "loss": 0.6593,
      "step": 3630
    },
    {
      "epoch": 0.2297979797979798,
      "grad_norm": 0.47460630536079407,
      "learning_rate": 0.0001962477087114713,
      "loss": 0.582,
      "step": 3640
    },
    {
      "epoch": 0.23042929292929293,
      "grad_norm": 0.7310224771499634,
      "learning_rate": 0.00019621991550670975,
      "loss": 0.5954,
      "step": 3650
    },
    {
      "epoch": 0.23106060606060605,
      "grad_norm": 0.3833620846271515,
      "learning_rate": 0.0001961920217320932,
      "loss": 0.8946,
      "step": 3660
    },
    {
      "epoch": 0.2316919191919192,
      "grad_norm": 0.4275381863117218,
      "learning_rate": 0.0001961640274167765,
      "loss": 0.7355,
      "step": 3670
    },
    {
      "epoch": 0.23232323232323232,
      "grad_norm": 0.45362481474876404,
      "learning_rate": 0.0001961359325900195,
      "loss": 0.6546,
      "step": 3680
    },
    {
      "epoch": 0.23295454545454544,
      "grad_norm": 0.4308529794216156,
      "learning_rate": 0.0001961077372811872,
      "loss": 0.5888,
      "step": 3690
    },
    {
      "epoch": 0.2335858585858586,
      "grad_norm": 0.6225800514221191,
      "learning_rate": 0.0001960794415197495,
      "loss": 0.6175,
      "step": 3700
    },
    {
      "epoch": 0.2342171717171717,
      "grad_norm": 0.39080938696861267,
      "learning_rate": 0.00019605104533528138,
      "loss": 0.856,
      "step": 3710
    },
    {
      "epoch": 0.23484848484848486,
      "grad_norm": 0.42048707604408264,
      "learning_rate": 0.00019602254875746283,
      "loss": 0.7273,
      "step": 3720
    },
    {
      "epoch": 0.23547979797979798,
      "grad_norm": 0.4681774079799652,
      "learning_rate": 0.00019599395181607864,
      "loss": 0.6609,
      "step": 3730
    },
    {
      "epoch": 0.2361111111111111,
      "grad_norm": 0.44815725088119507,
      "learning_rate": 0.00019596525454101856,
      "loss": 0.5428,
      "step": 3740
    },
    {
      "epoch": 0.23674242424242425,
      "grad_norm": 0.7128887176513672,
      "learning_rate": 0.0001959364569622773,
      "loss": 0.6199,
      "step": 3750
    },
    {
      "epoch": 0.23737373737373738,
      "grad_norm": 0.4024859070777893,
      "learning_rate": 0.00019590755910995426,
      "loss": 0.8656,
      "step": 3760
    },
    {
      "epoch": 0.2380050505050505,
      "grad_norm": 0.42903000116348267,
      "learning_rate": 0.00019587856101425377,
      "loss": 0.7263,
      "step": 3770
    },
    {
      "epoch": 0.23863636363636365,
      "grad_norm": 0.4616292417049408,
      "learning_rate": 0.00019584946270548482,
      "loss": 0.6246,
      "step": 3780
    },
    {
      "epoch": 0.23926767676767677,
      "grad_norm": 0.4750596284866333,
      "learning_rate": 0.00019582026421406125,
      "loss": 0.5925,
      "step": 3790
    },
    {
      "epoch": 0.2398989898989899,
      "grad_norm": 0.7134015560150146,
      "learning_rate": 0.00019579096557050156,
      "loss": 0.6004,
      "step": 3800
    },
    {
      "epoch": 0.24053030303030304,
      "grad_norm": 0.36563676595687866,
      "learning_rate": 0.0001957615668054289,
      "loss": 0.9062,
      "step": 3810
    },
    {
      "epoch": 0.24116161616161616,
      "grad_norm": 0.42556825280189514,
      "learning_rate": 0.00019573206794957116,
      "loss": 0.7141,
      "step": 3820
    },
    {
      "epoch": 0.24179292929292928,
      "grad_norm": 0.4527124762535095,
      "learning_rate": 0.00019570246903376071,
      "loss": 0.6396,
      "step": 3830
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 0.4524051547050476,
      "learning_rate": 0.00019567277008893466,
      "loss": 0.5814,
      "step": 3840
    },
    {
      "epoch": 0.24305555555555555,
      "grad_norm": 0.8041050434112549,
      "learning_rate": 0.0001956429711461346,
      "loss": 0.6312,
      "step": 3850
    },
    {
      "epoch": 0.24368686868686867,
      "grad_norm": 0.37380269169807434,
      "learning_rate": 0.00019561307223650654,
      "loss": 0.9009,
      "step": 3860
    },
    {
      "epoch": 0.24431818181818182,
      "grad_norm": 0.45515015721321106,
      "learning_rate": 0.00019558307339130116,
      "loss": 0.7222,
      "step": 3870
    },
    {
      "epoch": 0.24494949494949494,
      "grad_norm": 0.43123659491539,
      "learning_rate": 0.00019555297464187343,
      "loss": 0.6399,
      "step": 3880
    },
    {
      "epoch": 0.2455808080808081,
      "grad_norm": 0.47285959124565125,
      "learning_rate": 0.0001955227760196829,
      "loss": 0.5921,
      "step": 3890
    },
    {
      "epoch": 0.24621212121212122,
      "grad_norm": 0.8303335905075073,
      "learning_rate": 0.00019549247755629333,
      "loss": 0.6271,
      "step": 3900
    },
    {
      "epoch": 0.24684343434343434,
      "grad_norm": 0.37904393672943115,
      "learning_rate": 0.00019546207928337298,
      "loss": 0.8883,
      "step": 3910
    },
    {
      "epoch": 0.2474747474747475,
      "grad_norm": 0.39107823371887207,
      "learning_rate": 0.00019543158123269439,
      "loss": 0.7269,
      "step": 3920
    },
    {
      "epoch": 0.2481060606060606,
      "grad_norm": 0.4343271255493164,
      "learning_rate": 0.00019540098343613435,
      "loss": 0.6361,
      "step": 3930
    },
    {
      "epoch": 0.24873737373737373,
      "grad_norm": 0.4631684124469757,
      "learning_rate": 0.0001953702859256739,
      "loss": 0.5885,
      "step": 3940
    },
    {
      "epoch": 0.24936868686868688,
      "grad_norm": 0.6308715343475342,
      "learning_rate": 0.00019533948873339836,
      "loss": 0.5902,
      "step": 3950
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.3921145796775818,
      "learning_rate": 0.00019530859189149723,
      "loss": 0.8621,
      "step": 3960
    },
    {
      "epoch": 0.25063131313131315,
      "grad_norm": 0.40261584520339966,
      "learning_rate": 0.00019527759543226414,
      "loss": 0.7645,
      "step": 3970
    },
    {
      "epoch": 0.25126262626262624,
      "grad_norm": 0.43025997281074524,
      "learning_rate": 0.00019524649938809681,
      "loss": 0.6048,
      "step": 3980
    },
    {
      "epoch": 0.2518939393939394,
      "grad_norm": 0.4554912745952606,
      "learning_rate": 0.00019521530379149716,
      "loss": 0.5621,
      "step": 3990
    },
    {
      "epoch": 0.25252525252525254,
      "grad_norm": 0.668409526348114,
      "learning_rate": 0.00019518400867507102,
      "loss": 0.5792,
      "step": 4000
    },
    {
      "epoch": 0.25252525252525254,
      "eval_loss": 0.7124626040458679,
      "eval_runtime": 31.7758,
      "eval_samples_per_second": 80.565,
      "eval_steps_per_second": 10.071,
      "step": 4000
    },
    {
      "epoch": 0.25315656565656564,
      "grad_norm": 0.3763080835342407,
      "learning_rate": 0.0001951526140715283,
      "loss": 0.8892,
      "step": 4010
    },
    {
      "epoch": 0.2537878787878788,
      "grad_norm": 0.43125253915786743,
      "learning_rate": 0.00019512112001368297,
      "loss": 0.7174,
      "step": 4020
    },
    {
      "epoch": 0.25441919191919193,
      "grad_norm": 0.4303034543991089,
      "learning_rate": 0.0001950895265344528,
      "loss": 0.6471,
      "step": 4030
    },
    {
      "epoch": 0.255050505050505,
      "grad_norm": 0.41715168952941895,
      "learning_rate": 0.00019505783366685958,
      "loss": 0.6091,
      "step": 4040
    },
    {
      "epoch": 0.2556818181818182,
      "grad_norm": 1.1460380554199219,
      "learning_rate": 0.00019502604144402903,
      "loss": 0.5983,
      "step": 4050
    },
    {
      "epoch": 0.2563131313131313,
      "grad_norm": 0.37105029821395874,
      "learning_rate": 0.00019499414989919054,
      "loss": 0.8972,
      "step": 4060
    },
    {
      "epoch": 0.2569444444444444,
      "grad_norm": 0.40516555309295654,
      "learning_rate": 0.00019496215906567748,
      "loss": 0.7587,
      "step": 4070
    },
    {
      "epoch": 0.25757575757575757,
      "grad_norm": 0.4286646842956543,
      "learning_rate": 0.0001949300689769269,
      "loss": 0.6579,
      "step": 4080
    },
    {
      "epoch": 0.2582070707070707,
      "grad_norm": 0.4700028598308563,
      "learning_rate": 0.0001948978796664797,
      "loss": 0.5862,
      "step": 4090
    },
    {
      "epoch": 0.2588383838383838,
      "grad_norm": 0.8908843398094177,
      "learning_rate": 0.00019486559116798028,
      "loss": 0.6471,
      "step": 4100
    },
    {
      "epoch": 0.25946969696969696,
      "grad_norm": 0.3989599645137787,
      "learning_rate": 0.00019483320351517698,
      "loss": 0.8888,
      "step": 4110
    },
    {
      "epoch": 0.2601010101010101,
      "grad_norm": 0.4135248363018036,
      "learning_rate": 0.00019480071674192158,
      "loss": 0.7426,
      "step": 4120
    },
    {
      "epoch": 0.26073232323232326,
      "grad_norm": 0.4564741551876068,
      "learning_rate": 0.00019476813088216955,
      "loss": 0.6198,
      "step": 4130
    },
    {
      "epoch": 0.26136363636363635,
      "grad_norm": 0.4939608573913574,
      "learning_rate": 0.00019473544596997986,
      "loss": 0.5752,
      "step": 4140
    },
    {
      "epoch": 0.2619949494949495,
      "grad_norm": 0.708488941192627,
      "learning_rate": 0.0001947026620395151,
      "loss": 0.5999,
      "step": 4150
    },
    {
      "epoch": 0.26262626262626265,
      "grad_norm": 0.3820519745349884,
      "learning_rate": 0.00019466977912504127,
      "loss": 0.8321,
      "step": 4160
    },
    {
      "epoch": 0.26325757575757575,
      "grad_norm": 0.42977359890937805,
      "learning_rate": 0.00019463679726092791,
      "loss": 0.7091,
      "step": 4170
    },
    {
      "epoch": 0.2638888888888889,
      "grad_norm": 0.43725043535232544,
      "learning_rate": 0.0001946037164816479,
      "loss": 0.6435,
      "step": 4180
    },
    {
      "epoch": 0.26452020202020204,
      "grad_norm": 0.42156362533569336,
      "learning_rate": 0.00019457053682177754,
      "loss": 0.5717,
      "step": 4190
    },
    {
      "epoch": 0.26515151515151514,
      "grad_norm": 0.7050067782402039,
      "learning_rate": 0.00019453725831599652,
      "loss": 0.594,
      "step": 4200
    },
    {
      "epoch": 0.2657828282828283,
      "grad_norm": 0.4149351716041565,
      "learning_rate": 0.0001945038809990878,
      "loss": 0.8824,
      "step": 4210
    },
    {
      "epoch": 0.26641414141414144,
      "grad_norm": 0.4475421607494354,
      "learning_rate": 0.0001944704049059376,
      "loss": 0.7229,
      "step": 4220
    },
    {
      "epoch": 0.26704545454545453,
      "grad_norm": 0.4665261507034302,
      "learning_rate": 0.00019443683007153544,
      "loss": 0.6181,
      "step": 4230
    },
    {
      "epoch": 0.2676767676767677,
      "grad_norm": 0.4922696650028229,
      "learning_rate": 0.00019440315653097398,
      "loss": 0.5771,
      "step": 4240
    },
    {
      "epoch": 0.26830808080808083,
      "grad_norm": 0.7416155338287354,
      "learning_rate": 0.00019436938431944916,
      "loss": 0.5964,
      "step": 4250
    },
    {
      "epoch": 0.2689393939393939,
      "grad_norm": 0.4037087857723236,
      "learning_rate": 0.0001943355134722599,
      "loss": 0.8433,
      "step": 4260
    },
    {
      "epoch": 0.26957070707070707,
      "grad_norm": 0.4217888116836548,
      "learning_rate": 0.00019430154402480832,
      "loss": 0.7135,
      "step": 4270
    },
    {
      "epoch": 0.2702020202020202,
      "grad_norm": 0.4513973593711853,
      "learning_rate": 0.0001942674760125996,
      "loss": 0.6724,
      "step": 4280
    },
    {
      "epoch": 0.2708333333333333,
      "grad_norm": 0.49336615204811096,
      "learning_rate": 0.00019423330947124183,
      "loss": 0.5766,
      "step": 4290
    },
    {
      "epoch": 0.27146464646464646,
      "grad_norm": 0.8134427070617676,
      "learning_rate": 0.00019419904443644624,
      "loss": 0.6166,
      "step": 4300
    },
    {
      "epoch": 0.2720959595959596,
      "grad_norm": 0.36438074707984924,
      "learning_rate": 0.00019416468094402687,
      "loss": 0.855,
      "step": 4310
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.4420095384120941,
      "learning_rate": 0.00019413021902990078,
      "loss": 0.7312,
      "step": 4320
    },
    {
      "epoch": 0.27335858585858586,
      "grad_norm": 0.46921032667160034,
      "learning_rate": 0.00019409565873008782,
      "loss": 0.6525,
      "step": 4330
    },
    {
      "epoch": 0.273989898989899,
      "grad_norm": 0.43768244981765747,
      "learning_rate": 0.0001940610000807107,
      "loss": 0.5713,
      "step": 4340
    },
    {
      "epoch": 0.2746212121212121,
      "grad_norm": 0.8144502639770508,
      "learning_rate": 0.00019402624311799495,
      "loss": 0.5951,
      "step": 4350
    },
    {
      "epoch": 0.27525252525252525,
      "grad_norm": 0.3678363561630249,
      "learning_rate": 0.00019399138787826883,
      "loss": 0.8626,
      "step": 4360
    },
    {
      "epoch": 0.2758838383838384,
      "grad_norm": 0.41204315423965454,
      "learning_rate": 0.0001939564343979633,
      "loss": 0.7156,
      "step": 4370
    },
    {
      "epoch": 0.2765151515151515,
      "grad_norm": 0.4560472071170807,
      "learning_rate": 0.00019392138271361205,
      "loss": 0.6288,
      "step": 4380
    },
    {
      "epoch": 0.27714646464646464,
      "grad_norm": 0.508629560470581,
      "learning_rate": 0.00019388623286185138,
      "loss": 0.5965,
      "step": 4390
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.7561363577842712,
      "learning_rate": 0.00019385098487942023,
      "loss": 0.5776,
      "step": 4400
    },
    {
      "epoch": 0.2784090909090909,
      "grad_norm": 0.379712849855423,
      "learning_rate": 0.00019381563880316004,
      "loss": 0.9072,
      "step": 4410
    },
    {
      "epoch": 0.27904040404040403,
      "grad_norm": 0.40771400928497314,
      "learning_rate": 0.0001937801946700149,
      "loss": 0.7192,
      "step": 4420
    },
    {
      "epoch": 0.2796717171717172,
      "grad_norm": 0.43334996700286865,
      "learning_rate": 0.00019374465251703122,
      "loss": 0.6591,
      "step": 4430
    },
    {
      "epoch": 0.2803030303030303,
      "grad_norm": 0.4576507806777954,
      "learning_rate": 0.00019370901238135804,
      "loss": 0.6104,
      "step": 4440
    },
    {
      "epoch": 0.2809343434343434,
      "grad_norm": 0.7821081280708313,
      "learning_rate": 0.00019367327430024663,
      "loss": 0.5991,
      "step": 4450
    },
    {
      "epoch": 0.2815656565656566,
      "grad_norm": 0.3872219920158386,
      "learning_rate": 0.00019363743831105081,
      "loss": 0.8466,
      "step": 4460
    },
    {
      "epoch": 0.2821969696969697,
      "grad_norm": 0.4098997712135315,
      "learning_rate": 0.00019360150445122665,
      "loss": 0.7246,
      "step": 4470
    },
    {
      "epoch": 0.2828282828282828,
      "grad_norm": 0.44836002588272095,
      "learning_rate": 0.0001935654727583325,
      "loss": 0.6619,
      "step": 4480
    },
    {
      "epoch": 0.28345959595959597,
      "grad_norm": 0.45767542719841003,
      "learning_rate": 0.00019352934327002892,
      "loss": 0.554,
      "step": 4490
    },
    {
      "epoch": 0.2840909090909091,
      "grad_norm": 0.7291249632835388,
      "learning_rate": 0.00019349311602407884,
      "loss": 0.59,
      "step": 4500
    },
    {
      "epoch": 0.2847222222222222,
      "grad_norm": 0.39635372161865234,
      "learning_rate": 0.00019345679105834727,
      "loss": 0.8195,
      "step": 4510
    },
    {
      "epoch": 0.28535353535353536,
      "grad_norm": 0.41782698035240173,
      "learning_rate": 0.00019342036841080132,
      "loss": 0.7043,
      "step": 4520
    },
    {
      "epoch": 0.2859848484848485,
      "grad_norm": 0.46771711111068726,
      "learning_rate": 0.00019338384811951027,
      "loss": 0.6293,
      "step": 4530
    },
    {
      "epoch": 0.2866161616161616,
      "grad_norm": 0.4600197970867157,
      "learning_rate": 0.00019334723022264544,
      "loss": 0.5991,
      "step": 4540
    },
    {
      "epoch": 0.28724747474747475,
      "grad_norm": 0.660200834274292,
      "learning_rate": 0.00019331051475848018,
      "loss": 0.5975,
      "step": 4550
    },
    {
      "epoch": 0.2878787878787879,
      "grad_norm": 0.3880232274532318,
      "learning_rate": 0.0001932737017653897,
      "loss": 0.8572,
      "step": 4560
    },
    {
      "epoch": 0.288510101010101,
      "grad_norm": 0.41201967000961304,
      "learning_rate": 0.00019323679128185135,
      "loss": 0.7109,
      "step": 4570
    },
    {
      "epoch": 0.28914141414141414,
      "grad_norm": 0.41881483793258667,
      "learning_rate": 0.00019319978334644426,
      "loss": 0.6209,
      "step": 4580
    },
    {
      "epoch": 0.2897727272727273,
      "grad_norm": 0.4854350686073303,
      "learning_rate": 0.00019316267799784938,
      "loss": 0.5985,
      "step": 4590
    },
    {
      "epoch": 0.2904040404040404,
      "grad_norm": 0.7886999249458313,
      "learning_rate": 0.00019312547527484958,
      "loss": 0.6084,
      "step": 4600
    },
    {
      "epoch": 0.29103535353535354,
      "grad_norm": 0.4070093631744385,
      "learning_rate": 0.00019308817521632943,
      "loss": 0.8474,
      "step": 4610
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.39107125997543335,
      "learning_rate": 0.00019305077786127526,
      "loss": 0.7161,
      "step": 4620
    },
    {
      "epoch": 0.2922979797979798,
      "grad_norm": 0.4793693721294403,
      "learning_rate": 0.00019301328324877512,
      "loss": 0.6161,
      "step": 4630
    },
    {
      "epoch": 0.29292929292929293,
      "grad_norm": 0.5095090866088867,
      "learning_rate": 0.00019297569141801867,
      "loss": 0.5707,
      "step": 4640
    },
    {
      "epoch": 0.2935606060606061,
      "grad_norm": 0.7813356518745422,
      "learning_rate": 0.00019293800240829717,
      "loss": 0.5977,
      "step": 4650
    },
    {
      "epoch": 0.29419191919191917,
      "grad_norm": 0.43374884128570557,
      "learning_rate": 0.00019290021625900354,
      "loss": 0.9029,
      "step": 4660
    },
    {
      "epoch": 0.2948232323232323,
      "grad_norm": 0.4343441426753998,
      "learning_rate": 0.00019286233300963218,
      "loss": 0.7365,
      "step": 4670
    },
    {
      "epoch": 0.29545454545454547,
      "grad_norm": 0.40186938643455505,
      "learning_rate": 0.00019282435269977894,
      "loss": 0.6337,
      "step": 4680
    },
    {
      "epoch": 0.29608585858585856,
      "grad_norm": 0.459442675113678,
      "learning_rate": 0.00019278627536914117,
      "loss": 0.5307,
      "step": 4690
    },
    {
      "epoch": 0.2967171717171717,
      "grad_norm": 0.7740719318389893,
      "learning_rate": 0.00019274810105751762,
      "loss": 0.5757,
      "step": 4700
    },
    {
      "epoch": 0.29734848484848486,
      "grad_norm": 0.3968995213508606,
      "learning_rate": 0.0001927098298048084,
      "loss": 0.8322,
      "step": 4710
    },
    {
      "epoch": 0.29797979797979796,
      "grad_norm": 0.4420049786567688,
      "learning_rate": 0.00019267146165101491,
      "loss": 0.7224,
      "step": 4720
    },
    {
      "epoch": 0.2986111111111111,
      "grad_norm": 0.45030999183654785,
      "learning_rate": 0.0001926329966362399,
      "loss": 0.6322,
      "step": 4730
    },
    {
      "epoch": 0.29924242424242425,
      "grad_norm": 0.4827921390533447,
      "learning_rate": 0.0001925944348006873,
      "loss": 0.5877,
      "step": 4740
    },
    {
      "epoch": 0.29987373737373735,
      "grad_norm": 0.7837852239608765,
      "learning_rate": 0.00019255577618466227,
      "loss": 0.6206,
      "step": 4750
    },
    {
      "epoch": 0.3005050505050505,
      "grad_norm": 0.3987533450126648,
      "learning_rate": 0.0001925170208285711,
      "loss": 0.8666,
      "step": 4760
    },
    {
      "epoch": 0.30113636363636365,
      "grad_norm": 0.4245210587978363,
      "learning_rate": 0.00019247816877292125,
      "loss": 0.6986,
      "step": 4770
    },
    {
      "epoch": 0.30176767676767674,
      "grad_norm": 0.42239415645599365,
      "learning_rate": 0.0001924392200583212,
      "loss": 0.6224,
      "step": 4780
    },
    {
      "epoch": 0.3023989898989899,
      "grad_norm": 0.48009228706359863,
      "learning_rate": 0.00019240017472548044,
      "loss": 0.595,
      "step": 4790
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.7303432822227478,
      "learning_rate": 0.0001923610328152095,
      "loss": 0.589,
      "step": 4800
    },
    {
      "epoch": 0.3036616161616162,
      "grad_norm": 0.375401109457016,
      "learning_rate": 0.00019232179436841983,
      "loss": 0.8858,
      "step": 4810
    },
    {
      "epoch": 0.3042929292929293,
      "grad_norm": 0.42694878578186035,
      "learning_rate": 0.00019228245942612374,
      "loss": 0.6841,
      "step": 4820
    },
    {
      "epoch": 0.30492424242424243,
      "grad_norm": 0.4349174201488495,
      "learning_rate": 0.0001922430280294345,
      "loss": 0.6708,
      "step": 4830
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 0.45744362473487854,
      "learning_rate": 0.0001922035002195661,
      "loss": 0.5617,
      "step": 4840
    },
    {
      "epoch": 0.3061868686868687,
      "grad_norm": 0.712290346622467,
      "learning_rate": 0.00019216387603783334,
      "loss": 0.5842,
      "step": 4850
    },
    {
      "epoch": 0.3068181818181818,
      "grad_norm": 0.4064180254936218,
      "learning_rate": 0.00019212415552565174,
      "loss": 0.826,
      "step": 4860
    },
    {
      "epoch": 0.307449494949495,
      "grad_norm": 0.4075683057308197,
      "learning_rate": 0.00019208433872453754,
      "loss": 0.7143,
      "step": 4870
    },
    {
      "epoch": 0.30808080808080807,
      "grad_norm": 0.4114907383918762,
      "learning_rate": 0.00019204442567610756,
      "loss": 0.6241,
      "step": 4880
    },
    {
      "epoch": 0.3087121212121212,
      "grad_norm": 0.47960081696510315,
      "learning_rate": 0.00019200441642207923,
      "loss": 0.5906,
      "step": 4890
    },
    {
      "epoch": 0.30934343434343436,
      "grad_norm": 0.74140864610672,
      "learning_rate": 0.0001919643110042706,
      "loss": 0.576,
      "step": 4900
    },
    {
      "epoch": 0.30997474747474746,
      "grad_norm": 0.37656939029693604,
      "learning_rate": 0.00019192410946460015,
      "loss": 0.887,
      "step": 4910
    },
    {
      "epoch": 0.3106060606060606,
      "grad_norm": 0.4419526755809784,
      "learning_rate": 0.00019188381184508688,
      "loss": 0.7265,
      "step": 4920
    },
    {
      "epoch": 0.31123737373737376,
      "grad_norm": 0.45836278796195984,
      "learning_rate": 0.0001918434181878502,
      "loss": 0.6394,
      "step": 4930
    },
    {
      "epoch": 0.31186868686868685,
      "grad_norm": 0.43972936272621155,
      "learning_rate": 0.00019180292853510992,
      "loss": 0.5618,
      "step": 4940
    },
    {
      "epoch": 0.3125,
      "grad_norm": 0.6951815485954285,
      "learning_rate": 0.00019176234292918608,
      "loss": 0.6106,
      "step": 4950
    },
    {
      "epoch": 0.31313131313131315,
      "grad_norm": 0.4030090272426605,
      "learning_rate": 0.00019172166141249915,
      "loss": 0.8209,
      "step": 4960
    },
    {
      "epoch": 0.31376262626262624,
      "grad_norm": 0.434592604637146,
      "learning_rate": 0.00019168088402756985,
      "loss": 0.7121,
      "step": 4970
    },
    {
      "epoch": 0.3143939393939394,
      "grad_norm": 0.4368414282798767,
      "learning_rate": 0.0001916400108170189,
      "loss": 0.6502,
      "step": 4980
    },
    {
      "epoch": 0.31502525252525254,
      "grad_norm": 0.4942951202392578,
      "learning_rate": 0.00019159904182356746,
      "loss": 0.5741,
      "step": 4990
    },
    {
      "epoch": 0.31565656565656564,
      "grad_norm": 0.705095112323761,
      "learning_rate": 0.00019155797709003656,
      "loss": 0.5893,
      "step": 5000
    },
    {
      "epoch": 0.31565656565656564,
      "eval_loss": 0.7016334533691406,
      "eval_runtime": 31.7884,
      "eval_samples_per_second": 80.533,
      "eval_steps_per_second": 10.067,
      "step": 5000
    },
    {
      "epoch": 0.3162878787878788,
      "grad_norm": 0.3922106921672821,
      "learning_rate": 0.00019151681665934746,
      "loss": 0.8636,
      "step": 5010
    },
    {
      "epoch": 0.31691919191919193,
      "grad_norm": 0.4364156126976013,
      "learning_rate": 0.00019147556057452135,
      "loss": 0.7391,
      "step": 5020
    },
    {
      "epoch": 0.317550505050505,
      "grad_norm": 0.42145392298698425,
      "learning_rate": 0.00019143420887867945,
      "loss": 0.6338,
      "step": 5030
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 0.46027040481567383,
      "learning_rate": 0.00019139276161504287,
      "loss": 0.5846,
      "step": 5040
    },
    {
      "epoch": 0.3188131313131313,
      "grad_norm": 0.8381472229957581,
      "learning_rate": 0.00019135121882693268,
      "loss": 0.5885,
      "step": 5050
    },
    {
      "epoch": 0.3194444444444444,
      "grad_norm": 0.3917619287967682,
      "learning_rate": 0.00019130958055776969,
      "loss": 0.8454,
      "step": 5060
    },
    {
      "epoch": 0.32007575757575757,
      "grad_norm": 0.44352665543556213,
      "learning_rate": 0.00019126784685107463,
      "loss": 0.7281,
      "step": 5070
    },
    {
      "epoch": 0.3207070707070707,
      "grad_norm": 0.4034019112586975,
      "learning_rate": 0.00019122601775046789,
      "loss": 0.6431,
      "step": 5080
    },
    {
      "epoch": 0.3213383838383838,
      "grad_norm": 0.5232113003730774,
      "learning_rate": 0.00019118409329966956,
      "loss": 0.6028,
      "step": 5090
    },
    {
      "epoch": 0.32196969696969696,
      "grad_norm": 0.7263709902763367,
      "learning_rate": 0.00019114207354249948,
      "loss": 0.5836,
      "step": 5100
    },
    {
      "epoch": 0.3226010101010101,
      "grad_norm": 0.39116930961608887,
      "learning_rate": 0.00019109995852287698,
      "loss": 0.921,
      "step": 5110
    },
    {
      "epoch": 0.32323232323232326,
      "grad_norm": 0.45483240485191345,
      "learning_rate": 0.0001910577482848211,
      "loss": 0.7104,
      "step": 5120
    },
    {
      "epoch": 0.32386363636363635,
      "grad_norm": 0.40207910537719727,
      "learning_rate": 0.0001910154428724503,
      "loss": 0.6251,
      "step": 5130
    },
    {
      "epoch": 0.3244949494949495,
      "grad_norm": 0.48293283581733704,
      "learning_rate": 0.00019097304232998255,
      "loss": 0.5265,
      "step": 5140
    },
    {
      "epoch": 0.32512626262626265,
      "grad_norm": 0.640326201915741,
      "learning_rate": 0.00019093054670173522,
      "loss": 0.6172,
      "step": 5150
    },
    {
      "epoch": 0.32575757575757575,
      "grad_norm": 0.4201091229915619,
      "learning_rate": 0.00019088795603212517,
      "loss": 0.8379,
      "step": 5160
    },
    {
      "epoch": 0.3263888888888889,
      "grad_norm": 0.41733479499816895,
      "learning_rate": 0.00019084527036566847,
      "loss": 0.705,
      "step": 5170
    },
    {
      "epoch": 0.32702020202020204,
      "grad_norm": 0.4683533012866974,
      "learning_rate": 0.0001908024897469805,
      "loss": 0.6091,
      "step": 5180
    },
    {
      "epoch": 0.32765151515151514,
      "grad_norm": 0.4580214023590088,
      "learning_rate": 0.00019075961422077597,
      "loss": 0.5618,
      "step": 5190
    },
    {
      "epoch": 0.3282828282828283,
      "grad_norm": 0.7621403336524963,
      "learning_rate": 0.00019071664383186874,
      "loss": 0.6423,
      "step": 5200
    },
    {
      "epoch": 0.32891414141414144,
      "grad_norm": 0.3755031228065491,
      "learning_rate": 0.00019067357862517177,
      "loss": 0.9035,
      "step": 5210
    },
    {
      "epoch": 0.32954545454545453,
      "grad_norm": 0.38255593180656433,
      "learning_rate": 0.00019063041864569722,
      "loss": 0.7034,
      "step": 5220
    },
    {
      "epoch": 0.3301767676767677,
      "grad_norm": 0.40790966153144836,
      "learning_rate": 0.00019058716393855624,
      "loss": 0.6252,
      "step": 5230
    },
    {
      "epoch": 0.33080808080808083,
      "grad_norm": 0.4941380023956299,
      "learning_rate": 0.000190543814548959,
      "loss": 0.5486,
      "step": 5240
    },
    {
      "epoch": 0.3314393939393939,
      "grad_norm": 0.7739269137382507,
      "learning_rate": 0.00019050037052221463,
      "loss": 0.6138,
      "step": 5250
    },
    {
      "epoch": 0.33207070707070707,
      "grad_norm": 0.3894596993923187,
      "learning_rate": 0.00019045683190373124,
      "loss": 0.8705,
      "step": 5260
    },
    {
      "epoch": 0.3327020202020202,
      "grad_norm": 0.4522784650325775,
      "learning_rate": 0.00019041319873901573,
      "loss": 0.7267,
      "step": 5270
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.46348926424980164,
      "learning_rate": 0.00019036947107367382,
      "loss": 0.6483,
      "step": 5280
    },
    {
      "epoch": 0.33396464646464646,
      "grad_norm": 0.4732966125011444,
      "learning_rate": 0.00019032564895341006,
      "loss": 0.5641,
      "step": 5290
    },
    {
      "epoch": 0.3345959595959596,
      "grad_norm": 0.821956217288971,
      "learning_rate": 0.00019028173242402767,
      "loss": 0.6446,
      "step": 5300
    },
    {
      "epoch": 0.3352272727272727,
      "grad_norm": 0.4194863438606262,
      "learning_rate": 0.0001902377215314286,
      "loss": 0.8457,
      "step": 5310
    },
    {
      "epoch": 0.33585858585858586,
      "grad_norm": 0.38867178559303284,
      "learning_rate": 0.00019019361632161336,
      "loss": 0.7059,
      "step": 5320
    },
    {
      "epoch": 0.336489898989899,
      "grad_norm": 0.44815951585769653,
      "learning_rate": 0.00019014941684068114,
      "loss": 0.6362,
      "step": 5330
    },
    {
      "epoch": 0.3371212121212121,
      "grad_norm": 0.5004690289497375,
      "learning_rate": 0.00019010512313482955,
      "loss": 0.5663,
      "step": 5340
    },
    {
      "epoch": 0.33775252525252525,
      "grad_norm": 0.7502602338790894,
      "learning_rate": 0.00019006073525035477,
      "loss": 0.5962,
      "step": 5350
    },
    {
      "epoch": 0.3383838383838384,
      "grad_norm": 0.38762354850769043,
      "learning_rate": 0.0001900162532336514,
      "loss": 0.8331,
      "step": 5360
    },
    {
      "epoch": 0.3390151515151515,
      "grad_norm": 0.4460367262363434,
      "learning_rate": 0.00018997167713121236,
      "loss": 0.7302,
      "step": 5370
    },
    {
      "epoch": 0.33964646464646464,
      "grad_norm": 0.48357266187667847,
      "learning_rate": 0.000189927006989629,
      "loss": 0.6546,
      "step": 5380
    },
    {
      "epoch": 0.3402777777777778,
      "grad_norm": 0.4818354845046997,
      "learning_rate": 0.0001898822428555909,
      "loss": 0.5532,
      "step": 5390
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 0.6720193028450012,
      "learning_rate": 0.00018983738477588595,
      "loss": 0.5682,
      "step": 5400
    },
    {
      "epoch": 0.34154040404040403,
      "grad_norm": 0.4001424014568329,
      "learning_rate": 0.00018979243279740015,
      "loss": 0.8826,
      "step": 5410
    },
    {
      "epoch": 0.3421717171717172,
      "grad_norm": 0.42666366696357727,
      "learning_rate": 0.00018974738696711768,
      "loss": 0.7348,
      "step": 5420
    },
    {
      "epoch": 0.3428030303030303,
      "grad_norm": 0.4353691637516022,
      "learning_rate": 0.00018970224733212083,
      "loss": 0.6426,
      "step": 5430
    },
    {
      "epoch": 0.3434343434343434,
      "grad_norm": 0.46315157413482666,
      "learning_rate": 0.0001896570139395899,
      "loss": 0.5574,
      "step": 5440
    },
    {
      "epoch": 0.3440656565656566,
      "grad_norm": 0.7315667271614075,
      "learning_rate": 0.00018961168683680326,
      "loss": 0.5898,
      "step": 5450
    },
    {
      "epoch": 0.3446969696969697,
      "grad_norm": 0.44556403160095215,
      "learning_rate": 0.0001895662660711371,
      "loss": 0.8603,
      "step": 5460
    },
    {
      "epoch": 0.3453282828282828,
      "grad_norm": 0.44676002860069275,
      "learning_rate": 0.00018952075169006568,
      "loss": 0.6991,
      "step": 5470
    },
    {
      "epoch": 0.34595959595959597,
      "grad_norm": 0.42868638038635254,
      "learning_rate": 0.00018947514374116089,
      "loss": 0.6352,
      "step": 5480
    },
    {
      "epoch": 0.3465909090909091,
      "grad_norm": 0.479626327753067,
      "learning_rate": 0.00018942944227209264,
      "loss": 0.5819,
      "step": 5490
    },
    {
      "epoch": 0.3472222222222222,
      "grad_norm": 0.734047532081604,
      "learning_rate": 0.0001893836473306284,
      "loss": 0.5972,
      "step": 5500
    },
    {
      "epoch": 0.34785353535353536,
      "grad_norm": 0.407537043094635,
      "learning_rate": 0.00018933775896463347,
      "loss": 0.8954,
      "step": 5510
    },
    {
      "epoch": 0.3484848484848485,
      "grad_norm": 0.36007267236709595,
      "learning_rate": 0.00018929177722207076,
      "loss": 0.7189,
      "step": 5520
    },
    {
      "epoch": 0.3491161616161616,
      "grad_norm": 0.4329727590084076,
      "learning_rate": 0.00018924570215100075,
      "loss": 0.6971,
      "step": 5530
    },
    {
      "epoch": 0.34974747474747475,
      "grad_norm": 0.45185381174087524,
      "learning_rate": 0.0001891995337995815,
      "loss": 0.553,
      "step": 5540
    },
    {
      "epoch": 0.3503787878787879,
      "grad_norm": 0.7208488583564758,
      "learning_rate": 0.00018915327221606854,
      "loss": 0.579,
      "step": 5550
    },
    {
      "epoch": 0.351010101010101,
      "grad_norm": 0.3870282769203186,
      "learning_rate": 0.0001891069174488149,
      "loss": 0.8338,
      "step": 5560
    },
    {
      "epoch": 0.35164141414141414,
      "grad_norm": 0.41266173124313354,
      "learning_rate": 0.0001890604695462709,
      "loss": 0.7299,
      "step": 5570
    },
    {
      "epoch": 0.3522727272727273,
      "grad_norm": 0.42223578691482544,
      "learning_rate": 0.0001890139285569843,
      "loss": 0.6555,
      "step": 5580
    },
    {
      "epoch": 0.3529040404040404,
      "grad_norm": 0.4630308747291565,
      "learning_rate": 0.00018896729452960015,
      "loss": 0.5705,
      "step": 5590
    },
    {
      "epoch": 0.35353535353535354,
      "grad_norm": 0.8280799984931946,
      "learning_rate": 0.00018892056751286073,
      "loss": 0.5944,
      "step": 5600
    },
    {
      "epoch": 0.3541666666666667,
      "grad_norm": 0.3932120203971863,
      "learning_rate": 0.0001888737475556055,
      "loss": 0.8898,
      "step": 5610
    },
    {
      "epoch": 0.3547979797979798,
      "grad_norm": 0.38855764269828796,
      "learning_rate": 0.00018882683470677103,
      "loss": 0.7053,
      "step": 5620
    },
    {
      "epoch": 0.35542929292929293,
      "grad_norm": 0.45254337787628174,
      "learning_rate": 0.00018877982901539103,
      "loss": 0.6493,
      "step": 5630
    },
    {
      "epoch": 0.3560606060606061,
      "grad_norm": 0.4756447970867157,
      "learning_rate": 0.00018873273053059627,
      "loss": 0.5641,
      "step": 5640
    },
    {
      "epoch": 0.35669191919191917,
      "grad_norm": 0.8279054164886475,
      "learning_rate": 0.00018868553930161447,
      "loss": 0.5962,
      "step": 5650
    },
    {
      "epoch": 0.3573232323232323,
      "grad_norm": 0.38219213485717773,
      "learning_rate": 0.00018863825537777026,
      "loss": 0.8715,
      "step": 5660
    },
    {
      "epoch": 0.35795454545454547,
      "grad_norm": 0.4081600606441498,
      "learning_rate": 0.00018859087880848525,
      "loss": 0.7248,
      "step": 5670
    },
    {
      "epoch": 0.35858585858585856,
      "grad_norm": 0.4770353436470032,
      "learning_rate": 0.0001885434096432778,
      "loss": 0.6104,
      "step": 5680
    },
    {
      "epoch": 0.3592171717171717,
      "grad_norm": 0.47338932752609253,
      "learning_rate": 0.00018849584793176303,
      "loss": 0.5714,
      "step": 5690
    },
    {
      "epoch": 0.35984848484848486,
      "grad_norm": 0.727020263671875,
      "learning_rate": 0.00018844819372365286,
      "loss": 0.6242,
      "step": 5700
    },
    {
      "epoch": 0.36047979797979796,
      "grad_norm": 0.3860824704170227,
      "learning_rate": 0.0001884004470687559,
      "loss": 0.8094,
      "step": 5710
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 0.40516528487205505,
      "learning_rate": 0.00018835260801697734,
      "loss": 0.7252,
      "step": 5720
    },
    {
      "epoch": 0.36174242424242425,
      "grad_norm": 0.4752071797847748,
      "learning_rate": 0.00018830467661831891,
      "loss": 0.6236,
      "step": 5730
    },
    {
      "epoch": 0.36237373737373735,
      "grad_norm": 0.4662185609340668,
      "learning_rate": 0.00018825665292287894,
      "loss": 0.5547,
      "step": 5740
    },
    {
      "epoch": 0.3630050505050505,
      "grad_norm": 0.7058114409446716,
      "learning_rate": 0.0001882085369808522,
      "loss": 0.5895,
      "step": 5750
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.38776516914367676,
      "learning_rate": 0.00018816032884252988,
      "loss": 0.8814,
      "step": 5760
    },
    {
      "epoch": 0.36426767676767674,
      "grad_norm": 0.43628746271133423,
      "learning_rate": 0.0001881120285582995,
      "loss": 0.7617,
      "step": 5770
    },
    {
      "epoch": 0.3648989898989899,
      "grad_norm": 0.47471320629119873,
      "learning_rate": 0.00018806363617864493,
      "loss": 0.6484,
      "step": 5780
    },
    {
      "epoch": 0.36553030303030304,
      "grad_norm": 0.4608530104160309,
      "learning_rate": 0.00018801515175414629,
      "loss": 0.592,
      "step": 5790
    },
    {
      "epoch": 0.3661616161616162,
      "grad_norm": 0.6718423366546631,
      "learning_rate": 0.00018796657533547988,
      "loss": 0.5827,
      "step": 5800
    },
    {
      "epoch": 0.3667929292929293,
      "grad_norm": 0.40820831060409546,
      "learning_rate": 0.0001879179069734182,
      "loss": 0.8499,
      "step": 5810
    },
    {
      "epoch": 0.36742424242424243,
      "grad_norm": 0.40420350432395935,
      "learning_rate": 0.00018786914671882983,
      "loss": 0.7137,
      "step": 5820
    },
    {
      "epoch": 0.3680555555555556,
      "grad_norm": 0.4687645137310028,
      "learning_rate": 0.0001878202946226794,
      "loss": 0.6636,
      "step": 5830
    },
    {
      "epoch": 0.3686868686868687,
      "grad_norm": 0.47091683745384216,
      "learning_rate": 0.00018777135073602748,
      "loss": 0.601,
      "step": 5840
    },
    {
      "epoch": 0.3693181818181818,
      "grad_norm": 0.7796312570571899,
      "learning_rate": 0.00018772231511003068,
      "loss": 0.5673,
      "step": 5850
    },
    {
      "epoch": 0.369949494949495,
      "grad_norm": 0.3924722373485565,
      "learning_rate": 0.0001876731877959414,
      "loss": 0.867,
      "step": 5860
    },
    {
      "epoch": 0.37058080808080807,
      "grad_norm": 0.4304756820201874,
      "learning_rate": 0.00018762396884510797,
      "loss": 0.7171,
      "step": 5870
    },
    {
      "epoch": 0.3712121212121212,
      "grad_norm": 0.4799822270870209,
      "learning_rate": 0.0001875746583089744,
      "loss": 0.6458,
      "step": 5880
    },
    {
      "epoch": 0.37184343434343436,
      "grad_norm": 0.4810762107372284,
      "learning_rate": 0.0001875252562390805,
      "loss": 0.5481,
      "step": 5890
    },
    {
      "epoch": 0.37247474747474746,
      "grad_norm": 0.6989350318908691,
      "learning_rate": 0.00018747576268706172,
      "loss": 0.5917,
      "step": 5900
    },
    {
      "epoch": 0.3731060606060606,
      "grad_norm": 0.41079601645469666,
      "learning_rate": 0.0001874261777046491,
      "loss": 0.9389,
      "step": 5910
    },
    {
      "epoch": 0.37373737373737376,
      "grad_norm": 0.3961688280105591,
      "learning_rate": 0.00018737650134366927,
      "loss": 0.7333,
      "step": 5920
    },
    {
      "epoch": 0.37436868686868685,
      "grad_norm": 0.4297580122947693,
      "learning_rate": 0.00018732673365604447,
      "loss": 0.6184,
      "step": 5930
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.4522932171821594,
      "learning_rate": 0.0001872768746937922,
      "loss": 0.5812,
      "step": 5940
    },
    {
      "epoch": 0.37563131313131315,
      "grad_norm": 0.7187079787254333,
      "learning_rate": 0.00018722692450902551,
      "loss": 0.6091,
      "step": 5950
    },
    {
      "epoch": 0.37626262626262624,
      "grad_norm": 0.4044954180717468,
      "learning_rate": 0.0001871768831539527,
      "loss": 0.7955,
      "step": 5960
    },
    {
      "epoch": 0.3768939393939394,
      "grad_norm": 0.41925230622291565,
      "learning_rate": 0.00018712675068087746,
      "loss": 0.7243,
      "step": 5970
    },
    {
      "epoch": 0.37752525252525254,
      "grad_norm": 0.4172728359699249,
      "learning_rate": 0.00018707652714219868,
      "loss": 0.6053,
      "step": 5980
    },
    {
      "epoch": 0.37815656565656564,
      "grad_norm": 0.4754853844642639,
      "learning_rate": 0.00018702621259041036,
      "loss": 0.5659,
      "step": 5990
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 0.7594875693321228,
      "learning_rate": 0.00018697580707810173,
      "loss": 0.5805,
      "step": 6000
    },
    {
      "epoch": 0.3787878787878788,
      "eval_loss": 0.6978554725646973,
      "eval_runtime": 31.6373,
      "eval_samples_per_second": 80.917,
      "eval_steps_per_second": 10.115,
      "step": 6000
    },
    {
      "epoch": 0.37941919191919193,
      "grad_norm": 0.4188121557235718,
      "learning_rate": 0.00018692531065795702,
      "loss": 0.8347,
      "step": 6010
    },
    {
      "epoch": 0.380050505050505,
      "grad_norm": 0.415535032749176,
      "learning_rate": 0.00018687472338275557,
      "loss": 0.7205,
      "step": 6020
    },
    {
      "epoch": 0.3806818181818182,
      "grad_norm": 0.42297181487083435,
      "learning_rate": 0.00018682404530537155,
      "loss": 0.6142,
      "step": 6030
    },
    {
      "epoch": 0.3813131313131313,
      "grad_norm": 0.4627338945865631,
      "learning_rate": 0.00018677327647877412,
      "loss": 0.5696,
      "step": 6040
    },
    {
      "epoch": 0.3819444444444444,
      "grad_norm": 0.7780526280403137,
      "learning_rate": 0.00018672241695602733,
      "loss": 0.5995,
      "step": 6050
    },
    {
      "epoch": 0.38257575757575757,
      "grad_norm": 0.40693196654319763,
      "learning_rate": 0.0001866714667902899,
      "loss": 0.828,
      "step": 6060
    },
    {
      "epoch": 0.3832070707070707,
      "grad_norm": 0.3998897075653076,
      "learning_rate": 0.00018662042603481542,
      "loss": 0.731,
      "step": 6070
    },
    {
      "epoch": 0.3838383838383838,
      "grad_norm": 0.43422579765319824,
      "learning_rate": 0.00018656929474295209,
      "loss": 0.61,
      "step": 6080
    },
    {
      "epoch": 0.38446969696969696,
      "grad_norm": 0.50617516040802,
      "learning_rate": 0.00018651807296814278,
      "loss": 0.5756,
      "step": 6090
    },
    {
      "epoch": 0.3851010101010101,
      "grad_norm": 0.7707224488258362,
      "learning_rate": 0.0001864667607639249,
      "loss": 0.5997,
      "step": 6100
    },
    {
      "epoch": 0.38573232323232326,
      "grad_norm": 0.41991597414016724,
      "learning_rate": 0.0001864153581839304,
      "loss": 0.8781,
      "step": 6110
    },
    {
      "epoch": 0.38636363636363635,
      "grad_norm": 0.3813832402229309,
      "learning_rate": 0.00018636386528188568,
      "loss": 0.7026,
      "step": 6120
    },
    {
      "epoch": 0.3869949494949495,
      "grad_norm": 0.43615585565567017,
      "learning_rate": 0.00018631228211161152,
      "loss": 0.6333,
      "step": 6130
    },
    {
      "epoch": 0.38762626262626265,
      "grad_norm": 0.4383907914161682,
      "learning_rate": 0.00018626060872702313,
      "loss": 0.521,
      "step": 6140
    },
    {
      "epoch": 0.38825757575757575,
      "grad_norm": 0.9424976706504822,
      "learning_rate": 0.00018620884518212995,
      "loss": 0.6056,
      "step": 6150
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.3848761022090912,
      "learning_rate": 0.00018615699153103562,
      "loss": 0.8809,
      "step": 6160
    },
    {
      "epoch": 0.38952020202020204,
      "grad_norm": 0.4744691252708435,
      "learning_rate": 0.00018610504782793808,
      "loss": 0.7213,
      "step": 6170
    },
    {
      "epoch": 0.39015151515151514,
      "grad_norm": 0.44951802492141724,
      "learning_rate": 0.00018605301412712922,
      "loss": 0.6121,
      "step": 6180
    },
    {
      "epoch": 0.3907828282828283,
      "grad_norm": 0.4841908812522888,
      "learning_rate": 0.0001860008904829952,
      "loss": 0.5542,
      "step": 6190
    },
    {
      "epoch": 0.39141414141414144,
      "grad_norm": 0.746345579624176,
      "learning_rate": 0.00018594867695001605,
      "loss": 0.5762,
      "step": 6200
    },
    {
      "epoch": 0.39204545454545453,
      "grad_norm": 0.419131338596344,
      "learning_rate": 0.00018589637358276578,
      "loss": 0.9231,
      "step": 6210
    },
    {
      "epoch": 0.3926767676767677,
      "grad_norm": 0.42099711298942566,
      "learning_rate": 0.0001858439804359123,
      "loss": 0.6671,
      "step": 6220
    },
    {
      "epoch": 0.39330808080808083,
      "grad_norm": 0.4304157495498657,
      "learning_rate": 0.00018579149756421735,
      "loss": 0.6522,
      "step": 6230
    },
    {
      "epoch": 0.3939393939393939,
      "grad_norm": 0.4977884590625763,
      "learning_rate": 0.0001857389250225365,
      "loss": 0.5555,
      "step": 6240
    },
    {
      "epoch": 0.39457070707070707,
      "grad_norm": 0.7932303547859192,
      "learning_rate": 0.00018568626286581897,
      "loss": 0.5991,
      "step": 6250
    },
    {
      "epoch": 0.3952020202020202,
      "grad_norm": 0.38257917761802673,
      "learning_rate": 0.0001856335111491077,
      "loss": 0.8526,
      "step": 6260
    },
    {
      "epoch": 0.3958333333333333,
      "grad_norm": 0.4544815421104431,
      "learning_rate": 0.00018558066992753925,
      "loss": 0.7123,
      "step": 6270
    },
    {
      "epoch": 0.39646464646464646,
      "grad_norm": 0.4564586877822876,
      "learning_rate": 0.00018552773925634367,
      "loss": 0.6305,
      "step": 6280
    },
    {
      "epoch": 0.3970959595959596,
      "grad_norm": 0.46483704447746277,
      "learning_rate": 0.00018547471919084453,
      "loss": 0.5875,
      "step": 6290
    },
    {
      "epoch": 0.3977272727272727,
      "grad_norm": 0.712212085723877,
      "learning_rate": 0.00018542160978645886,
      "loss": 0.5872,
      "step": 6300
    },
    {
      "epoch": 0.39835858585858586,
      "grad_norm": 0.4110700190067291,
      "learning_rate": 0.00018536841109869704,
      "loss": 0.8538,
      "step": 6310
    },
    {
      "epoch": 0.398989898989899,
      "grad_norm": 0.4061870276927948,
      "learning_rate": 0.00018531512318316283,
      "loss": 0.7068,
      "step": 6320
    },
    {
      "epoch": 0.3996212121212121,
      "grad_norm": 0.43547025322914124,
      "learning_rate": 0.0001852617460955531,
      "loss": 0.661,
      "step": 6330
    },
    {
      "epoch": 0.40025252525252525,
      "grad_norm": 0.4260810911655426,
      "learning_rate": 0.00018520827989165813,
      "loss": 0.5883,
      "step": 6340
    },
    {
      "epoch": 0.4008838383838384,
      "grad_norm": 0.7499491572380066,
      "learning_rate": 0.0001851547246273612,
      "loss": 0.6104,
      "step": 6350
    },
    {
      "epoch": 0.4015151515151515,
      "grad_norm": 0.4179173409938812,
      "learning_rate": 0.00018510108035863868,
      "loss": 0.838,
      "step": 6360
    },
    {
      "epoch": 0.40214646464646464,
      "grad_norm": 0.4312395751476288,
      "learning_rate": 0.00018504734714156008,
      "loss": 0.7494,
      "step": 6370
    },
    {
      "epoch": 0.4027777777777778,
      "grad_norm": 0.4430065453052521,
      "learning_rate": 0.00018499352503228774,
      "loss": 0.6305,
      "step": 6380
    },
    {
      "epoch": 0.4034090909090909,
      "grad_norm": 0.44591107964515686,
      "learning_rate": 0.000184939614087077,
      "loss": 0.5558,
      "step": 6390
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 0.6720486283302307,
      "learning_rate": 0.00018488561436227603,
      "loss": 0.5751,
      "step": 6400
    },
    {
      "epoch": 0.4046717171717172,
      "grad_norm": 0.4105556607246399,
      "learning_rate": 0.0001848315259143258,
      "loss": 0.8768,
      "step": 6410
    },
    {
      "epoch": 0.4053030303030303,
      "grad_norm": 0.4076120853424072,
      "learning_rate": 0.00018477734879976,
      "loss": 0.7087,
      "step": 6420
    },
    {
      "epoch": 0.4059343434343434,
      "grad_norm": 0.4260729253292084,
      "learning_rate": 0.00018472308307520497,
      "loss": 0.6038,
      "step": 6430
    },
    {
      "epoch": 0.4065656565656566,
      "grad_norm": 0.4515727460384369,
      "learning_rate": 0.0001846687287973797,
      "loss": 0.5357,
      "step": 6440
    },
    {
      "epoch": 0.4071969696969697,
      "grad_norm": 0.8044649958610535,
      "learning_rate": 0.0001846142860230958,
      "loss": 0.6045,
      "step": 6450
    },
    {
      "epoch": 0.4078282828282828,
      "grad_norm": 0.41700279712677,
      "learning_rate": 0.00018455975480925722,
      "loss": 0.8756,
      "step": 6460
    },
    {
      "epoch": 0.40845959595959597,
      "grad_norm": 0.47817933559417725,
      "learning_rate": 0.0001845051352128605,
      "loss": 0.7191,
      "step": 6470
    },
    {
      "epoch": 0.4090909090909091,
      "grad_norm": 0.4666447937488556,
      "learning_rate": 0.00018445042729099445,
      "loss": 0.6639,
      "step": 6480
    },
    {
      "epoch": 0.4097222222222222,
      "grad_norm": 0.493257075548172,
      "learning_rate": 0.00018439563110084033,
      "loss": 0.556,
      "step": 6490
    },
    {
      "epoch": 0.41035353535353536,
      "grad_norm": 0.6930088400840759,
      "learning_rate": 0.00018434074669967148,
      "loss": 0.5943,
      "step": 6500
    },
    {
      "epoch": 0.4109848484848485,
      "grad_norm": 0.3738265335559845,
      "learning_rate": 0.00018428577414485357,
      "loss": 0.8746,
      "step": 6510
    },
    {
      "epoch": 0.4116161616161616,
      "grad_norm": 0.43326982855796814,
      "learning_rate": 0.00018423071349384435,
      "loss": 0.731,
      "step": 6520
    },
    {
      "epoch": 0.41224747474747475,
      "grad_norm": 0.45567891001701355,
      "learning_rate": 0.00018417556480419372,
      "loss": 0.6227,
      "step": 6530
    },
    {
      "epoch": 0.4128787878787879,
      "grad_norm": 0.47812190651893616,
      "learning_rate": 0.00018412032813354347,
      "loss": 0.5774,
      "step": 6540
    },
    {
      "epoch": 0.413510101010101,
      "grad_norm": 0.7046025395393372,
      "learning_rate": 0.0001840650035396275,
      "loss": 0.595,
      "step": 6550
    },
    {
      "epoch": 0.41414141414141414,
      "grad_norm": 0.4241725206375122,
      "learning_rate": 0.0001840095910802715,
      "loss": 0.8358,
      "step": 6560
    },
    {
      "epoch": 0.4147727272727273,
      "grad_norm": 0.41004592180252075,
      "learning_rate": 0.00018395409081339305,
      "loss": 0.7145,
      "step": 6570
    },
    {
      "epoch": 0.4154040404040404,
      "grad_norm": 0.4159409999847412,
      "learning_rate": 0.00018389850279700148,
      "loss": 0.6457,
      "step": 6580
    },
    {
      "epoch": 0.41603535353535354,
      "grad_norm": 0.4859406054019928,
      "learning_rate": 0.00018384282708919784,
      "loss": 0.5753,
      "step": 6590
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.7436560392379761,
      "learning_rate": 0.00018378706374817485,
      "loss": 0.5669,
      "step": 6600
    },
    {
      "epoch": 0.4172979797979798,
      "grad_norm": 0.41768285632133484,
      "learning_rate": 0.00018373121283221682,
      "loss": 0.819,
      "step": 6610
    },
    {
      "epoch": 0.41792929292929293,
      "grad_norm": 0.41179099678993225,
      "learning_rate": 0.00018367527439969958,
      "loss": 0.715,
      "step": 6620
    },
    {
      "epoch": 0.4185606060606061,
      "grad_norm": 0.5432329177856445,
      "learning_rate": 0.00018361924850909044,
      "loss": 0.5767,
      "step": 6630
    },
    {
      "epoch": 0.41919191919191917,
      "grad_norm": 0.4816562831401825,
      "learning_rate": 0.00018356313521894816,
      "loss": 0.5561,
      "step": 6640
    },
    {
      "epoch": 0.4198232323232323,
      "grad_norm": 0.734851598739624,
      "learning_rate": 0.00018350693458792279,
      "loss": 0.5866,
      "step": 6650
    },
    {
      "epoch": 0.42045454545454547,
      "grad_norm": 0.3941877484321594,
      "learning_rate": 0.0001834506466747557,
      "loss": 0.8467,
      "step": 6660
    },
    {
      "epoch": 0.42108585858585856,
      "grad_norm": 0.43431639671325684,
      "learning_rate": 0.0001833942715382795,
      "loss": 0.6952,
      "step": 6670
    },
    {
      "epoch": 0.4217171717171717,
      "grad_norm": 0.4476296007633209,
      "learning_rate": 0.00018333780923741788,
      "loss": 0.6077,
      "step": 6680
    },
    {
      "epoch": 0.42234848484848486,
      "grad_norm": 0.5109779834747314,
      "learning_rate": 0.0001832812598311858,
      "loss": 0.5883,
      "step": 6690
    },
    {
      "epoch": 0.42297979797979796,
      "grad_norm": 0.8144782185554504,
      "learning_rate": 0.00018322462337868914,
      "loss": 0.6094,
      "step": 6700
    },
    {
      "epoch": 0.4236111111111111,
      "grad_norm": 0.3966207802295685,
      "learning_rate": 0.00018316789993912477,
      "loss": 0.7958,
      "step": 6710
    },
    {
      "epoch": 0.42424242424242425,
      "grad_norm": 0.41125473380088806,
      "learning_rate": 0.0001831110895717805,
      "loss": 0.7134,
      "step": 6720
    },
    {
      "epoch": 0.42487373737373735,
      "grad_norm": 0.4433216452598572,
      "learning_rate": 0.00018305419233603508,
      "loss": 0.6437,
      "step": 6730
    },
    {
      "epoch": 0.4255050505050505,
      "grad_norm": 0.4871545732021332,
      "learning_rate": 0.00018299720829135786,
      "loss": 0.5773,
      "step": 6740
    },
    {
      "epoch": 0.42613636363636365,
      "grad_norm": 0.7740412354469299,
      "learning_rate": 0.00018294013749730904,
      "loss": 0.6429,
      "step": 6750
    },
    {
      "epoch": 0.42676767676767674,
      "grad_norm": 0.37375566363334656,
      "learning_rate": 0.00018288298001353957,
      "loss": 0.834,
      "step": 6760
    },
    {
      "epoch": 0.4273989898989899,
      "grad_norm": 0.40881073474884033,
      "learning_rate": 0.00018282573589979085,
      "loss": 0.7101,
      "step": 6770
    },
    {
      "epoch": 0.42803030303030304,
      "grad_norm": 0.45515722036361694,
      "learning_rate": 0.00018276840521589497,
      "loss": 0.6053,
      "step": 6780
    },
    {
      "epoch": 0.4286616161616162,
      "grad_norm": 0.45072251558303833,
      "learning_rate": 0.0001827109880217744,
      "loss": 0.5476,
      "step": 6790
    },
    {
      "epoch": 0.4292929292929293,
      "grad_norm": 0.7529823780059814,
      "learning_rate": 0.0001826534843774421,
      "loss": 0.5982,
      "step": 6800
    },
    {
      "epoch": 0.42992424242424243,
      "grad_norm": 0.39748749136924744,
      "learning_rate": 0.0001825958943430013,
      "loss": 0.8944,
      "step": 6810
    },
    {
      "epoch": 0.4305555555555556,
      "grad_norm": 0.4200434386730194,
      "learning_rate": 0.00018253821797864562,
      "loss": 0.7241,
      "step": 6820
    },
    {
      "epoch": 0.4311868686868687,
      "grad_norm": 0.4763912558555603,
      "learning_rate": 0.00018248045534465884,
      "loss": 0.6405,
      "step": 6830
    },
    {
      "epoch": 0.4318181818181818,
      "grad_norm": 0.48621535301208496,
      "learning_rate": 0.00018242260650141502,
      "loss": 0.5988,
      "step": 6840
    },
    {
      "epoch": 0.432449494949495,
      "grad_norm": 0.8303858041763306,
      "learning_rate": 0.0001823646715093782,
      "loss": 0.5682,
      "step": 6850
    },
    {
      "epoch": 0.43308080808080807,
      "grad_norm": 0.397946834564209,
      "learning_rate": 0.00018230665042910248,
      "loss": 0.857,
      "step": 6860
    },
    {
      "epoch": 0.4337121212121212,
      "grad_norm": 0.40436843037605286,
      "learning_rate": 0.00018224854332123206,
      "loss": 0.6927,
      "step": 6870
    },
    {
      "epoch": 0.43434343434343436,
      "grad_norm": 0.49015742540359497,
      "learning_rate": 0.0001821903502465009,
      "loss": 0.6203,
      "step": 6880
    },
    {
      "epoch": 0.43497474747474746,
      "grad_norm": 0.506808876991272,
      "learning_rate": 0.00018213207126573292,
      "loss": 0.5357,
      "step": 6890
    },
    {
      "epoch": 0.4356060606060606,
      "grad_norm": 0.7114576697349548,
      "learning_rate": 0.00018207370643984178,
      "loss": 0.5817,
      "step": 6900
    },
    {
      "epoch": 0.43623737373737376,
      "grad_norm": 0.41007959842681885,
      "learning_rate": 0.0001820152558298309,
      "loss": 0.8509,
      "step": 6910
    },
    {
      "epoch": 0.43686868686868685,
      "grad_norm": 0.40343308448791504,
      "learning_rate": 0.00018195671949679333,
      "loss": 0.7349,
      "step": 6920
    },
    {
      "epoch": 0.4375,
      "grad_norm": 0.4361518919467926,
      "learning_rate": 0.0001818980975019117,
      "loss": 0.6389,
      "step": 6930
    },
    {
      "epoch": 0.43813131313131315,
      "grad_norm": 0.5221399068832397,
      "learning_rate": 0.00018183938990645827,
      "loss": 0.5638,
      "step": 6940
    },
    {
      "epoch": 0.43876262626262624,
      "grad_norm": 0.815796434879303,
      "learning_rate": 0.00018178059677179467,
      "loss": 0.5488,
      "step": 6950
    },
    {
      "epoch": 0.4393939393939394,
      "grad_norm": 0.3805437684059143,
      "learning_rate": 0.00018172171815937195,
      "loss": 0.827,
      "step": 6960
    },
    {
      "epoch": 0.44002525252525254,
      "grad_norm": 0.41381528973579407,
      "learning_rate": 0.00018166275413073062,
      "loss": 0.7293,
      "step": 6970
    },
    {
      "epoch": 0.44065656565656564,
      "grad_norm": 0.4486306309700012,
      "learning_rate": 0.00018160370474750023,
      "loss": 0.6265,
      "step": 6980
    },
    {
      "epoch": 0.4412878787878788,
      "grad_norm": 0.43777331709861755,
      "learning_rate": 0.0001815445700713998,
      "loss": 0.5959,
      "step": 6990
    },
    {
      "epoch": 0.44191919191919193,
      "grad_norm": 0.9449087977409363,
      "learning_rate": 0.00018148535016423734,
      "loss": 0.6188,
      "step": 7000
    },
    {
      "epoch": 0.44191919191919193,
      "eval_loss": 0.6907533407211304,
      "eval_runtime": 31.7383,
      "eval_samples_per_second": 80.66,
      "eval_steps_per_second": 10.082,
      "step": 7000
    },
    {
      "epoch": 0.442550505050505,
      "grad_norm": 0.3838532269001007,
      "learning_rate": 0.00018142604508791,
      "loss": 0.8531,
      "step": 7010
    },
    {
      "epoch": 0.4431818181818182,
      "grad_norm": 0.41787683963775635,
      "learning_rate": 0.00018136665490440393,
      "loss": 0.7215,
      "step": 7020
    },
    {
      "epoch": 0.4438131313131313,
      "grad_norm": 0.45363765954971313,
      "learning_rate": 0.00018130717967579423,
      "loss": 0.623,
      "step": 7030
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.5209704041481018,
      "learning_rate": 0.00018124761946424492,
      "loss": 0.5635,
      "step": 7040
    },
    {
      "epoch": 0.44507575757575757,
      "grad_norm": 0.6806517243385315,
      "learning_rate": 0.00018118797433200882,
      "loss": 0.594,
      "step": 7050
    },
    {
      "epoch": 0.4457070707070707,
      "grad_norm": 0.3820900321006775,
      "learning_rate": 0.00018112824434142753,
      "loss": 0.8864,
      "step": 7060
    },
    {
      "epoch": 0.4463383838383838,
      "grad_norm": 0.4138188362121582,
      "learning_rate": 0.00018106842955493133,
      "loss": 0.688,
      "step": 7070
    },
    {
      "epoch": 0.44696969696969696,
      "grad_norm": 0.4519702196121216,
      "learning_rate": 0.00018100853003503916,
      "loss": 0.6555,
      "step": 7080
    },
    {
      "epoch": 0.4476010101010101,
      "grad_norm": 0.4797326326370239,
      "learning_rate": 0.00018094854584435843,
      "loss": 0.5887,
      "step": 7090
    },
    {
      "epoch": 0.44823232323232326,
      "grad_norm": 0.7420028448104858,
      "learning_rate": 0.00018088847704558517,
      "loss": 0.5694,
      "step": 7100
    },
    {
      "epoch": 0.44886363636363635,
      "grad_norm": 0.379117488861084,
      "learning_rate": 0.00018082832370150374,
      "loss": 0.8777,
      "step": 7110
    },
    {
      "epoch": 0.4494949494949495,
      "grad_norm": 0.43400248885154724,
      "learning_rate": 0.00018076808587498696,
      "loss": 0.7276,
      "step": 7120
    },
    {
      "epoch": 0.45012626262626265,
      "grad_norm": 0.4724447727203369,
      "learning_rate": 0.00018070776362899587,
      "loss": 0.657,
      "step": 7130
    },
    {
      "epoch": 0.45075757575757575,
      "grad_norm": 0.4629736840724945,
      "learning_rate": 0.0001806473570265798,
      "loss": 0.5783,
      "step": 7140
    },
    {
      "epoch": 0.4513888888888889,
      "grad_norm": 0.772071897983551,
      "learning_rate": 0.00018058686613087624,
      "loss": 0.5996,
      "step": 7150
    },
    {
      "epoch": 0.45202020202020204,
      "grad_norm": 0.37838196754455566,
      "learning_rate": 0.00018052629100511077,
      "loss": 0.8425,
      "step": 7160
    },
    {
      "epoch": 0.45265151515151514,
      "grad_norm": 0.38760703802108765,
      "learning_rate": 0.00018046563171259701,
      "loss": 0.7016,
      "step": 7170
    },
    {
      "epoch": 0.4532828282828283,
      "grad_norm": 0.4640325903892517,
      "learning_rate": 0.00018040488831673655,
      "loss": 0.6186,
      "step": 7180
    },
    {
      "epoch": 0.45391414141414144,
      "grad_norm": 0.5268521308898926,
      "learning_rate": 0.00018034406088101893,
      "loss": 0.5711,
      "step": 7190
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.7366604804992676,
      "learning_rate": 0.00018028314946902144,
      "loss": 0.5816,
      "step": 7200
    },
    {
      "epoch": 0.4551767676767677,
      "grad_norm": 0.4029639959335327,
      "learning_rate": 0.00018022215414440924,
      "loss": 0.8113,
      "step": 7210
    },
    {
      "epoch": 0.45580808080808083,
      "grad_norm": 0.43404385447502136,
      "learning_rate": 0.00018016107497093514,
      "loss": 0.735,
      "step": 7220
    },
    {
      "epoch": 0.4564393939393939,
      "grad_norm": 0.4508930444717407,
      "learning_rate": 0.00018009991201243955,
      "loss": 0.6305,
      "step": 7230
    },
    {
      "epoch": 0.45707070707070707,
      "grad_norm": 0.5153922438621521,
      "learning_rate": 0.00018003866533285054,
      "loss": 0.5524,
      "step": 7240
    },
    {
      "epoch": 0.4577020202020202,
      "grad_norm": 0.7794998288154602,
      "learning_rate": 0.00017997733499618365,
      "loss": 0.5805,
      "step": 7250
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.3815208077430725,
      "learning_rate": 0.00017991592106654186,
      "loss": 0.8022,
      "step": 7260
    },
    {
      "epoch": 0.45896464646464646,
      "grad_norm": 0.43813028931617737,
      "learning_rate": 0.00017985442360811553,
      "loss": 0.6968,
      "step": 7270
    },
    {
      "epoch": 0.4595959595959596,
      "grad_norm": 0.43707236647605896,
      "learning_rate": 0.00017979284268518228,
      "loss": 0.6343,
      "step": 7280
    },
    {
      "epoch": 0.4602272727272727,
      "grad_norm": 0.4610508680343628,
      "learning_rate": 0.00017973117836210702,
      "loss": 0.5624,
      "step": 7290
    },
    {
      "epoch": 0.46085858585858586,
      "grad_norm": 0.5968424677848816,
      "learning_rate": 0.00017966943070334184,
      "loss": 0.5811,
      "step": 7300
    },
    {
      "epoch": 0.461489898989899,
      "grad_norm": 0.3787730634212494,
      "learning_rate": 0.00017960759977342586,
      "loss": 0.8163,
      "step": 7310
    },
    {
      "epoch": 0.4621212121212121,
      "grad_norm": 0.4446588456630707,
      "learning_rate": 0.0001795456856369853,
      "loss": 0.6701,
      "step": 7320
    },
    {
      "epoch": 0.46275252525252525,
      "grad_norm": 0.44922909140586853,
      "learning_rate": 0.00017948368835873332,
      "loss": 0.6527,
      "step": 7330
    },
    {
      "epoch": 0.4633838383838384,
      "grad_norm": 0.4512815773487091,
      "learning_rate": 0.00017942160800347,
      "loss": 0.5622,
      "step": 7340
    },
    {
      "epoch": 0.4640151515151515,
      "grad_norm": 0.8998258113861084,
      "learning_rate": 0.00017935944463608227,
      "loss": 0.577,
      "step": 7350
    },
    {
      "epoch": 0.46464646464646464,
      "grad_norm": 0.42045727372169495,
      "learning_rate": 0.00017929719832154376,
      "loss": 0.83,
      "step": 7360
    },
    {
      "epoch": 0.4652777777777778,
      "grad_norm": 0.40953558683395386,
      "learning_rate": 0.00017923486912491482,
      "loss": 0.6878,
      "step": 7370
    },
    {
      "epoch": 0.4659090909090909,
      "grad_norm": 0.4562593996524811,
      "learning_rate": 0.0001791724571113425,
      "loss": 0.6427,
      "step": 7380
    },
    {
      "epoch": 0.46654040404040403,
      "grad_norm": 0.4416649341583252,
      "learning_rate": 0.0001791099623460603,
      "loss": 0.5759,
      "step": 7390
    },
    {
      "epoch": 0.4671717171717172,
      "grad_norm": 0.8668914437294006,
      "learning_rate": 0.00017904738489438836,
      "loss": 0.5969,
      "step": 7400
    },
    {
      "epoch": 0.4678030303030303,
      "grad_norm": 0.3955322206020355,
      "learning_rate": 0.00017898472482173302,
      "loss": 0.8542,
      "step": 7410
    },
    {
      "epoch": 0.4684343434343434,
      "grad_norm": 0.40944555401802063,
      "learning_rate": 0.0001789219821935872,
      "loss": 0.7572,
      "step": 7420
    },
    {
      "epoch": 0.4690656565656566,
      "grad_norm": 0.43655434250831604,
      "learning_rate": 0.00017885915707552998,
      "loss": 0.6358,
      "step": 7430
    },
    {
      "epoch": 0.4696969696969697,
      "grad_norm": 0.5364736318588257,
      "learning_rate": 0.0001787962495332267,
      "loss": 0.5676,
      "step": 7440
    },
    {
      "epoch": 0.4703282828282828,
      "grad_norm": 0.72092205286026,
      "learning_rate": 0.00017873325963242888,
      "loss": 0.5813,
      "step": 7450
    },
    {
      "epoch": 0.47095959595959597,
      "grad_norm": 0.408807635307312,
      "learning_rate": 0.00017867018743897406,
      "loss": 0.8195,
      "step": 7460
    },
    {
      "epoch": 0.4715909090909091,
      "grad_norm": 0.41940203309059143,
      "learning_rate": 0.0001786070330187858,
      "loss": 0.6696,
      "step": 7470
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 0.45250314474105835,
      "learning_rate": 0.00017854379643787363,
      "loss": 0.6564,
      "step": 7480
    },
    {
      "epoch": 0.47285353535353536,
      "grad_norm": 0.45826786756515503,
      "learning_rate": 0.000178480477762333,
      "loss": 0.5424,
      "step": 7490
    },
    {
      "epoch": 0.4734848484848485,
      "grad_norm": 0.7622692584991455,
      "learning_rate": 0.00017841707705834505,
      "loss": 0.5977,
      "step": 7500
    },
    {
      "epoch": 0.4741161616161616,
      "grad_norm": 0.4030795991420746,
      "learning_rate": 0.00017835359439217677,
      "loss": 0.8494,
      "step": 7510
    },
    {
      "epoch": 0.47474747474747475,
      "grad_norm": 0.40606337785720825,
      "learning_rate": 0.00017829002983018075,
      "loss": 0.7318,
      "step": 7520
    },
    {
      "epoch": 0.4753787878787879,
      "grad_norm": 0.4491689205169678,
      "learning_rate": 0.0001782263834387952,
      "loss": 0.6303,
      "step": 7530
    },
    {
      "epoch": 0.476010101010101,
      "grad_norm": 0.47923311591148376,
      "learning_rate": 0.00017816265528454382,
      "loss": 0.5935,
      "step": 7540
    },
    {
      "epoch": 0.47664141414141414,
      "grad_norm": 0.6971628665924072,
      "learning_rate": 0.0001780988454340359,
      "loss": 0.6015,
      "step": 7550
    },
    {
      "epoch": 0.4772727272727273,
      "grad_norm": 0.4107591509819031,
      "learning_rate": 0.00017803495395396593,
      "loss": 0.8154,
      "step": 7560
    },
    {
      "epoch": 0.4779040404040404,
      "grad_norm": 0.4214077293872833,
      "learning_rate": 0.0001779709809111139,
      "loss": 0.7028,
      "step": 7570
    },
    {
      "epoch": 0.47853535353535354,
      "grad_norm": 0.46693605184555054,
      "learning_rate": 0.00017790692637234488,
      "loss": 0.6347,
      "step": 7580
    },
    {
      "epoch": 0.4791666666666667,
      "grad_norm": 0.4552689790725708,
      "learning_rate": 0.00017784279040460924,
      "loss": 0.5645,
      "step": 7590
    },
    {
      "epoch": 0.4797979797979798,
      "grad_norm": 0.7488967776298523,
      "learning_rate": 0.00017777857307494247,
      "loss": 0.5964,
      "step": 7600
    },
    {
      "epoch": 0.48042929292929293,
      "grad_norm": 0.42153698205947876,
      "learning_rate": 0.000177714274450465,
      "loss": 0.8145,
      "step": 7610
    },
    {
      "epoch": 0.4810606060606061,
      "grad_norm": 0.40933963656425476,
      "learning_rate": 0.00017764989459838232,
      "loss": 0.7209,
      "step": 7620
    },
    {
      "epoch": 0.48169191919191917,
      "grad_norm": 0.44850075244903564,
      "learning_rate": 0.00017758543358598476,
      "loss": 0.6429,
      "step": 7630
    },
    {
      "epoch": 0.4823232323232323,
      "grad_norm": 0.5202820301055908,
      "learning_rate": 0.00017752089148064752,
      "loss": 0.5588,
      "step": 7640
    },
    {
      "epoch": 0.48295454545454547,
      "grad_norm": 0.7552527189254761,
      "learning_rate": 0.00017745626834983055,
      "loss": 0.6132,
      "step": 7650
    },
    {
      "epoch": 0.48358585858585856,
      "grad_norm": 0.38718196749687195,
      "learning_rate": 0.00017739156426107845,
      "loss": 0.8251,
      "step": 7660
    },
    {
      "epoch": 0.4842171717171717,
      "grad_norm": 0.42284855246543884,
      "learning_rate": 0.00017732677928202053,
      "loss": 0.7041,
      "step": 7670
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 0.4895118176937103,
      "learning_rate": 0.00017726191348037054,
      "loss": 0.6272,
      "step": 7680
    },
    {
      "epoch": 0.48547979797979796,
      "grad_norm": 0.4629121720790863,
      "learning_rate": 0.00017719696692392677,
      "loss": 0.55,
      "step": 7690
    },
    {
      "epoch": 0.4861111111111111,
      "grad_norm": 0.7579970359802246,
      "learning_rate": 0.0001771319396805719,
      "loss": 0.5506,
      "step": 7700
    },
    {
      "epoch": 0.48674242424242425,
      "grad_norm": 0.40679386258125305,
      "learning_rate": 0.00017706683181827295,
      "loss": 0.8612,
      "step": 7710
    },
    {
      "epoch": 0.48737373737373735,
      "grad_norm": 0.4105893075466156,
      "learning_rate": 0.00017700164340508117,
      "loss": 0.7051,
      "step": 7720
    },
    {
      "epoch": 0.4880050505050505,
      "grad_norm": 0.4041452705860138,
      "learning_rate": 0.0001769363745091321,
      "loss": 0.6126,
      "step": 7730
    },
    {
      "epoch": 0.48863636363636365,
      "grad_norm": 0.4690499007701874,
      "learning_rate": 0.00017687102519864525,
      "loss": 0.5434,
      "step": 7740
    },
    {
      "epoch": 0.48926767676767674,
      "grad_norm": 0.7439444661140442,
      "learning_rate": 0.0001768055955419243,
      "loss": 0.5823,
      "step": 7750
    },
    {
      "epoch": 0.4898989898989899,
      "grad_norm": 0.4041377007961273,
      "learning_rate": 0.0001767400856073569,
      "loss": 0.8594,
      "step": 7760
    },
    {
      "epoch": 0.49053030303030304,
      "grad_norm": 0.3995373845100403,
      "learning_rate": 0.00017667449546341453,
      "loss": 0.7221,
      "step": 7770
    },
    {
      "epoch": 0.4911616161616162,
      "grad_norm": 0.45533621311187744,
      "learning_rate": 0.00017660882517865254,
      "loss": 0.6312,
      "step": 7780
    },
    {
      "epoch": 0.4917929292929293,
      "grad_norm": 0.4862203598022461,
      "learning_rate": 0.00017654307482171014,
      "loss": 0.569,
      "step": 7790
    },
    {
      "epoch": 0.49242424242424243,
      "grad_norm": 0.7067985534667969,
      "learning_rate": 0.00017647724446131005,
      "loss": 0.6035,
      "step": 7800
    },
    {
      "epoch": 0.4930555555555556,
      "grad_norm": 0.37390050292015076,
      "learning_rate": 0.00017641133416625878,
      "loss": 0.8542,
      "step": 7810
    },
    {
      "epoch": 0.4936868686868687,
      "grad_norm": 0.41621527075767517,
      "learning_rate": 0.00017634534400544631,
      "loss": 0.7152,
      "step": 7820
    },
    {
      "epoch": 0.4943181818181818,
      "grad_norm": 0.48436734080314636,
      "learning_rate": 0.00017627927404784607,
      "loss": 0.6459,
      "step": 7830
    },
    {
      "epoch": 0.494949494949495,
      "grad_norm": 0.4439222514629364,
      "learning_rate": 0.00017621312436251496,
      "loss": 0.553,
      "step": 7840
    },
    {
      "epoch": 0.49558080808080807,
      "grad_norm": 0.7288021445274353,
      "learning_rate": 0.00017614689501859316,
      "loss": 0.5636,
      "step": 7850
    },
    {
      "epoch": 0.4962121212121212,
      "grad_norm": 0.3949030339717865,
      "learning_rate": 0.00017608058608530413,
      "loss": 0.8933,
      "step": 7860
    },
    {
      "epoch": 0.49684343434343436,
      "grad_norm": 0.4052608609199524,
      "learning_rate": 0.00017601419763195453,
      "loss": 0.7252,
      "step": 7870
    },
    {
      "epoch": 0.49747474747474746,
      "grad_norm": 0.5271382331848145,
      "learning_rate": 0.0001759477297279341,
      "loss": 0.6308,
      "step": 7880
    },
    {
      "epoch": 0.4981060606060606,
      "grad_norm": 0.5166057348251343,
      "learning_rate": 0.00017588118244271568,
      "loss": 0.5763,
      "step": 7890
    },
    {
      "epoch": 0.49873737373737376,
      "grad_norm": 0.7679633498191833,
      "learning_rate": 0.00017581455584585507,
      "loss": 0.5718,
      "step": 7900
    },
    {
      "epoch": 0.49936868686868685,
      "grad_norm": 0.41834408044815063,
      "learning_rate": 0.00017574785000699084,
      "loss": 0.8824,
      "step": 7910
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.40393438935279846,
      "learning_rate": 0.0001756810649958446,
      "loss": 0.7174,
      "step": 7920
    },
    {
      "epoch": 0.5006313131313131,
      "grad_norm": 0.43800821900367737,
      "learning_rate": 0.00017561420088222054,
      "loss": 0.623,
      "step": 7930
    },
    {
      "epoch": 0.5012626262626263,
      "grad_norm": 0.4504833221435547,
      "learning_rate": 0.0001755472577360056,
      "loss": 0.5917,
      "step": 7940
    },
    {
      "epoch": 0.5018939393939394,
      "grad_norm": 0.7888127565383911,
      "learning_rate": 0.0001754802356271693,
      "loss": 0.5911,
      "step": 7950
    },
    {
      "epoch": 0.5025252525252525,
      "grad_norm": 0.3851255476474762,
      "learning_rate": 0.00017541313462576368,
      "loss": 0.8329,
      "step": 7960
    },
    {
      "epoch": 0.5031565656565656,
      "grad_norm": 0.41838860511779785,
      "learning_rate": 0.0001753459548019233,
      "loss": 0.6918,
      "step": 7970
    },
    {
      "epoch": 0.5037878787878788,
      "grad_norm": 0.5080353021621704,
      "learning_rate": 0.0001752786962258651,
      "loss": 0.6073,
      "step": 7980
    },
    {
      "epoch": 0.5044191919191919,
      "grad_norm": 0.452402800321579,
      "learning_rate": 0.00017521135896788828,
      "loss": 0.5809,
      "step": 7990
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 0.8388983607292175,
      "learning_rate": 0.00017514394309837424,
      "loss": 0.594,
      "step": 8000
    },
    {
      "epoch": 0.5050505050505051,
      "eval_loss": 0.68694669008255,
      "eval_runtime": 31.692,
      "eval_samples_per_second": 80.777,
      "eval_steps_per_second": 10.097,
      "step": 8000
    },
    {
      "epoch": 0.5056818181818182,
      "grad_norm": 0.373368501663208,
      "learning_rate": 0.0001750764486877867,
      "loss": 0.8493,
      "step": 8010
    },
    {
      "epoch": 0.5063131313131313,
      "grad_norm": 0.43484067916870117,
      "learning_rate": 0.0001750088758066713,
      "loss": 0.7083,
      "step": 8020
    },
    {
      "epoch": 0.5069444444444444,
      "grad_norm": 0.44387009739875793,
      "learning_rate": 0.00017494122452565582,
      "loss": 0.6229,
      "step": 8030
    },
    {
      "epoch": 0.5075757575757576,
      "grad_norm": 0.502854585647583,
      "learning_rate": 0.00017487349491544996,
      "loss": 0.5659,
      "step": 8040
    },
    {
      "epoch": 0.5082070707070707,
      "grad_norm": 0.7618576884269714,
      "learning_rate": 0.00017480568704684521,
      "loss": 0.5227,
      "step": 8050
    },
    {
      "epoch": 0.5088383838383839,
      "grad_norm": 0.38144585490226746,
      "learning_rate": 0.00017473780099071498,
      "loss": 0.8439,
      "step": 8060
    },
    {
      "epoch": 0.509469696969697,
      "grad_norm": 0.3920883238315582,
      "learning_rate": 0.0001746698368180143,
      "loss": 0.7142,
      "step": 8070
    },
    {
      "epoch": 0.51010101010101,
      "grad_norm": 0.4381446838378906,
      "learning_rate": 0.0001746017945997799,
      "loss": 0.6197,
      "step": 8080
    },
    {
      "epoch": 0.5107323232323232,
      "grad_norm": 0.42488494515419006,
      "learning_rate": 0.00017453367440713007,
      "loss": 0.5585,
      "step": 8090
    },
    {
      "epoch": 0.5113636363636364,
      "grad_norm": 0.8614776134490967,
      "learning_rate": 0.00017446547631126463,
      "loss": 0.5906,
      "step": 8100
    },
    {
      "epoch": 0.5119949494949495,
      "grad_norm": 0.3985981345176697,
      "learning_rate": 0.00017439720038346472,
      "loss": 0.875,
      "step": 8110
    },
    {
      "epoch": 0.5126262626262627,
      "grad_norm": 0.39980363845825195,
      "learning_rate": 0.00017432884669509299,
      "loss": 0.7472,
      "step": 8120
    },
    {
      "epoch": 0.5132575757575758,
      "grad_norm": 0.417721152305603,
      "learning_rate": 0.0001742604153175932,
      "loss": 0.6005,
      "step": 8130
    },
    {
      "epoch": 0.5138888888888888,
      "grad_norm": 0.5192300081253052,
      "learning_rate": 0.00017419190632249053,
      "loss": 0.5742,
      "step": 8140
    },
    {
      "epoch": 0.514520202020202,
      "grad_norm": 0.7179785966873169,
      "learning_rate": 0.000174123319781391,
      "loss": 0.5739,
      "step": 8150
    },
    {
      "epoch": 0.5151515151515151,
      "grad_norm": 0.3938705027103424,
      "learning_rate": 0.0001740546557659819,
      "loss": 0.8373,
      "step": 8160
    },
    {
      "epoch": 0.5157828282828283,
      "grad_norm": 0.45027223229408264,
      "learning_rate": 0.00017398591434803143,
      "loss": 0.6959,
      "step": 8170
    },
    {
      "epoch": 0.5164141414141414,
      "grad_norm": 0.42369967699050903,
      "learning_rate": 0.0001739170955993887,
      "loss": 0.6393,
      "step": 8180
    },
    {
      "epoch": 0.5170454545454546,
      "grad_norm": 0.4506726562976837,
      "learning_rate": 0.0001738481995919836,
      "loss": 0.5757,
      "step": 8190
    },
    {
      "epoch": 0.5176767676767676,
      "grad_norm": 0.688136637210846,
      "learning_rate": 0.00017377922639782685,
      "loss": 0.6044,
      "step": 8200
    },
    {
      "epoch": 0.5183080808080808,
      "grad_norm": 0.4075978398323059,
      "learning_rate": 0.00017371017608900982,
      "loss": 0.8219,
      "step": 8210
    },
    {
      "epoch": 0.5189393939393939,
      "grad_norm": 0.40969714522361755,
      "learning_rate": 0.0001736410487377044,
      "loss": 0.724,
      "step": 8220
    },
    {
      "epoch": 0.5195707070707071,
      "grad_norm": 0.41157734394073486,
      "learning_rate": 0.0001735718444161631,
      "loss": 0.623,
      "step": 8230
    },
    {
      "epoch": 0.5202020202020202,
      "grad_norm": 0.516360878944397,
      "learning_rate": 0.00017350256319671888,
      "loss": 0.5593,
      "step": 8240
    },
    {
      "epoch": 0.5208333333333334,
      "grad_norm": 0.8920549154281616,
      "learning_rate": 0.000173433205151785,
      "loss": 0.5836,
      "step": 8250
    },
    {
      "epoch": 0.5214646464646465,
      "grad_norm": 0.40738487243652344,
      "learning_rate": 0.0001733637703538551,
      "loss": 0.8112,
      "step": 8260
    },
    {
      "epoch": 0.5220959595959596,
      "grad_norm": 0.39459818601608276,
      "learning_rate": 0.000173294258875503,
      "loss": 0.7371,
      "step": 8270
    },
    {
      "epoch": 0.5227272727272727,
      "grad_norm": 0.4111309349536896,
      "learning_rate": 0.0001732246707893827,
      "loss": 0.6198,
      "step": 8280
    },
    {
      "epoch": 0.5233585858585859,
      "grad_norm": 0.501974880695343,
      "learning_rate": 0.0001731550061682282,
      "loss": 0.5527,
      "step": 8290
    },
    {
      "epoch": 0.523989898989899,
      "grad_norm": 0.6858665347099304,
      "learning_rate": 0.00017308526508485352,
      "loss": 0.543,
      "step": 8300
    },
    {
      "epoch": 0.5246212121212122,
      "grad_norm": 0.38198694586753845,
      "learning_rate": 0.0001730154476121527,
      "loss": 0.8295,
      "step": 8310
    },
    {
      "epoch": 0.5252525252525253,
      "grad_norm": 0.39080092310905457,
      "learning_rate": 0.00017294555382309947,
      "loss": 0.6624,
      "step": 8320
    },
    {
      "epoch": 0.5258838383838383,
      "grad_norm": 0.47456270456314087,
      "learning_rate": 0.00017287558379074747,
      "loss": 0.6428,
      "step": 8330
    },
    {
      "epoch": 0.5265151515151515,
      "grad_norm": 0.48058196902275085,
      "learning_rate": 0.0001728055375882299,
      "loss": 0.552,
      "step": 8340
    },
    {
      "epoch": 0.5271464646464646,
      "grad_norm": 0.7194427847862244,
      "learning_rate": 0.00017273541528875966,
      "loss": 0.5716,
      "step": 8350
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 0.38480180501937866,
      "learning_rate": 0.00017266521696562913,
      "loss": 0.8656,
      "step": 8360
    },
    {
      "epoch": 0.5284090909090909,
      "grad_norm": 0.4199586808681488,
      "learning_rate": 0.0001725949426922102,
      "loss": 0.7511,
      "step": 8370
    },
    {
      "epoch": 0.5290404040404041,
      "grad_norm": 0.48128223419189453,
      "learning_rate": 0.00017252459254195413,
      "loss": 0.6541,
      "step": 8380
    },
    {
      "epoch": 0.5296717171717171,
      "grad_norm": 0.4559173882007599,
      "learning_rate": 0.00017245416658839152,
      "loss": 0.5397,
      "step": 8390
    },
    {
      "epoch": 0.5303030303030303,
      "grad_norm": 0.6923875212669373,
      "learning_rate": 0.00017238366490513206,
      "loss": 0.5454,
      "step": 8400
    },
    {
      "epoch": 0.5309343434343434,
      "grad_norm": 0.42016902565956116,
      "learning_rate": 0.00017231308756586477,
      "loss": 0.8782,
      "step": 8410
    },
    {
      "epoch": 0.5315656565656566,
      "grad_norm": 0.4007017910480499,
      "learning_rate": 0.00017224243464435766,
      "loss": 0.7078,
      "step": 8420
    },
    {
      "epoch": 0.5321969696969697,
      "grad_norm": 0.43170636892318726,
      "learning_rate": 0.00017217170621445775,
      "loss": 0.6504,
      "step": 8430
    },
    {
      "epoch": 0.5328282828282829,
      "grad_norm": 0.49653130769729614,
      "learning_rate": 0.00017210090235009098,
      "loss": 0.5382,
      "step": 8440
    },
    {
      "epoch": 0.5334595959595959,
      "grad_norm": 0.7479313611984253,
      "learning_rate": 0.00017203002312526214,
      "loss": 0.6017,
      "step": 8450
    },
    {
      "epoch": 0.5340909090909091,
      "grad_norm": 0.4298580586910248,
      "learning_rate": 0.00017195906861405477,
      "loss": 0.9331,
      "step": 8460
    },
    {
      "epoch": 0.5347222222222222,
      "grad_norm": 0.4370831251144409,
      "learning_rate": 0.00017188803889063112,
      "loss": 0.7065,
      "step": 8470
    },
    {
      "epoch": 0.5353535353535354,
      "grad_norm": 0.477287620306015,
      "learning_rate": 0.00017181693402923206,
      "loss": 0.6378,
      "step": 8480
    },
    {
      "epoch": 0.5359848484848485,
      "grad_norm": 0.48281097412109375,
      "learning_rate": 0.00017174575410417697,
      "loss": 0.5743,
      "step": 8490
    },
    {
      "epoch": 0.5366161616161617,
      "grad_norm": 0.7633206844329834,
      "learning_rate": 0.0001716744991898637,
      "loss": 0.5795,
      "step": 8500
    },
    {
      "epoch": 0.5372474747474747,
      "grad_norm": 0.4094119071960449,
      "learning_rate": 0.00017160316936076848,
      "loss": 0.8157,
      "step": 8510
    },
    {
      "epoch": 0.5378787878787878,
      "grad_norm": 0.42164021730422974,
      "learning_rate": 0.00017153176469144585,
      "loss": 0.7154,
      "step": 8520
    },
    {
      "epoch": 0.538510101010101,
      "grad_norm": 0.4608030915260315,
      "learning_rate": 0.00017146028525652856,
      "loss": 0.6411,
      "step": 8530
    },
    {
      "epoch": 0.5391414141414141,
      "grad_norm": 0.5494821667671204,
      "learning_rate": 0.0001713887311307275,
      "loss": 0.5988,
      "step": 8540
    },
    {
      "epoch": 0.5397727272727273,
      "grad_norm": 0.7463109493255615,
      "learning_rate": 0.00017131710238883164,
      "loss": 0.573,
      "step": 8550
    },
    {
      "epoch": 0.5404040404040404,
      "grad_norm": 0.42370057106018066,
      "learning_rate": 0.00017124539910570792,
      "loss": 0.8281,
      "step": 8560
    },
    {
      "epoch": 0.5410353535353535,
      "grad_norm": 0.4068855941295624,
      "learning_rate": 0.0001711736213563012,
      "loss": 0.6766,
      "step": 8570
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.4438983201980591,
      "learning_rate": 0.00017110176921563425,
      "loss": 0.6401,
      "step": 8580
    },
    {
      "epoch": 0.5422979797979798,
      "grad_norm": 0.44434985518455505,
      "learning_rate": 0.00017102984275880746,
      "loss": 0.5439,
      "step": 8590
    },
    {
      "epoch": 0.5429292929292929,
      "grad_norm": 0.7656099200248718,
      "learning_rate": 0.00017095784206099896,
      "loss": 0.6012,
      "step": 8600
    },
    {
      "epoch": 0.5435606060606061,
      "grad_norm": 0.36172300577163696,
      "learning_rate": 0.00017088576719746453,
      "loss": 0.87,
      "step": 8610
    },
    {
      "epoch": 0.5441919191919192,
      "grad_norm": 0.37410056591033936,
      "learning_rate": 0.00017081361824353736,
      "loss": 0.7247,
      "step": 8620
    },
    {
      "epoch": 0.5448232323232324,
      "grad_norm": 0.4853702187538147,
      "learning_rate": 0.00017074139527462818,
      "loss": 0.6294,
      "step": 8630
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.44500648975372314,
      "learning_rate": 0.000170669098366225,
      "loss": 0.5284,
      "step": 8640
    },
    {
      "epoch": 0.5460858585858586,
      "grad_norm": 0.7713720798492432,
      "learning_rate": 0.00017059672759389317,
      "loss": 0.5893,
      "step": 8650
    },
    {
      "epoch": 0.5467171717171717,
      "grad_norm": 0.37531596422195435,
      "learning_rate": 0.0001705242830332752,
      "loss": 0.874,
      "step": 8660
    },
    {
      "epoch": 0.5473484848484849,
      "grad_norm": 0.444714218378067,
      "learning_rate": 0.00017045176476009074,
      "loss": 0.7174,
      "step": 8670
    },
    {
      "epoch": 0.547979797979798,
      "grad_norm": 0.44192954897880554,
      "learning_rate": 0.00017037917285013654,
      "loss": 0.5995,
      "step": 8680
    },
    {
      "epoch": 0.5486111111111112,
      "grad_norm": 0.4665365219116211,
      "learning_rate": 0.00017030650737928627,
      "loss": 0.534,
      "step": 8690
    },
    {
      "epoch": 0.5492424242424242,
      "grad_norm": 0.6556959748268127,
      "learning_rate": 0.00017023376842349041,
      "loss": 0.5952,
      "step": 8700
    },
    {
      "epoch": 0.5498737373737373,
      "grad_norm": 0.38512519001960754,
      "learning_rate": 0.00017016095605877637,
      "loss": 0.8602,
      "step": 8710
    },
    {
      "epoch": 0.5505050505050505,
      "grad_norm": 0.39655575156211853,
      "learning_rate": 0.00017008807036124828,
      "loss": 0.6856,
      "step": 8720
    },
    {
      "epoch": 0.5511363636363636,
      "grad_norm": 0.4728635251522064,
      "learning_rate": 0.0001700151114070868,
      "loss": 0.6258,
      "step": 8730
    },
    {
      "epoch": 0.5517676767676768,
      "grad_norm": 0.4865548312664032,
      "learning_rate": 0.00016994207927254924,
      "loss": 0.5667,
      "step": 8740
    },
    {
      "epoch": 0.55239898989899,
      "grad_norm": 0.6710991859436035,
      "learning_rate": 0.00016986897403396944,
      "loss": 0.5661,
      "step": 8750
    },
    {
      "epoch": 0.553030303030303,
      "grad_norm": 0.38755089044570923,
      "learning_rate": 0.00016979579576775758,
      "loss": 0.8845,
      "step": 8760
    },
    {
      "epoch": 0.5536616161616161,
      "grad_norm": 0.43291619420051575,
      "learning_rate": 0.00016972254455040021,
      "loss": 0.6992,
      "step": 8770
    },
    {
      "epoch": 0.5542929292929293,
      "grad_norm": 0.4624379575252533,
      "learning_rate": 0.0001696492204584601,
      "loss": 0.6199,
      "step": 8780
    },
    {
      "epoch": 0.5549242424242424,
      "grad_norm": 0.4865303933620453,
      "learning_rate": 0.00016957582356857617,
      "loss": 0.55,
      "step": 8790
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.7246121764183044,
      "learning_rate": 0.00016950235395746347,
      "loss": 0.5716,
      "step": 8800
    },
    {
      "epoch": 0.5561868686868687,
      "grad_norm": 0.3934378921985626,
      "learning_rate": 0.0001694288117019131,
      "loss": 0.8293,
      "step": 8810
    },
    {
      "epoch": 0.5568181818181818,
      "grad_norm": 0.4022543132305145,
      "learning_rate": 0.000169355196878792,
      "loss": 0.7437,
      "step": 8820
    },
    {
      "epoch": 0.5574494949494949,
      "grad_norm": 0.4361529052257538,
      "learning_rate": 0.00016928150956504293,
      "loss": 0.6198,
      "step": 8830
    },
    {
      "epoch": 0.5580808080808081,
      "grad_norm": 0.49470070004463196,
      "learning_rate": 0.0001692077498376846,
      "loss": 0.587,
      "step": 8840
    },
    {
      "epoch": 0.5587121212121212,
      "grad_norm": 0.6698590517044067,
      "learning_rate": 0.00016913391777381124,
      "loss": 0.5899,
      "step": 8850
    },
    {
      "epoch": 0.5593434343434344,
      "grad_norm": 0.39969074726104736,
      "learning_rate": 0.00016906001345059273,
      "loss": 0.8252,
      "step": 8860
    },
    {
      "epoch": 0.5599747474747475,
      "grad_norm": 0.42836111783981323,
      "learning_rate": 0.00016898603694527443,
      "loss": 0.7109,
      "step": 8870
    },
    {
      "epoch": 0.5606060606060606,
      "grad_norm": 0.46367958188056946,
      "learning_rate": 0.00016891198833517729,
      "loss": 0.6047,
      "step": 8880
    },
    {
      "epoch": 0.5612373737373737,
      "grad_norm": 0.43841078877449036,
      "learning_rate": 0.00016883786769769752,
      "loss": 0.5716,
      "step": 8890
    },
    {
      "epoch": 0.5618686868686869,
      "grad_norm": 0.816278338432312,
      "learning_rate": 0.00016876367511030655,
      "loss": 0.5903,
      "step": 8900
    },
    {
      "epoch": 0.5625,
      "grad_norm": 0.40783360600471497,
      "learning_rate": 0.00016868941065055116,
      "loss": 0.8573,
      "step": 8910
    },
    {
      "epoch": 0.5631313131313131,
      "grad_norm": 0.39151227474212646,
      "learning_rate": 0.00016861507439605317,
      "loss": 0.7076,
      "step": 8920
    },
    {
      "epoch": 0.5637626262626263,
      "grad_norm": 0.4289484918117523,
      "learning_rate": 0.00016854066642450942,
      "loss": 0.6378,
      "step": 8930
    },
    {
      "epoch": 0.5643939393939394,
      "grad_norm": 0.4645409882068634,
      "learning_rate": 0.00016846618681369178,
      "loss": 0.5673,
      "step": 8940
    },
    {
      "epoch": 0.5650252525252525,
      "grad_norm": 0.6585806012153625,
      "learning_rate": 0.00016839163564144694,
      "loss": 0.5717,
      "step": 8950
    },
    {
      "epoch": 0.5656565656565656,
      "grad_norm": 0.3913829028606415,
      "learning_rate": 0.0001683170129856964,
      "loss": 0.8564,
      "step": 8960
    },
    {
      "epoch": 0.5662878787878788,
      "grad_norm": 0.40859970450401306,
      "learning_rate": 0.00016824231892443635,
      "loss": 0.7244,
      "step": 8970
    },
    {
      "epoch": 0.5669191919191919,
      "grad_norm": 0.4570856988430023,
      "learning_rate": 0.0001681675535357377,
      "loss": 0.6076,
      "step": 8980
    },
    {
      "epoch": 0.5675505050505051,
      "grad_norm": 0.5008096098899841,
      "learning_rate": 0.00016809271689774584,
      "loss": 0.5428,
      "step": 8990
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 0.6988887190818787,
      "learning_rate": 0.00016801780908868062,
      "loss": 0.5931,
      "step": 9000
    },
    {
      "epoch": 0.5681818181818182,
      "eval_loss": 0.6796126365661621,
      "eval_runtime": 31.8234,
      "eval_samples_per_second": 80.444,
      "eval_steps_per_second": 10.055,
      "step": 9000
    },
    {
      "epoch": 0.5688131313131313,
      "grad_norm": 0.4009682238101959,
      "learning_rate": 0.00016794283018683632,
      "loss": 0.8652,
      "step": 9010
    },
    {
      "epoch": 0.5694444444444444,
      "grad_norm": 0.43925949931144714,
      "learning_rate": 0.00016786778027058153,
      "loss": 0.701,
      "step": 9020
    },
    {
      "epoch": 0.5700757575757576,
      "grad_norm": 0.46836549043655396,
      "learning_rate": 0.00016779265941835897,
      "loss": 0.6577,
      "step": 9030
    },
    {
      "epoch": 0.5707070707070707,
      "grad_norm": 0.5597358345985413,
      "learning_rate": 0.00016771746770868567,
      "loss": 0.5915,
      "step": 9040
    },
    {
      "epoch": 0.5713383838383839,
      "grad_norm": 0.8587664365768433,
      "learning_rate": 0.00016764220522015263,
      "loss": 0.6104,
      "step": 9050
    },
    {
      "epoch": 0.571969696969697,
      "grad_norm": 0.3842219412326813,
      "learning_rate": 0.0001675668720314248,
      "loss": 0.9049,
      "step": 9060
    },
    {
      "epoch": 0.57260101010101,
      "grad_norm": 0.4339574873447418,
      "learning_rate": 0.00016749146822124097,
      "loss": 0.7213,
      "step": 9070
    },
    {
      "epoch": 0.5732323232323232,
      "grad_norm": 0.4239516258239746,
      "learning_rate": 0.00016741599386841397,
      "loss": 0.6219,
      "step": 9080
    },
    {
      "epoch": 0.5738636363636364,
      "grad_norm": 0.4742462635040283,
      "learning_rate": 0.00016734044905183012,
      "loss": 0.5736,
      "step": 9090
    },
    {
      "epoch": 0.5744949494949495,
      "grad_norm": 0.8710987567901611,
      "learning_rate": 0.00016726483385044958,
      "loss": 0.6,
      "step": 9100
    },
    {
      "epoch": 0.5751262626262627,
      "grad_norm": 0.41594424843788147,
      "learning_rate": 0.0001671891483433059,
      "loss": 0.872,
      "step": 9110
    },
    {
      "epoch": 0.5757575757575758,
      "grad_norm": 0.3817179799079895,
      "learning_rate": 0.00016711339260950622,
      "loss": 0.6989,
      "step": 9120
    },
    {
      "epoch": 0.5763888888888888,
      "grad_norm": 0.4446948170661926,
      "learning_rate": 0.0001670375667282311,
      "loss": 0.6094,
      "step": 9130
    },
    {
      "epoch": 0.577020202020202,
      "grad_norm": 0.4311649799346924,
      "learning_rate": 0.00016696167077873435,
      "loss": 0.5363,
      "step": 9140
    },
    {
      "epoch": 0.5776515151515151,
      "grad_norm": 0.6595962643623352,
      "learning_rate": 0.00016688570484034307,
      "loss": 0.6049,
      "step": 9150
    },
    {
      "epoch": 0.5782828282828283,
      "grad_norm": 0.3813289701938629,
      "learning_rate": 0.00016680966899245748,
      "loss": 0.8614,
      "step": 9160
    },
    {
      "epoch": 0.5789141414141414,
      "grad_norm": 0.44657930731773376,
      "learning_rate": 0.00016673356331455084,
      "loss": 0.7243,
      "step": 9170
    },
    {
      "epoch": 0.5795454545454546,
      "grad_norm": 0.4876100420951843,
      "learning_rate": 0.00016665738788616946,
      "loss": 0.6292,
      "step": 9180
    },
    {
      "epoch": 0.5801767676767676,
      "grad_norm": 0.4932308495044708,
      "learning_rate": 0.0001665811427869326,
      "loss": 0.5547,
      "step": 9190
    },
    {
      "epoch": 0.5808080808080808,
      "grad_norm": 0.7451329231262207,
      "learning_rate": 0.00016650482809653217,
      "loss": 0.5944,
      "step": 9200
    },
    {
      "epoch": 0.5814393939393939,
      "grad_norm": 0.41353994607925415,
      "learning_rate": 0.000166428443894733,
      "loss": 0.8368,
      "step": 9210
    },
    {
      "epoch": 0.5820707070707071,
      "grad_norm": 0.431100070476532,
      "learning_rate": 0.00016635199026137243,
      "loss": 0.6793,
      "step": 9220
    },
    {
      "epoch": 0.5827020202020202,
      "grad_norm": 0.49128299951553345,
      "learning_rate": 0.00016627546727636044,
      "loss": 0.6284,
      "step": 9230
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.47215157747268677,
      "learning_rate": 0.00016619887501967954,
      "loss": 0.5505,
      "step": 9240
    },
    {
      "epoch": 0.5839646464646465,
      "grad_norm": 0.6621851325035095,
      "learning_rate": 0.00016612221357138453,
      "loss": 0.5617,
      "step": 9250
    },
    {
      "epoch": 0.5845959595959596,
      "grad_norm": 0.4210949242115021,
      "learning_rate": 0.00016604548301160264,
      "loss": 0.8326,
      "step": 9260
    },
    {
      "epoch": 0.5852272727272727,
      "grad_norm": 0.4298464357852936,
      "learning_rate": 0.00016596868342053325,
      "loss": 0.7182,
      "step": 9270
    },
    {
      "epoch": 0.5858585858585859,
      "grad_norm": 0.42421016097068787,
      "learning_rate": 0.000165891814878448,
      "loss": 0.6076,
      "step": 9280
    },
    {
      "epoch": 0.586489898989899,
      "grad_norm": 0.45643123984336853,
      "learning_rate": 0.00016581487746569043,
      "loss": 0.5581,
      "step": 9290
    },
    {
      "epoch": 0.5871212121212122,
      "grad_norm": 0.7897077798843384,
      "learning_rate": 0.0001657378712626762,
      "loss": 0.5727,
      "step": 9300
    },
    {
      "epoch": 0.5877525252525253,
      "grad_norm": 0.3880464434623718,
      "learning_rate": 0.00016566079634989289,
      "loss": 0.8618,
      "step": 9310
    },
    {
      "epoch": 0.5883838383838383,
      "grad_norm": 0.4071453809738159,
      "learning_rate": 0.00016558365280789977,
      "loss": 0.6773,
      "step": 9320
    },
    {
      "epoch": 0.5890151515151515,
      "grad_norm": 0.49148842692375183,
      "learning_rate": 0.00016550644071732794,
      "loss": 0.6098,
      "step": 9330
    },
    {
      "epoch": 0.5896464646464646,
      "grad_norm": 0.479719340801239,
      "learning_rate": 0.0001654291601588801,
      "loss": 0.5559,
      "step": 9340
    },
    {
      "epoch": 0.5902777777777778,
      "grad_norm": 0.6978563070297241,
      "learning_rate": 0.00016535181121333058,
      "loss": 0.542,
      "step": 9350
    },
    {
      "epoch": 0.5909090909090909,
      "grad_norm": 0.40642011165618896,
      "learning_rate": 0.00016527439396152508,
      "loss": 0.8565,
      "step": 9360
    },
    {
      "epoch": 0.5915404040404041,
      "grad_norm": 0.4633709490299225,
      "learning_rate": 0.0001651969084843808,
      "loss": 0.7209,
      "step": 9370
    },
    {
      "epoch": 0.5921717171717171,
      "grad_norm": 0.46379798650741577,
      "learning_rate": 0.00016511935486288618,
      "loss": 0.6198,
      "step": 9380
    },
    {
      "epoch": 0.5928030303030303,
      "grad_norm": 0.5171828866004944,
      "learning_rate": 0.0001650417331781009,
      "loss": 0.5337,
      "step": 9390
    },
    {
      "epoch": 0.5934343434343434,
      "grad_norm": 0.7303241491317749,
      "learning_rate": 0.0001649640435111558,
      "loss": 0.554,
      "step": 9400
    },
    {
      "epoch": 0.5940656565656566,
      "grad_norm": 0.4006171226501465,
      "learning_rate": 0.00016488628594325277,
      "loss": 0.8726,
      "step": 9410
    },
    {
      "epoch": 0.5946969696969697,
      "grad_norm": 0.4310572147369385,
      "learning_rate": 0.0001648084605556647,
      "loss": 0.7186,
      "step": 9420
    },
    {
      "epoch": 0.5953282828282829,
      "grad_norm": 0.46270427107810974,
      "learning_rate": 0.00016473056742973526,
      "loss": 0.6068,
      "step": 9430
    },
    {
      "epoch": 0.5959595959595959,
      "grad_norm": 0.49812617897987366,
      "learning_rate": 0.00016465260664687902,
      "loss": 0.5817,
      "step": 9440
    },
    {
      "epoch": 0.5965909090909091,
      "grad_norm": 0.7427959442138672,
      "learning_rate": 0.0001645745782885813,
      "loss": 0.584,
      "step": 9450
    },
    {
      "epoch": 0.5972222222222222,
      "grad_norm": 0.3949665129184723,
      "learning_rate": 0.00016449648243639788,
      "loss": 0.893,
      "step": 9460
    },
    {
      "epoch": 0.5978535353535354,
      "grad_norm": 0.4785396158695221,
      "learning_rate": 0.0001644183191719553,
      "loss": 0.7311,
      "step": 9470
    },
    {
      "epoch": 0.5984848484848485,
      "grad_norm": 0.4292181730270386,
      "learning_rate": 0.00016434008857695037,
      "loss": 0.6186,
      "step": 9480
    },
    {
      "epoch": 0.5991161616161617,
      "grad_norm": 0.52422034740448,
      "learning_rate": 0.0001642617907331504,
      "loss": 0.5491,
      "step": 9490
    },
    {
      "epoch": 0.5997474747474747,
      "grad_norm": 0.6773059368133545,
      "learning_rate": 0.00016418342572239292,
      "loss": 0.5922,
      "step": 9500
    },
    {
      "epoch": 0.6003787878787878,
      "grad_norm": 0.39980462193489075,
      "learning_rate": 0.0001641049936265857,
      "loss": 0.8245,
      "step": 9510
    },
    {
      "epoch": 0.601010101010101,
      "grad_norm": 0.41937774419784546,
      "learning_rate": 0.00016402649452770666,
      "loss": 0.7019,
      "step": 9520
    },
    {
      "epoch": 0.6016414141414141,
      "grad_norm": 0.4275891184806824,
      "learning_rate": 0.00016394792850780364,
      "loss": 0.5972,
      "step": 9530
    },
    {
      "epoch": 0.6022727272727273,
      "grad_norm": 0.4829592704772949,
      "learning_rate": 0.00016386929564899457,
      "loss": 0.5372,
      "step": 9540
    },
    {
      "epoch": 0.6029040404040404,
      "grad_norm": 0.7352997064590454,
      "learning_rate": 0.0001637905960334671,
      "loss": 0.5801,
      "step": 9550
    },
    {
      "epoch": 0.6035353535353535,
      "grad_norm": 0.40179598331451416,
      "learning_rate": 0.00016371182974347876,
      "loss": 0.8228,
      "step": 9560
    },
    {
      "epoch": 0.6041666666666666,
      "grad_norm": 0.43279874324798584,
      "learning_rate": 0.0001636329968613567,
      "loss": 0.7145,
      "step": 9570
    },
    {
      "epoch": 0.6047979797979798,
      "grad_norm": 0.4343333840370178,
      "learning_rate": 0.00016355409746949778,
      "loss": 0.6109,
      "step": 9580
    },
    {
      "epoch": 0.6054292929292929,
      "grad_norm": 0.4863261878490448,
      "learning_rate": 0.0001634751316503682,
      "loss": 0.5481,
      "step": 9590
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 0.7001116871833801,
      "learning_rate": 0.00016339609948650376,
      "loss": 0.5509,
      "step": 9600
    },
    {
      "epoch": 0.6066919191919192,
      "grad_norm": 0.41055071353912354,
      "learning_rate": 0.0001633170010605095,
      "loss": 0.8436,
      "step": 9610
    },
    {
      "epoch": 0.6073232323232324,
      "grad_norm": 0.4107693135738373,
      "learning_rate": 0.00016323783645505975,
      "loss": 0.6966,
      "step": 9620
    },
    {
      "epoch": 0.6079545454545454,
      "grad_norm": 0.4708723723888397,
      "learning_rate": 0.000163158605752898,
      "loss": 0.6005,
      "step": 9630
    },
    {
      "epoch": 0.6085858585858586,
      "grad_norm": 0.5112484693527222,
      "learning_rate": 0.0001630793090368369,
      "loss": 0.5534,
      "step": 9640
    },
    {
      "epoch": 0.6092171717171717,
      "grad_norm": 0.7161701917648315,
      "learning_rate": 0.00016299994638975797,
      "loss": 0.6017,
      "step": 9650
    },
    {
      "epoch": 0.6098484848484849,
      "grad_norm": 0.3851141035556793,
      "learning_rate": 0.0001629205178946118,
      "loss": 0.843,
      "step": 9660
    },
    {
      "epoch": 0.610479797979798,
      "grad_norm": 0.41493353247642517,
      "learning_rate": 0.00016284102363441758,
      "loss": 0.6943,
      "step": 9670
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.44516682624816895,
      "learning_rate": 0.00016276146369226347,
      "loss": 0.6062,
      "step": 9680
    },
    {
      "epoch": 0.6117424242424242,
      "grad_norm": 0.5227181911468506,
      "learning_rate": 0.00016268183815130614,
      "loss": 0.5574,
      "step": 9690
    },
    {
      "epoch": 0.6123737373737373,
      "grad_norm": 0.7684870362281799,
      "learning_rate": 0.00016260214709477088,
      "loss": 0.5445,
      "step": 9700
    },
    {
      "epoch": 0.6130050505050505,
      "grad_norm": 0.43409523367881775,
      "learning_rate": 0.00016252239060595146,
      "loss": 0.8413,
      "step": 9710
    },
    {
      "epoch": 0.6136363636363636,
      "grad_norm": 0.41207340359687805,
      "learning_rate": 0.00016244256876821,
      "loss": 0.7261,
      "step": 9720
    },
    {
      "epoch": 0.6142676767676768,
      "grad_norm": 0.419304221868515,
      "learning_rate": 0.00016236268166497695,
      "loss": 0.604,
      "step": 9730
    },
    {
      "epoch": 0.61489898989899,
      "grad_norm": 0.5061937570571899,
      "learning_rate": 0.000162282729379751,
      "loss": 0.5782,
      "step": 9740
    },
    {
      "epoch": 0.615530303030303,
      "grad_norm": 0.5814034342765808,
      "learning_rate": 0.0001622027119960989,
      "loss": 0.5648,
      "step": 9750
    },
    {
      "epoch": 0.6161616161616161,
      "grad_norm": 0.3868893086910248,
      "learning_rate": 0.0001621226295976555,
      "loss": 0.8397,
      "step": 9760
    },
    {
      "epoch": 0.6167929292929293,
      "grad_norm": 0.39275869727134705,
      "learning_rate": 0.00016204248226812365,
      "loss": 0.7117,
      "step": 9770
    },
    {
      "epoch": 0.6174242424242424,
      "grad_norm": 0.4420825242996216,
      "learning_rate": 0.0001619622700912739,
      "loss": 0.6142,
      "step": 9780
    },
    {
      "epoch": 0.6180555555555556,
      "grad_norm": 0.41085076332092285,
      "learning_rate": 0.00016188199315094473,
      "loss": 0.5491,
      "step": 9790
    },
    {
      "epoch": 0.6186868686868687,
      "grad_norm": 0.6806663870811462,
      "learning_rate": 0.0001618016515310423,
      "loss": 0.5427,
      "step": 9800
    },
    {
      "epoch": 0.6193181818181818,
      "grad_norm": 0.3867488503456116,
      "learning_rate": 0.0001617212453155403,
      "loss": 0.8668,
      "step": 9810
    },
    {
      "epoch": 0.6199494949494949,
      "grad_norm": 0.4313066899776459,
      "learning_rate": 0.00016164077458847995,
      "loss": 0.6893,
      "step": 9820
    },
    {
      "epoch": 0.6205808080808081,
      "grad_norm": 0.4171628952026367,
      "learning_rate": 0.00016156023943396998,
      "loss": 0.5914,
      "step": 9830
    },
    {
      "epoch": 0.6212121212121212,
      "grad_norm": 0.46197620034217834,
      "learning_rate": 0.0001614796399361864,
      "loss": 0.5747,
      "step": 9840
    },
    {
      "epoch": 0.6218434343434344,
      "grad_norm": 0.708810567855835,
      "learning_rate": 0.00016139897617937238,
      "loss": 0.5941,
      "step": 9850
    },
    {
      "epoch": 0.6224747474747475,
      "grad_norm": 0.44061851501464844,
      "learning_rate": 0.00016131824824783847,
      "loss": 0.8606,
      "step": 9860
    },
    {
      "epoch": 0.6231060606060606,
      "grad_norm": 0.44626912474632263,
      "learning_rate": 0.00016123745622596212,
      "loss": 0.7018,
      "step": 9870
    },
    {
      "epoch": 0.6237373737373737,
      "grad_norm": 0.4255472719669342,
      "learning_rate": 0.0001611566001981878,
      "loss": 0.6028,
      "step": 9880
    },
    {
      "epoch": 0.6243686868686869,
      "grad_norm": 0.489614874124527,
      "learning_rate": 0.00016107568024902697,
      "loss": 0.546,
      "step": 9890
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.7167721390724182,
      "learning_rate": 0.00016099469646305777,
      "loss": 0.588,
      "step": 9900
    },
    {
      "epoch": 0.6256313131313131,
      "grad_norm": 0.38225969672203064,
      "learning_rate": 0.00016091364892492516,
      "loss": 0.8317,
      "step": 9910
    },
    {
      "epoch": 0.6262626262626263,
      "grad_norm": 0.4190126061439514,
      "learning_rate": 0.00016083253771934065,
      "loss": 0.6826,
      "step": 9920
    },
    {
      "epoch": 0.6268939393939394,
      "grad_norm": 0.491161048412323,
      "learning_rate": 0.00016075136293108245,
      "loss": 0.6347,
      "step": 9930
    },
    {
      "epoch": 0.6275252525252525,
      "grad_norm": 0.48111626505851746,
      "learning_rate": 0.000160670124644995,
      "loss": 0.5834,
      "step": 9940
    },
    {
      "epoch": 0.6281565656565656,
      "grad_norm": 0.7762089371681213,
      "learning_rate": 0.00016058882294598932,
      "loss": 0.5598,
      "step": 9950
    },
    {
      "epoch": 0.6287878787878788,
      "grad_norm": 0.38790324330329895,
      "learning_rate": 0.00016050745791904256,
      "loss": 0.8563,
      "step": 9960
    },
    {
      "epoch": 0.6294191919191919,
      "grad_norm": 0.40825584530830383,
      "learning_rate": 0.00016042602964919816,
      "loss": 0.7024,
      "step": 9970
    },
    {
      "epoch": 0.6300505050505051,
      "grad_norm": 0.4360995292663574,
      "learning_rate": 0.0001603445382215656,
      "loss": 0.616,
      "step": 9980
    },
    {
      "epoch": 0.6306818181818182,
      "grad_norm": 0.4249142110347748,
      "learning_rate": 0.00016026298372132046,
      "loss": 0.5342,
      "step": 9990
    },
    {
      "epoch": 0.6313131313131313,
      "grad_norm": 0.7646644711494446,
      "learning_rate": 0.00016018136623370408,
      "loss": 0.5725,
      "step": 10000
    },
    {
      "epoch": 0.6313131313131313,
      "eval_loss": 0.6743065118789673,
      "eval_runtime": 31.7128,
      "eval_samples_per_second": 80.724,
      "eval_steps_per_second": 10.091,
      "step": 10000
    },
    {
      "epoch": 0.6319444444444444,
      "grad_norm": 0.42191603779792786,
      "learning_rate": 0.00016009968584402383,
      "loss": 0.8065,
      "step": 10010
    },
    {
      "epoch": 0.6325757575757576,
      "grad_norm": 0.4238017797470093,
      "learning_rate": 0.00016001794263765265,
      "loss": 0.7196,
      "step": 10020
    },
    {
      "epoch": 0.6332070707070707,
      "grad_norm": 0.4852093756198883,
      "learning_rate": 0.0001599361367000293,
      "loss": 0.6413,
      "step": 10030
    },
    {
      "epoch": 0.6338383838383839,
      "grad_norm": 0.503449559211731,
      "learning_rate": 0.000159854268116658,
      "loss": 0.5468,
      "step": 10040
    },
    {
      "epoch": 0.634469696969697,
      "grad_norm": 0.8369807004928589,
      "learning_rate": 0.00015977233697310838,
      "loss": 0.5549,
      "step": 10050
    },
    {
      "epoch": 0.63510101010101,
      "grad_norm": 0.41767174005508423,
      "learning_rate": 0.00015969034335501568,
      "loss": 0.8185,
      "step": 10060
    },
    {
      "epoch": 0.6357323232323232,
      "grad_norm": 0.4306197166442871,
      "learning_rate": 0.00015960828734808027,
      "loss": 0.7277,
      "step": 10070
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.43360432982444763,
      "learning_rate": 0.00015952616903806775,
      "loss": 0.6061,
      "step": 10080
    },
    {
      "epoch": 0.6369949494949495,
      "grad_norm": 0.4999752938747406,
      "learning_rate": 0.00015944398851080885,
      "loss": 0.5788,
      "step": 10090
    },
    {
      "epoch": 0.6376262626262627,
      "grad_norm": 0.731171727180481,
      "learning_rate": 0.00015936174585219937,
      "loss": 0.5604,
      "step": 10100
    },
    {
      "epoch": 0.6382575757575758,
      "grad_norm": 0.4045703411102295,
      "learning_rate": 0.00015927944114820005,
      "loss": 0.8091,
      "step": 10110
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 0.4213419258594513,
      "learning_rate": 0.00015919707448483638,
      "loss": 0.7064,
      "step": 10120
    },
    {
      "epoch": 0.639520202020202,
      "grad_norm": 0.4091891050338745,
      "learning_rate": 0.0001591146459481987,
      "loss": 0.633,
      "step": 10130
    },
    {
      "epoch": 0.6401515151515151,
      "grad_norm": 0.47606709599494934,
      "learning_rate": 0.00015903215562444202,
      "loss": 0.5587,
      "step": 10140
    },
    {
      "epoch": 0.6407828282828283,
      "grad_norm": 0.6779358983039856,
      "learning_rate": 0.00015894960359978593,
      "loss": 0.5731,
      "step": 10150
    },
    {
      "epoch": 0.6414141414141414,
      "grad_norm": 0.40254074335098267,
      "learning_rate": 0.00015886698996051446,
      "loss": 0.8609,
      "step": 10160
    },
    {
      "epoch": 0.6420454545454546,
      "grad_norm": 0.41049593687057495,
      "learning_rate": 0.00015878431479297603,
      "loss": 0.7039,
      "step": 10170
    },
    {
      "epoch": 0.6426767676767676,
      "grad_norm": 0.4572261869907379,
      "learning_rate": 0.0001587015781835835,
      "loss": 0.6274,
      "step": 10180
    },
    {
      "epoch": 0.6433080808080808,
      "grad_norm": 0.49836811423301697,
      "learning_rate": 0.00015861878021881384,
      "loss": 0.536,
      "step": 10190
    },
    {
      "epoch": 0.6439393939393939,
      "grad_norm": 0.7206619381904602,
      "learning_rate": 0.0001585359209852081,
      "loss": 0.5957,
      "step": 10200
    },
    {
      "epoch": 0.6445707070707071,
      "grad_norm": 0.4254686236381531,
      "learning_rate": 0.00015845300056937153,
      "loss": 0.8533,
      "step": 10210
    },
    {
      "epoch": 0.6452020202020202,
      "grad_norm": 0.42316338419914246,
      "learning_rate": 0.0001583700190579732,
      "loss": 0.706,
      "step": 10220
    },
    {
      "epoch": 0.6458333333333334,
      "grad_norm": 0.4535283148288727,
      "learning_rate": 0.00015828697653774606,
      "loss": 0.6261,
      "step": 10230
    },
    {
      "epoch": 0.6464646464646465,
      "grad_norm": 0.5090728998184204,
      "learning_rate": 0.00015820387309548685,
      "loss": 0.5815,
      "step": 10240
    },
    {
      "epoch": 0.6470959595959596,
      "grad_norm": 0.8286582231521606,
      "learning_rate": 0.00015812070881805602,
      "loss": 0.5566,
      "step": 10250
    },
    {
      "epoch": 0.6477272727272727,
      "grad_norm": 0.4053003787994385,
      "learning_rate": 0.00015803748379237747,
      "loss": 0.8181,
      "step": 10260
    },
    {
      "epoch": 0.6483585858585859,
      "grad_norm": 0.41847535967826843,
      "learning_rate": 0.00015795419810543882,
      "loss": 0.6898,
      "step": 10270
    },
    {
      "epoch": 0.648989898989899,
      "grad_norm": 0.44468438625335693,
      "learning_rate": 0.00015787085184429086,
      "loss": 0.5698,
      "step": 10280
    },
    {
      "epoch": 0.6496212121212122,
      "grad_norm": 0.49084755778312683,
      "learning_rate": 0.0001577874450960478,
      "loss": 0.5352,
      "step": 10290
    },
    {
      "epoch": 0.6502525252525253,
      "grad_norm": 0.8154090642929077,
      "learning_rate": 0.00015770397794788706,
      "loss": 0.5671,
      "step": 10300
    },
    {
      "epoch": 0.6508838383838383,
      "grad_norm": 0.37386414408683777,
      "learning_rate": 0.00015762045048704927,
      "loss": 0.8584,
      "step": 10310
    },
    {
      "epoch": 0.6515151515151515,
      "grad_norm": 0.41814225912094116,
      "learning_rate": 0.00015753686280083797,
      "loss": 0.6939,
      "step": 10320
    },
    {
      "epoch": 0.6521464646464646,
      "grad_norm": 0.4435232877731323,
      "learning_rate": 0.00015745321497661973,
      "loss": 0.5835,
      "step": 10330
    },
    {
      "epoch": 0.6527777777777778,
      "grad_norm": 0.5572220683097839,
      "learning_rate": 0.00015736950710182392,
      "loss": 0.5503,
      "step": 10340
    },
    {
      "epoch": 0.6534090909090909,
      "grad_norm": 0.7354696393013,
      "learning_rate": 0.00015728573926394271,
      "loss": 0.58,
      "step": 10350
    },
    {
      "epoch": 0.6540404040404041,
      "grad_norm": 0.40027880668640137,
      "learning_rate": 0.00015720191155053098,
      "loss": 0.8084,
      "step": 10360
    },
    {
      "epoch": 0.6546717171717171,
      "grad_norm": 0.38504400849342346,
      "learning_rate": 0.0001571180240492061,
      "loss": 0.7056,
      "step": 10370
    },
    {
      "epoch": 0.6553030303030303,
      "grad_norm": 0.4328770041465759,
      "learning_rate": 0.00015703407684764802,
      "loss": 0.6317,
      "step": 10380
    },
    {
      "epoch": 0.6559343434343434,
      "grad_norm": 0.47838786244392395,
      "learning_rate": 0.00015695007003359908,
      "loss": 0.5577,
      "step": 10390
    },
    {
      "epoch": 0.6565656565656566,
      "grad_norm": 0.6952661275863647,
      "learning_rate": 0.0001568660036948638,
      "loss": 0.578,
      "step": 10400
    },
    {
      "epoch": 0.6571969696969697,
      "grad_norm": 0.384345680475235,
      "learning_rate": 0.0001567818779193091,
      "loss": 0.8584,
      "step": 10410
    },
    {
      "epoch": 0.6578282828282829,
      "grad_norm": 0.4073186218738556,
      "learning_rate": 0.0001566976927948639,
      "loss": 0.6908,
      "step": 10420
    },
    {
      "epoch": 0.6584595959595959,
      "grad_norm": 0.430889755487442,
      "learning_rate": 0.0001566134484095192,
      "loss": 0.6487,
      "step": 10430
    },
    {
      "epoch": 0.6590909090909091,
      "grad_norm": 0.41858479380607605,
      "learning_rate": 0.00015652914485132796,
      "loss": 0.5345,
      "step": 10440
    },
    {
      "epoch": 0.6597222222222222,
      "grad_norm": 0.7019746899604797,
      "learning_rate": 0.00015644478220840492,
      "loss": 0.5599,
      "step": 10450
    },
    {
      "epoch": 0.6603535353535354,
      "grad_norm": 0.4269649386405945,
      "learning_rate": 0.00015636036056892663,
      "loss": 0.8473,
      "step": 10460
    },
    {
      "epoch": 0.6609848484848485,
      "grad_norm": 0.42092782258987427,
      "learning_rate": 0.00015627588002113134,
      "loss": 0.6987,
      "step": 10470
    },
    {
      "epoch": 0.6616161616161617,
      "grad_norm": 0.4388461112976074,
      "learning_rate": 0.00015619134065331873,
      "loss": 0.6049,
      "step": 10480
    },
    {
      "epoch": 0.6622474747474747,
      "grad_norm": 0.5200571417808533,
      "learning_rate": 0.0001561067425538501,
      "loss": 0.5353,
      "step": 10490
    },
    {
      "epoch": 0.6628787878787878,
      "grad_norm": 0.7956357002258301,
      "learning_rate": 0.00015602208581114808,
      "loss": 0.594,
      "step": 10500
    },
    {
      "epoch": 0.663510101010101,
      "grad_norm": 0.38890981674194336,
      "learning_rate": 0.00015593737051369655,
      "loss": 0.8152,
      "step": 10510
    },
    {
      "epoch": 0.6641414141414141,
      "grad_norm": 0.3969663381576538,
      "learning_rate": 0.00015585259675004076,
      "loss": 0.6992,
      "step": 10520
    },
    {
      "epoch": 0.6647727272727273,
      "grad_norm": 0.4716900885105133,
      "learning_rate": 0.00015576776460878686,
      "loss": 0.6187,
      "step": 10530
    },
    {
      "epoch": 0.6654040404040404,
      "grad_norm": 0.5017859935760498,
      "learning_rate": 0.0001556828741786021,
      "loss": 0.5552,
      "step": 10540
    },
    {
      "epoch": 0.6660353535353535,
      "grad_norm": 0.6798177361488342,
      "learning_rate": 0.00015559792554821472,
      "loss": 0.5941,
      "step": 10550
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.379591166973114,
      "learning_rate": 0.00015551291880641372,
      "loss": 0.8489,
      "step": 10560
    },
    {
      "epoch": 0.6672979797979798,
      "grad_norm": 0.4076208174228668,
      "learning_rate": 0.00015542785404204883,
      "loss": 0.6717,
      "step": 10570
    },
    {
      "epoch": 0.6679292929292929,
      "grad_norm": 0.44688910245895386,
      "learning_rate": 0.00015534273134403047,
      "loss": 0.6098,
      "step": 10580
    },
    {
      "epoch": 0.6685606060606061,
      "grad_norm": 0.45217257738113403,
      "learning_rate": 0.00015525755080132955,
      "loss": 0.5695,
      "step": 10590
    },
    {
      "epoch": 0.6691919191919192,
      "grad_norm": 0.7299561500549316,
      "learning_rate": 0.0001551723125029775,
      "loss": 0.5974,
      "step": 10600
    },
    {
      "epoch": 0.6698232323232324,
      "grad_norm": 0.3935202956199646,
      "learning_rate": 0.00015508701653806615,
      "loss": 0.82,
      "step": 10610
    },
    {
      "epoch": 0.6704545454545454,
      "grad_norm": 0.41423672437667847,
      "learning_rate": 0.0001550016629957475,
      "loss": 0.7022,
      "step": 10620
    },
    {
      "epoch": 0.6710858585858586,
      "grad_norm": 0.5076875686645508,
      "learning_rate": 0.0001549162519652338,
      "loss": 0.5976,
      "step": 10630
    },
    {
      "epoch": 0.6717171717171717,
      "grad_norm": 0.4930507242679596,
      "learning_rate": 0.00015483078353579734,
      "loss": 0.541,
      "step": 10640
    },
    {
      "epoch": 0.6723484848484849,
      "grad_norm": 0.8618254661560059,
      "learning_rate": 0.00015474525779677047,
      "loss": 0.5984,
      "step": 10650
    },
    {
      "epoch": 0.672979797979798,
      "grad_norm": 0.3795359432697296,
      "learning_rate": 0.00015465967483754538,
      "loss": 0.8771,
      "step": 10660
    },
    {
      "epoch": 0.6736111111111112,
      "grad_norm": 0.41606026887893677,
      "learning_rate": 0.00015457403474757405,
      "loss": 0.7071,
      "step": 10670
    },
    {
      "epoch": 0.6742424242424242,
      "grad_norm": 0.4859228730201721,
      "learning_rate": 0.0001544883376163683,
      "loss": 0.6545,
      "step": 10680
    },
    {
      "epoch": 0.6748737373737373,
      "grad_norm": 0.4534260332584381,
      "learning_rate": 0.00015440258353349945,
      "loss": 0.534,
      "step": 10690
    },
    {
      "epoch": 0.6755050505050505,
      "grad_norm": 0.7350413799285889,
      "learning_rate": 0.00015431677258859837,
      "loss": 0.57,
      "step": 10700
    },
    {
      "epoch": 0.6761363636363636,
      "grad_norm": 0.38950178027153015,
      "learning_rate": 0.00015423090487135535,
      "loss": 0.8182,
      "step": 10710
    },
    {
      "epoch": 0.6767676767676768,
      "grad_norm": 0.4159404933452606,
      "learning_rate": 0.00015414498047152007,
      "loss": 0.6903,
      "step": 10720
    },
    {
      "epoch": 0.67739898989899,
      "grad_norm": 0.4483141601085663,
      "learning_rate": 0.00015405899947890146,
      "loss": 0.6404,
      "step": 10730
    },
    {
      "epoch": 0.678030303030303,
      "grad_norm": 0.506531834602356,
      "learning_rate": 0.0001539729619833675,
      "loss": 0.5825,
      "step": 10740
    },
    {
      "epoch": 0.6786616161616161,
      "grad_norm": 0.7041772603988647,
      "learning_rate": 0.00015388686807484532,
      "loss": 0.5537,
      "step": 10750
    },
    {
      "epoch": 0.6792929292929293,
      "grad_norm": 0.4034171998500824,
      "learning_rate": 0.000153800717843321,
      "loss": 0.8273,
      "step": 10760
    },
    {
      "epoch": 0.6799242424242424,
      "grad_norm": 0.4018371105194092,
      "learning_rate": 0.00015371451137883945,
      "loss": 0.68,
      "step": 10770
    },
    {
      "epoch": 0.6805555555555556,
      "grad_norm": 0.46492621302604675,
      "learning_rate": 0.00015362824877150443,
      "loss": 0.6238,
      "step": 10780
    },
    {
      "epoch": 0.6811868686868687,
      "grad_norm": 0.47299566864967346,
      "learning_rate": 0.0001535419301114783,
      "loss": 0.5671,
      "step": 10790
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 0.7236783504486084,
      "learning_rate": 0.00015345555548898206,
      "loss": 0.5992,
      "step": 10800
    },
    {
      "epoch": 0.6824494949494949,
      "grad_norm": 0.3979905843734741,
      "learning_rate": 0.00015336912499429515,
      "loss": 0.8507,
      "step": 10810
    },
    {
      "epoch": 0.6830808080808081,
      "grad_norm": 0.4042966365814209,
      "learning_rate": 0.00015328263871775544,
      "loss": 0.7069,
      "step": 10820
    },
    {
      "epoch": 0.6837121212121212,
      "grad_norm": 0.4539998769760132,
      "learning_rate": 0.0001531960967497592,
      "loss": 0.6036,
      "step": 10830
    },
    {
      "epoch": 0.6843434343434344,
      "grad_norm": 0.48568177223205566,
      "learning_rate": 0.00015310949918076068,
      "loss": 0.556,
      "step": 10840
    },
    {
      "epoch": 0.6849747474747475,
      "grad_norm": 0.7200063467025757,
      "learning_rate": 0.00015302284610127246,
      "loss": 0.5606,
      "step": 10850
    },
    {
      "epoch": 0.6856060606060606,
      "grad_norm": 0.42332252860069275,
      "learning_rate": 0.00015293613760186502,
      "loss": 0.7645,
      "step": 10860
    },
    {
      "epoch": 0.6862373737373737,
      "grad_norm": 0.41520920395851135,
      "learning_rate": 0.00015284937377316684,
      "loss": 0.6701,
      "step": 10870
    },
    {
      "epoch": 0.6868686868686869,
      "grad_norm": 0.4295734763145447,
      "learning_rate": 0.00015276255470586417,
      "loss": 0.6175,
      "step": 10880
    },
    {
      "epoch": 0.6875,
      "grad_norm": 0.4375039041042328,
      "learning_rate": 0.00015267568049070102,
      "loss": 0.503,
      "step": 10890
    },
    {
      "epoch": 0.6881313131313131,
      "grad_norm": 0.629347562789917,
      "learning_rate": 0.00015258875121847902,
      "loss": 0.5732,
      "step": 10900
    },
    {
      "epoch": 0.6887626262626263,
      "grad_norm": 0.3747410774230957,
      "learning_rate": 0.00015250176698005744,
      "loss": 0.8033,
      "step": 10910
    },
    {
      "epoch": 0.6893939393939394,
      "grad_norm": 0.4021148085594177,
      "learning_rate": 0.00015241472786635288,
      "loss": 0.724,
      "step": 10920
    },
    {
      "epoch": 0.6900252525252525,
      "grad_norm": 0.4595926105976105,
      "learning_rate": 0.0001523276339683393,
      "loss": 0.6355,
      "step": 10930
    },
    {
      "epoch": 0.6906565656565656,
      "grad_norm": 0.44848868250846863,
      "learning_rate": 0.0001522404853770481,
      "loss": 0.5429,
      "step": 10940
    },
    {
      "epoch": 0.6912878787878788,
      "grad_norm": 0.7155193090438843,
      "learning_rate": 0.00015215328218356756,
      "loss": 0.5678,
      "step": 10950
    },
    {
      "epoch": 0.6919191919191919,
      "grad_norm": 0.40425750613212585,
      "learning_rate": 0.00015206602447904327,
      "loss": 0.8145,
      "step": 10960
    },
    {
      "epoch": 0.6925505050505051,
      "grad_norm": 0.41528192162513733,
      "learning_rate": 0.00015197871235467765,
      "loss": 0.6604,
      "step": 10970
    },
    {
      "epoch": 0.6931818181818182,
      "grad_norm": 0.499453604221344,
      "learning_rate": 0.00015189134590173014,
      "loss": 0.6281,
      "step": 10980
    },
    {
      "epoch": 0.6938131313131313,
      "grad_norm": 0.5127883553504944,
      "learning_rate": 0.00015180392521151677,
      "loss": 0.5701,
      "step": 10990
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 0.7317492961883545,
      "learning_rate": 0.0001517164503754105,
      "loss": 0.5327,
      "step": 11000
    },
    {
      "epoch": 0.6944444444444444,
      "eval_loss": 0.6740230917930603,
      "eval_runtime": 31.7763,
      "eval_samples_per_second": 80.563,
      "eval_steps_per_second": 10.07,
      "step": 11000
    },
    {
      "epoch": 0.6950757575757576,
      "grad_norm": 0.37355682253837585,
      "learning_rate": 0.00015162892148484067,
      "loss": 0.8258,
      "step": 11010
    },
    {
      "epoch": 0.6957070707070707,
      "grad_norm": 0.4407990872859955,
      "learning_rate": 0.00015154133863129324,
      "loss": 0.6947,
      "step": 11020
    },
    {
      "epoch": 0.6963383838383839,
      "grad_norm": 0.4248354732990265,
      "learning_rate": 0.0001514537019063105,
      "loss": 0.627,
      "step": 11030
    },
    {
      "epoch": 0.696969696969697,
      "grad_norm": 0.46465909481048584,
      "learning_rate": 0.0001513660114014911,
      "loss": 0.5341,
      "step": 11040
    },
    {
      "epoch": 0.69760101010101,
      "grad_norm": 0.7854079604148865,
      "learning_rate": 0.00015127826720848995,
      "loss": 0.5742,
      "step": 11050
    },
    {
      "epoch": 0.6982323232323232,
      "grad_norm": 0.41912344098091125,
      "learning_rate": 0.00015119046941901787,
      "loss": 0.8386,
      "step": 11060
    },
    {
      "epoch": 0.6988636363636364,
      "grad_norm": 0.40627849102020264,
      "learning_rate": 0.00015110261812484197,
      "loss": 0.7155,
      "step": 11070
    },
    {
      "epoch": 0.6994949494949495,
      "grad_norm": 0.45331013202667236,
      "learning_rate": 0.00015101471341778507,
      "loss": 0.6281,
      "step": 11080
    },
    {
      "epoch": 0.7001262626262627,
      "grad_norm": 0.43650028109550476,
      "learning_rate": 0.0001509267553897259,
      "loss": 0.5146,
      "step": 11090
    },
    {
      "epoch": 0.7007575757575758,
      "grad_norm": 0.7089285254478455,
      "learning_rate": 0.000150838744132599,
      "loss": 0.5585,
      "step": 11100
    },
    {
      "epoch": 0.7013888888888888,
      "grad_norm": 0.39863675832748413,
      "learning_rate": 0.0001507506797383944,
      "loss": 0.8,
      "step": 11110
    },
    {
      "epoch": 0.702020202020202,
      "grad_norm": 0.41469570994377136,
      "learning_rate": 0.00015066256229915776,
      "loss": 0.7264,
      "step": 11120
    },
    {
      "epoch": 0.7026515151515151,
      "grad_norm": 0.46872881054878235,
      "learning_rate": 0.00015057439190699018,
      "loss": 0.6317,
      "step": 11130
    },
    {
      "epoch": 0.7032828282828283,
      "grad_norm": 0.4373462498188019,
      "learning_rate": 0.000150486168654048,
      "loss": 0.4996,
      "step": 11140
    },
    {
      "epoch": 0.7039141414141414,
      "grad_norm": 0.6925891041755676,
      "learning_rate": 0.00015039789263254303,
      "loss": 0.5589,
      "step": 11150
    },
    {
      "epoch": 0.7045454545454546,
      "grad_norm": 0.37480080127716064,
      "learning_rate": 0.00015030956393474198,
      "loss": 0.853,
      "step": 11160
    },
    {
      "epoch": 0.7051767676767676,
      "grad_norm": 0.4233093857765198,
      "learning_rate": 0.00015022118265296678,
      "loss": 0.7105,
      "step": 11170
    },
    {
      "epoch": 0.7058080808080808,
      "grad_norm": 0.4597311019897461,
      "learning_rate": 0.00015013274887959433,
      "loss": 0.5919,
      "step": 11180
    },
    {
      "epoch": 0.7064393939393939,
      "grad_norm": 0.4307326674461365,
      "learning_rate": 0.00015004426270705625,
      "loss": 0.5338,
      "step": 11190
    },
    {
      "epoch": 0.7070707070707071,
      "grad_norm": 0.7547082304954529,
      "learning_rate": 0.00014995572422783911,
      "loss": 0.5919,
      "step": 11200
    },
    {
      "epoch": 0.7077020202020202,
      "grad_norm": 0.3919515311717987,
      "learning_rate": 0.000149867133534484,
      "loss": 0.8564,
      "step": 11210
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.4222598671913147,
      "learning_rate": 0.0001497784907195867,
      "loss": 0.6936,
      "step": 11220
    },
    {
      "epoch": 0.7089646464646465,
      "grad_norm": 0.42735445499420166,
      "learning_rate": 0.00014968979587579736,
      "loss": 0.6108,
      "step": 11230
    },
    {
      "epoch": 0.7095959595959596,
      "grad_norm": 0.4462816119194031,
      "learning_rate": 0.0001496010490958206,
      "loss": 0.5521,
      "step": 11240
    },
    {
      "epoch": 0.7102272727272727,
      "grad_norm": 0.8036096096038818,
      "learning_rate": 0.00014951225047241535,
      "loss": 0.5884,
      "step": 11250
    },
    {
      "epoch": 0.7108585858585859,
      "grad_norm": 0.4196447730064392,
      "learning_rate": 0.00014942340009839458,
      "loss": 0.8611,
      "step": 11260
    },
    {
      "epoch": 0.711489898989899,
      "grad_norm": 0.39637133479118347,
      "learning_rate": 0.00014933449806662545,
      "loss": 0.6948,
      "step": 11270
    },
    {
      "epoch": 0.7121212121212122,
      "grad_norm": 0.4245402216911316,
      "learning_rate": 0.00014924554447002915,
      "loss": 0.5942,
      "step": 11280
    },
    {
      "epoch": 0.7127525252525253,
      "grad_norm": 0.46138742566108704,
      "learning_rate": 0.00014915653940158067,
      "loss": 0.5692,
      "step": 11290
    },
    {
      "epoch": 0.7133838383838383,
      "grad_norm": 0.7576135396957397,
      "learning_rate": 0.00014906748295430888,
      "loss": 0.591,
      "step": 11300
    },
    {
      "epoch": 0.7140151515151515,
      "grad_norm": 0.38529276847839355,
      "learning_rate": 0.0001489783752212963,
      "loss": 0.822,
      "step": 11310
    },
    {
      "epoch": 0.7146464646464646,
      "grad_norm": 0.4090179204940796,
      "learning_rate": 0.00014888921629567907,
      "loss": 0.7191,
      "step": 11320
    },
    {
      "epoch": 0.7152777777777778,
      "grad_norm": 0.3986714780330658,
      "learning_rate": 0.00014880000627064686,
      "loss": 0.6158,
      "step": 11330
    },
    {
      "epoch": 0.7159090909090909,
      "grad_norm": 0.4723992943763733,
      "learning_rate": 0.00014871074523944267,
      "loss": 0.56,
      "step": 11340
    },
    {
      "epoch": 0.7165404040404041,
      "grad_norm": 0.8075559139251709,
      "learning_rate": 0.000148621433295363,
      "loss": 0.5567,
      "step": 11350
    },
    {
      "epoch": 0.7171717171717171,
      "grad_norm": 0.3899381458759308,
      "learning_rate": 0.0001485320705317573,
      "loss": 0.8481,
      "step": 11360
    },
    {
      "epoch": 0.7178030303030303,
      "grad_norm": 0.4256030321121216,
      "learning_rate": 0.00014844265704202836,
      "loss": 0.6992,
      "step": 11370
    },
    {
      "epoch": 0.7184343434343434,
      "grad_norm": 0.42632630467414856,
      "learning_rate": 0.00014835319291963184,
      "loss": 0.5973,
      "step": 11380
    },
    {
      "epoch": 0.7190656565656566,
      "grad_norm": 0.44425168633461,
      "learning_rate": 0.0001482636782580764,
      "loss": 0.57,
      "step": 11390
    },
    {
      "epoch": 0.7196969696969697,
      "grad_norm": 0.7841507196426392,
      "learning_rate": 0.00014817411315092358,
      "loss": 0.5822,
      "step": 11400
    },
    {
      "epoch": 0.7203282828282829,
      "grad_norm": 0.4025212824344635,
      "learning_rate": 0.00014808449769178745,
      "loss": 0.8324,
      "step": 11410
    },
    {
      "epoch": 0.7209595959595959,
      "grad_norm": 0.42724311351776123,
      "learning_rate": 0.00014799483197433494,
      "loss": 0.7029,
      "step": 11420
    },
    {
      "epoch": 0.7215909090909091,
      "grad_norm": 0.4382633864879608,
      "learning_rate": 0.00014790511609228535,
      "loss": 0.5946,
      "step": 11430
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.48703277111053467,
      "learning_rate": 0.00014781535013941047,
      "loss": 0.5415,
      "step": 11440
    },
    {
      "epoch": 0.7228535353535354,
      "grad_norm": 0.6988332271575928,
      "learning_rate": 0.0001477255342095344,
      "loss": 0.5964,
      "step": 11450
    },
    {
      "epoch": 0.7234848484848485,
      "grad_norm": 0.38710835576057434,
      "learning_rate": 0.0001476356683965336,
      "loss": 0.8355,
      "step": 11460
    },
    {
      "epoch": 0.7241161616161617,
      "grad_norm": 0.41079068183898926,
      "learning_rate": 0.0001475457527943364,
      "loss": 0.6929,
      "step": 11470
    },
    {
      "epoch": 0.7247474747474747,
      "grad_norm": 0.44315218925476074,
      "learning_rate": 0.00014745578749692347,
      "loss": 0.6127,
      "step": 11480
    },
    {
      "epoch": 0.7253787878787878,
      "grad_norm": 0.49873799085617065,
      "learning_rate": 0.0001473657725983272,
      "loss": 0.5763,
      "step": 11490
    },
    {
      "epoch": 0.726010101010101,
      "grad_norm": 0.6905547976493835,
      "learning_rate": 0.00014727570819263197,
      "loss": 0.5447,
      "step": 11500
    },
    {
      "epoch": 0.7266414141414141,
      "grad_norm": 0.38986462354660034,
      "learning_rate": 0.00014718559437397382,
      "loss": 0.8298,
      "step": 11510
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.43311914801597595,
      "learning_rate": 0.00014709543123654043,
      "loss": 0.6906,
      "step": 11520
    },
    {
      "epoch": 0.7279040404040404,
      "grad_norm": 0.4450455605983734,
      "learning_rate": 0.00014700521887457116,
      "loss": 0.5982,
      "step": 11530
    },
    {
      "epoch": 0.7285353535353535,
      "grad_norm": 0.4902573525905609,
      "learning_rate": 0.00014691495738235658,
      "loss": 0.5532,
      "step": 11540
    },
    {
      "epoch": 0.7291666666666666,
      "grad_norm": 0.7286410331726074,
      "learning_rate": 0.00014682464685423886,
      "loss": 0.5629,
      "step": 11550
    },
    {
      "epoch": 0.7297979797979798,
      "grad_norm": 0.4271340072154999,
      "learning_rate": 0.00014673428738461126,
      "loss": 0.8691,
      "step": 11560
    },
    {
      "epoch": 0.7304292929292929,
      "grad_norm": 0.41660723090171814,
      "learning_rate": 0.00014664387906791827,
      "loss": 0.6886,
      "step": 11570
    },
    {
      "epoch": 0.7310606060606061,
      "grad_norm": 0.42229148745536804,
      "learning_rate": 0.00014655342199865543,
      "loss": 0.59,
      "step": 11580
    },
    {
      "epoch": 0.7316919191919192,
      "grad_norm": 0.4496455192565918,
      "learning_rate": 0.0001464629162713692,
      "loss": 0.5314,
      "step": 11590
    },
    {
      "epoch": 0.7323232323232324,
      "grad_norm": 0.6008057594299316,
      "learning_rate": 0.0001463723619806569,
      "loss": 0.5393,
      "step": 11600
    },
    {
      "epoch": 0.7329545454545454,
      "grad_norm": 0.3917795717716217,
      "learning_rate": 0.00014628175922116665,
      "loss": 0.8287,
      "step": 11610
    },
    {
      "epoch": 0.7335858585858586,
      "grad_norm": 0.40501078963279724,
      "learning_rate": 0.0001461911080875972,
      "loss": 0.7215,
      "step": 11620
    },
    {
      "epoch": 0.7342171717171717,
      "grad_norm": 0.511781632900238,
      "learning_rate": 0.00014610040867469787,
      "loss": 0.6374,
      "step": 11630
    },
    {
      "epoch": 0.7348484848484849,
      "grad_norm": 0.47348880767822266,
      "learning_rate": 0.00014600966107726845,
      "loss": 0.5391,
      "step": 11640
    },
    {
      "epoch": 0.735479797979798,
      "grad_norm": 0.7067840695381165,
      "learning_rate": 0.00014591886539015906,
      "loss": 0.586,
      "step": 11650
    },
    {
      "epoch": 0.7361111111111112,
      "grad_norm": 0.39044439792633057,
      "learning_rate": 0.00014582802170827014,
      "loss": 0.9076,
      "step": 11660
    },
    {
      "epoch": 0.7367424242424242,
      "grad_norm": 0.41078707575798035,
      "learning_rate": 0.00014573713012655222,
      "loss": 0.6565,
      "step": 11670
    },
    {
      "epoch": 0.7373737373737373,
      "grad_norm": 0.40659379959106445,
      "learning_rate": 0.00014564619074000596,
      "loss": 0.5883,
      "step": 11680
    },
    {
      "epoch": 0.7380050505050505,
      "grad_norm": 0.484119176864624,
      "learning_rate": 0.00014555520364368195,
      "loss": 0.5442,
      "step": 11690
    },
    {
      "epoch": 0.7386363636363636,
      "grad_norm": 0.6790262460708618,
      "learning_rate": 0.00014546416893268069,
      "loss": 0.6036,
      "step": 11700
    },
    {
      "epoch": 0.7392676767676768,
      "grad_norm": 0.39854755997657776,
      "learning_rate": 0.00014537308670215237,
      "loss": 0.8747,
      "step": 11710
    },
    {
      "epoch": 0.73989898989899,
      "grad_norm": 0.40913841128349304,
      "learning_rate": 0.00014528195704729692,
      "loss": 0.6716,
      "step": 11720
    },
    {
      "epoch": 0.740530303030303,
      "grad_norm": 0.470438688993454,
      "learning_rate": 0.00014519078006336382,
      "loss": 0.6196,
      "step": 11730
    },
    {
      "epoch": 0.7411616161616161,
      "grad_norm": 0.46601495146751404,
      "learning_rate": 0.000145099555845652,
      "loss": 0.5729,
      "step": 11740
    },
    {
      "epoch": 0.7417929292929293,
      "grad_norm": 0.7471237778663635,
      "learning_rate": 0.0001450082844895098,
      "loss": 0.5838,
      "step": 11750
    },
    {
      "epoch": 0.7424242424242424,
      "grad_norm": 0.4000641405582428,
      "learning_rate": 0.0001449169660903347,
      "loss": 0.8036,
      "step": 11760
    },
    {
      "epoch": 0.7430555555555556,
      "grad_norm": 0.4184749722480774,
      "learning_rate": 0.00014482560074357358,
      "loss": 0.7078,
      "step": 11770
    },
    {
      "epoch": 0.7436868686868687,
      "grad_norm": 0.47586241364479065,
      "learning_rate": 0.00014473418854472217,
      "loss": 0.6436,
      "step": 11780
    },
    {
      "epoch": 0.7443181818181818,
      "grad_norm": 0.47922879457473755,
      "learning_rate": 0.0001446427295893253,
      "loss": 0.5307,
      "step": 11790
    },
    {
      "epoch": 0.7449494949494949,
      "grad_norm": 0.8111808896064758,
      "learning_rate": 0.0001445512239729766,
      "loss": 0.5581,
      "step": 11800
    },
    {
      "epoch": 0.7455808080808081,
      "grad_norm": 0.3881637752056122,
      "learning_rate": 0.00014445967179131853,
      "loss": 0.8258,
      "step": 11810
    },
    {
      "epoch": 0.7462121212121212,
      "grad_norm": 0.40814366936683655,
      "learning_rate": 0.00014436807314004217,
      "loss": 0.7121,
      "step": 11820
    },
    {
      "epoch": 0.7468434343434344,
      "grad_norm": 0.47985583543777466,
      "learning_rate": 0.00014427642811488722,
      "loss": 0.6296,
      "step": 11830
    },
    {
      "epoch": 0.7474747474747475,
      "grad_norm": 0.5009323954582214,
      "learning_rate": 0.00014418473681164178,
      "loss": 0.5419,
      "step": 11840
    },
    {
      "epoch": 0.7481060606060606,
      "grad_norm": 0.7863016128540039,
      "learning_rate": 0.0001440929993261424,
      "loss": 0.5671,
      "step": 11850
    },
    {
      "epoch": 0.7487373737373737,
      "grad_norm": 0.404234915971756,
      "learning_rate": 0.00014400121575427384,
      "loss": 0.7942,
      "step": 11860
    },
    {
      "epoch": 0.7493686868686869,
      "grad_norm": 0.4192754924297333,
      "learning_rate": 0.00014390938619196906,
      "loss": 0.6693,
      "step": 11870
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.4592561721801758,
      "learning_rate": 0.0001438175107352091,
      "loss": 0.6155,
      "step": 11880
    },
    {
      "epoch": 0.7506313131313131,
      "grad_norm": 0.49892061948776245,
      "learning_rate": 0.0001437255894800229,
      "loss": 0.5461,
      "step": 11890
    },
    {
      "epoch": 0.7512626262626263,
      "grad_norm": 0.8253175616264343,
      "learning_rate": 0.0001436336225224874,
      "loss": 0.5756,
      "step": 11900
    },
    {
      "epoch": 0.7518939393939394,
      "grad_norm": 0.4303654134273529,
      "learning_rate": 0.0001435416099587271,
      "loss": 0.8449,
      "step": 11910
    },
    {
      "epoch": 0.7525252525252525,
      "grad_norm": 0.41442054510116577,
      "learning_rate": 0.00014344955188491442,
      "loss": 0.6621,
      "step": 11920
    },
    {
      "epoch": 0.7531565656565656,
      "grad_norm": 0.4323083758354187,
      "learning_rate": 0.00014335744839726917,
      "loss": 0.5915,
      "step": 11930
    },
    {
      "epoch": 0.7537878787878788,
      "grad_norm": 0.4786604642868042,
      "learning_rate": 0.0001432652995920587,
      "loss": 0.5437,
      "step": 11940
    },
    {
      "epoch": 0.7544191919191919,
      "grad_norm": 0.7814172506332397,
      "learning_rate": 0.00014317310556559765,
      "loss": 0.5646,
      "step": 11950
    },
    {
      "epoch": 0.7550505050505051,
      "grad_norm": 0.4201054275035858,
      "learning_rate": 0.0001430808664142481,
      "loss": 0.8876,
      "step": 11960
    },
    {
      "epoch": 0.7556818181818182,
      "grad_norm": 0.3979398012161255,
      "learning_rate": 0.00014298858223441902,
      "loss": 0.6737,
      "step": 11970
    },
    {
      "epoch": 0.7563131313131313,
      "grad_norm": 0.426853746175766,
      "learning_rate": 0.00014289625312256674,
      "loss": 0.5914,
      "step": 11980
    },
    {
      "epoch": 0.7569444444444444,
      "grad_norm": 0.47796735167503357,
      "learning_rate": 0.00014280387917519436,
      "loss": 0.5481,
      "step": 11990
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 0.7142711877822876,
      "learning_rate": 0.00014271146048885187,
      "loss": 0.5681,
      "step": 12000
    },
    {
      "epoch": 0.7575757575757576,
      "eval_loss": 0.6688094139099121,
      "eval_runtime": 31.8238,
      "eval_samples_per_second": 80.443,
      "eval_steps_per_second": 10.055,
      "step": 12000
    },
    {
      "epoch": 0.7582070707070707,
      "grad_norm": 0.38984981179237366,
      "learning_rate": 0.0001426189971601361,
      "loss": 0.7964,
      "step": 12010
    },
    {
      "epoch": 0.7588383838383839,
      "grad_norm": 0.4175244867801666,
      "learning_rate": 0.00014252648928569044,
      "loss": 0.7175,
      "step": 12020
    },
    {
      "epoch": 0.759469696969697,
      "grad_norm": 0.43180859088897705,
      "learning_rate": 0.00014243393696220492,
      "loss": 0.6132,
      "step": 12030
    },
    {
      "epoch": 0.76010101010101,
      "grad_norm": 0.46261066198349,
      "learning_rate": 0.000142341340286416,
      "loss": 0.5521,
      "step": 12040
    },
    {
      "epoch": 0.7607323232323232,
      "grad_norm": 0.8091698884963989,
      "learning_rate": 0.00014224869935510647,
      "loss": 0.5793,
      "step": 12050
    },
    {
      "epoch": 0.7613636363636364,
      "grad_norm": 0.38522958755493164,
      "learning_rate": 0.00014215601426510543,
      "loss": 0.7959,
      "step": 12060
    },
    {
      "epoch": 0.7619949494949495,
      "grad_norm": 0.42887264490127563,
      "learning_rate": 0.0001420632851132881,
      "loss": 0.7113,
      "step": 12070
    },
    {
      "epoch": 0.7626262626262627,
      "grad_norm": 0.48759254813194275,
      "learning_rate": 0.00014197051199657575,
      "loss": 0.6062,
      "step": 12080
    },
    {
      "epoch": 0.7632575757575758,
      "grad_norm": 0.4887999892234802,
      "learning_rate": 0.0001418776950119356,
      "loss": 0.551,
      "step": 12090
    },
    {
      "epoch": 0.7638888888888888,
      "grad_norm": 0.6605980396270752,
      "learning_rate": 0.00014178483425638077,
      "loss": 0.5086,
      "step": 12100
    },
    {
      "epoch": 0.764520202020202,
      "grad_norm": 0.4150230586528778,
      "learning_rate": 0.00014169192982697003,
      "loss": 0.8724,
      "step": 12110
    },
    {
      "epoch": 0.7651515151515151,
      "grad_norm": 0.4203437566757202,
      "learning_rate": 0.00014159898182080792,
      "loss": 0.7106,
      "step": 12120
    },
    {
      "epoch": 0.7657828282828283,
      "grad_norm": 0.4587154686450958,
      "learning_rate": 0.00014150599033504446,
      "loss": 0.6004,
      "step": 12130
    },
    {
      "epoch": 0.7664141414141414,
      "grad_norm": 0.44256168603897095,
      "learning_rate": 0.00014141295546687513,
      "loss": 0.5167,
      "step": 12140
    },
    {
      "epoch": 0.7670454545454546,
      "grad_norm": 0.6656980514526367,
      "learning_rate": 0.00014131987731354068,
      "loss": 0.593,
      "step": 12150
    },
    {
      "epoch": 0.7676767676767676,
      "grad_norm": 0.41358962655067444,
      "learning_rate": 0.00014122675597232726,
      "loss": 0.86,
      "step": 12160
    },
    {
      "epoch": 0.7683080808080808,
      "grad_norm": 0.41480815410614014,
      "learning_rate": 0.00014113359154056603,
      "loss": 0.7083,
      "step": 12170
    },
    {
      "epoch": 0.7689393939393939,
      "grad_norm": 0.4382536709308624,
      "learning_rate": 0.00014104038411563323,
      "loss": 0.6322,
      "step": 12180
    },
    {
      "epoch": 0.7695707070707071,
      "grad_norm": 0.5818092823028564,
      "learning_rate": 0.00014094713379495,
      "loss": 0.5258,
      "step": 12190
    },
    {
      "epoch": 0.7702020202020202,
      "grad_norm": 0.6411810517311096,
      "learning_rate": 0.00014085384067598242,
      "loss": 0.5336,
      "step": 12200
    },
    {
      "epoch": 0.7708333333333334,
      "grad_norm": 0.42943447828292847,
      "learning_rate": 0.00014076050485624118,
      "loss": 0.8705,
      "step": 12210
    },
    {
      "epoch": 0.7714646464646465,
      "grad_norm": 0.47742217779159546,
      "learning_rate": 0.00014066712643328165,
      "loss": 0.6964,
      "step": 12220
    },
    {
      "epoch": 0.7720959595959596,
      "grad_norm": 0.4521583914756775,
      "learning_rate": 0.0001405737055047038,
      "loss": 0.6025,
      "step": 12230
    },
    {
      "epoch": 0.7727272727272727,
      "grad_norm": 0.5201184749603271,
      "learning_rate": 0.00014048024216815186,
      "loss": 0.5373,
      "step": 12240
    },
    {
      "epoch": 0.7733585858585859,
      "grad_norm": 0.7048273086547852,
      "learning_rate": 0.00014038673652131455,
      "loss": 0.5705,
      "step": 12250
    },
    {
      "epoch": 0.773989898989899,
      "grad_norm": 0.40805086493492126,
      "learning_rate": 0.00014029318866192475,
      "loss": 0.8654,
      "step": 12260
    },
    {
      "epoch": 0.7746212121212122,
      "grad_norm": 0.4157496988773346,
      "learning_rate": 0.00014019959868775947,
      "loss": 0.682,
      "step": 12270
    },
    {
      "epoch": 0.7752525252525253,
      "grad_norm": 0.4149816036224365,
      "learning_rate": 0.00014010596669663966,
      "loss": 0.607,
      "step": 12280
    },
    {
      "epoch": 0.7758838383838383,
      "grad_norm": 0.48256298899650574,
      "learning_rate": 0.00014001229278643034,
      "loss": 0.5622,
      "step": 12290
    },
    {
      "epoch": 0.7765151515151515,
      "grad_norm": 0.8416755199432373,
      "learning_rate": 0.0001399185770550402,
      "loss": 0.5889,
      "step": 12300
    },
    {
      "epoch": 0.7771464646464646,
      "grad_norm": 0.40384411811828613,
      "learning_rate": 0.00013982481960042172,
      "loss": 0.8011,
      "step": 12310
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.43190136551856995,
      "learning_rate": 0.00013973102052057095,
      "loss": 0.6985,
      "step": 12320
    },
    {
      "epoch": 0.7784090909090909,
      "grad_norm": 0.43144333362579346,
      "learning_rate": 0.0001396371799135275,
      "loss": 0.5975,
      "step": 12330
    },
    {
      "epoch": 0.7790404040404041,
      "grad_norm": 0.4926276206970215,
      "learning_rate": 0.00013954329787737438,
      "loss": 0.5287,
      "step": 12340
    },
    {
      "epoch": 0.7796717171717171,
      "grad_norm": 0.8113794922828674,
      "learning_rate": 0.0001394493745102378,
      "loss": 0.6042,
      "step": 12350
    },
    {
      "epoch": 0.7803030303030303,
      "grad_norm": 0.4078025817871094,
      "learning_rate": 0.0001393554099102873,
      "loss": 0.847,
      "step": 12360
    },
    {
      "epoch": 0.7809343434343434,
      "grad_norm": 0.43292880058288574,
      "learning_rate": 0.00013926140417573542,
      "loss": 0.6927,
      "step": 12370
    },
    {
      "epoch": 0.7815656565656566,
      "grad_norm": 0.4796653687953949,
      "learning_rate": 0.00013916735740483776,
      "loss": 0.6062,
      "step": 12380
    },
    {
      "epoch": 0.7821969696969697,
      "grad_norm": 0.4515894949436188,
      "learning_rate": 0.00013907326969589276,
      "loss": 0.5766,
      "step": 12390
    },
    {
      "epoch": 0.7828282828282829,
      "grad_norm": 0.697572648525238,
      "learning_rate": 0.00013897914114724172,
      "loss": 0.59,
      "step": 12400
    },
    {
      "epoch": 0.7834595959595959,
      "grad_norm": 0.4123322367668152,
      "learning_rate": 0.00013888497185726855,
      "loss": 0.8397,
      "step": 12410
    },
    {
      "epoch": 0.7840909090909091,
      "grad_norm": 0.4195459485054016,
      "learning_rate": 0.00013879076192439976,
      "loss": 0.6863,
      "step": 12420
    },
    {
      "epoch": 0.7847222222222222,
      "grad_norm": 0.4477291405200958,
      "learning_rate": 0.0001386965114471044,
      "loss": 0.6407,
      "step": 12430
    },
    {
      "epoch": 0.7853535353535354,
      "grad_norm": 0.47965455055236816,
      "learning_rate": 0.0001386022205238938,
      "loss": 0.5165,
      "step": 12440
    },
    {
      "epoch": 0.7859848484848485,
      "grad_norm": 0.8128817081451416,
      "learning_rate": 0.00013850788925332167,
      "loss": 0.5891,
      "step": 12450
    },
    {
      "epoch": 0.7866161616161617,
      "grad_norm": 0.38307270407676697,
      "learning_rate": 0.0001384135177339838,
      "loss": 0.8662,
      "step": 12460
    },
    {
      "epoch": 0.7872474747474747,
      "grad_norm": 0.397906094789505,
      "learning_rate": 0.0001383191060645181,
      "loss": 0.6837,
      "step": 12470
    },
    {
      "epoch": 0.7878787878787878,
      "grad_norm": 0.4513985514640808,
      "learning_rate": 0.00013822465434360442,
      "loss": 0.6278,
      "step": 12480
    },
    {
      "epoch": 0.788510101010101,
      "grad_norm": 0.5143095254898071,
      "learning_rate": 0.00013813016266996447,
      "loss": 0.5318,
      "step": 12490
    },
    {
      "epoch": 0.7891414141414141,
      "grad_norm": 0.636981725692749,
      "learning_rate": 0.00013803563114236176,
      "loss": 0.5766,
      "step": 12500
    },
    {
      "epoch": 0.7897727272727273,
      "grad_norm": 0.3896467089653015,
      "learning_rate": 0.00013794105985960147,
      "loss": 0.7979,
      "step": 12510
    },
    {
      "epoch": 0.7904040404040404,
      "grad_norm": 0.404638409614563,
      "learning_rate": 0.00013784644892053018,
      "loss": 0.6902,
      "step": 12520
    },
    {
      "epoch": 0.7910353535353535,
      "grad_norm": 0.4295019209384918,
      "learning_rate": 0.00013775179842403612,
      "loss": 0.593,
      "step": 12530
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.46901798248291016,
      "learning_rate": 0.00013765710846904873,
      "loss": 0.5543,
      "step": 12540
    },
    {
      "epoch": 0.7922979797979798,
      "grad_norm": 0.6771798729896545,
      "learning_rate": 0.00013756237915453875,
      "loss": 0.5988,
      "step": 12550
    },
    {
      "epoch": 0.7929292929292929,
      "grad_norm": 0.3839336931705475,
      "learning_rate": 0.00013746761057951805,
      "loss": 0.9109,
      "step": 12560
    },
    {
      "epoch": 0.7935606060606061,
      "grad_norm": 0.4233282506465912,
      "learning_rate": 0.00013737280284303955,
      "loss": 0.7208,
      "step": 12570
    },
    {
      "epoch": 0.7941919191919192,
      "grad_norm": 0.4357168674468994,
      "learning_rate": 0.00013727795604419706,
      "loss": 0.5968,
      "step": 12580
    },
    {
      "epoch": 0.7948232323232324,
      "grad_norm": 0.5023078322410583,
      "learning_rate": 0.00013718307028212523,
      "loss": 0.5702,
      "step": 12590
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 0.8357389569282532,
      "learning_rate": 0.00013708814565599947,
      "loss": 0.609,
      "step": 12600
    },
    {
      "epoch": 0.7960858585858586,
      "grad_norm": 0.39431032538414,
      "learning_rate": 0.0001369931822650358,
      "loss": 0.805,
      "step": 12610
    },
    {
      "epoch": 0.7967171717171717,
      "grad_norm": 0.44602444767951965,
      "learning_rate": 0.0001368981802084907,
      "loss": 0.7101,
      "step": 12620
    },
    {
      "epoch": 0.7973484848484849,
      "grad_norm": 0.430719256401062,
      "learning_rate": 0.00013680313958566113,
      "loss": 0.6031,
      "step": 12630
    },
    {
      "epoch": 0.797979797979798,
      "grad_norm": 0.521497368812561,
      "learning_rate": 0.00013670806049588438,
      "loss": 0.5745,
      "step": 12640
    },
    {
      "epoch": 0.7986111111111112,
      "grad_norm": 0.6806516647338867,
      "learning_rate": 0.00013661294303853784,
      "loss": 0.56,
      "step": 12650
    },
    {
      "epoch": 0.7992424242424242,
      "grad_norm": 0.4030816853046417,
      "learning_rate": 0.0001365177873130391,
      "loss": 0.8716,
      "step": 12660
    },
    {
      "epoch": 0.7998737373737373,
      "grad_norm": 0.3861168324947357,
      "learning_rate": 0.00013642259341884572,
      "loss": 0.674,
      "step": 12670
    },
    {
      "epoch": 0.8005050505050505,
      "grad_norm": 0.4878477156162262,
      "learning_rate": 0.00013632736145545514,
      "loss": 0.5972,
      "step": 12680
    },
    {
      "epoch": 0.8011363636363636,
      "grad_norm": 0.49004000425338745,
      "learning_rate": 0.00013623209152240458,
      "loss": 0.5583,
      "step": 12690
    },
    {
      "epoch": 0.8017676767676768,
      "grad_norm": 0.7845889925956726,
      "learning_rate": 0.00013613678371927098,
      "loss": 0.5662,
      "step": 12700
    },
    {
      "epoch": 0.80239898989899,
      "grad_norm": 0.42607882618904114,
      "learning_rate": 0.00013604143814567086,
      "loss": 0.8005,
      "step": 12710
    },
    {
      "epoch": 0.803030303030303,
      "grad_norm": 0.4259932041168213,
      "learning_rate": 0.0001359460549012602,
      "loss": 0.7173,
      "step": 12720
    },
    {
      "epoch": 0.8036616161616161,
      "grad_norm": 0.46363234519958496,
      "learning_rate": 0.00013585063408573435,
      "loss": 0.6137,
      "step": 12730
    },
    {
      "epoch": 0.8042929292929293,
      "grad_norm": 0.45852065086364746,
      "learning_rate": 0.00013575517579882793,
      "loss": 0.5541,
      "step": 12740
    },
    {
      "epoch": 0.8049242424242424,
      "grad_norm": 0.910020112991333,
      "learning_rate": 0.00013565968014031478,
      "loss": 0.5602,
      "step": 12750
    },
    {
      "epoch": 0.8055555555555556,
      "grad_norm": 0.39986976981163025,
      "learning_rate": 0.00013556414721000768,
      "loss": 0.8635,
      "step": 12760
    },
    {
      "epoch": 0.8061868686868687,
      "grad_norm": 0.4602300226688385,
      "learning_rate": 0.00013546857710775852,
      "loss": 0.7129,
      "step": 12770
    },
    {
      "epoch": 0.8068181818181818,
      "grad_norm": 0.46429455280303955,
      "learning_rate": 0.00013537296993345794,
      "loss": 0.5919,
      "step": 12780
    },
    {
      "epoch": 0.8074494949494949,
      "grad_norm": 0.47115981578826904,
      "learning_rate": 0.00013527732578703534,
      "loss": 0.5565,
      "step": 12790
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 0.7067246437072754,
      "learning_rate": 0.00013518164476845876,
      "loss": 0.5693,
      "step": 12800
    },
    {
      "epoch": 0.8087121212121212,
      "grad_norm": 0.4275135099887848,
      "learning_rate": 0.00013508592697773485,
      "loss": 0.8343,
      "step": 12810
    },
    {
      "epoch": 0.8093434343434344,
      "grad_norm": 0.41341742873191833,
      "learning_rate": 0.0001349901725149086,
      "loss": 0.7185,
      "step": 12820
    },
    {
      "epoch": 0.8099747474747475,
      "grad_norm": 0.47066307067871094,
      "learning_rate": 0.00013489438148006332,
      "loss": 0.5874,
      "step": 12830
    },
    {
      "epoch": 0.8106060606060606,
      "grad_norm": 0.4272002577781677,
      "learning_rate": 0.0001347985539733207,
      "loss": 0.5377,
      "step": 12840
    },
    {
      "epoch": 0.8112373737373737,
      "grad_norm": 0.7232552766799927,
      "learning_rate": 0.00013470269009484037,
      "loss": 0.5423,
      "step": 12850
    },
    {
      "epoch": 0.8118686868686869,
      "grad_norm": 0.4015613794326782,
      "learning_rate": 0.00013460678994482012,
      "loss": 0.8524,
      "step": 12860
    },
    {
      "epoch": 0.8125,
      "grad_norm": 0.4343956708908081,
      "learning_rate": 0.0001345108536234955,
      "loss": 0.6444,
      "step": 12870
    },
    {
      "epoch": 0.8131313131313131,
      "grad_norm": 0.4314040243625641,
      "learning_rate": 0.00013441488123114003,
      "loss": 0.5968,
      "step": 12880
    },
    {
      "epoch": 0.8137626262626263,
      "grad_norm": 0.4678441882133484,
      "learning_rate": 0.0001343188728680648,
      "loss": 0.5524,
      "step": 12890
    },
    {
      "epoch": 0.8143939393939394,
      "grad_norm": 0.6854606866836548,
      "learning_rate": 0.00013422282863461855,
      "loss": 0.579,
      "step": 12900
    },
    {
      "epoch": 0.8150252525252525,
      "grad_norm": 0.3773426413536072,
      "learning_rate": 0.00013412674863118751,
      "loss": 0.8011,
      "step": 12910
    },
    {
      "epoch": 0.8156565656565656,
      "grad_norm": 0.4155985713005066,
      "learning_rate": 0.00013403063295819537,
      "loss": 0.7058,
      "step": 12920
    },
    {
      "epoch": 0.8162878787878788,
      "grad_norm": 0.43196067214012146,
      "learning_rate": 0.00013393448171610296,
      "loss": 0.6143,
      "step": 12930
    },
    {
      "epoch": 0.8169191919191919,
      "grad_norm": 0.45984938740730286,
      "learning_rate": 0.00013383829500540831,
      "loss": 0.5727,
      "step": 12940
    },
    {
      "epoch": 0.8175505050505051,
      "grad_norm": 0.7649791836738586,
      "learning_rate": 0.0001337420729266467,
      "loss": 0.5722,
      "step": 12950
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.3872847855091095,
      "learning_rate": 0.00013364581558039011,
      "loss": 0.8128,
      "step": 12960
    },
    {
      "epoch": 0.8188131313131313,
      "grad_norm": 0.41579070687294006,
      "learning_rate": 0.00013354952306724761,
      "loss": 0.6764,
      "step": 12970
    },
    {
      "epoch": 0.8194444444444444,
      "grad_norm": 0.47754842042922974,
      "learning_rate": 0.0001334531954878649,
      "loss": 0.5961,
      "step": 12980
    },
    {
      "epoch": 0.8200757575757576,
      "grad_norm": 0.4338254928588867,
      "learning_rate": 0.0001333568329429244,
      "loss": 0.568,
      "step": 12990
    },
    {
      "epoch": 0.8207070707070707,
      "grad_norm": 0.8476325869560242,
      "learning_rate": 0.00013326043553314498,
      "loss": 0.6046,
      "step": 13000
    },
    {
      "epoch": 0.8207070707070707,
      "eval_loss": 0.6646369099617004,
      "eval_runtime": 31.6816,
      "eval_samples_per_second": 80.804,
      "eval_steps_per_second": 10.101,
      "step": 13000
    },
    {
      "epoch": 0.8213383838383839,
      "grad_norm": 0.40835514664649963,
      "learning_rate": 0.00013316400335928208,
      "loss": 0.8101,
      "step": 13010
    },
    {
      "epoch": 0.821969696969697,
      "grad_norm": 0.4103347063064575,
      "learning_rate": 0.00013306753652212732,
      "loss": 0.6874,
      "step": 13020
    },
    {
      "epoch": 0.82260101010101,
      "grad_norm": 0.4257214367389679,
      "learning_rate": 0.00013297103512250875,
      "loss": 0.6175,
      "step": 13030
    },
    {
      "epoch": 0.8232323232323232,
      "grad_norm": 0.4386374354362488,
      "learning_rate": 0.00013287449926129038,
      "loss": 0.5372,
      "step": 13040
    },
    {
      "epoch": 0.8238636363636364,
      "grad_norm": 0.7796090841293335,
      "learning_rate": 0.00013277792903937228,
      "loss": 0.5679,
      "step": 13050
    },
    {
      "epoch": 0.8244949494949495,
      "grad_norm": 0.39057448506355286,
      "learning_rate": 0.00013268132455769046,
      "loss": 0.8241,
      "step": 13060
    },
    {
      "epoch": 0.8251262626262627,
      "grad_norm": 0.4221686124801636,
      "learning_rate": 0.00013258468591721673,
      "loss": 0.6983,
      "step": 13070
    },
    {
      "epoch": 0.8257575757575758,
      "grad_norm": 0.4335082173347473,
      "learning_rate": 0.0001324880132189586,
      "loss": 0.5993,
      "step": 13080
    },
    {
      "epoch": 0.8263888888888888,
      "grad_norm": 0.4841999411582947,
      "learning_rate": 0.00013239130656395918,
      "loss": 0.5485,
      "step": 13090
    },
    {
      "epoch": 0.827020202020202,
      "grad_norm": 0.6911115050315857,
      "learning_rate": 0.0001322945660532971,
      "loss": 0.5644,
      "step": 13100
    },
    {
      "epoch": 0.8276515151515151,
      "grad_norm": 0.41165804862976074,
      "learning_rate": 0.00013219779178808624,
      "loss": 0.8076,
      "step": 13110
    },
    {
      "epoch": 0.8282828282828283,
      "grad_norm": 0.4360223114490509,
      "learning_rate": 0.000132100983869476,
      "loss": 0.6851,
      "step": 13120
    },
    {
      "epoch": 0.8289141414141414,
      "grad_norm": 0.48048245906829834,
      "learning_rate": 0.00013200414239865072,
      "loss": 0.625,
      "step": 13130
    },
    {
      "epoch": 0.8295454545454546,
      "grad_norm": 0.46276018023490906,
      "learning_rate": 0.00013190726747682997,
      "loss": 0.5351,
      "step": 13140
    },
    {
      "epoch": 0.8301767676767676,
      "grad_norm": 0.7023227214813232,
      "learning_rate": 0.0001318103592052682,
      "loss": 0.5902,
      "step": 13150
    },
    {
      "epoch": 0.8308080808080808,
      "grad_norm": 0.36076533794403076,
      "learning_rate": 0.00013171341768525476,
      "loss": 0.803,
      "step": 13160
    },
    {
      "epoch": 0.8314393939393939,
      "grad_norm": 0.41885295510292053,
      "learning_rate": 0.00013161644301811368,
      "loss": 0.7198,
      "step": 13170
    },
    {
      "epoch": 0.8320707070707071,
      "grad_norm": 0.453347772359848,
      "learning_rate": 0.00013151943530520375,
      "loss": 0.598,
      "step": 13180
    },
    {
      "epoch": 0.8327020202020202,
      "grad_norm": 0.4829827547073364,
      "learning_rate": 0.0001314223946479182,
      "loss": 0.5519,
      "step": 13190
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.6935321688652039,
      "learning_rate": 0.00013132532114768473,
      "loss": 0.5626,
      "step": 13200
    },
    {
      "epoch": 0.8339646464646465,
      "grad_norm": 0.4175455868244171,
      "learning_rate": 0.00013122821490596536,
      "loss": 0.793,
      "step": 13210
    },
    {
      "epoch": 0.8345959595959596,
      "grad_norm": 0.42951449751853943,
      "learning_rate": 0.00013113107602425638,
      "loss": 0.6833,
      "step": 13220
    },
    {
      "epoch": 0.8352272727272727,
      "grad_norm": 0.4871407151222229,
      "learning_rate": 0.00013103390460408816,
      "loss": 0.597,
      "step": 13230
    },
    {
      "epoch": 0.8358585858585859,
      "grad_norm": 0.44362375140190125,
      "learning_rate": 0.000130936700747025,
      "loss": 0.5451,
      "step": 13240
    },
    {
      "epoch": 0.836489898989899,
      "grad_norm": 0.7222819328308105,
      "learning_rate": 0.00013083946455466527,
      "loss": 0.5418,
      "step": 13250
    },
    {
      "epoch": 0.8371212121212122,
      "grad_norm": 0.4090614914894104,
      "learning_rate": 0.000130742196128641,
      "loss": 0.7836,
      "step": 13260
    },
    {
      "epoch": 0.8377525252525253,
      "grad_norm": 0.4538322687149048,
      "learning_rate": 0.00013064489557061795,
      "loss": 0.6858,
      "step": 13270
    },
    {
      "epoch": 0.8383838383838383,
      "grad_norm": 0.48767927289009094,
      "learning_rate": 0.0001305475629822955,
      "loss": 0.6143,
      "step": 13280
    },
    {
      "epoch": 0.8390151515151515,
      "grad_norm": 0.5203067064285278,
      "learning_rate": 0.00013045019846540644,
      "loss": 0.5484,
      "step": 13290
    },
    {
      "epoch": 0.8396464646464646,
      "grad_norm": 0.8104034066200256,
      "learning_rate": 0.000130352802121717,
      "loss": 0.5754,
      "step": 13300
    },
    {
      "epoch": 0.8402777777777778,
      "grad_norm": 0.4128987491130829,
      "learning_rate": 0.00013025537405302666,
      "loss": 0.841,
      "step": 13310
    },
    {
      "epoch": 0.8409090909090909,
      "grad_norm": 0.3976184129714966,
      "learning_rate": 0.00013015791436116802,
      "loss": 0.6561,
      "step": 13320
    },
    {
      "epoch": 0.8415404040404041,
      "grad_norm": 0.4238503575325012,
      "learning_rate": 0.00013006042314800677,
      "loss": 0.6241,
      "step": 13330
    },
    {
      "epoch": 0.8421717171717171,
      "grad_norm": 0.5021119117736816,
      "learning_rate": 0.00012996290051544155,
      "loss": 0.5946,
      "step": 13340
    },
    {
      "epoch": 0.8428030303030303,
      "grad_norm": 0.7573778033256531,
      "learning_rate": 0.0001298653465654038,
      "loss": 0.5872,
      "step": 13350
    },
    {
      "epoch": 0.8434343434343434,
      "grad_norm": 0.4177238643169403,
      "learning_rate": 0.00012976776139985772,
      "loss": 0.8135,
      "step": 13360
    },
    {
      "epoch": 0.8440656565656566,
      "grad_norm": 0.4206421375274658,
      "learning_rate": 0.00012967014512080014,
      "loss": 0.6959,
      "step": 13370
    },
    {
      "epoch": 0.8446969696969697,
      "grad_norm": 0.43411895632743835,
      "learning_rate": 0.00012957249783026044,
      "loss": 0.5881,
      "step": 13380
    },
    {
      "epoch": 0.8453282828282829,
      "grad_norm": 0.45201462507247925,
      "learning_rate": 0.00012947481963030032,
      "loss": 0.5252,
      "step": 13390
    },
    {
      "epoch": 0.8459595959595959,
      "grad_norm": 0.7356660962104797,
      "learning_rate": 0.0001293771106230139,
      "loss": 0.578,
      "step": 13400
    },
    {
      "epoch": 0.8465909090909091,
      "grad_norm": 0.40057292580604553,
      "learning_rate": 0.00012927937091052744,
      "loss": 0.8381,
      "step": 13410
    },
    {
      "epoch": 0.8472222222222222,
      "grad_norm": 0.43308311700820923,
      "learning_rate": 0.00012918160059499924,
      "loss": 0.7008,
      "step": 13420
    },
    {
      "epoch": 0.8478535353535354,
      "grad_norm": 0.4517548084259033,
      "learning_rate": 0.0001290837997786197,
      "loss": 0.5985,
      "step": 13430
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 0.5230621695518494,
      "learning_rate": 0.00012898596856361104,
      "loss": 0.5249,
      "step": 13440
    },
    {
      "epoch": 0.8491161616161617,
      "grad_norm": 0.733771800994873,
      "learning_rate": 0.00012888810705222726,
      "loss": 0.5943,
      "step": 13450
    },
    {
      "epoch": 0.8497474747474747,
      "grad_norm": 0.4152091443538666,
      "learning_rate": 0.00012879021534675398,
      "loss": 0.8256,
      "step": 13460
    },
    {
      "epoch": 0.8503787878787878,
      "grad_norm": 0.39363232254981995,
      "learning_rate": 0.00012869229354950853,
      "loss": 0.6612,
      "step": 13470
    },
    {
      "epoch": 0.851010101010101,
      "grad_norm": 0.4171309173107147,
      "learning_rate": 0.00012859434176283946,
      "loss": 0.5937,
      "step": 13480
    },
    {
      "epoch": 0.8516414141414141,
      "grad_norm": 0.4470405578613281,
      "learning_rate": 0.00012849636008912686,
      "loss": 0.5503,
      "step": 13490
    },
    {
      "epoch": 0.8522727272727273,
      "grad_norm": 0.7230708599090576,
      "learning_rate": 0.00012839834863078198,
      "loss": 0.5541,
      "step": 13500
    },
    {
      "epoch": 0.8529040404040404,
      "grad_norm": 0.3759131133556366,
      "learning_rate": 0.00012830030749024721,
      "loss": 0.8153,
      "step": 13510
    },
    {
      "epoch": 0.8535353535353535,
      "grad_norm": 0.43848785758018494,
      "learning_rate": 0.00012820223676999597,
      "loss": 0.6902,
      "step": 13520
    },
    {
      "epoch": 0.8541666666666666,
      "grad_norm": 0.4598679840564728,
      "learning_rate": 0.00012810413657253259,
      "loss": 0.5982,
      "step": 13530
    },
    {
      "epoch": 0.8547979797979798,
      "grad_norm": 0.4838223159313202,
      "learning_rate": 0.0001280060070003922,
      "loss": 0.5664,
      "step": 13540
    },
    {
      "epoch": 0.8554292929292929,
      "grad_norm": 0.631023108959198,
      "learning_rate": 0.00012790784815614064,
      "loss": 0.5186,
      "step": 13550
    },
    {
      "epoch": 0.8560606060606061,
      "grad_norm": 0.37927475571632385,
      "learning_rate": 0.00012780966014237435,
      "loss": 0.7869,
      "step": 13560
    },
    {
      "epoch": 0.8566919191919192,
      "grad_norm": 0.38126081228256226,
      "learning_rate": 0.00012771144306172026,
      "loss": 0.6816,
      "step": 13570
    },
    {
      "epoch": 0.8573232323232324,
      "grad_norm": 0.46559417247772217,
      "learning_rate": 0.0001276131970168357,
      "loss": 0.6385,
      "step": 13580
    },
    {
      "epoch": 0.8579545454545454,
      "grad_norm": 0.47952020168304443,
      "learning_rate": 0.00012751492211040823,
      "loss": 0.5615,
      "step": 13590
    },
    {
      "epoch": 0.8585858585858586,
      "grad_norm": 0.7455291152000427,
      "learning_rate": 0.00012741661844515562,
      "loss": 0.5719,
      "step": 13600
    },
    {
      "epoch": 0.8592171717171717,
      "grad_norm": 0.4059703052043915,
      "learning_rate": 0.00012731828612382566,
      "loss": 0.8615,
      "step": 13610
    },
    {
      "epoch": 0.8598484848484849,
      "grad_norm": 0.41605040431022644,
      "learning_rate": 0.0001272199252491961,
      "loss": 0.6797,
      "step": 13620
    },
    {
      "epoch": 0.860479797979798,
      "grad_norm": 0.4604390859603882,
      "learning_rate": 0.00012712153592407457,
      "loss": 0.6162,
      "step": 13630
    },
    {
      "epoch": 0.8611111111111112,
      "grad_norm": 0.46702027320861816,
      "learning_rate": 0.00012702311825129838,
      "loss": 0.5782,
      "step": 13640
    },
    {
      "epoch": 0.8617424242424242,
      "grad_norm": 0.6887663006782532,
      "learning_rate": 0.00012692467233373452,
      "loss": 0.5944,
      "step": 13650
    },
    {
      "epoch": 0.8623737373737373,
      "grad_norm": 0.41526302695274353,
      "learning_rate": 0.00012682619827427945,
      "loss": 0.8389,
      "step": 13660
    },
    {
      "epoch": 0.8630050505050505,
      "grad_norm": 0.4050087034702301,
      "learning_rate": 0.00012672769617585916,
      "loss": 0.7063,
      "step": 13670
    },
    {
      "epoch": 0.8636363636363636,
      "grad_norm": 0.41956180334091187,
      "learning_rate": 0.00012662916614142875,
      "loss": 0.6105,
      "step": 13680
    },
    {
      "epoch": 0.8642676767676768,
      "grad_norm": 0.5342766046524048,
      "learning_rate": 0.0001265306082739727,
      "loss": 0.5467,
      "step": 13690
    },
    {
      "epoch": 0.86489898989899,
      "grad_norm": 0.7617638111114502,
      "learning_rate": 0.00012643202267650447,
      "loss": 0.5464,
      "step": 13700
    },
    {
      "epoch": 0.865530303030303,
      "grad_norm": 0.41123586893081665,
      "learning_rate": 0.0001263334094520666,
      "loss": 0.8211,
      "step": 13710
    },
    {
      "epoch": 0.8661616161616161,
      "grad_norm": 0.44836679100990295,
      "learning_rate": 0.00012623476870373041,
      "loss": 0.721,
      "step": 13720
    },
    {
      "epoch": 0.8667929292929293,
      "grad_norm": 0.4404003322124481,
      "learning_rate": 0.00012613610053459606,
      "loss": 0.6237,
      "step": 13730
    },
    {
      "epoch": 0.8674242424242424,
      "grad_norm": 0.47968870401382446,
      "learning_rate": 0.00012603740504779228,
      "loss": 0.5459,
      "step": 13740
    },
    {
      "epoch": 0.8680555555555556,
      "grad_norm": 0.6938720941543579,
      "learning_rate": 0.00012593868234647647,
      "loss": 0.5762,
      "step": 13750
    },
    {
      "epoch": 0.8686868686868687,
      "grad_norm": 0.40199920535087585,
      "learning_rate": 0.00012583993253383443,
      "loss": 0.8661,
      "step": 13760
    },
    {
      "epoch": 0.8693181818181818,
      "grad_norm": 0.4403010308742523,
      "learning_rate": 0.00012574115571308022,
      "loss": 0.6849,
      "step": 13770
    },
    {
      "epoch": 0.8699494949494949,
      "grad_norm": 0.45858609676361084,
      "learning_rate": 0.00012564235198745625,
      "loss": 0.6198,
      "step": 13780
    },
    {
      "epoch": 0.8705808080808081,
      "grad_norm": 0.4701538681983948,
      "learning_rate": 0.00012554352146023295,
      "loss": 0.5573,
      "step": 13790
    },
    {
      "epoch": 0.8712121212121212,
      "grad_norm": 0.6757790446281433,
      "learning_rate": 0.00012544466423470885,
      "loss": 0.5309,
      "step": 13800
    },
    {
      "epoch": 0.8718434343434344,
      "grad_norm": 0.4097801446914673,
      "learning_rate": 0.0001253457804142103,
      "loss": 0.8444,
      "step": 13810
    },
    {
      "epoch": 0.8724747474747475,
      "grad_norm": 0.3941170871257782,
      "learning_rate": 0.00012524687010209153,
      "loss": 0.7089,
      "step": 13820
    },
    {
      "epoch": 0.8731060606060606,
      "grad_norm": 0.42793476581573486,
      "learning_rate": 0.00012514793340173442,
      "loss": 0.5928,
      "step": 13830
    },
    {
      "epoch": 0.8737373737373737,
      "grad_norm": 0.4606638252735138,
      "learning_rate": 0.0001250489704165484,
      "loss": 0.5392,
      "step": 13840
    },
    {
      "epoch": 0.8743686868686869,
      "grad_norm": 0.6518364548683167,
      "learning_rate": 0.00012494998124997042,
      "loss": 0.5365,
      "step": 13850
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.40729132294654846,
      "learning_rate": 0.00012485096600546477,
      "loss": 0.9472,
      "step": 13860
    },
    {
      "epoch": 0.8756313131313131,
      "grad_norm": 0.4063562750816345,
      "learning_rate": 0.00012475192478652303,
      "loss": 0.6886,
      "step": 13870
    },
    {
      "epoch": 0.8762626262626263,
      "grad_norm": 0.4930334687232971,
      "learning_rate": 0.00012465285769666388,
      "loss": 0.6049,
      "step": 13880
    },
    {
      "epoch": 0.8768939393939394,
      "grad_norm": 0.4516308605670929,
      "learning_rate": 0.00012455376483943312,
      "loss": 0.5165,
      "step": 13890
    },
    {
      "epoch": 0.8775252525252525,
      "grad_norm": 0.7816421389579773,
      "learning_rate": 0.0001244546463184033,
      "loss": 0.5651,
      "step": 13900
    },
    {
      "epoch": 0.8781565656565656,
      "grad_norm": 0.4049707353115082,
      "learning_rate": 0.00012435550223717406,
      "loss": 0.8847,
      "step": 13910
    },
    {
      "epoch": 0.8787878787878788,
      "grad_norm": 0.4231446385383606,
      "learning_rate": 0.00012425633269937155,
      "loss": 0.6839,
      "step": 13920
    },
    {
      "epoch": 0.8794191919191919,
      "grad_norm": 0.44199612736701965,
      "learning_rate": 0.00012415713780864862,
      "loss": 0.5702,
      "step": 13930
    },
    {
      "epoch": 0.8800505050505051,
      "grad_norm": 0.4639078676700592,
      "learning_rate": 0.00012405791766868457,
      "loss": 0.5305,
      "step": 13940
    },
    {
      "epoch": 0.8806818181818182,
      "grad_norm": 0.806225597858429,
      "learning_rate": 0.00012395867238318512,
      "loss": 0.5554,
      "step": 13950
    },
    {
      "epoch": 0.8813131313131313,
      "grad_norm": 0.37645605206489563,
      "learning_rate": 0.00012385940205588227,
      "loss": 0.7865,
      "step": 13960
    },
    {
      "epoch": 0.8819444444444444,
      "grad_norm": 0.3771421015262604,
      "learning_rate": 0.0001237601067905342,
      "loss": 0.6677,
      "step": 13970
    },
    {
      "epoch": 0.8825757575757576,
      "grad_norm": 0.47178322076797485,
      "learning_rate": 0.00012366078669092512,
      "loss": 0.6121,
      "step": 13980
    },
    {
      "epoch": 0.8832070707070707,
      "grad_norm": 0.47072458267211914,
      "learning_rate": 0.00012356144186086528,
      "loss": 0.5308,
      "step": 13990
    },
    {
      "epoch": 0.8838383838383839,
      "grad_norm": 0.724148154258728,
      "learning_rate": 0.00012346207240419068,
      "loss": 0.5631,
      "step": 14000
    },
    {
      "epoch": 0.8838383838383839,
      "eval_loss": 0.6602758169174194,
      "eval_runtime": 31.6832,
      "eval_samples_per_second": 80.8,
      "eval_steps_per_second": 10.1,
      "step": 14000
    },
    {
      "epoch": 0.884469696969697,
      "grad_norm": 0.42788049578666687,
      "learning_rate": 0.00012336267842476308,
      "loss": 0.8054,
      "step": 14010
    },
    {
      "epoch": 0.88510101010101,
      "grad_norm": 0.4089519679546356,
      "learning_rate": 0.00012326326002646992,
      "loss": 0.6985,
      "step": 14020
    },
    {
      "epoch": 0.8857323232323232,
      "grad_norm": 0.4357617199420929,
      "learning_rate": 0.00012316381731322413,
      "loss": 0.646,
      "step": 14030
    },
    {
      "epoch": 0.8863636363636364,
      "grad_norm": 0.4335095286369324,
      "learning_rate": 0.00012306435038896409,
      "loss": 0.5246,
      "step": 14040
    },
    {
      "epoch": 0.8869949494949495,
      "grad_norm": 0.6921783089637756,
      "learning_rate": 0.00012296485935765338,
      "loss": 0.5635,
      "step": 14050
    },
    {
      "epoch": 0.8876262626262627,
      "grad_norm": 0.39155542850494385,
      "learning_rate": 0.00012286534432328096,
      "loss": 0.7995,
      "step": 14060
    },
    {
      "epoch": 0.8882575757575758,
      "grad_norm": 0.4158618748188019,
      "learning_rate": 0.00012276580538986067,
      "loss": 0.6844,
      "step": 14070
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.4563840627670288,
      "learning_rate": 0.00012266624266143148,
      "loss": 0.6168,
      "step": 14080
    },
    {
      "epoch": 0.889520202020202,
      "grad_norm": 0.4676637649536133,
      "learning_rate": 0.00012256665624205717,
      "loss": 0.5538,
      "step": 14090
    },
    {
      "epoch": 0.8901515151515151,
      "grad_norm": 0.8020414710044861,
      "learning_rate": 0.00012246704623582636,
      "loss": 0.5517,
      "step": 14100
    },
    {
      "epoch": 0.8907828282828283,
      "grad_norm": 0.4076377749443054,
      "learning_rate": 0.00012236741274685212,
      "loss": 0.8284,
      "step": 14110
    },
    {
      "epoch": 0.8914141414141414,
      "grad_norm": 0.3903372585773468,
      "learning_rate": 0.00012226775587927227,
      "loss": 0.6667,
      "step": 14120
    },
    {
      "epoch": 0.8920454545454546,
      "grad_norm": 0.4346112608909607,
      "learning_rate": 0.00012216807573724905,
      "loss": 0.6073,
      "step": 14130
    },
    {
      "epoch": 0.8926767676767676,
      "grad_norm": 0.47311460971832275,
      "learning_rate": 0.00012206837242496887,
      "loss": 0.5475,
      "step": 14140
    },
    {
      "epoch": 0.8933080808080808,
      "grad_norm": 0.6440660357475281,
      "learning_rate": 0.00012196864604664253,
      "loss": 0.5548,
      "step": 14150
    },
    {
      "epoch": 0.8939393939393939,
      "grad_norm": 0.3944689929485321,
      "learning_rate": 0.00012186889670650486,
      "loss": 0.8288,
      "step": 14160
    },
    {
      "epoch": 0.8945707070707071,
      "grad_norm": 0.3773377537727356,
      "learning_rate": 0.00012176912450881468,
      "loss": 0.6913,
      "step": 14170
    },
    {
      "epoch": 0.8952020202020202,
      "grad_norm": 0.41568541526794434,
      "learning_rate": 0.00012166932955785471,
      "loss": 0.6001,
      "step": 14180
    },
    {
      "epoch": 0.8958333333333334,
      "grad_norm": 0.48487791419029236,
      "learning_rate": 0.00012156951195793152,
      "loss": 0.5449,
      "step": 14190
    },
    {
      "epoch": 0.8964646464646465,
      "grad_norm": 0.7605670094490051,
      "learning_rate": 0.0001214696718133752,
      "loss": 0.5716,
      "step": 14200
    },
    {
      "epoch": 0.8970959595959596,
      "grad_norm": 0.3885903060436249,
      "learning_rate": 0.0001213698092285396,
      "loss": 0.8632,
      "step": 14210
    },
    {
      "epoch": 0.8977272727272727,
      "grad_norm": 0.46508580446243286,
      "learning_rate": 0.00012126992430780187,
      "loss": 0.7226,
      "step": 14220
    },
    {
      "epoch": 0.8983585858585859,
      "grad_norm": 0.4235909879207611,
      "learning_rate": 0.00012117001715556255,
      "loss": 0.6099,
      "step": 14230
    },
    {
      "epoch": 0.898989898989899,
      "grad_norm": 0.49782323837280273,
      "learning_rate": 0.00012107008787624546,
      "loss": 0.554,
      "step": 14240
    },
    {
      "epoch": 0.8996212121212122,
      "grad_norm": 0.7208977937698364,
      "learning_rate": 0.00012097013657429746,
      "loss": 0.5922,
      "step": 14250
    },
    {
      "epoch": 0.9002525252525253,
      "grad_norm": 0.40166938304901123,
      "learning_rate": 0.00012087016335418855,
      "loss": 0.8129,
      "step": 14260
    },
    {
      "epoch": 0.9008838383838383,
      "grad_norm": 0.4439374804496765,
      "learning_rate": 0.00012077016832041151,
      "loss": 0.6742,
      "step": 14270
    },
    {
      "epoch": 0.9015151515151515,
      "grad_norm": 0.4050210118293762,
      "learning_rate": 0.00012067015157748203,
      "loss": 0.578,
      "step": 14280
    },
    {
      "epoch": 0.9021464646464646,
      "grad_norm": 0.46065762639045715,
      "learning_rate": 0.00012057011322993838,
      "loss": 0.5061,
      "step": 14290
    },
    {
      "epoch": 0.9027777777777778,
      "grad_norm": 0.8514410853385925,
      "learning_rate": 0.00012047005338234155,
      "loss": 0.5548,
      "step": 14300
    },
    {
      "epoch": 0.9034090909090909,
      "grad_norm": 0.38600584864616394,
      "learning_rate": 0.00012036997213927484,
      "loss": 0.8185,
      "step": 14310
    },
    {
      "epoch": 0.9040404040404041,
      "grad_norm": 0.4366419017314911,
      "learning_rate": 0.00012026986960534406,
      "loss": 0.679,
      "step": 14320
    },
    {
      "epoch": 0.9046717171717171,
      "grad_norm": 0.4390747547149658,
      "learning_rate": 0.00012016974588517717,
      "loss": 0.6155,
      "step": 14330
    },
    {
      "epoch": 0.9053030303030303,
      "grad_norm": 0.43881291151046753,
      "learning_rate": 0.00012006960108342436,
      "loss": 0.5056,
      "step": 14340
    },
    {
      "epoch": 0.9059343434343434,
      "grad_norm": 0.6988786458969116,
      "learning_rate": 0.00011996943530475779,
      "loss": 0.5545,
      "step": 14350
    },
    {
      "epoch": 0.9065656565656566,
      "grad_norm": 0.4184214174747467,
      "learning_rate": 0.0001198692486538715,
      "loss": 0.8558,
      "step": 14360
    },
    {
      "epoch": 0.9071969696969697,
      "grad_norm": 0.40464258193969727,
      "learning_rate": 0.00011976904123548151,
      "loss": 0.6861,
      "step": 14370
    },
    {
      "epoch": 0.9078282828282829,
      "grad_norm": 0.48992007970809937,
      "learning_rate": 0.00011966881315432539,
      "loss": 0.5851,
      "step": 14380
    },
    {
      "epoch": 0.9084595959595959,
      "grad_norm": 0.5153314471244812,
      "learning_rate": 0.00011956856451516239,
      "loss": 0.5597,
      "step": 14390
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.7092933654785156,
      "learning_rate": 0.00011946829542277313,
      "loss": 0.5474,
      "step": 14400
    },
    {
      "epoch": 0.9097222222222222,
      "grad_norm": 0.3777289092540741,
      "learning_rate": 0.00011936800598195983,
      "loss": 0.8565,
      "step": 14410
    },
    {
      "epoch": 0.9103535353535354,
      "grad_norm": 0.40755805373191833,
      "learning_rate": 0.00011926769629754582,
      "loss": 0.685,
      "step": 14420
    },
    {
      "epoch": 0.9109848484848485,
      "grad_norm": 0.4375799298286438,
      "learning_rate": 0.00011916736647437552,
      "loss": 0.6065,
      "step": 14430
    },
    {
      "epoch": 0.9116161616161617,
      "grad_norm": 0.5308750867843628,
      "learning_rate": 0.00011906701661731461,
      "loss": 0.5329,
      "step": 14440
    },
    {
      "epoch": 0.9122474747474747,
      "grad_norm": 0.7480524182319641,
      "learning_rate": 0.00011896664683124949,
      "loss": 0.5468,
      "step": 14450
    },
    {
      "epoch": 0.9128787878787878,
      "grad_norm": 0.42764797806739807,
      "learning_rate": 0.00011886625722108759,
      "loss": 0.8035,
      "step": 14460
    },
    {
      "epoch": 0.913510101010101,
      "grad_norm": 0.3668133020401001,
      "learning_rate": 0.0001187658478917569,
      "loss": 0.6994,
      "step": 14470
    },
    {
      "epoch": 0.9141414141414141,
      "grad_norm": 0.45702362060546875,
      "learning_rate": 0.00011866541894820613,
      "loss": 0.6076,
      "step": 14480
    },
    {
      "epoch": 0.9147727272727273,
      "grad_norm": 0.4899498522281647,
      "learning_rate": 0.00011856497049540438,
      "loss": 0.5479,
      "step": 14490
    },
    {
      "epoch": 0.9154040404040404,
      "grad_norm": 0.7795534133911133,
      "learning_rate": 0.0001184645026383413,
      "loss": 0.5857,
      "step": 14500
    },
    {
      "epoch": 0.9160353535353535,
      "grad_norm": 0.38651201128959656,
      "learning_rate": 0.00011836401548202665,
      "loss": 0.8186,
      "step": 14510
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.43762415647506714,
      "learning_rate": 0.0001182635091314905,
      "loss": 0.6718,
      "step": 14520
    },
    {
      "epoch": 0.9172979797979798,
      "grad_norm": 0.43279486894607544,
      "learning_rate": 0.00011816298369178287,
      "loss": 0.5907,
      "step": 14530
    },
    {
      "epoch": 0.9179292929292929,
      "grad_norm": 0.43264245986938477,
      "learning_rate": 0.00011806243926797378,
      "loss": 0.5236,
      "step": 14540
    },
    {
      "epoch": 0.9185606060606061,
      "grad_norm": 0.7412869334220886,
      "learning_rate": 0.00011796187596515316,
      "loss": 0.5494,
      "step": 14550
    },
    {
      "epoch": 0.9191919191919192,
      "grad_norm": 0.3959885537624359,
      "learning_rate": 0.00011786129388843055,
      "loss": 0.8219,
      "step": 14560
    },
    {
      "epoch": 0.9198232323232324,
      "grad_norm": 0.4204117953777313,
      "learning_rate": 0.00011776069314293523,
      "loss": 0.7066,
      "step": 14570
    },
    {
      "epoch": 0.9204545454545454,
      "grad_norm": 0.4738713204860687,
      "learning_rate": 0.00011766007383381586,
      "loss": 0.618,
      "step": 14580
    },
    {
      "epoch": 0.9210858585858586,
      "grad_norm": 0.4939231276512146,
      "learning_rate": 0.00011755943606624064,
      "loss": 0.5619,
      "step": 14590
    },
    {
      "epoch": 0.9217171717171717,
      "grad_norm": 0.7559062242507935,
      "learning_rate": 0.00011745877994539696,
      "loss": 0.5879,
      "step": 14600
    },
    {
      "epoch": 0.9223484848484849,
      "grad_norm": 0.4079274535179138,
      "learning_rate": 0.00011735810557649148,
      "loss": 0.8226,
      "step": 14610
    },
    {
      "epoch": 0.922979797979798,
      "grad_norm": 0.42304134368896484,
      "learning_rate": 0.00011725741306474982,
      "loss": 0.7129,
      "step": 14620
    },
    {
      "epoch": 0.9236111111111112,
      "grad_norm": 0.4407094717025757,
      "learning_rate": 0.0001171567025154167,
      "loss": 0.6133,
      "step": 14630
    },
    {
      "epoch": 0.9242424242424242,
      "grad_norm": 0.4712275266647339,
      "learning_rate": 0.00011705597403375559,
      "loss": 0.5295,
      "step": 14640
    },
    {
      "epoch": 0.9248737373737373,
      "grad_norm": 0.7023929357528687,
      "learning_rate": 0.00011695522772504872,
      "loss": 0.5415,
      "step": 14650
    },
    {
      "epoch": 0.9255050505050505,
      "grad_norm": 0.3634699285030365,
      "learning_rate": 0.000116854463694597,
      "loss": 0.8833,
      "step": 14660
    },
    {
      "epoch": 0.9261363636363636,
      "grad_norm": 0.39813223481178284,
      "learning_rate": 0.00011675368204771979,
      "loss": 0.6718,
      "step": 14670
    },
    {
      "epoch": 0.9267676767676768,
      "grad_norm": 0.4547695517539978,
      "learning_rate": 0.00011665288288975498,
      "loss": 0.5968,
      "step": 14680
    },
    {
      "epoch": 0.92739898989899,
      "grad_norm": 0.4529993534088135,
      "learning_rate": 0.0001165520663260586,
      "loss": 0.5211,
      "step": 14690
    },
    {
      "epoch": 0.928030303030303,
      "grad_norm": 0.6980299353599548,
      "learning_rate": 0.00011645123246200502,
      "loss": 0.5923,
      "step": 14700
    },
    {
      "epoch": 0.9286616161616161,
      "grad_norm": 0.3772542476654053,
      "learning_rate": 0.0001163503814029866,
      "loss": 0.7663,
      "step": 14710
    },
    {
      "epoch": 0.9292929292929293,
      "grad_norm": 0.41288384795188904,
      "learning_rate": 0.0001162495132544137,
      "loss": 0.6895,
      "step": 14720
    },
    {
      "epoch": 0.9299242424242424,
      "grad_norm": 0.46760642528533936,
      "learning_rate": 0.00011614862812171458,
      "loss": 0.6074,
      "step": 14730
    },
    {
      "epoch": 0.9305555555555556,
      "grad_norm": 0.4947351813316345,
      "learning_rate": 0.00011604772611033522,
      "loss": 0.56,
      "step": 14740
    },
    {
      "epoch": 0.9311868686868687,
      "grad_norm": 0.639689564704895,
      "learning_rate": 0.0001159468073257392,
      "loss": 0.5598,
      "step": 14750
    },
    {
      "epoch": 0.9318181818181818,
      "grad_norm": 0.3957979381084442,
      "learning_rate": 0.0001158458718734077,
      "loss": 0.7862,
      "step": 14760
    },
    {
      "epoch": 0.9324494949494949,
      "grad_norm": 0.40098583698272705,
      "learning_rate": 0.00011574491985883931,
      "loss": 0.6895,
      "step": 14770
    },
    {
      "epoch": 0.9330808080808081,
      "grad_norm": 0.43454307317733765,
      "learning_rate": 0.00011564395138754984,
      "loss": 0.5862,
      "step": 14780
    },
    {
      "epoch": 0.9337121212121212,
      "grad_norm": 0.44166627526283264,
      "learning_rate": 0.00011554296656507246,
      "loss": 0.51,
      "step": 14790
    },
    {
      "epoch": 0.9343434343434344,
      "grad_norm": 0.822049081325531,
      "learning_rate": 0.0001154419654969573,
      "loss": 0.6098,
      "step": 14800
    },
    {
      "epoch": 0.9349747474747475,
      "grad_norm": 0.3803097605705261,
      "learning_rate": 0.00011534094828877154,
      "loss": 0.842,
      "step": 14810
    },
    {
      "epoch": 0.9356060606060606,
      "grad_norm": 0.4166465997695923,
      "learning_rate": 0.00011523991504609917,
      "loss": 0.701,
      "step": 14820
    },
    {
      "epoch": 0.9362373737373737,
      "grad_norm": 0.42946329712867737,
      "learning_rate": 0.00011513886587454101,
      "loss": 0.6253,
      "step": 14830
    },
    {
      "epoch": 0.9368686868686869,
      "grad_norm": 0.47377562522888184,
      "learning_rate": 0.00011503780087971448,
      "loss": 0.5293,
      "step": 14840
    },
    {
      "epoch": 0.9375,
      "grad_norm": 0.7041848301887512,
      "learning_rate": 0.00011493672016725356,
      "loss": 0.5476,
      "step": 14850
    },
    {
      "epoch": 0.9381313131313131,
      "grad_norm": 0.38447532057762146,
      "learning_rate": 0.00011483562384280864,
      "loss": 0.8109,
      "step": 14860
    },
    {
      "epoch": 0.9387626262626263,
      "grad_norm": 0.4161527454853058,
      "learning_rate": 0.00011473451201204644,
      "loss": 0.6913,
      "step": 14870
    },
    {
      "epoch": 0.9393939393939394,
      "grad_norm": 0.4284558594226837,
      "learning_rate": 0.00011463338478064989,
      "loss": 0.5894,
      "step": 14880
    },
    {
      "epoch": 0.9400252525252525,
      "grad_norm": 0.46336302161216736,
      "learning_rate": 0.00011453224225431797,
      "loss": 0.5303,
      "step": 14890
    },
    {
      "epoch": 0.9406565656565656,
      "grad_norm": 0.610905647277832,
      "learning_rate": 0.00011443108453876579,
      "loss": 0.5412,
      "step": 14900
    },
    {
      "epoch": 0.9412878787878788,
      "grad_norm": 0.4192424416542053,
      "learning_rate": 0.00011432991173972412,
      "loss": 0.8546,
      "step": 14910
    },
    {
      "epoch": 0.9419191919191919,
      "grad_norm": 0.4213845729827881,
      "learning_rate": 0.0001142287239629397,
      "loss": 0.6714,
      "step": 14920
    },
    {
      "epoch": 0.9425505050505051,
      "grad_norm": 0.434122771024704,
      "learning_rate": 0.00011412752131417477,
      "loss": 0.6016,
      "step": 14930
    },
    {
      "epoch": 0.9431818181818182,
      "grad_norm": 0.4454551339149475,
      "learning_rate": 0.00011402630389920723,
      "loss": 0.5337,
      "step": 14940
    },
    {
      "epoch": 0.9438131313131313,
      "grad_norm": 0.7335276007652283,
      "learning_rate": 0.0001139250718238303,
      "loss": 0.5618,
      "step": 14950
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 0.37566083669662476,
      "learning_rate": 0.00011382382519385266,
      "loss": 0.8322,
      "step": 14960
    },
    {
      "epoch": 0.9450757575757576,
      "grad_norm": 0.41348570585250854,
      "learning_rate": 0.00011372256411509808,
      "loss": 0.6754,
      "step": 14970
    },
    {
      "epoch": 0.9457070707070707,
      "grad_norm": 0.43709030747413635,
      "learning_rate": 0.00011362128869340549,
      "loss": 0.6007,
      "step": 14980
    },
    {
      "epoch": 0.9463383838383839,
      "grad_norm": 0.45085644721984863,
      "learning_rate": 0.00011351999903462882,
      "loss": 0.5501,
      "step": 14990
    },
    {
      "epoch": 0.946969696969697,
      "grad_norm": 0.7417927980422974,
      "learning_rate": 0.00011341869524463684,
      "loss": 0.5492,
      "step": 15000
    },
    {
      "epoch": 0.946969696969697,
      "eval_loss": 0.6554573178291321,
      "eval_runtime": 31.6739,
      "eval_samples_per_second": 80.824,
      "eval_steps_per_second": 10.103,
      "step": 15000
    },
    {
      "epoch": 0.94760101010101,
      "grad_norm": 0.4102729856967926,
      "learning_rate": 0.00011331737742931314,
      "loss": 0.8318,
      "step": 15010
    },
    {
      "epoch": 0.9482323232323232,
      "grad_norm": 0.45132431387901306,
      "learning_rate": 0.00011321604569455591,
      "loss": 0.6784,
      "step": 15020
    },
    {
      "epoch": 0.9488636363636364,
      "grad_norm": 0.4406030476093292,
      "learning_rate": 0.00011311470014627792,
      "loss": 0.5823,
      "step": 15030
    },
    {
      "epoch": 0.9494949494949495,
      "grad_norm": 0.5018042325973511,
      "learning_rate": 0.00011301334089040641,
      "loss": 0.5272,
      "step": 15040
    },
    {
      "epoch": 0.9501262626262627,
      "grad_norm": 0.7167516946792603,
      "learning_rate": 0.00011291196803288291,
      "loss": 0.5353,
      "step": 15050
    },
    {
      "epoch": 0.9507575757575758,
      "grad_norm": 0.4005897343158722,
      "learning_rate": 0.00011281058167966313,
      "loss": 0.8043,
      "step": 15060
    },
    {
      "epoch": 0.9513888888888888,
      "grad_norm": 0.45400261878967285,
      "learning_rate": 0.000112709181936717,
      "loss": 0.6968,
      "step": 15070
    },
    {
      "epoch": 0.952020202020202,
      "grad_norm": 0.39645203948020935,
      "learning_rate": 0.00011260776891002831,
      "loss": 0.5921,
      "step": 15080
    },
    {
      "epoch": 0.9526515151515151,
      "grad_norm": 0.5125328302383423,
      "learning_rate": 0.00011250634270559484,
      "loss": 0.5426,
      "step": 15090
    },
    {
      "epoch": 0.9532828282828283,
      "grad_norm": 0.6870701313018799,
      "learning_rate": 0.00011240490342942806,
      "loss": 0.5775,
      "step": 15100
    },
    {
      "epoch": 0.9539141414141414,
      "grad_norm": 0.372134804725647,
      "learning_rate": 0.00011230345118755319,
      "loss": 0.7973,
      "step": 15110
    },
    {
      "epoch": 0.9545454545454546,
      "grad_norm": 0.4136431813240051,
      "learning_rate": 0.00011220198608600894,
      "loss": 0.6652,
      "step": 15120
    },
    {
      "epoch": 0.9551767676767676,
      "grad_norm": 0.491595059633255,
      "learning_rate": 0.00011210050823084746,
      "loss": 0.6169,
      "step": 15130
    },
    {
      "epoch": 0.9558080808080808,
      "grad_norm": 0.4354867935180664,
      "learning_rate": 0.00011199901772813426,
      "loss": 0.5248,
      "step": 15140
    },
    {
      "epoch": 0.9564393939393939,
      "grad_norm": 0.7879035472869873,
      "learning_rate": 0.00011189751468394805,
      "loss": 0.5428,
      "step": 15150
    },
    {
      "epoch": 0.9570707070707071,
      "grad_norm": 0.38792240619659424,
      "learning_rate": 0.00011179599920438066,
      "loss": 0.8051,
      "step": 15160
    },
    {
      "epoch": 0.9577020202020202,
      "grad_norm": 0.4376166760921478,
      "learning_rate": 0.00011169447139553691,
      "loss": 0.6744,
      "step": 15170
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 0.46887460350990295,
      "learning_rate": 0.00011159293136353452,
      "loss": 0.6076,
      "step": 15180
    },
    {
      "epoch": 0.9589646464646465,
      "grad_norm": 0.48365020751953125,
      "learning_rate": 0.00011149137921450396,
      "loss": 0.5641,
      "step": 15190
    },
    {
      "epoch": 0.9595959595959596,
      "grad_norm": 0.828226625919342,
      "learning_rate": 0.00011138981505458841,
      "loss": 0.5692,
      "step": 15200
    },
    {
      "epoch": 0.9602272727272727,
      "grad_norm": 0.4023291766643524,
      "learning_rate": 0.00011128823898994351,
      "loss": 0.8184,
      "step": 15210
    },
    {
      "epoch": 0.9608585858585859,
      "grad_norm": 0.3986116647720337,
      "learning_rate": 0.00011118665112673749,
      "loss": 0.6531,
      "step": 15220
    },
    {
      "epoch": 0.961489898989899,
      "grad_norm": 0.4188527464866638,
      "learning_rate": 0.00011108505157115079,
      "loss": 0.6222,
      "step": 15230
    },
    {
      "epoch": 0.9621212121212122,
      "grad_norm": 0.4313940107822418,
      "learning_rate": 0.00011098344042937605,
      "loss": 0.5244,
      "step": 15240
    },
    {
      "epoch": 0.9627525252525253,
      "grad_norm": 0.6391387581825256,
      "learning_rate": 0.0001108818178076182,
      "loss": 0.5281,
      "step": 15250
    },
    {
      "epoch": 0.9633838383838383,
      "grad_norm": 0.3870745897293091,
      "learning_rate": 0.00011078018381209392,
      "loss": 0.8214,
      "step": 15260
    },
    {
      "epoch": 0.9640151515151515,
      "grad_norm": 0.4556797444820404,
      "learning_rate": 0.000110678538549032,
      "loss": 0.6784,
      "step": 15270
    },
    {
      "epoch": 0.9646464646464646,
      "grad_norm": 0.42930614948272705,
      "learning_rate": 0.00011057688212467287,
      "loss": 0.5966,
      "step": 15280
    },
    {
      "epoch": 0.9652777777777778,
      "grad_norm": 0.47973641753196716,
      "learning_rate": 0.00011047521464526871,
      "loss": 0.5135,
      "step": 15290
    },
    {
      "epoch": 0.9659090909090909,
      "grad_norm": 0.7528154253959656,
      "learning_rate": 0.00011037353621708315,
      "loss": 0.5381,
      "step": 15300
    },
    {
      "epoch": 0.9665404040404041,
      "grad_norm": 0.3926270008087158,
      "learning_rate": 0.00011027184694639139,
      "loss": 0.8201,
      "step": 15310
    },
    {
      "epoch": 0.9671717171717171,
      "grad_norm": 0.4088374376296997,
      "learning_rate": 0.00011017014693947987,
      "loss": 0.6931,
      "step": 15320
    },
    {
      "epoch": 0.9678030303030303,
      "grad_norm": 0.4694957137107849,
      "learning_rate": 0.00011006843630264628,
      "loss": 0.6032,
      "step": 15330
    },
    {
      "epoch": 0.9684343434343434,
      "grad_norm": 0.4740636646747589,
      "learning_rate": 0.00010996671514219943,
      "loss": 0.5322,
      "step": 15340
    },
    {
      "epoch": 0.9690656565656566,
      "grad_norm": 0.6540841460227966,
      "learning_rate": 0.00010986498356445912,
      "loss": 0.5412,
      "step": 15350
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 0.3836665153503418,
      "learning_rate": 0.00010976324167575604,
      "loss": 0.8319,
      "step": 15360
    },
    {
      "epoch": 0.9703282828282829,
      "grad_norm": 0.41627421975135803,
      "learning_rate": 0.00010966148958243165,
      "loss": 0.6746,
      "step": 15370
    },
    {
      "epoch": 0.9709595959595959,
      "grad_norm": 0.4410651624202728,
      "learning_rate": 0.00010955972739083811,
      "loss": 0.5809,
      "step": 15380
    },
    {
      "epoch": 0.9715909090909091,
      "grad_norm": 0.5013641715049744,
      "learning_rate": 0.00010945795520733808,
      "loss": 0.5075,
      "step": 15390
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 0.7169002890586853,
      "learning_rate": 0.0001093561731383047,
      "loss": 0.5621,
      "step": 15400
    },
    {
      "epoch": 0.9728535353535354,
      "grad_norm": 0.3983255624771118,
      "learning_rate": 0.00010925438129012142,
      "loss": 0.8149,
      "step": 15410
    },
    {
      "epoch": 0.9734848484848485,
      "grad_norm": 0.42440590262413025,
      "learning_rate": 0.00010915257976918196,
      "loss": 0.7129,
      "step": 15420
    },
    {
      "epoch": 0.9741161616161617,
      "grad_norm": 0.42855212092399597,
      "learning_rate": 0.00010905076868189008,
      "loss": 0.6029,
      "step": 15430
    },
    {
      "epoch": 0.9747474747474747,
      "grad_norm": 0.4746481478214264,
      "learning_rate": 0.00010894894813465961,
      "loss": 0.5147,
      "step": 15440
    },
    {
      "epoch": 0.9753787878787878,
      "grad_norm": 0.7695639133453369,
      "learning_rate": 0.00010884711823391418,
      "loss": 0.5478,
      "step": 15450
    },
    {
      "epoch": 0.976010101010101,
      "grad_norm": 0.38739314675331116,
      "learning_rate": 0.00010874527908608732,
      "loss": 0.84,
      "step": 15460
    },
    {
      "epoch": 0.9766414141414141,
      "grad_norm": 0.44229286909103394,
      "learning_rate": 0.00010864343079762209,
      "loss": 0.7032,
      "step": 15470
    },
    {
      "epoch": 0.9772727272727273,
      "grad_norm": 0.4456184208393097,
      "learning_rate": 0.00010854157347497118,
      "loss": 0.5777,
      "step": 15480
    },
    {
      "epoch": 0.9779040404040404,
      "grad_norm": 0.4372856914997101,
      "learning_rate": 0.00010843970722459675,
      "loss": 0.5124,
      "step": 15490
    },
    {
      "epoch": 0.9785353535353535,
      "grad_norm": 0.6481226682662964,
      "learning_rate": 0.00010833783215297019,
      "loss": 0.5522,
      "step": 15500
    },
    {
      "epoch": 0.9791666666666666,
      "grad_norm": 0.3829697370529175,
      "learning_rate": 0.00010823594836657223,
      "loss": 0.8315,
      "step": 15510
    },
    {
      "epoch": 0.9797979797979798,
      "grad_norm": 0.43208009004592896,
      "learning_rate": 0.00010813405597189259,
      "loss": 0.671,
      "step": 15520
    },
    {
      "epoch": 0.9804292929292929,
      "grad_norm": 0.430987685918808,
      "learning_rate": 0.0001080321550754301,
      "loss": 0.5893,
      "step": 15530
    },
    {
      "epoch": 0.9810606060606061,
      "grad_norm": 0.4601426422595978,
      "learning_rate": 0.0001079302457836924,
      "loss": 0.5383,
      "step": 15540
    },
    {
      "epoch": 0.9816919191919192,
      "grad_norm": 0.7079986333847046,
      "learning_rate": 0.00010782832820319593,
      "loss": 0.5563,
      "step": 15550
    },
    {
      "epoch": 0.9823232323232324,
      "grad_norm": 0.4296407699584961,
      "learning_rate": 0.00010772640244046576,
      "loss": 0.8429,
      "step": 15560
    },
    {
      "epoch": 0.9829545454545454,
      "grad_norm": 0.40231817960739136,
      "learning_rate": 0.00010762446860203563,
      "loss": 0.669,
      "step": 15570
    },
    {
      "epoch": 0.9835858585858586,
      "grad_norm": 0.41770851612091064,
      "learning_rate": 0.00010752252679444755,
      "loss": 0.5869,
      "step": 15580
    },
    {
      "epoch": 0.9842171717171717,
      "grad_norm": 0.49466225504875183,
      "learning_rate": 0.00010742057712425199,
      "loss": 0.5351,
      "step": 15590
    },
    {
      "epoch": 0.9848484848484849,
      "grad_norm": 0.6917340755462646,
      "learning_rate": 0.00010731861969800758,
      "loss": 0.5682,
      "step": 15600
    },
    {
      "epoch": 0.985479797979798,
      "grad_norm": 0.41410085558891296,
      "learning_rate": 0.00010721665462228105,
      "loss": 0.8005,
      "step": 15610
    },
    {
      "epoch": 0.9861111111111112,
      "grad_norm": 0.4417104125022888,
      "learning_rate": 0.00010711468200364718,
      "loss": 0.7066,
      "step": 15620
    },
    {
      "epoch": 0.9867424242424242,
      "grad_norm": 0.43248382210731506,
      "learning_rate": 0.00010701270194868856,
      "loss": 0.5942,
      "step": 15630
    },
    {
      "epoch": 0.9873737373737373,
      "grad_norm": 0.4684816002845764,
      "learning_rate": 0.00010691071456399561,
      "loss": 0.5491,
      "step": 15640
    },
    {
      "epoch": 0.9880050505050505,
      "grad_norm": 0.714117169380188,
      "learning_rate": 0.00010680871995616639,
      "loss": 0.5387,
      "step": 15650
    },
    {
      "epoch": 0.9886363636363636,
      "grad_norm": 0.4158186912536621,
      "learning_rate": 0.00010670671823180651,
      "loss": 0.8852,
      "step": 15660
    },
    {
      "epoch": 0.9892676767676768,
      "grad_norm": 0.41978946328163147,
      "learning_rate": 0.00010660470949752903,
      "loss": 0.6819,
      "step": 15670
    },
    {
      "epoch": 0.98989898989899,
      "grad_norm": 0.4281819760799408,
      "learning_rate": 0.00010650269385995433,
      "loss": 0.6224,
      "step": 15680
    },
    {
      "epoch": 0.990530303030303,
      "grad_norm": 0.47746744751930237,
      "learning_rate": 0.00010640067142570995,
      "loss": 0.5336,
      "step": 15690
    },
    {
      "epoch": 0.9911616161616161,
      "grad_norm": 0.737670361995697,
      "learning_rate": 0.00010629864230143067,
      "loss": 0.5649,
      "step": 15700
    },
    {
      "epoch": 0.9917929292929293,
      "grad_norm": 0.4266386032104492,
      "learning_rate": 0.00010619660659375814,
      "loss": 0.8145,
      "step": 15710
    },
    {
      "epoch": 0.9924242424242424,
      "grad_norm": 0.4299318492412567,
      "learning_rate": 0.0001060945644093409,
      "loss": 0.6514,
      "step": 15720
    },
    {
      "epoch": 0.9930555555555556,
      "grad_norm": 0.4141492247581482,
      "learning_rate": 0.0001059925158548343,
      "loss": 0.6077,
      "step": 15730
    },
    {
      "epoch": 0.9936868686868687,
      "grad_norm": 0.46187371015548706,
      "learning_rate": 0.00010589046103690035,
      "loss": 0.5399,
      "step": 15740
    },
    {
      "epoch": 0.9943181818181818,
      "grad_norm": 0.7786952257156372,
      "learning_rate": 0.00010578840006220763,
      "loss": 0.5368,
      "step": 15750
    },
    {
      "epoch": 0.9949494949494949,
      "grad_norm": 0.39749422669410706,
      "learning_rate": 0.00010568633303743102,
      "loss": 0.8327,
      "step": 15760
    },
    {
      "epoch": 0.9955808080808081,
      "grad_norm": 0.4417003095149994,
      "learning_rate": 0.00010558426006925195,
      "loss": 0.6854,
      "step": 15770
    },
    {
      "epoch": 0.9962121212121212,
      "grad_norm": 0.44338297843933105,
      "learning_rate": 0.0001054821812643578,
      "loss": 0.6136,
      "step": 15780
    },
    {
      "epoch": 0.9968434343434344,
      "grad_norm": 0.49987879395484924,
      "learning_rate": 0.00010538009672944231,
      "loss": 0.5312,
      "step": 15790
    },
    {
      "epoch": 0.9974747474747475,
      "grad_norm": 0.7082954049110413,
      "learning_rate": 0.000105278006571205,
      "loss": 0.5447,
      "step": 15800
    },
    {
      "epoch": 0.9981060606060606,
      "grad_norm": 0.41318991780281067,
      "learning_rate": 0.0001051759108963514,
      "loss": 0.8004,
      "step": 15810
    },
    {
      "epoch": 0.9987373737373737,
      "grad_norm": 0.41791775822639465,
      "learning_rate": 0.00010507380981159272,
      "loss": 0.6177,
      "step": 15820
    },
    {
      "epoch": 0.9993686868686869,
      "grad_norm": 0.40846577286720276,
      "learning_rate": 0.00010497170342364586,
      "loss": 0.5635,
      "step": 15830
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.8246739506721497,
      "learning_rate": 0.0001048695918392333,
      "loss": 0.5431,
      "step": 15840
    },
    {
      "epoch": 1.0006313131313131,
      "grad_norm": 0.37637418508529663,
      "learning_rate": 0.0001047674751650829,
      "loss": 0.7421,
      "step": 15850
    },
    {
      "epoch": 1.0012626262626263,
      "grad_norm": 0.42997756600379944,
      "learning_rate": 0.00010466535350792786,
      "loss": 0.6479,
      "step": 15860
    },
    {
      "epoch": 1.0018939393939394,
      "grad_norm": 0.43426236510276794,
      "learning_rate": 0.00010456322697450654,
      "loss": 0.5919,
      "step": 15870
    },
    {
      "epoch": 1.0025252525252526,
      "grad_norm": 0.5045300126075745,
      "learning_rate": 0.00010446109567156252,
      "loss": 0.4836,
      "step": 15880
    },
    {
      "epoch": 1.0031565656565657,
      "grad_norm": 0.7320950627326965,
      "learning_rate": 0.00010435895970584422,
      "loss": 0.5065,
      "step": 15890
    },
    {
      "epoch": 1.003787878787879,
      "grad_norm": 0.4099649488925934,
      "learning_rate": 0.00010425681918410506,
      "loss": 0.8144,
      "step": 15900
    },
    {
      "epoch": 1.0044191919191918,
      "grad_norm": 0.42308539152145386,
      "learning_rate": 0.00010415467421310308,
      "loss": 0.6368,
      "step": 15910
    },
    {
      "epoch": 1.005050505050505,
      "grad_norm": 0.44454237818717957,
      "learning_rate": 0.00010405252489960113,
      "loss": 0.5851,
      "step": 15920
    },
    {
      "epoch": 1.0056818181818181,
      "grad_norm": 0.5226922035217285,
      "learning_rate": 0.00010395037135036649,
      "loss": 0.5283,
      "step": 15930
    },
    {
      "epoch": 1.0063131313131313,
      "grad_norm": 0.5933412909507751,
      "learning_rate": 0.0001038482136721709,
      "loss": 0.4921,
      "step": 15940
    },
    {
      "epoch": 1.0069444444444444,
      "grad_norm": 0.4007224440574646,
      "learning_rate": 0.00010374605197179044,
      "loss": 0.753,
      "step": 15950
    },
    {
      "epoch": 1.0075757575757576,
      "grad_norm": 0.41831499338150024,
      "learning_rate": 0.00010364388635600529,
      "loss": 0.6703,
      "step": 15960
    },
    {
      "epoch": 1.0082070707070707,
      "grad_norm": 0.47179847955703735,
      "learning_rate": 0.00010354171693159989,
      "loss": 0.5813,
      "step": 15970
    },
    {
      "epoch": 1.0088383838383839,
      "grad_norm": 0.46716779470443726,
      "learning_rate": 0.0001034395438053625,
      "loss": 0.5041,
      "step": 15980
    },
    {
      "epoch": 1.009469696969697,
      "grad_norm": 0.7118711471557617,
      "learning_rate": 0.00010333736708408537,
      "loss": 0.5178,
      "step": 15990
    },
    {
      "epoch": 1.0101010101010102,
      "grad_norm": 0.40058913826942444,
      "learning_rate": 0.00010323518687456442,
      "loss": 0.7886,
      "step": 16000
    },
    {
      "epoch": 1.0101010101010102,
      "eval_loss": 0.6537434458732605,
      "eval_runtime": 31.8318,
      "eval_samples_per_second": 80.423,
      "eval_steps_per_second": 10.053,
      "step": 16000
    },
    {
      "epoch": 1.0107323232323233,
      "grad_norm": 0.41854897141456604,
      "learning_rate": 0.0001031330032835993,
      "loss": 0.6795,
      "step": 16010
    },
    {
      "epoch": 1.0113636363636365,
      "grad_norm": 0.46103334426879883,
      "learning_rate": 0.0001030308164179931,
      "loss": 0.5791,
      "step": 16020
    },
    {
      "epoch": 1.0119949494949494,
      "grad_norm": 0.4788380265235901,
      "learning_rate": 0.00010292862638455242,
      "loss": 0.5118,
      "step": 16030
    },
    {
      "epoch": 1.0126262626262625,
      "grad_norm": 0.6869479417800903,
      "learning_rate": 0.0001028264332900871,
      "loss": 0.5233,
      "step": 16040
    },
    {
      "epoch": 1.0132575757575757,
      "grad_norm": 0.40825706720352173,
      "learning_rate": 0.00010272423724141025,
      "loss": 0.7605,
      "step": 16050
    },
    {
      "epoch": 1.0138888888888888,
      "grad_norm": 0.4223894476890564,
      "learning_rate": 0.00010262203834533801,
      "loss": 0.6529,
      "step": 16060
    },
    {
      "epoch": 1.014520202020202,
      "grad_norm": 0.4253254532814026,
      "learning_rate": 0.00010251983670868947,
      "loss": 0.5965,
      "step": 16070
    },
    {
      "epoch": 1.0151515151515151,
      "grad_norm": 0.4931151270866394,
      "learning_rate": 0.00010241763243828671,
      "loss": 0.5065,
      "step": 16080
    },
    {
      "epoch": 1.0157828282828283,
      "grad_norm": 0.7993422150611877,
      "learning_rate": 0.00010231542564095442,
      "loss": 0.5216,
      "step": 16090
    },
    {
      "epoch": 1.0164141414141414,
      "grad_norm": 0.4308284521102905,
      "learning_rate": 0.00010221321642352002,
      "loss": 0.8057,
      "step": 16100
    },
    {
      "epoch": 1.0170454545454546,
      "grad_norm": 0.42484942078590393,
      "learning_rate": 0.00010211100489281342,
      "loss": 0.6643,
      "step": 16110
    },
    {
      "epoch": 1.0176767676767677,
      "grad_norm": 0.4346823990345001,
      "learning_rate": 0.000102008791155667,
      "loss": 0.56,
      "step": 16120
    },
    {
      "epoch": 1.0183080808080809,
      "grad_norm": 0.47582128643989563,
      "learning_rate": 0.00010190657531891535,
      "loss": 0.4967,
      "step": 16130
    },
    {
      "epoch": 1.018939393939394,
      "grad_norm": 0.7623319029808044,
      "learning_rate": 0.00010180435748939533,
      "loss": 0.5094,
      "step": 16140
    },
    {
      "epoch": 1.0195707070707072,
      "grad_norm": 0.44875872135162354,
      "learning_rate": 0.00010170213777394591,
      "loss": 0.771,
      "step": 16150
    },
    {
      "epoch": 1.02020202020202,
      "grad_norm": 0.4169301688671112,
      "learning_rate": 0.00010159991627940793,
      "loss": 0.6583,
      "step": 16160
    },
    {
      "epoch": 1.0208333333333333,
      "grad_norm": 0.4330001175403595,
      "learning_rate": 0.00010149769311262413,
      "loss": 0.5809,
      "step": 16170
    },
    {
      "epoch": 1.0214646464646464,
      "grad_norm": 0.4810887575149536,
      "learning_rate": 0.00010139546838043909,
      "loss": 0.5371,
      "step": 16180
    },
    {
      "epoch": 1.0220959595959596,
      "grad_norm": 0.6918138861656189,
      "learning_rate": 0.00010129324218969894,
      "loss": 0.5332,
      "step": 16190
    },
    {
      "epoch": 1.0227272727272727,
      "grad_norm": 0.42057764530181885,
      "learning_rate": 0.00010119101464725125,
      "loss": 0.8173,
      "step": 16200
    },
    {
      "epoch": 1.0233585858585859,
      "grad_norm": 0.41253191232681274,
      "learning_rate": 0.00010108878585994517,
      "loss": 0.6765,
      "step": 16210
    },
    {
      "epoch": 1.023989898989899,
      "grad_norm": 0.42354780435562134,
      "learning_rate": 0.00010098655593463107,
      "loss": 0.5805,
      "step": 16220
    },
    {
      "epoch": 1.0246212121212122,
      "grad_norm": 0.4569184482097626,
      "learning_rate": 0.00010088432497816052,
      "loss": 0.5134,
      "step": 16230
    },
    {
      "epoch": 1.0252525252525253,
      "grad_norm": 0.6027874946594238,
      "learning_rate": 0.00010078209309738614,
      "loss": 0.5285,
      "step": 16240
    },
    {
      "epoch": 1.0258838383838385,
      "grad_norm": 0.424534410238266,
      "learning_rate": 0.00010067986039916157,
      "loss": 0.8552,
      "step": 16250
    },
    {
      "epoch": 1.0265151515151516,
      "grad_norm": 0.4223536550998688,
      "learning_rate": 0.00010057762699034123,
      "loss": 0.6478,
      "step": 16260
    },
    {
      "epoch": 1.0271464646464648,
      "grad_norm": 0.4014786183834076,
      "learning_rate": 0.00010047539297778042,
      "loss": 0.5814,
      "step": 16270
    },
    {
      "epoch": 1.0277777777777777,
      "grad_norm": 0.47112956643104553,
      "learning_rate": 0.00010037315846833489,
      "loss": 0.5212,
      "step": 16280
    },
    {
      "epoch": 1.0284090909090908,
      "grad_norm": 0.6816920042037964,
      "learning_rate": 0.00010027092356886102,
      "loss": 0.5425,
      "step": 16290
    },
    {
      "epoch": 1.029040404040404,
      "grad_norm": 0.40762466192245483,
      "learning_rate": 0.0001001686883862156,
      "loss": 0.8633,
      "step": 16300
    },
    {
      "epoch": 1.0296717171717171,
      "grad_norm": 0.3834340572357178,
      "learning_rate": 0.00010006645302725568,
      "loss": 0.6808,
      "step": 16310
    },
    {
      "epoch": 1.0303030303030303,
      "grad_norm": 0.4730844795703888,
      "learning_rate": 9.996421759883849e-05,
      "loss": 0.5925,
      "step": 16320
    },
    {
      "epoch": 1.0309343434343434,
      "grad_norm": 0.4281693398952484,
      "learning_rate": 9.986198220782135e-05,
      "loss": 0.5294,
      "step": 16330
    },
    {
      "epoch": 1.0315656565656566,
      "grad_norm": 0.7912710905075073,
      "learning_rate": 9.97597469610616e-05,
      "loss": 0.5151,
      "step": 16340
    },
    {
      "epoch": 1.0321969696969697,
      "grad_norm": 0.4433063268661499,
      "learning_rate": 9.965751196541628e-05,
      "loss": 0.8065,
      "step": 16350
    },
    {
      "epoch": 1.0328282828282829,
      "grad_norm": 0.40210413932800293,
      "learning_rate": 9.955527732774232e-05,
      "loss": 0.6258,
      "step": 16360
    },
    {
      "epoch": 1.033459595959596,
      "grad_norm": 0.4286053776741028,
      "learning_rate": 9.945304315489617e-05,
      "loss": 0.5643,
      "step": 16370
    },
    {
      "epoch": 1.0340909090909092,
      "grad_norm": 0.47549939155578613,
      "learning_rate": 9.93508095537339e-05,
      "loss": 0.513,
      "step": 16380
    },
    {
      "epoch": 1.0347222222222223,
      "grad_norm": 0.7549278736114502,
      "learning_rate": 9.924857663111083e-05,
      "loss": 0.481,
      "step": 16390
    },
    {
      "epoch": 1.0353535353535352,
      "grad_norm": 0.40826213359832764,
      "learning_rate": 9.914634449388173e-05,
      "loss": 0.8011,
      "step": 16400
    },
    {
      "epoch": 1.0359848484848484,
      "grad_norm": 0.4228736460208893,
      "learning_rate": 9.904411324890047e-05,
      "loss": 0.6488,
      "step": 16410
    },
    {
      "epoch": 1.0366161616161615,
      "grad_norm": 0.4546844959259033,
      "learning_rate": 9.894188300301996e-05,
      "loss": 0.5854,
      "step": 16420
    },
    {
      "epoch": 1.0372474747474747,
      "grad_norm": 0.45419570803642273,
      "learning_rate": 9.883965386309214e-05,
      "loss": 0.5081,
      "step": 16430
    },
    {
      "epoch": 1.0378787878787878,
      "grad_norm": 0.620713472366333,
      "learning_rate": 9.87374259359677e-05,
      "loss": 0.504,
      "step": 16440
    },
    {
      "epoch": 1.038510101010101,
      "grad_norm": 0.4067384600639343,
      "learning_rate": 9.863519932849622e-05,
      "loss": 0.8395,
      "step": 16450
    },
    {
      "epoch": 1.0391414141414141,
      "grad_norm": 0.4533483386039734,
      "learning_rate": 9.853297414752569e-05,
      "loss": 0.6803,
      "step": 16460
    },
    {
      "epoch": 1.0397727272727273,
      "grad_norm": 0.4677666425704956,
      "learning_rate": 9.84307504999028e-05,
      "loss": 0.5994,
      "step": 16470
    },
    {
      "epoch": 1.0404040404040404,
      "grad_norm": 0.47854501008987427,
      "learning_rate": 9.83285284924725e-05,
      "loss": 0.5266,
      "step": 16480
    },
    {
      "epoch": 1.0410353535353536,
      "grad_norm": 0.7662001848220825,
      "learning_rate": 9.822630823207813e-05,
      "loss": 0.5161,
      "step": 16490
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 0.38045045733451843,
      "learning_rate": 9.81240898255611e-05,
      "loss": 0.7883,
      "step": 16500
    },
    {
      "epoch": 1.04229797979798,
      "grad_norm": 0.4327259957790375,
      "learning_rate": 9.8021873379761e-05,
      "loss": 0.6752,
      "step": 16510
    },
    {
      "epoch": 1.0429292929292928,
      "grad_norm": 0.4444340169429779,
      "learning_rate": 9.791965900151525e-05,
      "loss": 0.5625,
      "step": 16520
    },
    {
      "epoch": 1.043560606060606,
      "grad_norm": 0.5312319397926331,
      "learning_rate": 9.78174467976592e-05,
      "loss": 0.507,
      "step": 16530
    },
    {
      "epoch": 1.0441919191919191,
      "grad_norm": 0.7626638412475586,
      "learning_rate": 9.771523687502591e-05,
      "loss": 0.5496,
      "step": 16540
    },
    {
      "epoch": 1.0448232323232323,
      "grad_norm": 0.4364401400089264,
      "learning_rate": 9.761302934044598e-05,
      "loss": 0.8186,
      "step": 16550
    },
    {
      "epoch": 1.0454545454545454,
      "grad_norm": 0.4267435073852539,
      "learning_rate": 9.751082430074765e-05,
      "loss": 0.665,
      "step": 16560
    },
    {
      "epoch": 1.0460858585858586,
      "grad_norm": 0.46395987272262573,
      "learning_rate": 9.74086218627564e-05,
      "loss": 0.5698,
      "step": 16570
    },
    {
      "epoch": 1.0467171717171717,
      "grad_norm": 0.4736422300338745,
      "learning_rate": 9.730642213329514e-05,
      "loss": 0.4922,
      "step": 16580
    },
    {
      "epoch": 1.0473484848484849,
      "grad_norm": 0.6460751295089722,
      "learning_rate": 9.720422521918381e-05,
      "loss": 0.5091,
      "step": 16590
    },
    {
      "epoch": 1.047979797979798,
      "grad_norm": 0.40145501494407654,
      "learning_rate": 9.710203122723954e-05,
      "loss": 0.8195,
      "step": 16600
    },
    {
      "epoch": 1.0486111111111112,
      "grad_norm": 0.4360848367214203,
      "learning_rate": 9.699984026427626e-05,
      "loss": 0.6506,
      "step": 16610
    },
    {
      "epoch": 1.0492424242424243,
      "grad_norm": 0.4468562602996826,
      "learning_rate": 9.689765243710489e-05,
      "loss": 0.6101,
      "step": 16620
    },
    {
      "epoch": 1.0498737373737375,
      "grad_norm": 0.5236707329750061,
      "learning_rate": 9.679546785253294e-05,
      "loss": 0.5275,
      "step": 16630
    },
    {
      "epoch": 1.0505050505050506,
      "grad_norm": 0.734307050704956,
      "learning_rate": 9.669328661736464e-05,
      "loss": 0.5116,
      "step": 16640
    },
    {
      "epoch": 1.0511363636363635,
      "grad_norm": 0.42792779207229614,
      "learning_rate": 9.659110883840064e-05,
      "loss": 0.7847,
      "step": 16650
    },
    {
      "epoch": 1.0517676767676767,
      "grad_norm": 0.45372772216796875,
      "learning_rate": 9.648893462243797e-05,
      "loss": 0.6677,
      "step": 16660
    },
    {
      "epoch": 1.0523989898989898,
      "grad_norm": 0.4986220896244049,
      "learning_rate": 9.638676407627002e-05,
      "loss": 0.5902,
      "step": 16670
    },
    {
      "epoch": 1.053030303030303,
      "grad_norm": 0.44015011191368103,
      "learning_rate": 9.628459730668625e-05,
      "loss": 0.5053,
      "step": 16680
    },
    {
      "epoch": 1.0536616161616161,
      "grad_norm": 0.6253780722618103,
      "learning_rate": 9.618243442047227e-05,
      "loss": 0.5451,
      "step": 16690
    },
    {
      "epoch": 1.0542929292929293,
      "grad_norm": 0.39248859882354736,
      "learning_rate": 9.608027552440951e-05,
      "loss": 0.7943,
      "step": 16700
    },
    {
      "epoch": 1.0549242424242424,
      "grad_norm": 0.4390207827091217,
      "learning_rate": 9.597812072527535e-05,
      "loss": 0.639,
      "step": 16710
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 0.4365966320037842,
      "learning_rate": 9.587597012984282e-05,
      "loss": 0.5808,
      "step": 16720
    },
    {
      "epoch": 1.0561868686868687,
      "grad_norm": 0.5014045834541321,
      "learning_rate": 9.577382384488057e-05,
      "loss": 0.51,
      "step": 16730
    },
    {
      "epoch": 1.0568181818181819,
      "grad_norm": 0.7516974210739136,
      "learning_rate": 9.567168197715273e-05,
      "loss": 0.5362,
      "step": 16740
    },
    {
      "epoch": 1.057449494949495,
      "grad_norm": 0.40236231684684753,
      "learning_rate": 9.556954463341886e-05,
      "loss": 0.7691,
      "step": 16750
    },
    {
      "epoch": 1.0580808080808082,
      "grad_norm": 0.4297971725463867,
      "learning_rate": 9.546741192043373e-05,
      "loss": 0.6648,
      "step": 16760
    },
    {
      "epoch": 1.058712121212121,
      "grad_norm": 0.4667983949184418,
      "learning_rate": 9.536528394494733e-05,
      "loss": 0.6046,
      "step": 16770
    },
    {
      "epoch": 1.0593434343434343,
      "grad_norm": 0.5039950609207153,
      "learning_rate": 9.526316081370465e-05,
      "loss": 0.5048,
      "step": 16780
    },
    {
      "epoch": 1.0599747474747474,
      "grad_norm": 0.6864810585975647,
      "learning_rate": 9.516104263344562e-05,
      "loss": 0.507,
      "step": 16790
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 0.3954821825027466,
      "learning_rate": 9.505892951090504e-05,
      "loss": 0.807,
      "step": 16800
    },
    {
      "epoch": 1.0612373737373737,
      "grad_norm": 0.4163360893726349,
      "learning_rate": 9.495682155281236e-05,
      "loss": 0.6882,
      "step": 16810
    },
    {
      "epoch": 1.0618686868686869,
      "grad_norm": 0.4290304183959961,
      "learning_rate": 9.485471886589171e-05,
      "loss": 0.5607,
      "step": 16820
    },
    {
      "epoch": 1.0625,
      "grad_norm": 0.4586707055568695,
      "learning_rate": 9.475262155686162e-05,
      "loss": 0.4868,
      "step": 16830
    },
    {
      "epoch": 1.0631313131313131,
      "grad_norm": 0.7256348729133606,
      "learning_rate": 9.465052973243509e-05,
      "loss": 0.5667,
      "step": 16840
    },
    {
      "epoch": 1.0637626262626263,
      "grad_norm": 0.3960825800895691,
      "learning_rate": 9.45484434993193e-05,
      "loss": 0.8095,
      "step": 16850
    },
    {
      "epoch": 1.0643939393939394,
      "grad_norm": 0.43919217586517334,
      "learning_rate": 9.444636296421567e-05,
      "loss": 0.6865,
      "step": 16860
    },
    {
      "epoch": 1.0650252525252526,
      "grad_norm": 0.43587929010391235,
      "learning_rate": 9.434428823381959e-05,
      "loss": 0.5897,
      "step": 16870
    },
    {
      "epoch": 1.0656565656565657,
      "grad_norm": 0.5393941402435303,
      "learning_rate": 9.424221941482044e-05,
      "loss": 0.501,
      "step": 16880
    },
    {
      "epoch": 1.066287878787879,
      "grad_norm": 0.729978084564209,
      "learning_rate": 9.41401566139014e-05,
      "loss": 0.5474,
      "step": 16890
    },
    {
      "epoch": 1.0669191919191918,
      "grad_norm": 0.4207863211631775,
      "learning_rate": 9.403809993773931e-05,
      "loss": 0.7592,
      "step": 16900
    },
    {
      "epoch": 1.067550505050505,
      "grad_norm": 0.43006205558776855,
      "learning_rate": 9.393604949300472e-05,
      "loss": 0.675,
      "step": 16910
    },
    {
      "epoch": 1.0681818181818181,
      "grad_norm": 0.4856518507003784,
      "learning_rate": 9.383400538636155e-05,
      "loss": 0.6061,
      "step": 16920
    },
    {
      "epoch": 1.0688131313131313,
      "grad_norm": 0.5204713344573975,
      "learning_rate": 9.37319677244672e-05,
      "loss": 0.5001,
      "step": 16930
    },
    {
      "epoch": 1.0694444444444444,
      "grad_norm": 0.7100063562393188,
      "learning_rate": 9.362993661397222e-05,
      "loss": 0.5117,
      "step": 16940
    },
    {
      "epoch": 1.0700757575757576,
      "grad_norm": 0.410814493894577,
      "learning_rate": 9.352791216152043e-05,
      "loss": 0.8024,
      "step": 16950
    },
    {
      "epoch": 1.0707070707070707,
      "grad_norm": 0.4060390889644623,
      "learning_rate": 9.342589447374859e-05,
      "loss": 0.6643,
      "step": 16960
    },
    {
      "epoch": 1.0713383838383839,
      "grad_norm": 0.4096003770828247,
      "learning_rate": 9.332388365728648e-05,
      "loss": 0.5575,
      "step": 16970
    },
    {
      "epoch": 1.071969696969697,
      "grad_norm": 0.5102066397666931,
      "learning_rate": 9.322187981875661e-05,
      "loss": 0.4987,
      "step": 16980
    },
    {
      "epoch": 1.0726010101010102,
      "grad_norm": 0.6628770232200623,
      "learning_rate": 9.311988306477427e-05,
      "loss": 0.5151,
      "step": 16990
    },
    {
      "epoch": 1.0732323232323233,
      "grad_norm": 0.4077882170677185,
      "learning_rate": 9.301789350194732e-05,
      "loss": 0.797,
      "step": 17000
    },
    {
      "epoch": 1.0732323232323233,
      "eval_loss": 0.6515650153160095,
      "eval_runtime": 31.7873,
      "eval_samples_per_second": 80.535,
      "eval_steps_per_second": 10.067,
      "step": 17000
    },
    {
      "epoch": 1.0738636363636365,
      "grad_norm": 0.43689969182014465,
      "learning_rate": 9.291591123687604e-05,
      "loss": 0.663,
      "step": 17010
    },
    {
      "epoch": 1.0744949494949494,
      "grad_norm": 0.44369909167289734,
      "learning_rate": 9.281393637615322e-05,
      "loss": 0.5887,
      "step": 17020
    },
    {
      "epoch": 1.0751262626262625,
      "grad_norm": 0.469542533159256,
      "learning_rate": 9.271196902636376e-05,
      "loss": 0.5613,
      "step": 17030
    },
    {
      "epoch": 1.0757575757575757,
      "grad_norm": 0.6898037195205688,
      "learning_rate": 9.261000929408485e-05,
      "loss": 0.5227,
      "step": 17040
    },
    {
      "epoch": 1.0763888888888888,
      "grad_norm": 0.414147287607193,
      "learning_rate": 9.250805728588559e-05,
      "loss": 0.817,
      "step": 17050
    },
    {
      "epoch": 1.077020202020202,
      "grad_norm": 0.4239162504673004,
      "learning_rate": 9.24061131083271e-05,
      "loss": 0.6557,
      "step": 17060
    },
    {
      "epoch": 1.0776515151515151,
      "grad_norm": 0.4361567795276642,
      "learning_rate": 9.230417686796227e-05,
      "loss": 0.5637,
      "step": 17070
    },
    {
      "epoch": 1.0782828282828283,
      "grad_norm": 0.46944981813430786,
      "learning_rate": 9.220224867133572e-05,
      "loss": 0.544,
      "step": 17080
    },
    {
      "epoch": 1.0789141414141414,
      "grad_norm": 0.7874929308891296,
      "learning_rate": 9.21003286249836e-05,
      "loss": 0.537,
      "step": 17090
    },
    {
      "epoch": 1.0795454545454546,
      "grad_norm": 0.40842828154563904,
      "learning_rate": 9.199841683543365e-05,
      "loss": 0.8495,
      "step": 17100
    },
    {
      "epoch": 1.0801767676767677,
      "grad_norm": 0.4166153371334076,
      "learning_rate": 9.189651340920488e-05,
      "loss": 0.686,
      "step": 17110
    },
    {
      "epoch": 1.0808080808080809,
      "grad_norm": 0.4332253336906433,
      "learning_rate": 9.179461845280762e-05,
      "loss": 0.5898,
      "step": 17120
    },
    {
      "epoch": 1.081439393939394,
      "grad_norm": 0.5064159035682678,
      "learning_rate": 9.169273207274333e-05,
      "loss": 0.5011,
      "step": 17130
    },
    {
      "epoch": 1.0820707070707072,
      "grad_norm": 0.7443277835845947,
      "learning_rate": 9.159085437550445e-05,
      "loss": 0.4924,
      "step": 17140
    },
    {
      "epoch": 1.08270202020202,
      "grad_norm": 0.4328727424144745,
      "learning_rate": 9.148898546757445e-05,
      "loss": 0.7939,
      "step": 17150
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.44491875171661377,
      "learning_rate": 9.138712545542754e-05,
      "loss": 0.6614,
      "step": 17160
    },
    {
      "epoch": 1.0839646464646464,
      "grad_norm": 0.44322073459625244,
      "learning_rate": 9.128527444552864e-05,
      "loss": 0.5613,
      "step": 17170
    },
    {
      "epoch": 1.0845959595959596,
      "grad_norm": 0.4743420481681824,
      "learning_rate": 9.118343254433329e-05,
      "loss": 0.4985,
      "step": 17180
    },
    {
      "epoch": 1.0852272727272727,
      "grad_norm": 0.7735625505447388,
      "learning_rate": 9.108159985828749e-05,
      "loss": 0.5103,
      "step": 17190
    },
    {
      "epoch": 1.0858585858585859,
      "grad_norm": 0.4153669476509094,
      "learning_rate": 9.097977649382758e-05,
      "loss": 0.811,
      "step": 17200
    },
    {
      "epoch": 1.086489898989899,
      "grad_norm": 0.44142937660217285,
      "learning_rate": 9.087796255738024e-05,
      "loss": 0.6766,
      "step": 17210
    },
    {
      "epoch": 1.0871212121212122,
      "grad_norm": 0.48759838938713074,
      "learning_rate": 9.077615815536219e-05,
      "loss": 0.5699,
      "step": 17220
    },
    {
      "epoch": 1.0877525252525253,
      "grad_norm": 0.49312639236450195,
      "learning_rate": 9.067436339418027e-05,
      "loss": 0.5151,
      "step": 17230
    },
    {
      "epoch": 1.0883838383838385,
      "grad_norm": 0.7081851959228516,
      "learning_rate": 9.05725783802312e-05,
      "loss": 0.531,
      "step": 17240
    },
    {
      "epoch": 1.0890151515151516,
      "grad_norm": 0.4036579728126526,
      "learning_rate": 9.04708032199015e-05,
      "loss": 0.7858,
      "step": 17250
    },
    {
      "epoch": 1.0896464646464645,
      "grad_norm": 0.4505506157875061,
      "learning_rate": 9.036903801956746e-05,
      "loss": 0.637,
      "step": 17260
    },
    {
      "epoch": 1.0902777777777777,
      "grad_norm": 0.4516341984272003,
      "learning_rate": 9.026728288559487e-05,
      "loss": 0.5925,
      "step": 17270
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.5047511458396912,
      "learning_rate": 9.016553792433907e-05,
      "loss": 0.5103,
      "step": 17280
    },
    {
      "epoch": 1.091540404040404,
      "grad_norm": 0.8165682554244995,
      "learning_rate": 9.00638032421447e-05,
      "loss": 0.5099,
      "step": 17290
    },
    {
      "epoch": 1.0921717171717171,
      "grad_norm": 0.3961057960987091,
      "learning_rate": 8.996207894534573e-05,
      "loss": 0.8196,
      "step": 17300
    },
    {
      "epoch": 1.0928030303030303,
      "grad_norm": 0.4142899811267853,
      "learning_rate": 8.986036514026523e-05,
      "loss": 0.6385,
      "step": 17310
    },
    {
      "epoch": 1.0934343434343434,
      "grad_norm": 0.4401889443397522,
      "learning_rate": 8.975866193321533e-05,
      "loss": 0.5918,
      "step": 17320
    },
    {
      "epoch": 1.0940656565656566,
      "grad_norm": 0.4817618727684021,
      "learning_rate": 8.965696943049703e-05,
      "loss": 0.5001,
      "step": 17330
    },
    {
      "epoch": 1.0946969696969697,
      "grad_norm": 0.7110857963562012,
      "learning_rate": 8.955528773840022e-05,
      "loss": 0.5266,
      "step": 17340
    },
    {
      "epoch": 1.0953282828282829,
      "grad_norm": 0.4210379719734192,
      "learning_rate": 8.945361696320341e-05,
      "loss": 0.8349,
      "step": 17350
    },
    {
      "epoch": 1.095959595959596,
      "grad_norm": 0.4201394021511078,
      "learning_rate": 8.935195721117379e-05,
      "loss": 0.6951,
      "step": 17360
    },
    {
      "epoch": 1.0965909090909092,
      "grad_norm": 0.4383458197116852,
      "learning_rate": 8.925030858856693e-05,
      "loss": 0.5767,
      "step": 17370
    },
    {
      "epoch": 1.0972222222222223,
      "grad_norm": 0.4691868722438812,
      "learning_rate": 8.914867120162681e-05,
      "loss": 0.4902,
      "step": 17380
    },
    {
      "epoch": 1.0978535353535355,
      "grad_norm": 0.7015037536621094,
      "learning_rate": 8.904704515658572e-05,
      "loss": 0.5331,
      "step": 17390
    },
    {
      "epoch": 1.0984848484848484,
      "grad_norm": 0.39762118458747864,
      "learning_rate": 8.8945430559664e-05,
      "loss": 0.8088,
      "step": 17400
    },
    {
      "epoch": 1.0991161616161615,
      "grad_norm": 0.40533632040023804,
      "learning_rate": 8.88438275170701e-05,
      "loss": 0.6665,
      "step": 17410
    },
    {
      "epoch": 1.0997474747474747,
      "grad_norm": 0.4754107892513275,
      "learning_rate": 8.874223613500033e-05,
      "loss": 0.5882,
      "step": 17420
    },
    {
      "epoch": 1.1003787878787878,
      "grad_norm": 0.5126242637634277,
      "learning_rate": 8.86406565196389e-05,
      "loss": 0.4817,
      "step": 17430
    },
    {
      "epoch": 1.101010101010101,
      "grad_norm": 0.7102018594741821,
      "learning_rate": 8.853908877715762e-05,
      "loss": 0.517,
      "step": 17440
    },
    {
      "epoch": 1.1016414141414141,
      "grad_norm": 0.39945560693740845,
      "learning_rate": 8.843753301371596e-05,
      "loss": 0.7861,
      "step": 17450
    },
    {
      "epoch": 1.1022727272727273,
      "grad_norm": 0.4422730803489685,
      "learning_rate": 8.833598933546083e-05,
      "loss": 0.6516,
      "step": 17460
    },
    {
      "epoch": 1.1029040404040404,
      "grad_norm": 0.4873642027378082,
      "learning_rate": 8.823445784852657e-05,
      "loss": 0.5696,
      "step": 17470
    },
    {
      "epoch": 1.1035353535353536,
      "grad_norm": 0.5275599956512451,
      "learning_rate": 8.81329386590347e-05,
      "loss": 0.5147,
      "step": 17480
    },
    {
      "epoch": 1.1041666666666667,
      "grad_norm": 0.8240005373954773,
      "learning_rate": 8.803143187309388e-05,
      "loss": 0.5699,
      "step": 17490
    },
    {
      "epoch": 1.10479797979798,
      "grad_norm": 0.4095676839351654,
      "learning_rate": 8.792993759679993e-05,
      "loss": 0.8475,
      "step": 17500
    },
    {
      "epoch": 1.1054292929292928,
      "grad_norm": 0.3986596465110779,
      "learning_rate": 8.782845593623544e-05,
      "loss": 0.6749,
      "step": 17510
    },
    {
      "epoch": 1.106060606060606,
      "grad_norm": 0.4265003204345703,
      "learning_rate": 8.772698699746993e-05,
      "loss": 0.5819,
      "step": 17520
    },
    {
      "epoch": 1.1066919191919191,
      "grad_norm": 0.49655207991600037,
      "learning_rate": 8.762553088655954e-05,
      "loss": 0.5074,
      "step": 17530
    },
    {
      "epoch": 1.1073232323232323,
      "grad_norm": 0.6977995038032532,
      "learning_rate": 8.75240877095471e-05,
      "loss": 0.5226,
      "step": 17540
    },
    {
      "epoch": 1.1079545454545454,
      "grad_norm": 0.3777778446674347,
      "learning_rate": 8.742265757246179e-05,
      "loss": 0.7786,
      "step": 17550
    },
    {
      "epoch": 1.1085858585858586,
      "grad_norm": 0.42944997549057007,
      "learning_rate": 8.732124058131928e-05,
      "loss": 0.6662,
      "step": 17560
    },
    {
      "epoch": 1.1092171717171717,
      "grad_norm": 0.44920483231544495,
      "learning_rate": 8.721983684212143e-05,
      "loss": 0.5936,
      "step": 17570
    },
    {
      "epoch": 1.1098484848484849,
      "grad_norm": 0.4586697220802307,
      "learning_rate": 8.71184464608563e-05,
      "loss": 0.5336,
      "step": 17580
    },
    {
      "epoch": 1.110479797979798,
      "grad_norm": 0.874282717704773,
      "learning_rate": 8.701706954349791e-05,
      "loss": 0.5167,
      "step": 17590
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.4132348597049713,
      "learning_rate": 8.691570619600629e-05,
      "loss": 0.7812,
      "step": 17600
    },
    {
      "epoch": 1.1117424242424243,
      "grad_norm": 0.40053483843803406,
      "learning_rate": 8.681435652432724e-05,
      "loss": 0.6312,
      "step": 17610
    },
    {
      "epoch": 1.1123737373737375,
      "grad_norm": 0.481662780046463,
      "learning_rate": 8.671302063439227e-05,
      "loss": 0.5863,
      "step": 17620
    },
    {
      "epoch": 1.1130050505050506,
      "grad_norm": 0.4417303204536438,
      "learning_rate": 8.661169863211853e-05,
      "loss": 0.5232,
      "step": 17630
    },
    {
      "epoch": 1.1136363636363635,
      "grad_norm": 0.6587544679641724,
      "learning_rate": 8.651039062340858e-05,
      "loss": 0.5423,
      "step": 17640
    },
    {
      "epoch": 1.1142676767676767,
      "grad_norm": 0.42927685379981995,
      "learning_rate": 8.640909671415043e-05,
      "loss": 0.82,
      "step": 17650
    },
    {
      "epoch": 1.1148989898989898,
      "grad_norm": 0.43303897976875305,
      "learning_rate": 8.630781701021726e-05,
      "loss": 0.6389,
      "step": 17660
    },
    {
      "epoch": 1.115530303030303,
      "grad_norm": 0.4321282207965851,
      "learning_rate": 8.620655161746752e-05,
      "loss": 0.5805,
      "step": 17670
    },
    {
      "epoch": 1.1161616161616161,
      "grad_norm": 0.4614081084728241,
      "learning_rate": 8.610530064174459e-05,
      "loss": 0.5048,
      "step": 17680
    },
    {
      "epoch": 1.1167929292929293,
      "grad_norm": 0.884528398513794,
      "learning_rate": 8.600406418887688e-05,
      "loss": 0.5172,
      "step": 17690
    },
    {
      "epoch": 1.1174242424242424,
      "grad_norm": 0.40983492136001587,
      "learning_rate": 8.590284236467751e-05,
      "loss": 0.7647,
      "step": 17700
    },
    {
      "epoch": 1.1180555555555556,
      "grad_norm": 0.43024927377700806,
      "learning_rate": 8.580163527494443e-05,
      "loss": 0.6486,
      "step": 17710
    },
    {
      "epoch": 1.1186868686868687,
      "grad_norm": 0.4588288962841034,
      "learning_rate": 8.57004430254601e-05,
      "loss": 0.5337,
      "step": 17720
    },
    {
      "epoch": 1.1193181818181819,
      "grad_norm": 0.4353034496307373,
      "learning_rate": 8.55992657219915e-05,
      "loss": 0.4874,
      "step": 17730
    },
    {
      "epoch": 1.119949494949495,
      "grad_norm": 0.7974393963813782,
      "learning_rate": 8.549810347029001e-05,
      "loss": 0.5106,
      "step": 17740
    },
    {
      "epoch": 1.1205808080808082,
      "grad_norm": 0.42054301500320435,
      "learning_rate": 8.53969563760912e-05,
      "loss": 0.8138,
      "step": 17750
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 0.4363821744918823,
      "learning_rate": 8.529582454511493e-05,
      "loss": 0.6594,
      "step": 17760
    },
    {
      "epoch": 1.1218434343434343,
      "grad_norm": 0.47206196188926697,
      "learning_rate": 8.519470808306497e-05,
      "loss": 0.5674,
      "step": 17770
    },
    {
      "epoch": 1.1224747474747474,
      "grad_norm": 0.4513121247291565,
      "learning_rate": 8.509360709562912e-05,
      "loss": 0.5109,
      "step": 17780
    },
    {
      "epoch": 1.1231060606060606,
      "grad_norm": 0.8112536072731018,
      "learning_rate": 8.499252168847896e-05,
      "loss": 0.5399,
      "step": 17790
    },
    {
      "epoch": 1.1237373737373737,
      "grad_norm": 0.4271901845932007,
      "learning_rate": 8.48914519672698e-05,
      "loss": 0.8568,
      "step": 17800
    },
    {
      "epoch": 1.1243686868686869,
      "grad_norm": 0.4398418664932251,
      "learning_rate": 8.479039803764053e-05,
      "loss": 0.6907,
      "step": 17810
    },
    {
      "epoch": 1.125,
      "grad_norm": 0.5171748995780945,
      "learning_rate": 8.46893600052136e-05,
      "loss": 0.5816,
      "step": 17820
    },
    {
      "epoch": 1.1256313131313131,
      "grad_norm": 0.5107201933860779,
      "learning_rate": 8.458833797559477e-05,
      "loss": 0.4945,
      "step": 17830
    },
    {
      "epoch": 1.1262626262626263,
      "grad_norm": 0.7533595561981201,
      "learning_rate": 8.44873320543731e-05,
      "loss": 0.5225,
      "step": 17840
    },
    {
      "epoch": 1.1268939393939394,
      "grad_norm": 0.42707574367523193,
      "learning_rate": 8.438634234712085e-05,
      "loss": 0.7865,
      "step": 17850
    },
    {
      "epoch": 1.1275252525252526,
      "grad_norm": 0.4102030396461487,
      "learning_rate": 8.428536895939327e-05,
      "loss": 0.6606,
      "step": 17860
    },
    {
      "epoch": 1.1281565656565657,
      "grad_norm": 0.4953867793083191,
      "learning_rate": 8.41844119967286e-05,
      "loss": 0.605,
      "step": 17870
    },
    {
      "epoch": 1.128787878787879,
      "grad_norm": 0.5153169631958008,
      "learning_rate": 8.408347156464787e-05,
      "loss": 0.4881,
      "step": 17880
    },
    {
      "epoch": 1.1294191919191918,
      "grad_norm": 0.7814686298370361,
      "learning_rate": 8.398254776865491e-05,
      "loss": 0.539,
      "step": 17890
    },
    {
      "epoch": 1.130050505050505,
      "grad_norm": 0.4028628468513489,
      "learning_rate": 8.388164071423604e-05,
      "loss": 0.8293,
      "step": 17900
    },
    {
      "epoch": 1.1306818181818181,
      "grad_norm": 0.4164751172065735,
      "learning_rate": 8.378075050686023e-05,
      "loss": 0.6489,
      "step": 17910
    },
    {
      "epoch": 1.1313131313131313,
      "grad_norm": 0.4554752707481384,
      "learning_rate": 8.367987725197868e-05,
      "loss": 0.5511,
      "step": 17920
    },
    {
      "epoch": 1.1319444444444444,
      "grad_norm": 0.46922874450683594,
      "learning_rate": 8.357902105502504e-05,
      "loss": 0.4831,
      "step": 17930
    },
    {
      "epoch": 1.1325757575757576,
      "grad_norm": 0.8782879710197449,
      "learning_rate": 8.347818202141497e-05,
      "loss": 0.5754,
      "step": 17940
    },
    {
      "epoch": 1.1332070707070707,
      "grad_norm": 0.4060918092727661,
      "learning_rate": 8.337736025654633e-05,
      "loss": 0.8642,
      "step": 17950
    },
    {
      "epoch": 1.1338383838383839,
      "grad_norm": 0.43536412715911865,
      "learning_rate": 8.32765558657988e-05,
      "loss": 0.6657,
      "step": 17960
    },
    {
      "epoch": 1.134469696969697,
      "grad_norm": 0.442756712436676,
      "learning_rate": 8.3175768954534e-05,
      "loss": 0.5798,
      "step": 17970
    },
    {
      "epoch": 1.1351010101010102,
      "grad_norm": 0.48292186856269836,
      "learning_rate": 8.307499962809528e-05,
      "loss": 0.5044,
      "step": 17980
    },
    {
      "epoch": 1.1357323232323233,
      "grad_norm": 0.7547537088394165,
      "learning_rate": 8.29742479918075e-05,
      "loss": 0.5312,
      "step": 17990
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 0.40675634145736694,
      "learning_rate": 8.28735141509772e-05,
      "loss": 0.816,
      "step": 18000
    },
    {
      "epoch": 1.1363636363636362,
      "eval_loss": 0.6497641801834106,
      "eval_runtime": 31.7605,
      "eval_samples_per_second": 80.603,
      "eval_steps_per_second": 10.075,
      "step": 18000
    },
    {
      "epoch": 1.1369949494949494,
      "grad_norm": 0.41817715764045715,
      "learning_rate": 8.277279821089213e-05,
      "loss": 0.6284,
      "step": 18010
    },
    {
      "epoch": 1.1376262626262625,
      "grad_norm": 0.4861487150192261,
      "learning_rate": 8.267210027682151e-05,
      "loss": 0.5676,
      "step": 18020
    },
    {
      "epoch": 1.1382575757575757,
      "grad_norm": 0.4569685459136963,
      "learning_rate": 8.257142045401559e-05,
      "loss": 0.4674,
      "step": 18030
    },
    {
      "epoch": 1.1388888888888888,
      "grad_norm": 0.6595017910003662,
      "learning_rate": 8.247075884770583e-05,
      "loss": 0.534,
      "step": 18040
    },
    {
      "epoch": 1.139520202020202,
      "grad_norm": 0.40405410528182983,
      "learning_rate": 8.23701155631045e-05,
      "loss": 0.8349,
      "step": 18050
    },
    {
      "epoch": 1.1401515151515151,
      "grad_norm": 0.406789094209671,
      "learning_rate": 8.226949070540487e-05,
      "loss": 0.6656,
      "step": 18060
    },
    {
      "epoch": 1.1407828282828283,
      "grad_norm": 0.4304566979408264,
      "learning_rate": 8.216888437978082e-05,
      "loss": 0.5696,
      "step": 18070
    },
    {
      "epoch": 1.1414141414141414,
      "grad_norm": 0.43476173281669617,
      "learning_rate": 8.206829669138693e-05,
      "loss": 0.5369,
      "step": 18080
    },
    {
      "epoch": 1.1420454545454546,
      "grad_norm": 0.7440270185470581,
      "learning_rate": 8.196772774535832e-05,
      "loss": 0.5225,
      "step": 18090
    },
    {
      "epoch": 1.1426767676767677,
      "grad_norm": 0.42064639925956726,
      "learning_rate": 8.186717764681042e-05,
      "loss": 0.8231,
      "step": 18100
    },
    {
      "epoch": 1.1433080808080809,
      "grad_norm": 0.43421700596809387,
      "learning_rate": 8.176664650083911e-05,
      "loss": 0.6736,
      "step": 18110
    },
    {
      "epoch": 1.143939393939394,
      "grad_norm": 0.4516768157482147,
      "learning_rate": 8.166613441252034e-05,
      "loss": 0.5801,
      "step": 18120
    },
    {
      "epoch": 1.1445707070707072,
      "grad_norm": 0.4485583007335663,
      "learning_rate": 8.156564148691018e-05,
      "loss": 0.5173,
      "step": 18130
    },
    {
      "epoch": 1.14520202020202,
      "grad_norm": 0.7184070348739624,
      "learning_rate": 8.146516782904464e-05,
      "loss": 0.519,
      "step": 18140
    },
    {
      "epoch": 1.1458333333333333,
      "grad_norm": 0.4126015603542328,
      "learning_rate": 8.136471354393969e-05,
      "loss": 0.7906,
      "step": 18150
    },
    {
      "epoch": 1.1464646464646464,
      "grad_norm": 0.4019117057323456,
      "learning_rate": 8.126427873659091e-05,
      "loss": 0.6771,
      "step": 18160
    },
    {
      "epoch": 1.1470959595959596,
      "grad_norm": 0.433698832988739,
      "learning_rate": 8.116386351197367e-05,
      "loss": 0.5579,
      "step": 18170
    },
    {
      "epoch": 1.1477272727272727,
      "grad_norm": 0.5152361989021301,
      "learning_rate": 8.106346797504276e-05,
      "loss": 0.4943,
      "step": 18180
    },
    {
      "epoch": 1.1483585858585859,
      "grad_norm": 0.6766521334648132,
      "learning_rate": 8.096309223073239e-05,
      "loss": 0.5228,
      "step": 18190
    },
    {
      "epoch": 1.148989898989899,
      "grad_norm": 0.40940871834754944,
      "learning_rate": 8.086273638395619e-05,
      "loss": 0.8146,
      "step": 18200
    },
    {
      "epoch": 1.1496212121212122,
      "grad_norm": 0.4513130187988281,
      "learning_rate": 8.076240053960686e-05,
      "loss": 0.6487,
      "step": 18210
    },
    {
      "epoch": 1.1502525252525253,
      "grad_norm": 0.4363804757595062,
      "learning_rate": 8.066208480255633e-05,
      "loss": 0.5832,
      "step": 18220
    },
    {
      "epoch": 1.1508838383838385,
      "grad_norm": 0.47246840596199036,
      "learning_rate": 8.056178927765536e-05,
      "loss": 0.5038,
      "step": 18230
    },
    {
      "epoch": 1.1515151515151516,
      "grad_norm": 0.7379527688026428,
      "learning_rate": 8.046151406973376e-05,
      "loss": 0.5314,
      "step": 18240
    },
    {
      "epoch": 1.1521464646464645,
      "grad_norm": 0.3911227583885193,
      "learning_rate": 8.036125928359992e-05,
      "loss": 0.7829,
      "step": 18250
    },
    {
      "epoch": 1.1527777777777777,
      "grad_norm": 0.4478697180747986,
      "learning_rate": 8.026102502404104e-05,
      "loss": 0.6561,
      "step": 18260
    },
    {
      "epoch": 1.1534090909090908,
      "grad_norm": 0.43704378604888916,
      "learning_rate": 8.016081139582276e-05,
      "loss": 0.5841,
      "step": 18270
    },
    {
      "epoch": 1.154040404040404,
      "grad_norm": 0.4399907886981964,
      "learning_rate": 8.006061850368923e-05,
      "loss": 0.5207,
      "step": 18280
    },
    {
      "epoch": 1.1546717171717171,
      "grad_norm": 0.7712368965148926,
      "learning_rate": 7.996044645236283e-05,
      "loss": 0.5271,
      "step": 18290
    },
    {
      "epoch": 1.1553030303030303,
      "grad_norm": 0.43295449018478394,
      "learning_rate": 7.986029534654431e-05,
      "loss": 0.7691,
      "step": 18300
    },
    {
      "epoch": 1.1559343434343434,
      "grad_norm": 0.4194464683532715,
      "learning_rate": 7.97601652909124e-05,
      "loss": 0.647,
      "step": 18310
    },
    {
      "epoch": 1.1565656565656566,
      "grad_norm": 0.441564679145813,
      "learning_rate": 7.966005639012383e-05,
      "loss": 0.5691,
      "step": 18320
    },
    {
      "epoch": 1.1571969696969697,
      "grad_norm": 0.5162746906280518,
      "learning_rate": 7.95599687488133e-05,
      "loss": 0.5528,
      "step": 18330
    },
    {
      "epoch": 1.1578282828282829,
      "grad_norm": 0.7434961795806885,
      "learning_rate": 7.945990247159321e-05,
      "loss": 0.5454,
      "step": 18340
    },
    {
      "epoch": 1.158459595959596,
      "grad_norm": 0.41086068749427795,
      "learning_rate": 7.93598576630537e-05,
      "loss": 0.762,
      "step": 18350
    },
    {
      "epoch": 1.1590909090909092,
      "grad_norm": 0.4377158582210541,
      "learning_rate": 7.925983442776241e-05,
      "loss": 0.6741,
      "step": 18360
    },
    {
      "epoch": 1.1597222222222223,
      "grad_norm": 0.4763686954975128,
      "learning_rate": 7.915983287026447e-05,
      "loss": 0.6001,
      "step": 18370
    },
    {
      "epoch": 1.1603535353535355,
      "grad_norm": 0.46259573101997375,
      "learning_rate": 7.905985309508232e-05,
      "loss": 0.5233,
      "step": 18380
    },
    {
      "epoch": 1.1609848484848484,
      "grad_norm": 0.7256913185119629,
      "learning_rate": 7.895989520671567e-05,
      "loss": 0.5124,
      "step": 18390
    },
    {
      "epoch": 1.1616161616161615,
      "grad_norm": 0.406796395778656,
      "learning_rate": 7.885995930964131e-05,
      "loss": 0.8302,
      "step": 18400
    },
    {
      "epoch": 1.1622474747474747,
      "grad_norm": 0.4457390606403351,
      "learning_rate": 7.876004550831313e-05,
      "loss": 0.648,
      "step": 18410
    },
    {
      "epoch": 1.1628787878787878,
      "grad_norm": 0.43056175112724304,
      "learning_rate": 7.866015390716182e-05,
      "loss": 0.5661,
      "step": 18420
    },
    {
      "epoch": 1.163510101010101,
      "grad_norm": 0.46518674492836,
      "learning_rate": 7.856028461059488e-05,
      "loss": 0.5194,
      "step": 18430
    },
    {
      "epoch": 1.1641414141414141,
      "grad_norm": 0.797951877117157,
      "learning_rate": 7.846043772299658e-05,
      "loss": 0.5475,
      "step": 18440
    },
    {
      "epoch": 1.1647727272727273,
      "grad_norm": 0.3978939354419708,
      "learning_rate": 7.83606133487277e-05,
      "loss": 0.7915,
      "step": 18450
    },
    {
      "epoch": 1.1654040404040404,
      "grad_norm": 0.4375913441181183,
      "learning_rate": 7.826081159212551e-05,
      "loss": 0.6697,
      "step": 18460
    },
    {
      "epoch": 1.1660353535353536,
      "grad_norm": 0.46155187487602234,
      "learning_rate": 7.816103255750361e-05,
      "loss": 0.5754,
      "step": 18470
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.5195637345314026,
      "learning_rate": 7.806127634915192e-05,
      "loss": 0.5341,
      "step": 18480
    },
    {
      "epoch": 1.16729797979798,
      "grad_norm": 0.6406044363975525,
      "learning_rate": 7.796154307133642e-05,
      "loss": 0.5023,
      "step": 18490
    },
    {
      "epoch": 1.1679292929292928,
      "grad_norm": 0.42629310488700867,
      "learning_rate": 7.786183282829918e-05,
      "loss": 0.8221,
      "step": 18500
    },
    {
      "epoch": 1.168560606060606,
      "grad_norm": 0.42355164885520935,
      "learning_rate": 7.776214572425816e-05,
      "loss": 0.6705,
      "step": 18510
    },
    {
      "epoch": 1.1691919191919191,
      "grad_norm": 0.4523802399635315,
      "learning_rate": 7.766248186340718e-05,
      "loss": 0.5893,
      "step": 18520
    },
    {
      "epoch": 1.1698232323232323,
      "grad_norm": 0.4879468083381653,
      "learning_rate": 7.756284134991568e-05,
      "loss": 0.5096,
      "step": 18530
    },
    {
      "epoch": 1.1704545454545454,
      "grad_norm": 0.856860876083374,
      "learning_rate": 7.746322428792882e-05,
      "loss": 0.5489,
      "step": 18540
    },
    {
      "epoch": 1.1710858585858586,
      "grad_norm": 0.4406171143054962,
      "learning_rate": 7.736363078156714e-05,
      "loss": 0.7662,
      "step": 18550
    },
    {
      "epoch": 1.1717171717171717,
      "grad_norm": 0.490296334028244,
      "learning_rate": 7.726406093492659e-05,
      "loss": 0.6553,
      "step": 18560
    },
    {
      "epoch": 1.1723484848484849,
      "grad_norm": 0.4700033962726593,
      "learning_rate": 7.716451485207843e-05,
      "loss": 0.5903,
      "step": 18570
    },
    {
      "epoch": 1.172979797979798,
      "grad_norm": 0.4883473813533783,
      "learning_rate": 7.706499263706901e-05,
      "loss": 0.5083,
      "step": 18580
    },
    {
      "epoch": 1.1736111111111112,
      "grad_norm": 0.7195966839790344,
      "learning_rate": 7.696549439391985e-05,
      "loss": 0.5382,
      "step": 18590
    },
    {
      "epoch": 1.1742424242424243,
      "grad_norm": 0.40182968974113464,
      "learning_rate": 7.686602022662725e-05,
      "loss": 0.7906,
      "step": 18600
    },
    {
      "epoch": 1.1748737373737375,
      "grad_norm": 0.436972439289093,
      "learning_rate": 7.676657023916252e-05,
      "loss": 0.6269,
      "step": 18610
    },
    {
      "epoch": 1.1755050505050506,
      "grad_norm": 0.4882041811943054,
      "learning_rate": 7.666714453547155e-05,
      "loss": 0.6131,
      "step": 18620
    },
    {
      "epoch": 1.1761363636363638,
      "grad_norm": 0.5243234038352966,
      "learning_rate": 7.656774321947495e-05,
      "loss": 0.5069,
      "step": 18630
    },
    {
      "epoch": 1.1767676767676767,
      "grad_norm": 0.7602398991584778,
      "learning_rate": 7.646836639506778e-05,
      "loss": 0.5622,
      "step": 18640
    },
    {
      "epoch": 1.1773989898989898,
      "grad_norm": 0.403030127286911,
      "learning_rate": 7.636901416611954e-05,
      "loss": 0.8337,
      "step": 18650
    },
    {
      "epoch": 1.178030303030303,
      "grad_norm": 0.43632587790489197,
      "learning_rate": 7.626968663647401e-05,
      "loss": 0.6386,
      "step": 18660
    },
    {
      "epoch": 1.1786616161616161,
      "grad_norm": 0.46833914518356323,
      "learning_rate": 7.617038390994909e-05,
      "loss": 0.5954,
      "step": 18670
    },
    {
      "epoch": 1.1792929292929293,
      "grad_norm": 0.47699999809265137,
      "learning_rate": 7.60711060903369e-05,
      "loss": 0.4963,
      "step": 18680
    },
    {
      "epoch": 1.1799242424242424,
      "grad_norm": 0.7453601956367493,
      "learning_rate": 7.597185328140337e-05,
      "loss": 0.5039,
      "step": 18690
    },
    {
      "epoch": 1.1805555555555556,
      "grad_norm": 0.42221689224243164,
      "learning_rate": 7.587262558688841e-05,
      "loss": 0.8173,
      "step": 18700
    },
    {
      "epoch": 1.1811868686868687,
      "grad_norm": 0.4348530173301697,
      "learning_rate": 7.57734231105056e-05,
      "loss": 0.6228,
      "step": 18710
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 0.4750975966453552,
      "learning_rate": 7.567424595594221e-05,
      "loss": 0.615,
      "step": 18720
    },
    {
      "epoch": 1.182449494949495,
      "grad_norm": 0.4773416519165039,
      "learning_rate": 7.5575094226859e-05,
      "loss": 0.4993,
      "step": 18730
    },
    {
      "epoch": 1.1830808080808082,
      "grad_norm": 0.6599456667900085,
      "learning_rate": 7.547596802689023e-05,
      "loss": 0.5148,
      "step": 18740
    },
    {
      "epoch": 1.183712121212121,
      "grad_norm": 0.4071122407913208,
      "learning_rate": 7.537686745964339e-05,
      "loss": 0.7842,
      "step": 18750
    },
    {
      "epoch": 1.1843434343434343,
      "grad_norm": 0.4604470729827881,
      "learning_rate": 7.527779262869921e-05,
      "loss": 0.6617,
      "step": 18760
    },
    {
      "epoch": 1.1849747474747474,
      "grad_norm": 0.41870027780532837,
      "learning_rate": 7.517874363761158e-05,
      "loss": 0.5678,
      "step": 18770
    },
    {
      "epoch": 1.1856060606060606,
      "grad_norm": 0.4932459890842438,
      "learning_rate": 7.507972058990725e-05,
      "loss": 0.5191,
      "step": 18780
    },
    {
      "epoch": 1.1862373737373737,
      "grad_norm": 0.7546570897102356,
      "learning_rate": 7.498072358908598e-05,
      "loss": 0.5193,
      "step": 18790
    },
    {
      "epoch": 1.1868686868686869,
      "grad_norm": 0.4011492133140564,
      "learning_rate": 7.488175273862023e-05,
      "loss": 0.8268,
      "step": 18800
    },
    {
      "epoch": 1.1875,
      "grad_norm": 0.4174717962741852,
      "learning_rate": 7.478280814195521e-05,
      "loss": 0.6674,
      "step": 18810
    },
    {
      "epoch": 1.1881313131313131,
      "grad_norm": 0.4475518763065338,
      "learning_rate": 7.468388990250857e-05,
      "loss": 0.563,
      "step": 18820
    },
    {
      "epoch": 1.1887626262626263,
      "grad_norm": 0.45688703656196594,
      "learning_rate": 7.458499812367052e-05,
      "loss": 0.5049,
      "step": 18830
    },
    {
      "epoch": 1.1893939393939394,
      "grad_norm": 0.6997200846672058,
      "learning_rate": 7.448613290880353e-05,
      "loss": 0.5203,
      "step": 18840
    },
    {
      "epoch": 1.1900252525252526,
      "grad_norm": 0.42525553703308105,
      "learning_rate": 7.438729436124239e-05,
      "loss": 0.8015,
      "step": 18850
    },
    {
      "epoch": 1.1906565656565657,
      "grad_norm": 0.435749888420105,
      "learning_rate": 7.428848258429393e-05,
      "loss": 0.6425,
      "step": 18860
    },
    {
      "epoch": 1.191287878787879,
      "grad_norm": 0.445121169090271,
      "learning_rate": 7.418969768123707e-05,
      "loss": 0.5628,
      "step": 18870
    },
    {
      "epoch": 1.1919191919191918,
      "grad_norm": 0.536883533000946,
      "learning_rate": 7.409093975532258e-05,
      "loss": 0.5197,
      "step": 18880
    },
    {
      "epoch": 1.192550505050505,
      "grad_norm": 0.7404107451438904,
      "learning_rate": 7.399220890977313e-05,
      "loss": 0.4948,
      "step": 18890
    },
    {
      "epoch": 1.1931818181818181,
      "grad_norm": 0.4104001224040985,
      "learning_rate": 7.389350524778293e-05,
      "loss": 0.7935,
      "step": 18900
    },
    {
      "epoch": 1.1938131313131313,
      "grad_norm": 0.421471506357193,
      "learning_rate": 7.379482887251792e-05,
      "loss": 0.6567,
      "step": 18910
    },
    {
      "epoch": 1.1944444444444444,
      "grad_norm": 0.40246474742889404,
      "learning_rate": 7.369617988711545e-05,
      "loss": 0.5966,
      "step": 18920
    },
    {
      "epoch": 1.1950757575757576,
      "grad_norm": 0.4946107864379883,
      "learning_rate": 7.359755839468427e-05,
      "loss": 0.5179,
      "step": 18930
    },
    {
      "epoch": 1.1957070707070707,
      "grad_norm": 0.743615984916687,
      "learning_rate": 7.34989644983044e-05,
      "loss": 0.5239,
      "step": 18940
    },
    {
      "epoch": 1.1963383838383839,
      "grad_norm": 0.4419601261615753,
      "learning_rate": 7.340039830102693e-05,
      "loss": 0.7992,
      "step": 18950
    },
    {
      "epoch": 1.196969696969697,
      "grad_norm": 0.4193497896194458,
      "learning_rate": 7.330185990587418e-05,
      "loss": 0.6704,
      "step": 18960
    },
    {
      "epoch": 1.1976010101010102,
      "grad_norm": 0.4395389258861542,
      "learning_rate": 7.32033494158392e-05,
      "loss": 0.5654,
      "step": 18970
    },
    {
      "epoch": 1.1982323232323233,
      "grad_norm": 0.4914807975292206,
      "learning_rate": 7.3104866933886e-05,
      "loss": 0.5014,
      "step": 18980
    },
    {
      "epoch": 1.1988636363636362,
      "grad_norm": 0.7160587310791016,
      "learning_rate": 7.300641256294931e-05,
      "loss": 0.5421,
      "step": 18990
    },
    {
      "epoch": 1.1994949494949494,
      "grad_norm": 0.4217836260795593,
      "learning_rate": 7.290798640593446e-05,
      "loss": 0.7915,
      "step": 19000
    },
    {
      "epoch": 1.1994949494949494,
      "eval_loss": 0.6450358629226685,
      "eval_runtime": 31.891,
      "eval_samples_per_second": 80.274,
      "eval_steps_per_second": 10.034,
      "step": 19000
    },
    {
      "epoch": 1.2001262626262625,
      "grad_norm": 0.4494868814945221,
      "learning_rate": 7.280958856571727e-05,
      "loss": 0.6456,
      "step": 19010
    },
    {
      "epoch": 1.2007575757575757,
      "grad_norm": 0.43029189109802246,
      "learning_rate": 7.271121914514396e-05,
      "loss": 0.6019,
      "step": 19020
    },
    {
      "epoch": 1.2013888888888888,
      "grad_norm": 0.45147445797920227,
      "learning_rate": 7.261287824703109e-05,
      "loss": 0.4999,
      "step": 19030
    },
    {
      "epoch": 1.202020202020202,
      "grad_norm": 0.6573380827903748,
      "learning_rate": 7.251456597416537e-05,
      "loss": 0.5174,
      "step": 19040
    },
    {
      "epoch": 1.2026515151515151,
      "grad_norm": 0.4166720509529114,
      "learning_rate": 7.241628242930362e-05,
      "loss": 0.855,
      "step": 19050
    },
    {
      "epoch": 1.2032828282828283,
      "grad_norm": 0.44660839438438416,
      "learning_rate": 7.231802771517263e-05,
      "loss": 0.652,
      "step": 19060
    },
    {
      "epoch": 1.2039141414141414,
      "grad_norm": 0.4723118841648102,
      "learning_rate": 7.2219801934469e-05,
      "loss": 0.5882,
      "step": 19070
    },
    {
      "epoch": 1.2045454545454546,
      "grad_norm": 0.49925562739372253,
      "learning_rate": 7.212160518985916e-05,
      "loss": 0.5124,
      "step": 19080
    },
    {
      "epoch": 1.2051767676767677,
      "grad_norm": 0.7472469210624695,
      "learning_rate": 7.202343758397918e-05,
      "loss": 0.5141,
      "step": 19090
    },
    {
      "epoch": 1.2058080808080809,
      "grad_norm": 0.41443943977355957,
      "learning_rate": 7.192529921943462e-05,
      "loss": 0.7856,
      "step": 19100
    },
    {
      "epoch": 1.206439393939394,
      "grad_norm": 0.40638771653175354,
      "learning_rate": 7.182719019880055e-05,
      "loss": 0.6625,
      "step": 19110
    },
    {
      "epoch": 1.2070707070707072,
      "grad_norm": 0.4366365373134613,
      "learning_rate": 7.172911062462127e-05,
      "loss": 0.5449,
      "step": 19120
    },
    {
      "epoch": 1.20770202020202,
      "grad_norm": 0.4726068675518036,
      "learning_rate": 7.163106059941046e-05,
      "loss": 0.5388,
      "step": 19130
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 0.6271906495094299,
      "learning_rate": 7.153304022565077e-05,
      "loss": 0.501,
      "step": 19140
    },
    {
      "epoch": 1.2089646464646464,
      "grad_norm": 0.4060162901878357,
      "learning_rate": 7.143504960579386e-05,
      "loss": 0.7976,
      "step": 19150
    },
    {
      "epoch": 1.2095959595959596,
      "grad_norm": 0.4471953213214874,
      "learning_rate": 7.133708884226044e-05,
      "loss": 0.6656,
      "step": 19160
    },
    {
      "epoch": 1.2102272727272727,
      "grad_norm": 0.47020384669303894,
      "learning_rate": 7.123915803743982e-05,
      "loss": 0.5832,
      "step": 19170
    },
    {
      "epoch": 1.2108585858585859,
      "grad_norm": 0.4495004117488861,
      "learning_rate": 7.114125729369017e-05,
      "loss": 0.4933,
      "step": 19180
    },
    {
      "epoch": 1.211489898989899,
      "grad_norm": 0.7242642641067505,
      "learning_rate": 7.10433867133381e-05,
      "loss": 0.5358,
      "step": 19190
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 0.4265076518058777,
      "learning_rate": 7.09455463986788e-05,
      "loss": 0.7798,
      "step": 19200
    },
    {
      "epoch": 1.2127525252525253,
      "grad_norm": 0.4314489960670471,
      "learning_rate": 7.084773645197573e-05,
      "loss": 0.6773,
      "step": 19210
    },
    {
      "epoch": 1.2133838383838385,
      "grad_norm": 0.4648510813713074,
      "learning_rate": 7.074995697546072e-05,
      "loss": 0.5745,
      "step": 19220
    },
    {
      "epoch": 1.2140151515151516,
      "grad_norm": 0.4817498028278351,
      "learning_rate": 7.065220807133364e-05,
      "loss": 0.4928,
      "step": 19230
    },
    {
      "epoch": 1.2146464646464645,
      "grad_norm": 0.82808917760849,
      "learning_rate": 7.055448984176248e-05,
      "loss": 0.5308,
      "step": 19240
    },
    {
      "epoch": 1.2152777777777777,
      "grad_norm": 0.39945799112319946,
      "learning_rate": 7.045680238888315e-05,
      "loss": 0.787,
      "step": 19250
    },
    {
      "epoch": 1.2159090909090908,
      "grad_norm": 0.39489302039146423,
      "learning_rate": 7.035914581479934e-05,
      "loss": 0.666,
      "step": 19260
    },
    {
      "epoch": 1.216540404040404,
      "grad_norm": 0.4883556365966797,
      "learning_rate": 7.026152022158259e-05,
      "loss": 0.5708,
      "step": 19270
    },
    {
      "epoch": 1.2171717171717171,
      "grad_norm": 0.45725396275520325,
      "learning_rate": 7.01639257112719e-05,
      "loss": 0.4662,
      "step": 19280
    },
    {
      "epoch": 1.2178030303030303,
      "grad_norm": 0.6735079288482666,
      "learning_rate": 7.006636238587394e-05,
      "loss": 0.5332,
      "step": 19290
    },
    {
      "epoch": 1.2184343434343434,
      "grad_norm": 0.4353508949279785,
      "learning_rate": 6.996883034736265e-05,
      "loss": 0.8247,
      "step": 19300
    },
    {
      "epoch": 1.2190656565656566,
      "grad_norm": 0.4371180236339569,
      "learning_rate": 6.987132969767935e-05,
      "loss": 0.6574,
      "step": 19310
    },
    {
      "epoch": 1.2196969696969697,
      "grad_norm": 0.5048404932022095,
      "learning_rate": 6.977386053873253e-05,
      "loss": 0.5843,
      "step": 19320
    },
    {
      "epoch": 1.2203282828282829,
      "grad_norm": 0.4787929058074951,
      "learning_rate": 6.967642297239778e-05,
      "loss": 0.4892,
      "step": 19330
    },
    {
      "epoch": 1.220959595959596,
      "grad_norm": 0.7346585392951965,
      "learning_rate": 6.95790171005176e-05,
      "loss": 0.5438,
      "step": 19340
    },
    {
      "epoch": 1.2215909090909092,
      "grad_norm": 0.4177452027797699,
      "learning_rate": 6.948164302490148e-05,
      "loss": 0.7731,
      "step": 19350
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.42948535084724426,
      "learning_rate": 6.938430084732559e-05,
      "loss": 0.6685,
      "step": 19360
    },
    {
      "epoch": 1.2228535353535355,
      "grad_norm": 0.4224274754524231,
      "learning_rate": 6.928699066953275e-05,
      "loss": 0.5627,
      "step": 19370
    },
    {
      "epoch": 1.2234848484848484,
      "grad_norm": 0.5142901539802551,
      "learning_rate": 6.918971259323242e-05,
      "loss": 0.5365,
      "step": 19380
    },
    {
      "epoch": 1.2241161616161615,
      "grad_norm": 0.7501926422119141,
      "learning_rate": 6.909246672010039e-05,
      "loss": 0.5144,
      "step": 19390
    },
    {
      "epoch": 1.2247474747474747,
      "grad_norm": 0.4227486848831177,
      "learning_rate": 6.899525315177893e-05,
      "loss": 0.8145,
      "step": 19400
    },
    {
      "epoch": 1.2253787878787878,
      "grad_norm": 0.4214194118976593,
      "learning_rate": 6.889807198987641e-05,
      "loss": 0.6658,
      "step": 19410
    },
    {
      "epoch": 1.226010101010101,
      "grad_norm": 0.44231489300727844,
      "learning_rate": 6.880092333596741e-05,
      "loss": 0.5787,
      "step": 19420
    },
    {
      "epoch": 1.2266414141414141,
      "grad_norm": 0.4872717559337616,
      "learning_rate": 6.870380729159251e-05,
      "loss": 0.5321,
      "step": 19430
    },
    {
      "epoch": 1.2272727272727273,
      "grad_norm": 0.7570958733558655,
      "learning_rate": 6.860672395825822e-05,
      "loss": 0.5348,
      "step": 19440
    },
    {
      "epoch": 1.2279040404040404,
      "grad_norm": 0.4427579641342163,
      "learning_rate": 6.850967343743679e-05,
      "loss": 0.7869,
      "step": 19450
    },
    {
      "epoch": 1.2285353535353536,
      "grad_norm": 0.4674310088157654,
      "learning_rate": 6.841265583056632e-05,
      "loss": 0.6686,
      "step": 19460
    },
    {
      "epoch": 1.2291666666666667,
      "grad_norm": 0.46234625577926636,
      "learning_rate": 6.831567123905034e-05,
      "loss": 0.5633,
      "step": 19470
    },
    {
      "epoch": 1.22979797979798,
      "grad_norm": 0.46399959921836853,
      "learning_rate": 6.8218719764258e-05,
      "loss": 0.5039,
      "step": 19480
    },
    {
      "epoch": 1.2304292929292928,
      "grad_norm": 0.7546144127845764,
      "learning_rate": 6.812180150752377e-05,
      "loss": 0.545,
      "step": 19490
    },
    {
      "epoch": 1.231060606060606,
      "grad_norm": 0.3974730372428894,
      "learning_rate": 6.802491657014739e-05,
      "loss": 0.7779,
      "step": 19500
    },
    {
      "epoch": 1.2316919191919191,
      "grad_norm": 0.4204888641834259,
      "learning_rate": 6.792806505339384e-05,
      "loss": 0.6263,
      "step": 19510
    },
    {
      "epoch": 1.2323232323232323,
      "grad_norm": 0.413665771484375,
      "learning_rate": 6.783124705849311e-05,
      "loss": 0.5589,
      "step": 19520
    },
    {
      "epoch": 1.2329545454545454,
      "grad_norm": 0.5270133018493652,
      "learning_rate": 6.77344626866402e-05,
      "loss": 0.5004,
      "step": 19530
    },
    {
      "epoch": 1.2335858585858586,
      "grad_norm": 0.7086285948753357,
      "learning_rate": 6.763771203899488e-05,
      "loss": 0.5448,
      "step": 19540
    },
    {
      "epoch": 1.2342171717171717,
      "grad_norm": 0.4047442674636841,
      "learning_rate": 6.75409952166818e-05,
      "loss": 0.8222,
      "step": 19550
    },
    {
      "epoch": 1.2348484848484849,
      "grad_norm": 0.42858830094337463,
      "learning_rate": 6.744431232079013e-05,
      "loss": 0.647,
      "step": 19560
    },
    {
      "epoch": 1.235479797979798,
      "grad_norm": 0.4454658627510071,
      "learning_rate": 6.734766345237369e-05,
      "loss": 0.5456,
      "step": 19570
    },
    {
      "epoch": 1.2361111111111112,
      "grad_norm": 0.4949605166912079,
      "learning_rate": 6.725104871245063e-05,
      "loss": 0.5237,
      "step": 19580
    },
    {
      "epoch": 1.2367424242424243,
      "grad_norm": 0.7092539668083191,
      "learning_rate": 6.715446820200351e-05,
      "loss": 0.4992,
      "step": 19590
    },
    {
      "epoch": 1.2373737373737375,
      "grad_norm": 0.43510887026786804,
      "learning_rate": 6.705792202197908e-05,
      "loss": 0.7602,
      "step": 19600
    },
    {
      "epoch": 1.2380050505050506,
      "grad_norm": 0.4648519456386566,
      "learning_rate": 6.696141027328817e-05,
      "loss": 0.637,
      "step": 19610
    },
    {
      "epoch": 1.2386363636363638,
      "grad_norm": 0.4613785743713379,
      "learning_rate": 6.686493305680572e-05,
      "loss": 0.5915,
      "step": 19620
    },
    {
      "epoch": 1.2392676767676767,
      "grad_norm": 0.5052450895309448,
      "learning_rate": 6.676849047337048e-05,
      "loss": 0.4933,
      "step": 19630
    },
    {
      "epoch": 1.2398989898989898,
      "grad_norm": 0.7824845314025879,
      "learning_rate": 6.667208262378505e-05,
      "loss": 0.5262,
      "step": 19640
    },
    {
      "epoch": 1.240530303030303,
      "grad_norm": 0.39157968759536743,
      "learning_rate": 6.65757096088157e-05,
      "loss": 0.784,
      "step": 19650
    },
    {
      "epoch": 1.2411616161616161,
      "grad_norm": 0.44698968529701233,
      "learning_rate": 6.647937152919235e-05,
      "loss": 0.6536,
      "step": 19660
    },
    {
      "epoch": 1.2417929292929293,
      "grad_norm": 0.47868722677230835,
      "learning_rate": 6.63830684856083e-05,
      "loss": 0.6026,
      "step": 19670
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 0.4794768989086151,
      "learning_rate": 6.628680057872034e-05,
      "loss": 0.4987,
      "step": 19680
    },
    {
      "epoch": 1.2430555555555556,
      "grad_norm": 0.8108012080192566,
      "learning_rate": 6.619056790914846e-05,
      "loss": 0.5314,
      "step": 19690
    },
    {
      "epoch": 1.2436868686868687,
      "grad_norm": 0.4579092264175415,
      "learning_rate": 6.609437057747587e-05,
      "loss": 0.8303,
      "step": 19700
    },
    {
      "epoch": 1.2443181818181819,
      "grad_norm": 0.4272776246070862,
      "learning_rate": 6.599820868424882e-05,
      "loss": 0.6428,
      "step": 19710
    },
    {
      "epoch": 1.244949494949495,
      "grad_norm": 0.4598644971847534,
      "learning_rate": 6.590208232997646e-05,
      "loss": 0.552,
      "step": 19720
    },
    {
      "epoch": 1.2455808080808082,
      "grad_norm": 0.518125593662262,
      "learning_rate": 6.580599161513093e-05,
      "loss": 0.5018,
      "step": 19730
    },
    {
      "epoch": 1.246212121212121,
      "grad_norm": 0.7192078828811646,
      "learning_rate": 6.5709936640147e-05,
      "loss": 0.5307,
      "step": 19740
    },
    {
      "epoch": 1.2468434343434343,
      "grad_norm": 0.42809411883354187,
      "learning_rate": 6.561391750542214e-05,
      "loss": 0.7986,
      "step": 19750
    },
    {
      "epoch": 1.2474747474747474,
      "grad_norm": 0.4409399628639221,
      "learning_rate": 6.551793431131635e-05,
      "loss": 0.6669,
      "step": 19760
    },
    {
      "epoch": 1.2481060606060606,
      "grad_norm": 0.42329126596450806,
      "learning_rate": 6.542198715815205e-05,
      "loss": 0.5939,
      "step": 19770
    },
    {
      "epoch": 1.2487373737373737,
      "grad_norm": 0.4359026551246643,
      "learning_rate": 6.532607614621401e-05,
      "loss": 0.489,
      "step": 19780
    },
    {
      "epoch": 1.2493686868686869,
      "grad_norm": 0.7722359299659729,
      "learning_rate": 6.523020137574923e-05,
      "loss": 0.5573,
      "step": 19790
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.40776699781417847,
      "learning_rate": 6.513436294696678e-05,
      "loss": 0.8289,
      "step": 19800
    },
    {
      "epoch": 1.2506313131313131,
      "grad_norm": 0.4417164921760559,
      "learning_rate": 6.503856096003783e-05,
      "loss": 0.6906,
      "step": 19810
    },
    {
      "epoch": 1.2512626262626263,
      "grad_norm": 0.45934852957725525,
      "learning_rate": 6.494279551509537e-05,
      "loss": 0.5876,
      "step": 19820
    },
    {
      "epoch": 1.2518939393939394,
      "grad_norm": 0.4437900185585022,
      "learning_rate": 6.484706671223427e-05,
      "loss": 0.4721,
      "step": 19830
    },
    {
      "epoch": 1.2525252525252526,
      "grad_norm": 0.6525704264640808,
      "learning_rate": 6.475137465151107e-05,
      "loss": 0.5269,
      "step": 19840
    },
    {
      "epoch": 1.2531565656565657,
      "grad_norm": 0.40391334891319275,
      "learning_rate": 6.465571943294385e-05,
      "loss": 0.7659,
      "step": 19850
    },
    {
      "epoch": 1.253787878787879,
      "grad_norm": 0.4194711148738861,
      "learning_rate": 6.45601011565123e-05,
      "loss": 0.6616,
      "step": 19860
    },
    {
      "epoch": 1.254419191919192,
      "grad_norm": 0.45587095618247986,
      "learning_rate": 6.446451992215744e-05,
      "loss": 0.5838,
      "step": 19870
    },
    {
      "epoch": 1.255050505050505,
      "grad_norm": 0.4821510910987854,
      "learning_rate": 6.436897582978151e-05,
      "loss": 0.4866,
      "step": 19880
    },
    {
      "epoch": 1.2556818181818181,
      "grad_norm": 0.7578423619270325,
      "learning_rate": 6.427346897924804e-05,
      "loss": 0.5227,
      "step": 19890
    },
    {
      "epoch": 1.2563131313131313,
      "grad_norm": 0.40476757287979126,
      "learning_rate": 6.417799947038159e-05,
      "loss": 0.7803,
      "step": 19900
    },
    {
      "epoch": 1.2569444444444444,
      "grad_norm": 0.4430878460407257,
      "learning_rate": 6.408256740296766e-05,
      "loss": 0.6639,
      "step": 19910
    },
    {
      "epoch": 1.2575757575757576,
      "grad_norm": 0.463030606508255,
      "learning_rate": 6.398717287675265e-05,
      "loss": 0.5822,
      "step": 19920
    },
    {
      "epoch": 1.2582070707070707,
      "grad_norm": 0.47586965560913086,
      "learning_rate": 6.389181599144367e-05,
      "loss": 0.5031,
      "step": 19930
    },
    {
      "epoch": 1.2588383838383839,
      "grad_norm": 0.6899352669715881,
      "learning_rate": 6.37964968467086e-05,
      "loss": 0.5184,
      "step": 19940
    },
    {
      "epoch": 1.259469696969697,
      "grad_norm": 0.42546647787094116,
      "learning_rate": 6.370121554217576e-05,
      "loss": 0.7847,
      "step": 19950
    },
    {
      "epoch": 1.2601010101010102,
      "grad_norm": 0.4771040976047516,
      "learning_rate": 6.360597217743396e-05,
      "loss": 0.6623,
      "step": 19960
    },
    {
      "epoch": 1.2607323232323233,
      "grad_norm": 0.47740769386291504,
      "learning_rate": 6.351076685203236e-05,
      "loss": 0.5359,
      "step": 19970
    },
    {
      "epoch": 1.2613636363636362,
      "grad_norm": 0.4570713937282562,
      "learning_rate": 6.341559966548036e-05,
      "loss": 0.4681,
      "step": 19980
    },
    {
      "epoch": 1.2619949494949494,
      "grad_norm": 0.6711466908454895,
      "learning_rate": 6.332047071724751e-05,
      "loss": 0.5425,
      "step": 19990
    },
    {
      "epoch": 1.2626262626262625,
      "grad_norm": 0.4166159927845001,
      "learning_rate": 6.322538010676334e-05,
      "loss": 0.8036,
      "step": 20000
    },
    {
      "epoch": 1.2626262626262625,
      "eval_loss": 0.6444680094718933,
      "eval_runtime": 31.7549,
      "eval_samples_per_second": 80.618,
      "eval_steps_per_second": 10.077,
      "step": 20000
    },
    {
      "epoch": 1.2632575757575757,
      "grad_norm": 0.4471845030784607,
      "learning_rate": 6.313032793341739e-05,
      "loss": 0.6721,
      "step": 20010
    },
    {
      "epoch": 1.2638888888888888,
      "grad_norm": 0.4755266606807709,
      "learning_rate": 6.303531429655893e-05,
      "loss": 0.5969,
      "step": 20020
    },
    {
      "epoch": 1.264520202020202,
      "grad_norm": 0.4984815716743469,
      "learning_rate": 6.294033929549707e-05,
      "loss": 0.5441,
      "step": 20030
    },
    {
      "epoch": 1.2651515151515151,
      "grad_norm": 0.7560636401176453,
      "learning_rate": 6.28454030295004e-05,
      "loss": 0.555,
      "step": 20040
    },
    {
      "epoch": 1.2657828282828283,
      "grad_norm": 0.41524508595466614,
      "learning_rate": 6.275050559779714e-05,
      "loss": 0.8171,
      "step": 20050
    },
    {
      "epoch": 1.2664141414141414,
      "grad_norm": 0.44458985328674316,
      "learning_rate": 6.265564709957483e-05,
      "loss": 0.6707,
      "step": 20060
    },
    {
      "epoch": 1.2670454545454546,
      "grad_norm": 0.45801037549972534,
      "learning_rate": 6.256082763398042e-05,
      "loss": 0.5711,
      "step": 20070
    },
    {
      "epoch": 1.2676767676767677,
      "grad_norm": 0.4680320620536804,
      "learning_rate": 6.246604730011998e-05,
      "loss": 0.5007,
      "step": 20080
    },
    {
      "epoch": 1.2683080808080809,
      "grad_norm": 0.7338193655014038,
      "learning_rate": 6.237130619705866e-05,
      "loss": 0.5222,
      "step": 20090
    },
    {
      "epoch": 1.268939393939394,
      "grad_norm": 0.3849430978298187,
      "learning_rate": 6.227660442382071e-05,
      "loss": 0.7742,
      "step": 20100
    },
    {
      "epoch": 1.2695707070707072,
      "grad_norm": 0.40056148171424866,
      "learning_rate": 6.218194207938917e-05,
      "loss": 0.622,
      "step": 20110
    },
    {
      "epoch": 1.2702020202020203,
      "grad_norm": 0.46981385350227356,
      "learning_rate": 6.208731926270592e-05,
      "loss": 0.5696,
      "step": 20120
    },
    {
      "epoch": 1.2708333333333333,
      "grad_norm": 0.4940813183784485,
      "learning_rate": 6.199273607267151e-05,
      "loss": 0.4989,
      "step": 20130
    },
    {
      "epoch": 1.2714646464646464,
      "grad_norm": 0.7308007478713989,
      "learning_rate": 6.18981926081451e-05,
      "loss": 0.5191,
      "step": 20140
    },
    {
      "epoch": 1.2720959595959596,
      "grad_norm": 0.4418826401233673,
      "learning_rate": 6.180368896794426e-05,
      "loss": 0.8163,
      "step": 20150
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 0.41342514753341675,
      "learning_rate": 6.170922525084504e-05,
      "loss": 0.6686,
      "step": 20160
    },
    {
      "epoch": 1.2733585858585859,
      "grad_norm": 0.4789275825023651,
      "learning_rate": 6.161480155558163e-05,
      "loss": 0.5756,
      "step": 20170
    },
    {
      "epoch": 1.273989898989899,
      "grad_norm": 0.47732725739479065,
      "learning_rate": 6.152041798084653e-05,
      "loss": 0.4993,
      "step": 20180
    },
    {
      "epoch": 1.2746212121212122,
      "grad_norm": 0.7475916147232056,
      "learning_rate": 6.142607462529019e-05,
      "loss": 0.5113,
      "step": 20190
    },
    {
      "epoch": 1.2752525252525253,
      "grad_norm": 0.4275875389575958,
      "learning_rate": 6.133177158752108e-05,
      "loss": 0.8084,
      "step": 20200
    },
    {
      "epoch": 1.2758838383838385,
      "grad_norm": 0.4426858425140381,
      "learning_rate": 6.123750896610555e-05,
      "loss": 0.6428,
      "step": 20210
    },
    {
      "epoch": 1.2765151515151514,
      "grad_norm": 0.4514177739620209,
      "learning_rate": 6.114328685956759e-05,
      "loss": 0.6077,
      "step": 20220
    },
    {
      "epoch": 1.2771464646464645,
      "grad_norm": 0.5021898150444031,
      "learning_rate": 6.104910536638903e-05,
      "loss": 0.5226,
      "step": 20230
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 0.6105430126190186,
      "learning_rate": 6.095496458500907e-05,
      "loss": 0.497,
      "step": 20240
    },
    {
      "epoch": 1.2784090909090908,
      "grad_norm": 0.4063798785209656,
      "learning_rate": 6.0860864613824496e-05,
      "loss": 0.7896,
      "step": 20250
    },
    {
      "epoch": 1.279040404040404,
      "grad_norm": 0.4245878756046295,
      "learning_rate": 6.0766805551189346e-05,
      "loss": 0.6567,
      "step": 20260
    },
    {
      "epoch": 1.2796717171717171,
      "grad_norm": 0.4198213219642639,
      "learning_rate": 6.067278749541495e-05,
      "loss": 0.5825,
      "step": 20270
    },
    {
      "epoch": 1.2803030303030303,
      "grad_norm": 0.46612149477005005,
      "learning_rate": 6.057881054476974e-05,
      "loss": 0.509,
      "step": 20280
    },
    {
      "epoch": 1.2809343434343434,
      "grad_norm": 0.6923873424530029,
      "learning_rate": 6.048487479747924e-05,
      "loss": 0.4951,
      "step": 20290
    },
    {
      "epoch": 1.2815656565656566,
      "grad_norm": 0.3927284777164459,
      "learning_rate": 6.0390980351725834e-05,
      "loss": 0.7979,
      "step": 20300
    },
    {
      "epoch": 1.2821969696969697,
      "grad_norm": 0.4290507137775421,
      "learning_rate": 6.029712730564879e-05,
      "loss": 0.6392,
      "step": 20310
    },
    {
      "epoch": 1.2828282828282829,
      "grad_norm": 0.42847615480422974,
      "learning_rate": 6.020331575734411e-05,
      "loss": 0.5518,
      "step": 20320
    },
    {
      "epoch": 1.283459595959596,
      "grad_norm": 0.44330036640167236,
      "learning_rate": 6.010954580486439e-05,
      "loss": 0.5043,
      "step": 20330
    },
    {
      "epoch": 1.2840909090909092,
      "grad_norm": 0.7030863165855408,
      "learning_rate": 6.001581754621875e-05,
      "loss": 0.5222,
      "step": 20340
    },
    {
      "epoch": 1.2847222222222223,
      "grad_norm": 0.4235060214996338,
      "learning_rate": 5.992213107937276e-05,
      "loss": 0.7988,
      "step": 20350
    },
    {
      "epoch": 1.2853535353535355,
      "grad_norm": 0.39605268836021423,
      "learning_rate": 5.98284865022483e-05,
      "loss": 0.6587,
      "step": 20360
    },
    {
      "epoch": 1.2859848484848486,
      "grad_norm": 0.45490652322769165,
      "learning_rate": 5.9734883912723425e-05,
      "loss": 0.5499,
      "step": 20370
    },
    {
      "epoch": 1.2866161616161615,
      "grad_norm": 0.47418302297592163,
      "learning_rate": 5.96413234086324e-05,
      "loss": 0.5029,
      "step": 20380
    },
    {
      "epoch": 1.2872474747474747,
      "grad_norm": 0.6778990030288696,
      "learning_rate": 5.954780508776537e-05,
      "loss": 0.5313,
      "step": 20390
    },
    {
      "epoch": 1.2878787878787878,
      "grad_norm": 0.4181135892868042,
      "learning_rate": 5.9454329047868526e-05,
      "loss": 0.81,
      "step": 20400
    },
    {
      "epoch": 1.288510101010101,
      "grad_norm": 0.44087696075439453,
      "learning_rate": 5.9360895386643756e-05,
      "loss": 0.645,
      "step": 20410
    },
    {
      "epoch": 1.2891414141414141,
      "grad_norm": 0.45325371623039246,
      "learning_rate": 5.9267504201748725e-05,
      "loss": 0.5708,
      "step": 20420
    },
    {
      "epoch": 1.2897727272727273,
      "grad_norm": 0.5231882333755493,
      "learning_rate": 5.917415559079666e-05,
      "loss": 0.4846,
      "step": 20430
    },
    {
      "epoch": 1.2904040404040404,
      "grad_norm": 0.7699835300445557,
      "learning_rate": 5.9080849651356294e-05,
      "loss": 0.5254,
      "step": 20440
    },
    {
      "epoch": 1.2910353535353536,
      "grad_norm": 0.41361433267593384,
      "learning_rate": 5.898758648095182e-05,
      "loss": 0.7952,
      "step": 20450
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 0.4000220000743866,
      "learning_rate": 5.889436617706261e-05,
      "loss": 0.6485,
      "step": 20460
    },
    {
      "epoch": 1.2922979797979797,
      "grad_norm": 0.4799007177352905,
      "learning_rate": 5.8801188837123356e-05,
      "loss": 0.5624,
      "step": 20470
    },
    {
      "epoch": 1.2929292929292928,
      "grad_norm": 0.45171457529067993,
      "learning_rate": 5.870805455852375e-05,
      "loss": 0.5182,
      "step": 20480
    },
    {
      "epoch": 1.293560606060606,
      "grad_norm": 0.6684824824333191,
      "learning_rate": 5.861496343860855e-05,
      "loss": 0.5202,
      "step": 20490
    },
    {
      "epoch": 1.2941919191919191,
      "grad_norm": 0.44333282113075256,
      "learning_rate": 5.8521915574677323e-05,
      "loss": 0.7805,
      "step": 20500
    },
    {
      "epoch": 1.2948232323232323,
      "grad_norm": 0.4316853880882263,
      "learning_rate": 5.842891106398451e-05,
      "loss": 0.634,
      "step": 20510
    },
    {
      "epoch": 1.2954545454545454,
      "grad_norm": 0.4471173584461212,
      "learning_rate": 5.833595000373917e-05,
      "loss": 0.5895,
      "step": 20520
    },
    {
      "epoch": 1.2960858585858586,
      "grad_norm": 0.49974316358566284,
      "learning_rate": 5.8243032491105e-05,
      "loss": 0.5062,
      "step": 20530
    },
    {
      "epoch": 1.2967171717171717,
      "grad_norm": 0.66298907995224,
      "learning_rate": 5.8150158623200143e-05,
      "loss": 0.5357,
      "step": 20540
    },
    {
      "epoch": 1.2973484848484849,
      "grad_norm": 0.41261598467826843,
      "learning_rate": 5.805732849709714e-05,
      "loss": 0.8354,
      "step": 20550
    },
    {
      "epoch": 1.297979797979798,
      "grad_norm": 0.43574127554893494,
      "learning_rate": 5.796454220982277e-05,
      "loss": 0.681,
      "step": 20560
    },
    {
      "epoch": 1.2986111111111112,
      "grad_norm": 0.42683979868888855,
      "learning_rate": 5.787179985835811e-05,
      "loss": 0.551,
      "step": 20570
    },
    {
      "epoch": 1.2992424242424243,
      "grad_norm": 0.47132155299186707,
      "learning_rate": 5.777910153963819e-05,
      "loss": 0.5202,
      "step": 20580
    },
    {
      "epoch": 1.2998737373737375,
      "grad_norm": 0.7455496788024902,
      "learning_rate": 5.7686447350552065e-05,
      "loss": 0.5042,
      "step": 20590
    },
    {
      "epoch": 1.3005050505050506,
      "grad_norm": 0.43786099553108215,
      "learning_rate": 5.759383738794263e-05,
      "loss": 0.7571,
      "step": 20600
    },
    {
      "epoch": 1.3011363636363638,
      "grad_norm": 0.4210129976272583,
      "learning_rate": 5.750127174860667e-05,
      "loss": 0.6439,
      "step": 20610
    },
    {
      "epoch": 1.3017676767676767,
      "grad_norm": 0.4431641697883606,
      "learning_rate": 5.74087505292945e-05,
      "loss": 0.5755,
      "step": 20620
    },
    {
      "epoch": 1.3023989898989898,
      "grad_norm": 0.4822089672088623,
      "learning_rate": 5.731627382671008e-05,
      "loss": 0.5011,
      "step": 20630
    },
    {
      "epoch": 1.303030303030303,
      "grad_norm": 0.6925951838493347,
      "learning_rate": 5.7223841737510786e-05,
      "loss": 0.5052,
      "step": 20640
    },
    {
      "epoch": 1.3036616161616161,
      "grad_norm": 0.4115133583545685,
      "learning_rate": 5.713145435830749e-05,
      "loss": 0.7758,
      "step": 20650
    },
    {
      "epoch": 1.3042929292929293,
      "grad_norm": 0.42563876509666443,
      "learning_rate": 5.7039111785664215e-05,
      "loss": 0.644,
      "step": 20660
    },
    {
      "epoch": 1.3049242424242424,
      "grad_norm": 0.4803997874259949,
      "learning_rate": 5.694681411609817e-05,
      "loss": 0.5484,
      "step": 20670
    },
    {
      "epoch": 1.3055555555555556,
      "grad_norm": 0.49707382917404175,
      "learning_rate": 5.685456144607966e-05,
      "loss": 0.4811,
      "step": 20680
    },
    {
      "epoch": 1.3061868686868687,
      "grad_norm": 0.7546204924583435,
      "learning_rate": 5.676235387203188e-05,
      "loss": 0.5066,
      "step": 20690
    },
    {
      "epoch": 1.3068181818181819,
      "grad_norm": 0.42508986592292786,
      "learning_rate": 5.6670191490331036e-05,
      "loss": 0.8211,
      "step": 20700
    },
    {
      "epoch": 1.307449494949495,
      "grad_norm": 0.44827041029930115,
      "learning_rate": 5.6578074397306e-05,
      "loss": 0.6295,
      "step": 20710
    },
    {
      "epoch": 1.308080808080808,
      "grad_norm": 0.457287460565567,
      "learning_rate": 5.648600268923832e-05,
      "loss": 0.5564,
      "step": 20720
    },
    {
      "epoch": 1.308712121212121,
      "grad_norm": 0.4721066951751709,
      "learning_rate": 5.639397646236204e-05,
      "loss": 0.4971,
      "step": 20730
    },
    {
      "epoch": 1.3093434343434343,
      "grad_norm": 0.7324030995368958,
      "learning_rate": 5.630199581286384e-05,
      "loss": 0.5166,
      "step": 20740
    },
    {
      "epoch": 1.3099747474747474,
      "grad_norm": 0.39813894033432007,
      "learning_rate": 5.62100608368826e-05,
      "loss": 0.8026,
      "step": 20750
    },
    {
      "epoch": 1.3106060606060606,
      "grad_norm": 0.4548540413379669,
      "learning_rate": 5.611817163050954e-05,
      "loss": 0.6671,
      "step": 20760
    },
    {
      "epoch": 1.3112373737373737,
      "grad_norm": 0.45226895809173584,
      "learning_rate": 5.602632828978799e-05,
      "loss": 0.5429,
      "step": 20770
    },
    {
      "epoch": 1.3118686868686869,
      "grad_norm": 0.5180386900901794,
      "learning_rate": 5.5934530910713426e-05,
      "loss": 0.5194,
      "step": 20780
    },
    {
      "epoch": 1.3125,
      "grad_norm": 0.7457020878791809,
      "learning_rate": 5.584277958923321e-05,
      "loss": 0.5296,
      "step": 20790
    },
    {
      "epoch": 1.3131313131313131,
      "grad_norm": 0.43067875504493713,
      "learning_rate": 5.5751074421246565e-05,
      "loss": 0.7759,
      "step": 20800
    },
    {
      "epoch": 1.3137626262626263,
      "grad_norm": 0.4012519419193268,
      "learning_rate": 5.5659415502604515e-05,
      "loss": 0.6315,
      "step": 20810
    },
    {
      "epoch": 1.3143939393939394,
      "grad_norm": 0.47751834988594055,
      "learning_rate": 5.556780292910968e-05,
      "loss": 0.5654,
      "step": 20820
    },
    {
      "epoch": 1.3150252525252526,
      "grad_norm": 0.43210944533348083,
      "learning_rate": 5.547623679651637e-05,
      "loss": 0.4645,
      "step": 20830
    },
    {
      "epoch": 1.3156565656565657,
      "grad_norm": 0.727574348449707,
      "learning_rate": 5.5384717200530204e-05,
      "loss": 0.5192,
      "step": 20840
    },
    {
      "epoch": 1.316287878787879,
      "grad_norm": 0.3908596336841583,
      "learning_rate": 5.529324423680824e-05,
      "loss": 0.8318,
      "step": 20850
    },
    {
      "epoch": 1.316919191919192,
      "grad_norm": 0.4179915189743042,
      "learning_rate": 5.520181800095874e-05,
      "loss": 0.6346,
      "step": 20860
    },
    {
      "epoch": 1.317550505050505,
      "grad_norm": 0.4749850928783417,
      "learning_rate": 5.511043858854124e-05,
      "loss": 0.5652,
      "step": 20870
    },
    {
      "epoch": 1.3181818181818181,
      "grad_norm": 0.43591997027397156,
      "learning_rate": 5.501910609506621e-05,
      "loss": 0.4996,
      "step": 20880
    },
    {
      "epoch": 1.3188131313131313,
      "grad_norm": 0.750872790813446,
      "learning_rate": 5.492782061599515e-05,
      "loss": 0.5305,
      "step": 20890
    },
    {
      "epoch": 1.3194444444444444,
      "grad_norm": 0.4321485459804535,
      "learning_rate": 5.48365822467404e-05,
      "loss": 0.8109,
      "step": 20900
    },
    {
      "epoch": 1.3200757575757576,
      "grad_norm": 0.41416794061660767,
      "learning_rate": 5.474539108266501e-05,
      "loss": 0.6565,
      "step": 20910
    },
    {
      "epoch": 1.3207070707070707,
      "grad_norm": 0.4545000493526459,
      "learning_rate": 5.4654247219082833e-05,
      "loss": 0.5736,
      "step": 20920
    },
    {
      "epoch": 1.3213383838383839,
      "grad_norm": 0.4779270887374878,
      "learning_rate": 5.456315075125814e-05,
      "loss": 0.5191,
      "step": 20930
    },
    {
      "epoch": 1.321969696969697,
      "grad_norm": 0.7829553484916687,
      "learning_rate": 5.4472101774405726e-05,
      "loss": 0.5452,
      "step": 20940
    },
    {
      "epoch": 1.3226010101010102,
      "grad_norm": 0.4044997990131378,
      "learning_rate": 5.43811003836907e-05,
      "loss": 0.8166,
      "step": 20950
    },
    {
      "epoch": 1.3232323232323233,
      "grad_norm": 0.4412634074687958,
      "learning_rate": 5.429014667422854e-05,
      "loss": 0.649,
      "step": 20960
    },
    {
      "epoch": 1.3238636363636362,
      "grad_norm": 0.4788423180580139,
      "learning_rate": 5.419924074108479e-05,
      "loss": 0.537,
      "step": 20970
    },
    {
      "epoch": 1.3244949494949494,
      "grad_norm": 0.46375009417533875,
      "learning_rate": 5.410838267927507e-05,
      "loss": 0.5053,
      "step": 20980
    },
    {
      "epoch": 1.3251262626262625,
      "grad_norm": 0.6899106502532959,
      "learning_rate": 5.401757258376496e-05,
      "loss": 0.5326,
      "step": 20990
    },
    {
      "epoch": 1.3257575757575757,
      "grad_norm": 0.4176788628101349,
      "learning_rate": 5.3926810549470016e-05,
      "loss": 0.8243,
      "step": 21000
    },
    {
      "epoch": 1.3257575757575757,
      "eval_loss": 0.6435986757278442,
      "eval_runtime": 33.7763,
      "eval_samples_per_second": 75.793,
      "eval_steps_per_second": 9.474,
      "step": 21000
    },
    {
      "epoch": 1.3263888888888888,
      "grad_norm": 0.43148452043533325,
      "learning_rate": 5.3836096671255396e-05,
      "loss": 0.6405,
      "step": 21010
    },
    {
      "epoch": 1.327020202020202,
      "grad_norm": 0.4148733615875244,
      "learning_rate": 5.374543104393601e-05,
      "loss": 0.557,
      "step": 21020
    },
    {
      "epoch": 1.3276515151515151,
      "grad_norm": 0.5252333283424377,
      "learning_rate": 5.3654813762276346e-05,
      "loss": 0.4947,
      "step": 21030
    },
    {
      "epoch": 1.3282828282828283,
      "grad_norm": 0.8124145865440369,
      "learning_rate": 5.356424492099028e-05,
      "loss": 0.5393,
      "step": 21040
    },
    {
      "epoch": 1.3289141414141414,
      "grad_norm": 0.4137613773345947,
      "learning_rate": 5.3473724614741204e-05,
      "loss": 0.7779,
      "step": 21050
    },
    {
      "epoch": 1.3295454545454546,
      "grad_norm": 0.4154757857322693,
      "learning_rate": 5.338325293814165e-05,
      "loss": 0.6399,
      "step": 21060
    },
    {
      "epoch": 1.3301767676767677,
      "grad_norm": 0.44959232211112976,
      "learning_rate": 5.3292829985753354e-05,
      "loss": 0.5952,
      "step": 21070
    },
    {
      "epoch": 1.3308080808080809,
      "grad_norm": 0.5180746912956238,
      "learning_rate": 5.320245585208713e-05,
      "loss": 0.5568,
      "step": 21080
    },
    {
      "epoch": 1.331439393939394,
      "grad_norm": 0.6760265827178955,
      "learning_rate": 5.311213063160283e-05,
      "loss": 0.5161,
      "step": 21090
    },
    {
      "epoch": 1.3320707070707072,
      "grad_norm": 0.4217928946018219,
      "learning_rate": 5.302185441870908e-05,
      "loss": 0.7887,
      "step": 21100
    },
    {
      "epoch": 1.3327020202020203,
      "grad_norm": 0.42852675914764404,
      "learning_rate": 5.293162730776336e-05,
      "loss": 0.6613,
      "step": 21110
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.4619160592556,
      "learning_rate": 5.2841449393071696e-05,
      "loss": 0.5684,
      "step": 21120
    },
    {
      "epoch": 1.3339646464646464,
      "grad_norm": 0.464976042509079,
      "learning_rate": 5.275132076888893e-05,
      "loss": 0.5012,
      "step": 21130
    },
    {
      "epoch": 1.3345959595959596,
      "grad_norm": 0.6551287174224854,
      "learning_rate": 5.266124152941819e-05,
      "loss": 0.5226,
      "step": 21140
    },
    {
      "epoch": 1.3352272727272727,
      "grad_norm": 0.4212140142917633,
      "learning_rate": 5.257121176881103e-05,
      "loss": 0.7947,
      "step": 21150
    },
    {
      "epoch": 1.3358585858585859,
      "grad_norm": 0.42818906903266907,
      "learning_rate": 5.2481231581167336e-05,
      "loss": 0.6319,
      "step": 21160
    },
    {
      "epoch": 1.336489898989899,
      "grad_norm": 0.4263778626918793,
      "learning_rate": 5.239130106053509e-05,
      "loss": 0.5775,
      "step": 21170
    },
    {
      "epoch": 1.3371212121212122,
      "grad_norm": 0.4673944115638733,
      "learning_rate": 5.230142030091049e-05,
      "loss": 0.5004,
      "step": 21180
    },
    {
      "epoch": 1.3377525252525253,
      "grad_norm": 0.7591480612754822,
      "learning_rate": 5.221158939623761e-05,
      "loss": 0.5137,
      "step": 21190
    },
    {
      "epoch": 1.3383838383838385,
      "grad_norm": 0.404070645570755,
      "learning_rate": 5.212180844040848e-05,
      "loss": 0.7768,
      "step": 21200
    },
    {
      "epoch": 1.3390151515151514,
      "grad_norm": 0.39202001690864563,
      "learning_rate": 5.2032077527262854e-05,
      "loss": 0.6364,
      "step": 21210
    },
    {
      "epoch": 1.3396464646464645,
      "grad_norm": 0.4402666389942169,
      "learning_rate": 5.1942396750588294e-05,
      "loss": 0.5894,
      "step": 21220
    },
    {
      "epoch": 1.3402777777777777,
      "grad_norm": 0.4825628399848938,
      "learning_rate": 5.1852766204119876e-05,
      "loss": 0.4964,
      "step": 21230
    },
    {
      "epoch": 1.3409090909090908,
      "grad_norm": 0.7100677490234375,
      "learning_rate": 5.176318598154017e-05,
      "loss": 0.5256,
      "step": 21240
    },
    {
      "epoch": 1.341540404040404,
      "grad_norm": 0.41154977679252625,
      "learning_rate": 5.167365617647917e-05,
      "loss": 0.7986,
      "step": 21250
    },
    {
      "epoch": 1.3421717171717171,
      "grad_norm": 0.4298781156539917,
      "learning_rate": 5.158417688251415e-05,
      "loss": 0.6461,
      "step": 21260
    },
    {
      "epoch": 1.3428030303030303,
      "grad_norm": 0.4711237847805023,
      "learning_rate": 5.149474819316966e-05,
      "loss": 0.5682,
      "step": 21270
    },
    {
      "epoch": 1.3434343434343434,
      "grad_norm": 0.5154097676277161,
      "learning_rate": 5.140537020191731e-05,
      "loss": 0.4915,
      "step": 21280
    },
    {
      "epoch": 1.3440656565656566,
      "grad_norm": 0.7325214743614197,
      "learning_rate": 5.1316043002175695e-05,
      "loss": 0.5219,
      "step": 21290
    },
    {
      "epoch": 1.3446969696969697,
      "grad_norm": 0.4141232967376709,
      "learning_rate": 5.12267666873103e-05,
      "loss": 0.8019,
      "step": 21300
    },
    {
      "epoch": 1.3453282828282829,
      "grad_norm": 0.4123886227607727,
      "learning_rate": 5.1137541350633566e-05,
      "loss": 0.6561,
      "step": 21310
    },
    {
      "epoch": 1.345959595959596,
      "grad_norm": 0.4994353652000427,
      "learning_rate": 5.10483670854045e-05,
      "loss": 0.574,
      "step": 21320
    },
    {
      "epoch": 1.3465909090909092,
      "grad_norm": 0.4688580334186554,
      "learning_rate": 5.0959243984828805e-05,
      "loss": 0.5194,
      "step": 21330
    },
    {
      "epoch": 1.3472222222222223,
      "grad_norm": 0.8002917170524597,
      "learning_rate": 5.087017214205865e-05,
      "loss": 0.5026,
      "step": 21340
    },
    {
      "epoch": 1.3478535353535355,
      "grad_norm": 0.39379486441612244,
      "learning_rate": 5.07811516501927e-05,
      "loss": 0.8003,
      "step": 21350
    },
    {
      "epoch": 1.3484848484848486,
      "grad_norm": 0.40005260705947876,
      "learning_rate": 5.0692182602275926e-05,
      "loss": 0.6435,
      "step": 21360
    },
    {
      "epoch": 1.3491161616161615,
      "grad_norm": 0.4678109884262085,
      "learning_rate": 5.060326509129949e-05,
      "loss": 0.5725,
      "step": 21370
    },
    {
      "epoch": 1.3497474747474747,
      "grad_norm": 0.5286290645599365,
      "learning_rate": 5.0514399210200715e-05,
      "loss": 0.4755,
      "step": 21380
    },
    {
      "epoch": 1.3503787878787878,
      "grad_norm": 0.7266642451286316,
      "learning_rate": 5.0425585051862926e-05,
      "loss": 0.5353,
      "step": 21390
    },
    {
      "epoch": 1.351010101010101,
      "grad_norm": 0.419116348028183,
      "learning_rate": 5.0336822709115474e-05,
      "loss": 0.7753,
      "step": 21400
    },
    {
      "epoch": 1.3516414141414141,
      "grad_norm": 0.48275306820869446,
      "learning_rate": 5.024811227473348e-05,
      "loss": 0.6866,
      "step": 21410
    },
    {
      "epoch": 1.3522727272727273,
      "grad_norm": 0.47382041811943054,
      "learning_rate": 5.015945384143783e-05,
      "loss": 0.5771,
      "step": 21420
    },
    {
      "epoch": 1.3529040404040404,
      "grad_norm": 0.4579179883003235,
      "learning_rate": 5.007084750189501e-05,
      "loss": 0.4818,
      "step": 21430
    },
    {
      "epoch": 1.3535353535353536,
      "grad_norm": 0.704153299331665,
      "learning_rate": 4.998229334871718e-05,
      "loss": 0.5297,
      "step": 21440
    },
    {
      "epoch": 1.3541666666666667,
      "grad_norm": 0.40559834241867065,
      "learning_rate": 4.989379147446183e-05,
      "loss": 0.788,
      "step": 21450
    },
    {
      "epoch": 1.3547979797979797,
      "grad_norm": 0.4890791177749634,
      "learning_rate": 4.980534197163186e-05,
      "loss": 0.6818,
      "step": 21460
    },
    {
      "epoch": 1.3554292929292928,
      "grad_norm": 0.46046289801597595,
      "learning_rate": 4.971694493267539e-05,
      "loss": 0.5828,
      "step": 21470
    },
    {
      "epoch": 1.356060606060606,
      "grad_norm": 0.4379886984825134,
      "learning_rate": 4.962860044998582e-05,
      "loss": 0.4881,
      "step": 21480
    },
    {
      "epoch": 1.3566919191919191,
      "grad_norm": 0.7168340086936951,
      "learning_rate": 4.9540308615901474e-05,
      "loss": 0.5031,
      "step": 21490
    },
    {
      "epoch": 1.3573232323232323,
      "grad_norm": 0.40599891543388367,
      "learning_rate": 4.945206952270575e-05,
      "loss": 0.7659,
      "step": 21500
    },
    {
      "epoch": 1.3579545454545454,
      "grad_norm": 0.43105655908584595,
      "learning_rate": 4.9363883262626845e-05,
      "loss": 0.6403,
      "step": 21510
    },
    {
      "epoch": 1.3585858585858586,
      "grad_norm": 0.4022470712661743,
      "learning_rate": 4.927574992783774e-05,
      "loss": 0.5406,
      "step": 21520
    },
    {
      "epoch": 1.3592171717171717,
      "grad_norm": 0.44705238938331604,
      "learning_rate": 4.918766961045623e-05,
      "loss": 0.4802,
      "step": 21530
    },
    {
      "epoch": 1.3598484848484849,
      "grad_norm": 0.6834387183189392,
      "learning_rate": 4.909964240254453e-05,
      "loss": 0.5204,
      "step": 21540
    },
    {
      "epoch": 1.360479797979798,
      "grad_norm": 0.459176242351532,
      "learning_rate": 4.9011668396109424e-05,
      "loss": 0.8316,
      "step": 21550
    },
    {
      "epoch": 1.3611111111111112,
      "grad_norm": 0.42681989073753357,
      "learning_rate": 4.892374768310203e-05,
      "loss": 0.6388,
      "step": 21560
    },
    {
      "epoch": 1.3617424242424243,
      "grad_norm": 0.45609062910079956,
      "learning_rate": 4.8835880355417874e-05,
      "loss": 0.5627,
      "step": 21570
    },
    {
      "epoch": 1.3623737373737375,
      "grad_norm": 0.4767473638057709,
      "learning_rate": 4.874806650489658e-05,
      "loss": 0.5309,
      "step": 21580
    },
    {
      "epoch": 1.3630050505050506,
      "grad_norm": 0.8341626524925232,
      "learning_rate": 4.866030622332194e-05,
      "loss": 0.506,
      "step": 21590
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.4093655049800873,
      "learning_rate": 4.857259960242171e-05,
      "loss": 0.7778,
      "step": 21600
    },
    {
      "epoch": 1.3642676767676767,
      "grad_norm": 0.4153463542461395,
      "learning_rate": 4.848494673386753e-05,
      "loss": 0.6329,
      "step": 21610
    },
    {
      "epoch": 1.3648989898989898,
      "grad_norm": 0.46196743845939636,
      "learning_rate": 4.839734770927501e-05,
      "loss": 0.5657,
      "step": 21620
    },
    {
      "epoch": 1.365530303030303,
      "grad_norm": 0.5264285802841187,
      "learning_rate": 4.830980262020331e-05,
      "loss": 0.4957,
      "step": 21630
    },
    {
      "epoch": 1.3661616161616161,
      "grad_norm": 0.6846432089805603,
      "learning_rate": 4.8222311558155306e-05,
      "loss": 0.5119,
      "step": 21640
    },
    {
      "epoch": 1.3667929292929293,
      "grad_norm": 0.39907488226890564,
      "learning_rate": 4.813487461457734e-05,
      "loss": 0.7473,
      "step": 21650
    },
    {
      "epoch": 1.3674242424242424,
      "grad_norm": 0.4308079481124878,
      "learning_rate": 4.804749188085931e-05,
      "loss": 0.6292,
      "step": 21660
    },
    {
      "epoch": 1.3680555555555556,
      "grad_norm": 0.45549482107162476,
      "learning_rate": 4.7960163448334316e-05,
      "loss": 0.5858,
      "step": 21670
    },
    {
      "epoch": 1.3686868686868687,
      "grad_norm": 0.44962865114212036,
      "learning_rate": 4.787288940827879e-05,
      "loss": 0.4958,
      "step": 21680
    },
    {
      "epoch": 1.3693181818181819,
      "grad_norm": 0.721386730670929,
      "learning_rate": 4.778566985191223e-05,
      "loss": 0.5392,
      "step": 21690
    },
    {
      "epoch": 1.369949494949495,
      "grad_norm": 0.4313265085220337,
      "learning_rate": 4.769850487039732e-05,
      "loss": 0.8326,
      "step": 21700
    },
    {
      "epoch": 1.370580808080808,
      "grad_norm": 0.41946280002593994,
      "learning_rate": 4.7611394554839594e-05,
      "loss": 0.6367,
      "step": 21710
    },
    {
      "epoch": 1.371212121212121,
      "grad_norm": 0.41298359632492065,
      "learning_rate": 4.7524338996287456e-05,
      "loss": 0.5601,
      "step": 21720
    },
    {
      "epoch": 1.3718434343434343,
      "grad_norm": 0.5161378979682922,
      "learning_rate": 4.7437338285732116e-05,
      "loss": 0.4844,
      "step": 21730
    },
    {
      "epoch": 1.3724747474747474,
      "grad_norm": 0.7853905558586121,
      "learning_rate": 4.73503925141074e-05,
      "loss": 0.5202,
      "step": 21740
    },
    {
      "epoch": 1.3731060606060606,
      "grad_norm": 0.43856266140937805,
      "learning_rate": 4.726350177228982e-05,
      "loss": 0.7543,
      "step": 21750
    },
    {
      "epoch": 1.3737373737373737,
      "grad_norm": 0.42894628643989563,
      "learning_rate": 4.717666615109827e-05,
      "loss": 0.6469,
      "step": 21760
    },
    {
      "epoch": 1.3743686868686869,
      "grad_norm": 0.4589499533176422,
      "learning_rate": 4.708988574129406e-05,
      "loss": 0.5982,
      "step": 21770
    },
    {
      "epoch": 1.375,
      "grad_norm": 0.4905661642551422,
      "learning_rate": 4.7003160633580754e-05,
      "loss": 0.5003,
      "step": 21780
    },
    {
      "epoch": 1.3756313131313131,
      "grad_norm": 0.6602858901023865,
      "learning_rate": 4.6916490918604226e-05,
      "loss": 0.5214,
      "step": 21790
    },
    {
      "epoch": 1.3762626262626263,
      "grad_norm": 0.38965898752212524,
      "learning_rate": 4.6829876686952365e-05,
      "loss": 0.7382,
      "step": 21800
    },
    {
      "epoch": 1.3768939393939394,
      "grad_norm": 0.4507516622543335,
      "learning_rate": 4.674331802915507e-05,
      "loss": 0.6667,
      "step": 21810
    },
    {
      "epoch": 1.3775252525252526,
      "grad_norm": 0.44653087854385376,
      "learning_rate": 4.665681503568412e-05,
      "loss": 0.5803,
      "step": 21820
    },
    {
      "epoch": 1.3781565656565657,
      "grad_norm": 0.5130974054336548,
      "learning_rate": 4.657036779695326e-05,
      "loss": 0.5127,
      "step": 21830
    },
    {
      "epoch": 1.378787878787879,
      "grad_norm": 0.7406257390975952,
      "learning_rate": 4.648397640331782e-05,
      "loss": 0.5,
      "step": 21840
    },
    {
      "epoch": 1.379419191919192,
      "grad_norm": 0.40717533230781555,
      "learning_rate": 4.6397640945074796e-05,
      "loss": 0.8325,
      "step": 21850
    },
    {
      "epoch": 1.380050505050505,
      "grad_norm": 0.42125651240348816,
      "learning_rate": 4.6311361512462724e-05,
      "loss": 0.6863,
      "step": 21860
    },
    {
      "epoch": 1.3806818181818181,
      "grad_norm": 0.43882039189338684,
      "learning_rate": 4.622513819566156e-05,
      "loss": 0.5761,
      "step": 21870
    },
    {
      "epoch": 1.3813131313131313,
      "grad_norm": 0.41614821553230286,
      "learning_rate": 4.613897108479268e-05,
      "loss": 0.4637,
      "step": 21880
    },
    {
      "epoch": 1.3819444444444444,
      "grad_norm": 0.6833891868591309,
      "learning_rate": 4.605286026991866e-05,
      "loss": 0.5145,
      "step": 21890
    },
    {
      "epoch": 1.3825757575757576,
      "grad_norm": 0.42454826831817627,
      "learning_rate": 4.5966805841043213e-05,
      "loss": 0.8172,
      "step": 21900
    },
    {
      "epoch": 1.3832070707070707,
      "grad_norm": 0.4202449917793274,
      "learning_rate": 4.588080788811111e-05,
      "loss": 0.6659,
      "step": 21910
    },
    {
      "epoch": 1.3838383838383839,
      "grad_norm": 0.4577372670173645,
      "learning_rate": 4.5794866501008216e-05,
      "loss": 0.5643,
      "step": 21920
    },
    {
      "epoch": 1.384469696969697,
      "grad_norm": 0.48928356170654297,
      "learning_rate": 4.570898176956112e-05,
      "loss": 0.5339,
      "step": 21930
    },
    {
      "epoch": 1.3851010101010102,
      "grad_norm": 0.7488025426864624,
      "learning_rate": 4.562315378353729e-05,
      "loss": 0.5242,
      "step": 21940
    },
    {
      "epoch": 1.3857323232323233,
      "grad_norm": 0.41269609332084656,
      "learning_rate": 4.5537382632644774e-05,
      "loss": 0.7891,
      "step": 21950
    },
    {
      "epoch": 1.3863636363636362,
      "grad_norm": 0.4210665225982666,
      "learning_rate": 4.54516684065324e-05,
      "loss": 0.6348,
      "step": 21960
    },
    {
      "epoch": 1.3869949494949494,
      "grad_norm": 0.4494664967060089,
      "learning_rate": 4.536601119478932e-05,
      "loss": 0.5596,
      "step": 21970
    },
    {
      "epoch": 1.3876262626262625,
      "grad_norm": 0.4928082227706909,
      "learning_rate": 4.52804110869452e-05,
      "loss": 0.5189,
      "step": 21980
    },
    {
      "epoch": 1.3882575757575757,
      "grad_norm": 0.766897439956665,
      "learning_rate": 4.5194868172469964e-05,
      "loss": 0.545,
      "step": 21990
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 0.392879843711853,
      "learning_rate": 4.510938254077373e-05,
      "loss": 0.8804,
      "step": 22000
    },
    {
      "epoch": 1.3888888888888888,
      "eval_loss": 0.6402956247329712,
      "eval_runtime": 36.2981,
      "eval_samples_per_second": 70.527,
      "eval_steps_per_second": 8.816,
      "step": 22000
    },
    {
      "epoch": 1.389520202020202,
      "grad_norm": 0.4295845329761505,
      "learning_rate": 4.502395428120687e-05,
      "loss": 0.6671,
      "step": 22010
    },
    {
      "epoch": 1.3901515151515151,
      "grad_norm": 0.46694791316986084,
      "learning_rate": 4.4938583483059695e-05,
      "loss": 0.591,
      "step": 22020
    },
    {
      "epoch": 1.3907828282828283,
      "grad_norm": 0.45223522186279297,
      "learning_rate": 4.485327023556244e-05,
      "loss": 0.517,
      "step": 22030
    },
    {
      "epoch": 1.3914141414141414,
      "grad_norm": 0.7066787481307983,
      "learning_rate": 4.47680146278852e-05,
      "loss": 0.534,
      "step": 22040
    },
    {
      "epoch": 1.3920454545454546,
      "grad_norm": 0.39431092143058777,
      "learning_rate": 4.468281674913794e-05,
      "loss": 0.7928,
      "step": 22050
    },
    {
      "epoch": 1.3926767676767677,
      "grad_norm": 0.4497796297073364,
      "learning_rate": 4.4597676688370106e-05,
      "loss": 0.6252,
      "step": 22060
    },
    {
      "epoch": 1.3933080808080809,
      "grad_norm": 0.4858047664165497,
      "learning_rate": 4.451259453457083e-05,
      "loss": 0.567,
      "step": 22070
    },
    {
      "epoch": 1.393939393939394,
      "grad_norm": 0.48630213737487793,
      "learning_rate": 4.442757037666867e-05,
      "loss": 0.4951,
      "step": 22080
    },
    {
      "epoch": 1.3945707070707072,
      "grad_norm": 0.8198539614677429,
      "learning_rate": 4.4342604303531555e-05,
      "loss": 0.5428,
      "step": 22090
    },
    {
      "epoch": 1.3952020202020203,
      "grad_norm": 0.40441200137138367,
      "learning_rate": 4.425769640396681e-05,
      "loss": 0.7959,
      "step": 22100
    },
    {
      "epoch": 1.3958333333333333,
      "grad_norm": 0.41533422470092773,
      "learning_rate": 4.417284676672082e-05,
      "loss": 0.6493,
      "step": 22110
    },
    {
      "epoch": 1.3964646464646464,
      "grad_norm": 0.41782742738723755,
      "learning_rate": 4.4088055480479154e-05,
      "loss": 0.568,
      "step": 22120
    },
    {
      "epoch": 1.3970959595959596,
      "grad_norm": 0.4629150927066803,
      "learning_rate": 4.400332263386632e-05,
      "loss": 0.5312,
      "step": 22130
    },
    {
      "epoch": 1.3977272727272727,
      "grad_norm": 0.660281240940094,
      "learning_rate": 4.391864831544585e-05,
      "loss": 0.5177,
      "step": 22140
    },
    {
      "epoch": 1.3983585858585859,
      "grad_norm": 0.39929619431495667,
      "learning_rate": 4.383403261372004e-05,
      "loss": 0.8239,
      "step": 22150
    },
    {
      "epoch": 1.398989898989899,
      "grad_norm": 0.40335267782211304,
      "learning_rate": 4.374947561712992e-05,
      "loss": 0.6492,
      "step": 22160
    },
    {
      "epoch": 1.3996212121212122,
      "grad_norm": 0.47118738293647766,
      "learning_rate": 4.3664977414055095e-05,
      "loss": 0.5891,
      "step": 22170
    },
    {
      "epoch": 1.4002525252525253,
      "grad_norm": 0.5216065645217896,
      "learning_rate": 4.3580538092813904e-05,
      "loss": 0.5122,
      "step": 22180
    },
    {
      "epoch": 1.4008838383838385,
      "grad_norm": 0.7351699471473694,
      "learning_rate": 4.349615774166297e-05,
      "loss": 0.5186,
      "step": 22190
    },
    {
      "epoch": 1.4015151515151514,
      "grad_norm": 0.3819426894187927,
      "learning_rate": 4.3411836448797326e-05,
      "loss": 0.796,
      "step": 22200
    },
    {
      "epoch": 1.4021464646464645,
      "grad_norm": 0.4057577848434448,
      "learning_rate": 4.3327574302350305e-05,
      "loss": 0.6279,
      "step": 22210
    },
    {
      "epoch": 1.4027777777777777,
      "grad_norm": 0.4996820390224457,
      "learning_rate": 4.324337139039337e-05,
      "loss": 0.5937,
      "step": 22220
    },
    {
      "epoch": 1.4034090909090908,
      "grad_norm": 0.49788451194763184,
      "learning_rate": 4.3159227800936166e-05,
      "loss": 0.4933,
      "step": 22230
    },
    {
      "epoch": 1.404040404040404,
      "grad_norm": 0.6885098218917847,
      "learning_rate": 4.307514362192623e-05,
      "loss": 0.5074,
      "step": 22240
    },
    {
      "epoch": 1.4046717171717171,
      "grad_norm": 0.43490806221961975,
      "learning_rate": 4.299111894124905e-05,
      "loss": 0.782,
      "step": 22250
    },
    {
      "epoch": 1.4053030303030303,
      "grad_norm": 0.382238507270813,
      "learning_rate": 4.29071538467279e-05,
      "loss": 0.6433,
      "step": 22260
    },
    {
      "epoch": 1.4059343434343434,
      "grad_norm": 0.4956451654434204,
      "learning_rate": 4.282324842612384e-05,
      "loss": 0.5974,
      "step": 22270
    },
    {
      "epoch": 1.4065656565656566,
      "grad_norm": 0.44955721497535706,
      "learning_rate": 4.273940276713549e-05,
      "loss": 0.5203,
      "step": 22280
    },
    {
      "epoch": 1.4071969696969697,
      "grad_norm": 0.7792690396308899,
      "learning_rate": 4.265561695739904e-05,
      "loss": 0.5367,
      "step": 22290
    },
    {
      "epoch": 1.4078282828282829,
      "grad_norm": 0.39479243755340576,
      "learning_rate": 4.257189108448806e-05,
      "loss": 0.7863,
      "step": 22300
    },
    {
      "epoch": 1.408459595959596,
      "grad_norm": 0.3995639979839325,
      "learning_rate": 4.24882252359136e-05,
      "loss": 0.6561,
      "step": 22310
    },
    {
      "epoch": 1.4090909090909092,
      "grad_norm": 0.47146183252334595,
      "learning_rate": 4.240461949912388e-05,
      "loss": 0.569,
      "step": 22320
    },
    {
      "epoch": 1.4097222222222223,
      "grad_norm": 0.493060827255249,
      "learning_rate": 4.2321073961504297e-05,
      "loss": 0.5091,
      "step": 22330
    },
    {
      "epoch": 1.4103535353535355,
      "grad_norm": 0.8513369560241699,
      "learning_rate": 4.223758871037734e-05,
      "loss": 0.5198,
      "step": 22340
    },
    {
      "epoch": 1.4109848484848486,
      "grad_norm": 0.42660024762153625,
      "learning_rate": 4.215416383300247e-05,
      "loss": 0.7473,
      "step": 22350
    },
    {
      "epoch": 1.4116161616161615,
      "grad_norm": 0.42835184931755066,
      "learning_rate": 4.2070799416576104e-05,
      "loss": 0.6625,
      "step": 22360
    },
    {
      "epoch": 1.4122474747474747,
      "grad_norm": 0.46268483996391296,
      "learning_rate": 4.198749554823141e-05,
      "loss": 0.566,
      "step": 22370
    },
    {
      "epoch": 1.4128787878787878,
      "grad_norm": 0.49154290556907654,
      "learning_rate": 4.190425231503827e-05,
      "loss": 0.493,
      "step": 22380
    },
    {
      "epoch": 1.413510101010101,
      "grad_norm": 0.819926381111145,
      "learning_rate": 4.1821069804003186e-05,
      "loss": 0.5355,
      "step": 22390
    },
    {
      "epoch": 1.4141414141414141,
      "grad_norm": 0.4036133885383606,
      "learning_rate": 4.1737948102069256e-05,
      "loss": 0.7925,
      "step": 22400
    },
    {
      "epoch": 1.4147727272727273,
      "grad_norm": 0.4335852563381195,
      "learning_rate": 4.165488729611596e-05,
      "loss": 0.6633,
      "step": 22410
    },
    {
      "epoch": 1.4154040404040404,
      "grad_norm": 0.44690579175949097,
      "learning_rate": 4.157188747295913e-05,
      "loss": 0.5648,
      "step": 22420
    },
    {
      "epoch": 1.4160353535353536,
      "grad_norm": 0.5024848580360413,
      "learning_rate": 4.148894871935086e-05,
      "loss": 0.5243,
      "step": 22430
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.6508081555366516,
      "learning_rate": 4.140607112197941e-05,
      "loss": 0.4966,
      "step": 22440
    },
    {
      "epoch": 1.4172979797979797,
      "grad_norm": 0.3949246108531952,
      "learning_rate": 4.132325476746919e-05,
      "loss": 0.7643,
      "step": 22450
    },
    {
      "epoch": 1.4179292929292928,
      "grad_norm": 0.45935752987861633,
      "learning_rate": 4.1240499742380526e-05,
      "loss": 0.6773,
      "step": 22460
    },
    {
      "epoch": 1.418560606060606,
      "grad_norm": 0.42558297514915466,
      "learning_rate": 4.115780613320962e-05,
      "loss": 0.5613,
      "step": 22470
    },
    {
      "epoch": 1.4191919191919191,
      "grad_norm": 0.46716564893722534,
      "learning_rate": 4.107517402638853e-05,
      "loss": 0.4956,
      "step": 22480
    },
    {
      "epoch": 1.4198232323232323,
      "grad_norm": 0.6723323464393616,
      "learning_rate": 4.099260350828506e-05,
      "loss": 0.5311,
      "step": 22490
    },
    {
      "epoch": 1.4204545454545454,
      "grad_norm": 0.41297030448913574,
      "learning_rate": 4.091009466520257e-05,
      "loss": 0.7935,
      "step": 22500
    },
    {
      "epoch": 1.4210858585858586,
      "grad_norm": 0.4734841585159302,
      "learning_rate": 4.0827647583379994e-05,
      "loss": 0.6749,
      "step": 22510
    },
    {
      "epoch": 1.4217171717171717,
      "grad_norm": 0.45005002617836,
      "learning_rate": 4.074526234899166e-05,
      "loss": 0.5813,
      "step": 22520
    },
    {
      "epoch": 1.4223484848484849,
      "grad_norm": 0.4782038629055023,
      "learning_rate": 4.06629390481474e-05,
      "loss": 0.5019,
      "step": 22530
    },
    {
      "epoch": 1.422979797979798,
      "grad_norm": 0.7673088312149048,
      "learning_rate": 4.058067776689214e-05,
      "loss": 0.5148,
      "step": 22540
    },
    {
      "epoch": 1.4236111111111112,
      "grad_norm": 0.39693599939346313,
      "learning_rate": 4.049847859120608e-05,
      "loss": 0.7842,
      "step": 22550
    },
    {
      "epoch": 1.4242424242424243,
      "grad_norm": 0.41298308968544006,
      "learning_rate": 4.041634160700447e-05,
      "loss": 0.6336,
      "step": 22560
    },
    {
      "epoch": 1.4248737373737375,
      "grad_norm": 0.42608875036239624,
      "learning_rate": 4.033426690013754e-05,
      "loss": 0.5902,
      "step": 22570
    },
    {
      "epoch": 1.4255050505050506,
      "grad_norm": 0.4639263451099396,
      "learning_rate": 4.0252254556390513e-05,
      "loss": 0.5256,
      "step": 22580
    },
    {
      "epoch": 1.4261363636363638,
      "grad_norm": 0.7670562267303467,
      "learning_rate": 4.017030466148335e-05,
      "loss": 0.5139,
      "step": 22590
    },
    {
      "epoch": 1.4267676767676767,
      "grad_norm": 0.40447917580604553,
      "learning_rate": 4.008841730107076e-05,
      "loss": 0.7767,
      "step": 22600
    },
    {
      "epoch": 1.4273989898989898,
      "grad_norm": 0.42367395758628845,
      "learning_rate": 4.0006592560742054e-05,
      "loss": 0.6768,
      "step": 22610
    },
    {
      "epoch": 1.428030303030303,
      "grad_norm": 0.45545679330825806,
      "learning_rate": 3.992483052602119e-05,
      "loss": 0.5848,
      "step": 22620
    },
    {
      "epoch": 1.4286616161616161,
      "grad_norm": 0.4827161431312561,
      "learning_rate": 3.9843131282366506e-05,
      "loss": 0.488,
      "step": 22630
    },
    {
      "epoch": 1.4292929292929293,
      "grad_norm": 0.7595845460891724,
      "learning_rate": 3.976149491517073e-05,
      "loss": 0.5418,
      "step": 22640
    },
    {
      "epoch": 1.4299242424242424,
      "grad_norm": 0.40662330389022827,
      "learning_rate": 3.967992150976083e-05,
      "loss": 0.7598,
      "step": 22650
    },
    {
      "epoch": 1.4305555555555556,
      "grad_norm": 0.4118725657463074,
      "learning_rate": 3.959841115139807e-05,
      "loss": 0.6752,
      "step": 22660
    },
    {
      "epoch": 1.4311868686868687,
      "grad_norm": 0.4832860827445984,
      "learning_rate": 3.951696392527774e-05,
      "loss": 0.5742,
      "step": 22670
    },
    {
      "epoch": 1.4318181818181819,
      "grad_norm": 0.4419824779033661,
      "learning_rate": 3.943557991652914e-05,
      "loss": 0.4865,
      "step": 22680
    },
    {
      "epoch": 1.432449494949495,
      "grad_norm": 0.7465087175369263,
      "learning_rate": 3.935425921021552e-05,
      "loss": 0.5287,
      "step": 22690
    },
    {
      "epoch": 1.433080808080808,
      "grad_norm": 0.379428893327713,
      "learning_rate": 3.9273001891333906e-05,
      "loss": 0.7692,
      "step": 22700
    },
    {
      "epoch": 1.433712121212121,
      "grad_norm": 0.4536398947238922,
      "learning_rate": 3.9191808044815214e-05,
      "loss": 0.676,
      "step": 22710
    },
    {
      "epoch": 1.4343434343434343,
      "grad_norm": 0.4110669195652008,
      "learning_rate": 3.9110677755523894e-05,
      "loss": 0.553,
      "step": 22720
    },
    {
      "epoch": 1.4349747474747474,
      "grad_norm": 0.495761513710022,
      "learning_rate": 3.902961110825798e-05,
      "loss": 0.4968,
      "step": 22730
    },
    {
      "epoch": 1.4356060606060606,
      "grad_norm": 0.7141978144645691,
      "learning_rate": 3.894860818774897e-05,
      "loss": 0.5235,
      "step": 22740
    },
    {
      "epoch": 1.4362373737373737,
      "grad_norm": 0.39791199564933777,
      "learning_rate": 3.886766907866187e-05,
      "loss": 0.8255,
      "step": 22750
    },
    {
      "epoch": 1.4368686868686869,
      "grad_norm": 0.43719276785850525,
      "learning_rate": 3.878679386559488e-05,
      "loss": 0.6464,
      "step": 22760
    },
    {
      "epoch": 1.4375,
      "grad_norm": 0.4442656338214874,
      "learning_rate": 3.870598263307944e-05,
      "loss": 0.5915,
      "step": 22770
    },
    {
      "epoch": 1.4381313131313131,
      "grad_norm": 0.44960829615592957,
      "learning_rate": 3.862523546558011e-05,
      "loss": 0.4911,
      "step": 22780
    },
    {
      "epoch": 1.4387626262626263,
      "grad_norm": 0.7317538261413574,
      "learning_rate": 3.854455244749448e-05,
      "loss": 0.4905,
      "step": 22790
    },
    {
      "epoch": 1.4393939393939394,
      "grad_norm": 0.40403130650520325,
      "learning_rate": 3.846393366315316e-05,
      "loss": 0.8025,
      "step": 22800
    },
    {
      "epoch": 1.4400252525252526,
      "grad_norm": 0.43990635871887207,
      "learning_rate": 3.8383379196819556e-05,
      "loss": 0.6283,
      "step": 22810
    },
    {
      "epoch": 1.4406565656565657,
      "grad_norm": 0.4141784608364105,
      "learning_rate": 3.8302889132689865e-05,
      "loss": 0.5183,
      "step": 22820
    },
    {
      "epoch": 1.441287878787879,
      "grad_norm": 0.49068474769592285,
      "learning_rate": 3.822246355489294e-05,
      "loss": 0.4816,
      "step": 22830
    },
    {
      "epoch": 1.441919191919192,
      "grad_norm": 0.7788237929344177,
      "learning_rate": 3.8142102547490324e-05,
      "loss": 0.5048,
      "step": 22840
    },
    {
      "epoch": 1.442550505050505,
      "grad_norm": 0.41124919056892395,
      "learning_rate": 3.806180619447596e-05,
      "loss": 0.7769,
      "step": 22850
    },
    {
      "epoch": 1.4431818181818181,
      "grad_norm": 0.46569955348968506,
      "learning_rate": 3.798157457977629e-05,
      "loss": 0.6172,
      "step": 22860
    },
    {
      "epoch": 1.4438131313131313,
      "grad_norm": 0.43139559030532837,
      "learning_rate": 3.790140778725002e-05,
      "loss": 0.5438,
      "step": 22870
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.5034111738204956,
      "learning_rate": 3.7821305900688205e-05,
      "loss": 0.4973,
      "step": 22880
    },
    {
      "epoch": 1.4450757575757576,
      "grad_norm": 0.836421549320221,
      "learning_rate": 3.7741269003813985e-05,
      "loss": 0.5521,
      "step": 22890
    },
    {
      "epoch": 1.4457070707070707,
      "grad_norm": 0.42292627692222595,
      "learning_rate": 3.7661297180282584e-05,
      "loss": 0.7926,
      "step": 22900
    },
    {
      "epoch": 1.4463383838383839,
      "grad_norm": 0.4230225086212158,
      "learning_rate": 3.7581390513681205e-05,
      "loss": 0.6673,
      "step": 22910
    },
    {
      "epoch": 1.446969696969697,
      "grad_norm": 0.4895731210708618,
      "learning_rate": 3.750154908752894e-05,
      "loss": 0.5543,
      "step": 22920
    },
    {
      "epoch": 1.4476010101010102,
      "grad_norm": 0.48486456274986267,
      "learning_rate": 3.742177298527678e-05,
      "loss": 0.4962,
      "step": 22930
    },
    {
      "epoch": 1.4482323232323233,
      "grad_norm": 0.752871036529541,
      "learning_rate": 3.734206229030732e-05,
      "loss": 0.5157,
      "step": 22940
    },
    {
      "epoch": 1.4488636363636362,
      "grad_norm": 0.42715927958488464,
      "learning_rate": 3.726241708593485e-05,
      "loss": 0.7814,
      "step": 22950
    },
    {
      "epoch": 1.4494949494949494,
      "grad_norm": 0.43450093269348145,
      "learning_rate": 3.718283745540517e-05,
      "loss": 0.6324,
      "step": 22960
    },
    {
      "epoch": 1.4501262626262625,
      "grad_norm": 0.5033349990844727,
      "learning_rate": 3.710332348189565e-05,
      "loss": 0.5573,
      "step": 22970
    },
    {
      "epoch": 1.4507575757575757,
      "grad_norm": 0.4939670264720917,
      "learning_rate": 3.702387524851488e-05,
      "loss": 0.4891,
      "step": 22980
    },
    {
      "epoch": 1.4513888888888888,
      "grad_norm": 0.7505592107772827,
      "learning_rate": 3.6944492838302846e-05,
      "loss": 0.5037,
      "step": 22990
    },
    {
      "epoch": 1.452020202020202,
      "grad_norm": 0.3698158860206604,
      "learning_rate": 3.6865176334230665e-05,
      "loss": 0.7911,
      "step": 23000
    },
    {
      "epoch": 1.452020202020202,
      "eval_loss": 0.6372643709182739,
      "eval_runtime": 36.2096,
      "eval_samples_per_second": 70.699,
      "eval_steps_per_second": 8.837,
      "step": 23000
    },
    {
      "epoch": 1.4526515151515151,
      "grad_norm": 0.4719233214855194,
      "learning_rate": 3.6785925819200676e-05,
      "loss": 0.6494,
      "step": 23010
    },
    {
      "epoch": 1.4532828282828283,
      "grad_norm": 0.4295694828033447,
      "learning_rate": 3.6706741376046136e-05,
      "loss": 0.582,
      "step": 23020
    },
    {
      "epoch": 1.4539141414141414,
      "grad_norm": 0.4763098657131195,
      "learning_rate": 3.6627623087531285e-05,
      "loss": 0.4964,
      "step": 23030
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.6698194742202759,
      "learning_rate": 3.6548571036351197e-05,
      "loss": 0.5324,
      "step": 23040
    },
    {
      "epoch": 1.4551767676767677,
      "grad_norm": 0.38994261622428894,
      "learning_rate": 3.6469585305131726e-05,
      "loss": 0.8159,
      "step": 23050
    },
    {
      "epoch": 1.4558080808080809,
      "grad_norm": 0.440846711397171,
      "learning_rate": 3.639066597642948e-05,
      "loss": 0.6327,
      "step": 23060
    },
    {
      "epoch": 1.456439393939394,
      "grad_norm": 0.4211067259311676,
      "learning_rate": 3.631181313273154e-05,
      "loss": 0.5715,
      "step": 23070
    },
    {
      "epoch": 1.4570707070707072,
      "grad_norm": 0.5082303285598755,
      "learning_rate": 3.6233026856455573e-05,
      "loss": 0.4876,
      "step": 23080
    },
    {
      "epoch": 1.4577020202020203,
      "grad_norm": 0.7389759421348572,
      "learning_rate": 3.6154307229949624e-05,
      "loss": 0.5407,
      "step": 23090
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 0.4189814627170563,
      "learning_rate": 3.607565433549214e-05,
      "loss": 0.7996,
      "step": 23100
    },
    {
      "epoch": 1.4589646464646464,
      "grad_norm": 0.40986037254333496,
      "learning_rate": 3.599706825529177e-05,
      "loss": 0.6213,
      "step": 23110
    },
    {
      "epoch": 1.4595959595959596,
      "grad_norm": 0.4928225874900818,
      "learning_rate": 3.5918549071487317e-05,
      "loss": 0.5804,
      "step": 23120
    },
    {
      "epoch": 1.4602272727272727,
      "grad_norm": 0.4709970951080322,
      "learning_rate": 3.58400968661477e-05,
      "loss": 0.5071,
      "step": 23130
    },
    {
      "epoch": 1.4608585858585859,
      "grad_norm": 0.6840043663978577,
      "learning_rate": 3.5761711721271774e-05,
      "loss": 0.5303,
      "step": 23140
    },
    {
      "epoch": 1.461489898989899,
      "grad_norm": 0.428265780210495,
      "learning_rate": 3.5683393718788424e-05,
      "loss": 0.7988,
      "step": 23150
    },
    {
      "epoch": 1.4621212121212122,
      "grad_norm": 0.42540809512138367,
      "learning_rate": 3.5605142940556224e-05,
      "loss": 0.6579,
      "step": 23160
    },
    {
      "epoch": 1.4627525252525253,
      "grad_norm": 0.44766491651535034,
      "learning_rate": 3.552695946836355e-05,
      "loss": 0.5605,
      "step": 23170
    },
    {
      "epoch": 1.4633838383838385,
      "grad_norm": 0.4369848072528839,
      "learning_rate": 3.5448843383928386e-05,
      "loss": 0.4698,
      "step": 23180
    },
    {
      "epoch": 1.4640151515151514,
      "grad_norm": 0.7508913278579712,
      "learning_rate": 3.537079476889837e-05,
      "loss": 0.5127,
      "step": 23190
    },
    {
      "epoch": 1.4646464646464645,
      "grad_norm": 0.4005962610244751,
      "learning_rate": 3.5292813704850536e-05,
      "loss": 0.8163,
      "step": 23200
    },
    {
      "epoch": 1.4652777777777777,
      "grad_norm": 0.41507986187934875,
      "learning_rate": 3.521490027329133e-05,
      "loss": 0.6789,
      "step": 23210
    },
    {
      "epoch": 1.4659090909090908,
      "grad_norm": 0.4664764404296875,
      "learning_rate": 3.513705455565649e-05,
      "loss": 0.5411,
      "step": 23220
    },
    {
      "epoch": 1.466540404040404,
      "grad_norm": 0.4943993389606476,
      "learning_rate": 3.505927663331108e-05,
      "loss": 0.4902,
      "step": 23230
    },
    {
      "epoch": 1.4671717171717171,
      "grad_norm": 0.6247360110282898,
      "learning_rate": 3.49815665875492e-05,
      "loss": 0.4852,
      "step": 23240
    },
    {
      "epoch": 1.4678030303030303,
      "grad_norm": 0.42451924085617065,
      "learning_rate": 3.490392449959401e-05,
      "loss": 0.8211,
      "step": 23250
    },
    {
      "epoch": 1.4684343434343434,
      "grad_norm": 0.4296819865703583,
      "learning_rate": 3.482635045059769e-05,
      "loss": 0.6419,
      "step": 23260
    },
    {
      "epoch": 1.4690656565656566,
      "grad_norm": 0.4320942759513855,
      "learning_rate": 3.4748844521641245e-05,
      "loss": 0.596,
      "step": 23270
    },
    {
      "epoch": 1.4696969696969697,
      "grad_norm": 0.4692571461200714,
      "learning_rate": 3.467140679373456e-05,
      "loss": 0.5341,
      "step": 23280
    },
    {
      "epoch": 1.4703282828282829,
      "grad_norm": 0.7566947340965271,
      "learning_rate": 3.45940373478162e-05,
      "loss": 0.5194,
      "step": 23290
    },
    {
      "epoch": 1.470959595959596,
      "grad_norm": 0.4295487701892853,
      "learning_rate": 3.4516736264753313e-05,
      "loss": 0.793,
      "step": 23300
    },
    {
      "epoch": 1.4715909090909092,
      "grad_norm": 0.4141160249710083,
      "learning_rate": 3.4439503625341605e-05,
      "loss": 0.662,
      "step": 23310
    },
    {
      "epoch": 1.4722222222222223,
      "grad_norm": 0.38997137546539307,
      "learning_rate": 3.436233951030536e-05,
      "loss": 0.5556,
      "step": 23320
    },
    {
      "epoch": 1.4728535353535355,
      "grad_norm": 0.5118337869644165,
      "learning_rate": 3.4285244000297114e-05,
      "loss": 0.4956,
      "step": 23330
    },
    {
      "epoch": 1.4734848484848486,
      "grad_norm": 0.7020584344863892,
      "learning_rate": 3.420821717589773e-05,
      "loss": 0.4964,
      "step": 23340
    },
    {
      "epoch": 1.4741161616161615,
      "grad_norm": 0.4241845905780792,
      "learning_rate": 3.4131259117616235e-05,
      "loss": 0.7762,
      "step": 23350
    },
    {
      "epoch": 1.4747474747474747,
      "grad_norm": 0.4320143759250641,
      "learning_rate": 3.405436990588992e-05,
      "loss": 0.652,
      "step": 23360
    },
    {
      "epoch": 1.4753787878787878,
      "grad_norm": 0.4364858865737915,
      "learning_rate": 3.397754962108398e-05,
      "loss": 0.544,
      "step": 23370
    },
    {
      "epoch": 1.476010101010101,
      "grad_norm": 0.4523293077945709,
      "learning_rate": 3.390079834349163e-05,
      "loss": 0.4946,
      "step": 23380
    },
    {
      "epoch": 1.4766414141414141,
      "grad_norm": 0.6488606333732605,
      "learning_rate": 3.382411615333392e-05,
      "loss": 0.5123,
      "step": 23390
    },
    {
      "epoch": 1.4772727272727273,
      "grad_norm": 0.4068598449230194,
      "learning_rate": 3.374750313075969e-05,
      "loss": 0.7574,
      "step": 23400
    },
    {
      "epoch": 1.4779040404040404,
      "grad_norm": 0.43593829870224,
      "learning_rate": 3.36709593558456e-05,
      "loss": 0.6523,
      "step": 23410
    },
    {
      "epoch": 1.4785353535353536,
      "grad_norm": 0.4389702379703522,
      "learning_rate": 3.359448490859577e-05,
      "loss": 0.5485,
      "step": 23420
    },
    {
      "epoch": 1.4791666666666667,
      "grad_norm": 0.47356748580932617,
      "learning_rate": 3.351807986894193e-05,
      "loss": 0.4946,
      "step": 23430
    },
    {
      "epoch": 1.4797979797979797,
      "grad_norm": 0.7037277817726135,
      "learning_rate": 3.3441744316743264e-05,
      "loss": 0.5106,
      "step": 23440
    },
    {
      "epoch": 1.4804292929292928,
      "grad_norm": 0.4093282222747803,
      "learning_rate": 3.336547833178635e-05,
      "loss": 0.7636,
      "step": 23450
    },
    {
      "epoch": 1.481060606060606,
      "grad_norm": 0.4018779695034027,
      "learning_rate": 3.328928199378502e-05,
      "loss": 0.6544,
      "step": 23460
    },
    {
      "epoch": 1.4816919191919191,
      "grad_norm": 0.42554786801338196,
      "learning_rate": 3.3213155382380304e-05,
      "loss": 0.5528,
      "step": 23470
    },
    {
      "epoch": 1.4823232323232323,
      "grad_norm": 0.5049254894256592,
      "learning_rate": 3.3137098577140345e-05,
      "loss": 0.514,
      "step": 23480
    },
    {
      "epoch": 1.4829545454545454,
      "grad_norm": 0.7600453495979309,
      "learning_rate": 3.3061111657560416e-05,
      "loss": 0.4951,
      "step": 23490
    },
    {
      "epoch": 1.4835858585858586,
      "grad_norm": 0.41147786378860474,
      "learning_rate": 3.2985194703062636e-05,
      "loss": 0.8443,
      "step": 23500
    },
    {
      "epoch": 1.4842171717171717,
      "grad_norm": 0.4432637691497803,
      "learning_rate": 3.290934779299605e-05,
      "loss": 0.6711,
      "step": 23510
    },
    {
      "epoch": 1.4848484848484849,
      "grad_norm": 0.45762959122657776,
      "learning_rate": 3.283357100663647e-05,
      "loss": 0.5675,
      "step": 23520
    },
    {
      "epoch": 1.485479797979798,
      "grad_norm": 0.5125603079795837,
      "learning_rate": 3.2757864423186376e-05,
      "loss": 0.5224,
      "step": 23530
    },
    {
      "epoch": 1.4861111111111112,
      "grad_norm": 0.7835707664489746,
      "learning_rate": 3.2682228121775006e-05,
      "loss": 0.5242,
      "step": 23540
    },
    {
      "epoch": 1.4867424242424243,
      "grad_norm": 0.36460843682289124,
      "learning_rate": 3.2606662181458e-05,
      "loss": 0.8156,
      "step": 23550
    },
    {
      "epoch": 1.4873737373737375,
      "grad_norm": 0.3952728807926178,
      "learning_rate": 3.253116668121752e-05,
      "loss": 0.6599,
      "step": 23560
    },
    {
      "epoch": 1.4880050505050506,
      "grad_norm": 0.48948934674263,
      "learning_rate": 3.245574169996205e-05,
      "loss": 0.5622,
      "step": 23570
    },
    {
      "epoch": 1.4886363636363638,
      "grad_norm": 0.5336061120033264,
      "learning_rate": 3.2380387316526485e-05,
      "loss": 0.5008,
      "step": 23580
    },
    {
      "epoch": 1.4892676767676767,
      "grad_norm": 0.6535754799842834,
      "learning_rate": 3.2305103609671796e-05,
      "loss": 0.5308,
      "step": 23590
    },
    {
      "epoch": 1.4898989898989898,
      "grad_norm": 0.3796122968196869,
      "learning_rate": 3.222989065808516e-05,
      "loss": 0.8188,
      "step": 23600
    },
    {
      "epoch": 1.490530303030303,
      "grad_norm": 0.42613422870635986,
      "learning_rate": 3.215474854037976e-05,
      "loss": 0.628,
      "step": 23610
    },
    {
      "epoch": 1.4911616161616161,
      "grad_norm": 0.45387670397758484,
      "learning_rate": 3.2079677335094746e-05,
      "loss": 0.5959,
      "step": 23620
    },
    {
      "epoch": 1.4917929292929293,
      "grad_norm": 0.47958728671073914,
      "learning_rate": 3.200467712069523e-05,
      "loss": 0.4923,
      "step": 23630
    },
    {
      "epoch": 1.4924242424242424,
      "grad_norm": 0.7237853407859802,
      "learning_rate": 3.192974797557201e-05,
      "loss": 0.5252,
      "step": 23640
    },
    {
      "epoch": 1.4930555555555556,
      "grad_norm": 0.43181633949279785,
      "learning_rate": 3.185488997804167e-05,
      "loss": 0.7666,
      "step": 23650
    },
    {
      "epoch": 1.4936868686868687,
      "grad_norm": 0.466714084148407,
      "learning_rate": 3.1780103206346366e-05,
      "loss": 0.6059,
      "step": 23660
    },
    {
      "epoch": 1.4943181818181819,
      "grad_norm": 0.520751953125,
      "learning_rate": 3.1705387738653924e-05,
      "loss": 0.5867,
      "step": 23670
    },
    {
      "epoch": 1.494949494949495,
      "grad_norm": 0.4549154043197632,
      "learning_rate": 3.1630743653057536e-05,
      "loss": 0.4885,
      "step": 23680
    },
    {
      "epoch": 1.495580808080808,
      "grad_norm": 0.6659927368164062,
      "learning_rate": 3.155617102757583e-05,
      "loss": 0.5356,
      "step": 23690
    },
    {
      "epoch": 1.496212121212121,
      "grad_norm": 0.41322848200798035,
      "learning_rate": 3.148166994015268e-05,
      "loss": 0.7247,
      "step": 23700
    },
    {
      "epoch": 1.4968434343434343,
      "grad_norm": 0.42892971634864807,
      "learning_rate": 3.140724046865733e-05,
      "loss": 0.6369,
      "step": 23710
    },
    {
      "epoch": 1.4974747474747474,
      "grad_norm": 0.4409247636795044,
      "learning_rate": 3.1332882690884026e-05,
      "loss": 0.596,
      "step": 23720
    },
    {
      "epoch": 1.4981060606060606,
      "grad_norm": 0.41632604598999023,
      "learning_rate": 3.1258596684552155e-05,
      "loss": 0.5041,
      "step": 23730
    },
    {
      "epoch": 1.4987373737373737,
      "grad_norm": 0.8210018873214722,
      "learning_rate": 3.118438252730607e-05,
      "loss": 0.553,
      "step": 23740
    },
    {
      "epoch": 1.4993686868686869,
      "grad_norm": 0.43225544691085815,
      "learning_rate": 3.1110240296714966e-05,
      "loss": 0.8106,
      "step": 23750
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.44314804673194885,
      "learning_rate": 3.103617007027301e-05,
      "loss": 0.6599,
      "step": 23760
    },
    {
      "epoch": 1.5006313131313131,
      "grad_norm": 0.4413216710090637,
      "learning_rate": 3.0962171925398966e-05,
      "loss": 0.5455,
      "step": 23770
    },
    {
      "epoch": 1.5012626262626263,
      "grad_norm": 0.4875830113887787,
      "learning_rate": 3.0888245939436324e-05,
      "loss": 0.4645,
      "step": 23780
    },
    {
      "epoch": 1.5018939393939394,
      "grad_norm": 0.7757530808448792,
      "learning_rate": 3.0814392189653095e-05,
      "loss": 0.5158,
      "step": 23790
    },
    {
      "epoch": 1.5025252525252526,
      "grad_norm": 0.40642860531806946,
      "learning_rate": 3.074061075324191e-05,
      "loss": 0.7201,
      "step": 23800
    },
    {
      "epoch": 1.5031565656565657,
      "grad_norm": 0.4490605592727661,
      "learning_rate": 3.0666901707319706e-05,
      "loss": 0.6367,
      "step": 23810
    },
    {
      "epoch": 1.503787878787879,
      "grad_norm": 0.4818282723426819,
      "learning_rate": 3.0593265128927774e-05,
      "loss": 0.5708,
      "step": 23820
    },
    {
      "epoch": 1.504419191919192,
      "grad_norm": 0.501018226146698,
      "learning_rate": 3.0519701095031663e-05,
      "loss": 0.5165,
      "step": 23830
    },
    {
      "epoch": 1.5050505050505052,
      "grad_norm": 0.7978143692016602,
      "learning_rate": 3.0446209682521152e-05,
      "loss": 0.5499,
      "step": 23840
    },
    {
      "epoch": 1.5056818181818183,
      "grad_norm": 0.4289892315864563,
      "learning_rate": 3.0372790968210075e-05,
      "loss": 0.8202,
      "step": 23850
    },
    {
      "epoch": 1.5063131313131313,
      "grad_norm": 0.45021378993988037,
      "learning_rate": 3.029944502883626e-05,
      "loss": 0.6483,
      "step": 23860
    },
    {
      "epoch": 1.5069444444444444,
      "grad_norm": 0.4422750473022461,
      "learning_rate": 3.0226171941061508e-05,
      "loss": 0.5582,
      "step": 23870
    },
    {
      "epoch": 1.5075757575757576,
      "grad_norm": 0.4861195385456085,
      "learning_rate": 3.0152971781471416e-05,
      "loss": 0.5012,
      "step": 23880
    },
    {
      "epoch": 1.5082070707070707,
      "grad_norm": 0.8717333078384399,
      "learning_rate": 3.007984462657546e-05,
      "loss": 0.5368,
      "step": 23890
    },
    {
      "epoch": 1.5088383838383839,
      "grad_norm": 0.386904239654541,
      "learning_rate": 3.000679055280673e-05,
      "loss": 0.7872,
      "step": 23900
    },
    {
      "epoch": 1.509469696969697,
      "grad_norm": 0.4283917248249054,
      "learning_rate": 2.9933809636521935e-05,
      "loss": 0.6481,
      "step": 23910
    },
    {
      "epoch": 1.51010101010101,
      "grad_norm": 0.4685085415840149,
      "learning_rate": 2.986090195400132e-05,
      "loss": 0.5626,
      "step": 23920
    },
    {
      "epoch": 1.510732323232323,
      "grad_norm": 0.5020893812179565,
      "learning_rate": 2.9788067581448653e-05,
      "loss": 0.4939,
      "step": 23930
    },
    {
      "epoch": 1.5113636363636362,
      "grad_norm": 0.6642596125602722,
      "learning_rate": 2.9715306594991e-05,
      "loss": 0.4873,
      "step": 23940
    },
    {
      "epoch": 1.5119949494949494,
      "grad_norm": 0.416656494140625,
      "learning_rate": 2.9642619070678735e-05,
      "loss": 0.7521,
      "step": 23950
    },
    {
      "epoch": 1.5126262626262625,
      "grad_norm": 0.43392202258110046,
      "learning_rate": 2.9570005084485485e-05,
      "loss": 0.6414,
      "step": 23960
    },
    {
      "epoch": 1.5132575757575757,
      "grad_norm": 0.4521009027957916,
      "learning_rate": 2.9497464712307944e-05,
      "loss": 0.5779,
      "step": 23970
    },
    {
      "epoch": 1.5138888888888888,
      "grad_norm": 0.4976590573787689,
      "learning_rate": 2.9424998029965967e-05,
      "loss": 0.5171,
      "step": 23980
    },
    {
      "epoch": 1.514520202020202,
      "grad_norm": 0.728065013885498,
      "learning_rate": 2.9352605113202326e-05,
      "loss": 0.5232,
      "step": 23990
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 0.42466285824775696,
      "learning_rate": 2.928028603768268e-05,
      "loss": 0.8174,
      "step": 24000
    },
    {
      "epoch": 1.5151515151515151,
      "eval_loss": 0.6372917890548706,
      "eval_runtime": 35.3602,
      "eval_samples_per_second": 72.398,
      "eval_steps_per_second": 9.05,
      "step": 24000
    },
    {
      "epoch": 1.5157828282828283,
      "grad_norm": 0.4330010414123535,
      "learning_rate": 2.920804087899549e-05,
      "loss": 0.6649,
      "step": 24010
    },
    {
      "epoch": 1.5164141414141414,
      "grad_norm": 0.44617873430252075,
      "learning_rate": 2.9135869712652087e-05,
      "loss": 0.5669,
      "step": 24020
    },
    {
      "epoch": 1.5170454545454546,
      "grad_norm": 0.4560008645057678,
      "learning_rate": 2.9063772614086315e-05,
      "loss": 0.4801,
      "step": 24030
    },
    {
      "epoch": 1.5176767676767677,
      "grad_norm": 0.7054782509803772,
      "learning_rate": 2.8991749658654676e-05,
      "loss": 0.5379,
      "step": 24040
    },
    {
      "epoch": 1.5183080808080809,
      "grad_norm": 0.4064374566078186,
      "learning_rate": 2.891980092163612e-05,
      "loss": 0.7638,
      "step": 24050
    },
    {
      "epoch": 1.518939393939394,
      "grad_norm": 0.4402170181274414,
      "learning_rate": 2.8847926478232122e-05,
      "loss": 0.624,
      "step": 24060
    },
    {
      "epoch": 1.5195707070707072,
      "grad_norm": 0.46643123030662537,
      "learning_rate": 2.877612640356644e-05,
      "loss": 0.5602,
      "step": 24070
    },
    {
      "epoch": 1.5202020202020203,
      "grad_norm": 0.47278285026550293,
      "learning_rate": 2.870440077268509e-05,
      "loss": 0.5191,
      "step": 24080
    },
    {
      "epoch": 1.5208333333333335,
      "grad_norm": 0.7508648037910461,
      "learning_rate": 2.86327496605563e-05,
      "loss": 0.5384,
      "step": 24090
    },
    {
      "epoch": 1.5214646464646466,
      "grad_norm": 0.43021807074546814,
      "learning_rate": 2.8561173142070373e-05,
      "loss": 0.8093,
      "step": 24100
    },
    {
      "epoch": 1.5220959595959596,
      "grad_norm": 0.42284610867500305,
      "learning_rate": 2.8489671292039755e-05,
      "loss": 0.6492,
      "step": 24110
    },
    {
      "epoch": 1.5227272727272727,
      "grad_norm": 0.4167053997516632,
      "learning_rate": 2.8418244185198728e-05,
      "loss": 0.5625,
      "step": 24120
    },
    {
      "epoch": 1.5233585858585859,
      "grad_norm": 0.4597463309764862,
      "learning_rate": 2.8346891896203508e-05,
      "loss": 0.5139,
      "step": 24130
    },
    {
      "epoch": 1.523989898989899,
      "grad_norm": 0.71907639503479,
      "learning_rate": 2.8275614499632063e-05,
      "loss": 0.5161,
      "step": 24140
    },
    {
      "epoch": 1.5246212121212122,
      "grad_norm": 0.39071592688560486,
      "learning_rate": 2.8204412069984186e-05,
      "loss": 0.7571,
      "step": 24150
    },
    {
      "epoch": 1.5252525252525253,
      "grad_norm": 0.46827825903892517,
      "learning_rate": 2.8133284681681216e-05,
      "loss": 0.6518,
      "step": 24160
    },
    {
      "epoch": 1.5258838383838382,
      "grad_norm": 0.43097376823425293,
      "learning_rate": 2.8062232409066093e-05,
      "loss": 0.5452,
      "step": 24170
    },
    {
      "epoch": 1.5265151515151514,
      "grad_norm": 0.5031785368919373,
      "learning_rate": 2.79912553264032e-05,
      "loss": 0.4877,
      "step": 24180
    },
    {
      "epoch": 1.5271464646464645,
      "grad_norm": 0.7390169501304626,
      "learning_rate": 2.792035350787845e-05,
      "loss": 0.5285,
      "step": 24190
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 0.40501269698143005,
      "learning_rate": 2.7849527027598975e-05,
      "loss": 0.7771,
      "step": 24200
    },
    {
      "epoch": 1.5284090909090908,
      "grad_norm": 0.448458194732666,
      "learning_rate": 2.7778775959593195e-05,
      "loss": 0.6354,
      "step": 24210
    },
    {
      "epoch": 1.529040404040404,
      "grad_norm": 0.4529464840888977,
      "learning_rate": 2.770810037781073e-05,
      "loss": 0.5566,
      "step": 24220
    },
    {
      "epoch": 1.5296717171717171,
      "grad_norm": 0.48422813415527344,
      "learning_rate": 2.7637500356122237e-05,
      "loss": 0.4681,
      "step": 24230
    },
    {
      "epoch": 1.5303030303030303,
      "grad_norm": 0.722750186920166,
      "learning_rate": 2.7566975968319507e-05,
      "loss": 0.5372,
      "step": 24240
    },
    {
      "epoch": 1.5309343434343434,
      "grad_norm": 0.437062531709671,
      "learning_rate": 2.7496527288115205e-05,
      "loss": 0.7987,
      "step": 24250
    },
    {
      "epoch": 1.5315656565656566,
      "grad_norm": 0.4230830669403076,
      "learning_rate": 2.7426154389142856e-05,
      "loss": 0.7116,
      "step": 24260
    },
    {
      "epoch": 1.5321969696969697,
      "grad_norm": 0.4458599388599396,
      "learning_rate": 2.7355857344956783e-05,
      "loss": 0.5701,
      "step": 24270
    },
    {
      "epoch": 1.5328282828282829,
      "grad_norm": 0.5488540530204773,
      "learning_rate": 2.7285636229032085e-05,
      "loss": 0.5172,
      "step": 24280
    },
    {
      "epoch": 1.533459595959596,
      "grad_norm": 0.7775558233261108,
      "learning_rate": 2.7215491114764446e-05,
      "loss": 0.5301,
      "step": 24290
    },
    {
      "epoch": 1.5340909090909092,
      "grad_norm": 0.3945433497428894,
      "learning_rate": 2.7145422075470096e-05,
      "loss": 0.7443,
      "step": 24300
    },
    {
      "epoch": 1.5347222222222223,
      "grad_norm": 0.44774767756462097,
      "learning_rate": 2.7075429184385802e-05,
      "loss": 0.659,
      "step": 24310
    },
    {
      "epoch": 1.5353535353535355,
      "grad_norm": 0.4602750241756439,
      "learning_rate": 2.7005512514668674e-05,
      "loss": 0.5674,
      "step": 24320
    },
    {
      "epoch": 1.5359848484848486,
      "grad_norm": 0.4810922145843506,
      "learning_rate": 2.693567213939625e-05,
      "loss": 0.514,
      "step": 24330
    },
    {
      "epoch": 1.5366161616161618,
      "grad_norm": 0.7207184433937073,
      "learning_rate": 2.6865908131566255e-05,
      "loss": 0.5098,
      "step": 24340
    },
    {
      "epoch": 1.5372474747474747,
      "grad_norm": 0.4257236123085022,
      "learning_rate": 2.6796220564096608e-05,
      "loss": 0.7992,
      "step": 24350
    },
    {
      "epoch": 1.5378787878787878,
      "grad_norm": 0.4469432532787323,
      "learning_rate": 2.6726609509825286e-05,
      "loss": 0.6598,
      "step": 24360
    },
    {
      "epoch": 1.538510101010101,
      "grad_norm": 0.4534340798854828,
      "learning_rate": 2.6657075041510416e-05,
      "loss": 0.5754,
      "step": 24370
    },
    {
      "epoch": 1.5391414141414141,
      "grad_norm": 0.44365349411964417,
      "learning_rate": 2.6587617231829966e-05,
      "loss": 0.5005,
      "step": 24380
    },
    {
      "epoch": 1.5397727272727273,
      "grad_norm": 0.829616129398346,
      "learning_rate": 2.6518236153381815e-05,
      "loss": 0.5208,
      "step": 24390
    },
    {
      "epoch": 1.5404040404040404,
      "grad_norm": 0.38317573070526123,
      "learning_rate": 2.6448931878683626e-05,
      "loss": 0.7793,
      "step": 24400
    },
    {
      "epoch": 1.5410353535353534,
      "grad_norm": 0.4247084856033325,
      "learning_rate": 2.637970448017284e-05,
      "loss": 0.6306,
      "step": 24410
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 0.4678822457790375,
      "learning_rate": 2.6310554030206502e-05,
      "loss": 0.5456,
      "step": 24420
    },
    {
      "epoch": 1.5422979797979797,
      "grad_norm": 0.4494796395301819,
      "learning_rate": 2.6241480601061218e-05,
      "loss": 0.4961,
      "step": 24430
    },
    {
      "epoch": 1.5429292929292928,
      "grad_norm": 0.7228438854217529,
      "learning_rate": 2.6172484264933115e-05,
      "loss": 0.5514,
      "step": 24440
    },
    {
      "epoch": 1.543560606060606,
      "grad_norm": 0.3969835340976715,
      "learning_rate": 2.6103565093937722e-05,
      "loss": 0.7843,
      "step": 24450
    },
    {
      "epoch": 1.5441919191919191,
      "grad_norm": 0.4291761815547943,
      "learning_rate": 2.6034723160109965e-05,
      "loss": 0.6609,
      "step": 24460
    },
    {
      "epoch": 1.5448232323232323,
      "grad_norm": 0.4363035261631012,
      "learning_rate": 2.596595853540399e-05,
      "loss": 0.5363,
      "step": 24470
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 0.49109965562820435,
      "learning_rate": 2.5897271291693147e-05,
      "loss": 0.5221,
      "step": 24480
    },
    {
      "epoch": 1.5460858585858586,
      "grad_norm": 0.7967458367347717,
      "learning_rate": 2.5828661500769902e-05,
      "loss": 0.5344,
      "step": 24490
    },
    {
      "epoch": 1.5467171717171717,
      "grad_norm": 0.38802361488342285,
      "learning_rate": 2.576012923434581e-05,
      "loss": 0.7611,
      "step": 24500
    },
    {
      "epoch": 1.5473484848484849,
      "grad_norm": 0.42354679107666016,
      "learning_rate": 2.5691674564051348e-05,
      "loss": 0.651,
      "step": 24510
    },
    {
      "epoch": 1.547979797979798,
      "grad_norm": 0.431822806596756,
      "learning_rate": 2.5623297561435912e-05,
      "loss": 0.5603,
      "step": 24520
    },
    {
      "epoch": 1.5486111111111112,
      "grad_norm": 0.4177646338939667,
      "learning_rate": 2.555499829796768e-05,
      "loss": 0.5033,
      "step": 24530
    },
    {
      "epoch": 1.5492424242424243,
      "grad_norm": 0.6811342239379883,
      "learning_rate": 2.5486776845033654e-05,
      "loss": 0.5351,
      "step": 24540
    },
    {
      "epoch": 1.5498737373737375,
      "grad_norm": 0.4004029333591461,
      "learning_rate": 2.5418633273939462e-05,
      "loss": 0.7732,
      "step": 24550
    },
    {
      "epoch": 1.5505050505050506,
      "grad_norm": 0.4360256493091583,
      "learning_rate": 2.5350567655909308e-05,
      "loss": 0.6726,
      "step": 24560
    },
    {
      "epoch": 1.5511363636363638,
      "grad_norm": 0.47814667224884033,
      "learning_rate": 2.528258006208596e-05,
      "loss": 0.5992,
      "step": 24570
    },
    {
      "epoch": 1.551767676767677,
      "grad_norm": 0.44856923818588257,
      "learning_rate": 2.521467056353056e-05,
      "loss": 0.4761,
      "step": 24580
    },
    {
      "epoch": 1.55239898989899,
      "grad_norm": 0.7428015470504761,
      "learning_rate": 2.5146839231222764e-05,
      "loss": 0.505,
      "step": 24590
    },
    {
      "epoch": 1.553030303030303,
      "grad_norm": 0.40450796484947205,
      "learning_rate": 2.5079086136060393e-05,
      "loss": 0.765,
      "step": 24600
    },
    {
      "epoch": 1.5536616161616161,
      "grad_norm": 0.46073615550994873,
      "learning_rate": 2.501141134885957e-05,
      "loss": 0.6895,
      "step": 24610
    },
    {
      "epoch": 1.5542929292929293,
      "grad_norm": 0.4434994161128998,
      "learning_rate": 2.4943814940354492e-05,
      "loss": 0.5648,
      "step": 24620
    },
    {
      "epoch": 1.5549242424242424,
      "grad_norm": 0.49104225635528564,
      "learning_rate": 2.4876296981197555e-05,
      "loss": 0.4949,
      "step": 24630
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.7065548896789551,
      "learning_rate": 2.4808857541959074e-05,
      "loss": 0.5348,
      "step": 24640
    },
    {
      "epoch": 1.5561868686868687,
      "grad_norm": 0.4024820625782013,
      "learning_rate": 2.4741496693127287e-05,
      "loss": 0.7932,
      "step": 24650
    },
    {
      "epoch": 1.5568181818181817,
      "grad_norm": 0.4665620028972626,
      "learning_rate": 2.4674214505108294e-05,
      "loss": 0.6321,
      "step": 24660
    },
    {
      "epoch": 1.5574494949494948,
      "grad_norm": 0.4334457218647003,
      "learning_rate": 2.460701104822607e-05,
      "loss": 0.5919,
      "step": 24670
    },
    {
      "epoch": 1.558080808080808,
      "grad_norm": 0.4564291834831238,
      "learning_rate": 2.4539886392722178e-05,
      "loss": 0.5406,
      "step": 24680
    },
    {
      "epoch": 1.558712121212121,
      "grad_norm": 0.6633625626564026,
      "learning_rate": 2.4472840608755864e-05,
      "loss": 0.4919,
      "step": 24690
    },
    {
      "epoch": 1.5593434343434343,
      "grad_norm": 0.4040870666503906,
      "learning_rate": 2.440587376640395e-05,
      "loss": 0.7431,
      "step": 24700
    },
    {
      "epoch": 1.5599747474747474,
      "grad_norm": 0.40070080757141113,
      "learning_rate": 2.433898593566071e-05,
      "loss": 0.651,
      "step": 24710
    },
    {
      "epoch": 1.5606060606060606,
      "grad_norm": 0.47635072469711304,
      "learning_rate": 2.4272177186437896e-05,
      "loss": 0.5728,
      "step": 24720
    },
    {
      "epoch": 1.5612373737373737,
      "grad_norm": 0.4556596577167511,
      "learning_rate": 2.4205447588564566e-05,
      "loss": 0.5098,
      "step": 24730
    },
    {
      "epoch": 1.5618686868686869,
      "grad_norm": 0.6722652912139893,
      "learning_rate": 2.4138797211787034e-05,
      "loss": 0.5487,
      "step": 24740
    },
    {
      "epoch": 1.5625,
      "grad_norm": 0.40084460377693176,
      "learning_rate": 2.4072226125768795e-05,
      "loss": 0.7935,
      "step": 24750
    },
    {
      "epoch": 1.5631313131313131,
      "grad_norm": 0.4536131024360657,
      "learning_rate": 2.4005734400090574e-05,
      "loss": 0.6406,
      "step": 24760
    },
    {
      "epoch": 1.5637626262626263,
      "grad_norm": 0.43831995129585266,
      "learning_rate": 2.393932210425004e-05,
      "loss": 0.5605,
      "step": 24770
    },
    {
      "epoch": 1.5643939393939394,
      "grad_norm": 0.4624350965023041,
      "learning_rate": 2.3872989307661865e-05,
      "loss": 0.4958,
      "step": 24780
    },
    {
      "epoch": 1.5650252525252526,
      "grad_norm": 0.723550021648407,
      "learning_rate": 2.3806736079657654e-05,
      "loss": 0.5424,
      "step": 24790
    },
    {
      "epoch": 1.5656565656565657,
      "grad_norm": 0.38288867473602295,
      "learning_rate": 2.3740562489485806e-05,
      "loss": 0.7639,
      "step": 24800
    },
    {
      "epoch": 1.566287878787879,
      "grad_norm": 0.4431985020637512,
      "learning_rate": 2.367446860631154e-05,
      "loss": 0.6415,
      "step": 24810
    },
    {
      "epoch": 1.566919191919192,
      "grad_norm": 0.4469379484653473,
      "learning_rate": 2.3608454499216727e-05,
      "loss": 0.557,
      "step": 24820
    },
    {
      "epoch": 1.5675505050505052,
      "grad_norm": 0.5274122357368469,
      "learning_rate": 2.354252023719985e-05,
      "loss": 0.4779,
      "step": 24830
    },
    {
      "epoch": 1.5681818181818183,
      "grad_norm": 0.6929005980491638,
      "learning_rate": 2.347666588917592e-05,
      "loss": 0.5063,
      "step": 24840
    },
    {
      "epoch": 1.5688131313131313,
      "grad_norm": 0.4358207583427429,
      "learning_rate": 2.3410891523976507e-05,
      "loss": 0.8115,
      "step": 24850
    },
    {
      "epoch": 1.5694444444444444,
      "grad_norm": 0.38529258966445923,
      "learning_rate": 2.3345197210349502e-05,
      "loss": 0.6357,
      "step": 24860
    },
    {
      "epoch": 1.5700757575757576,
      "grad_norm": 0.46066585183143616,
      "learning_rate": 2.3279583016959128e-05,
      "loss": 0.5748,
      "step": 24870
    },
    {
      "epoch": 1.5707070707070707,
      "grad_norm": 0.46276891231536865,
      "learning_rate": 2.3214049012385874e-05,
      "loss": 0.525,
      "step": 24880
    },
    {
      "epoch": 1.5713383838383839,
      "grad_norm": 0.6029815077781677,
      "learning_rate": 2.3148595265126483e-05,
      "loss": 0.5089,
      "step": 24890
    },
    {
      "epoch": 1.571969696969697,
      "grad_norm": 0.4018890857696533,
      "learning_rate": 2.3083221843593716e-05,
      "loss": 0.7557,
      "step": 24900
    },
    {
      "epoch": 1.57260101010101,
      "grad_norm": 0.43234047293663025,
      "learning_rate": 2.301792881611642e-05,
      "loss": 0.675,
      "step": 24910
    },
    {
      "epoch": 1.573232323232323,
      "grad_norm": 0.44911786913871765,
      "learning_rate": 2.295271625093941e-05,
      "loss": 0.5469,
      "step": 24920
    },
    {
      "epoch": 1.5738636363636362,
      "grad_norm": 0.4674142301082611,
      "learning_rate": 2.2887584216223378e-05,
      "loss": 0.4852,
      "step": 24930
    },
    {
      "epoch": 1.5744949494949494,
      "grad_norm": 0.8809202909469604,
      "learning_rate": 2.2822532780044914e-05,
      "loss": 0.5174,
      "step": 24940
    },
    {
      "epoch": 1.5751262626262625,
      "grad_norm": 0.4108004868030548,
      "learning_rate": 2.2757562010396306e-05,
      "loss": 0.78,
      "step": 24950
    },
    {
      "epoch": 1.5757575757575757,
      "grad_norm": 0.41016897559165955,
      "learning_rate": 2.269267197518552e-05,
      "loss": 0.6655,
      "step": 24960
    },
    {
      "epoch": 1.5763888888888888,
      "grad_norm": 0.4636913537979126,
      "learning_rate": 2.262786274223616e-05,
      "loss": 0.5606,
      "step": 24970
    },
    {
      "epoch": 1.577020202020202,
      "grad_norm": 0.4765782058238983,
      "learning_rate": 2.256313437928741e-05,
      "loss": 0.4797,
      "step": 24980
    },
    {
      "epoch": 1.5776515151515151,
      "grad_norm": 0.6929013729095459,
      "learning_rate": 2.249848695399387e-05,
      "loss": 0.5008,
      "step": 24990
    },
    {
      "epoch": 1.5782828282828283,
      "grad_norm": 0.41554898023605347,
      "learning_rate": 2.243392053392557e-05,
      "loss": 0.7646,
      "step": 25000
    },
    {
      "epoch": 1.5782828282828283,
      "eval_loss": 0.6321786046028137,
      "eval_runtime": 31.6483,
      "eval_samples_per_second": 80.889,
      "eval_steps_per_second": 10.111,
      "step": 25000
    },
    {
      "epoch": 1.5789141414141414,
      "grad_norm": 0.4148867726325989,
      "learning_rate": 2.236943518656782e-05,
      "loss": 0.6181,
      "step": 25010
    },
    {
      "epoch": 1.5795454545454546,
      "grad_norm": 0.45767873525619507,
      "learning_rate": 2.230503097932134e-05,
      "loss": 0.5864,
      "step": 25020
    },
    {
      "epoch": 1.5801767676767677,
      "grad_norm": 0.47669801115989685,
      "learning_rate": 2.2240707979501874e-05,
      "loss": 0.5113,
      "step": 25030
    },
    {
      "epoch": 1.5808080808080809,
      "grad_norm": 0.7507054805755615,
      "learning_rate": 2.2176466254340366e-05,
      "loss": 0.5054,
      "step": 25040
    },
    {
      "epoch": 1.581439393939394,
      "grad_norm": 0.4065714478492737,
      "learning_rate": 2.211230587098281e-05,
      "loss": 0.7712,
      "step": 25050
    },
    {
      "epoch": 1.5820707070707072,
      "grad_norm": 0.4032250642776489,
      "learning_rate": 2.2048226896490142e-05,
      "loss": 0.6624,
      "step": 25060
    },
    {
      "epoch": 1.5827020202020203,
      "grad_norm": 0.47658389806747437,
      "learning_rate": 2.1984229397838285e-05,
      "loss": 0.5555,
      "step": 25070
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.4581820070743561,
      "learning_rate": 2.192031344191794e-05,
      "loss": 0.4816,
      "step": 25080
    },
    {
      "epoch": 1.5839646464646466,
      "grad_norm": 0.6689023971557617,
      "learning_rate": 2.185647909553459e-05,
      "loss": 0.533,
      "step": 25090
    },
    {
      "epoch": 1.5845959595959596,
      "grad_norm": 0.4105689823627472,
      "learning_rate": 2.1792726425408383e-05,
      "loss": 0.7988,
      "step": 25100
    },
    {
      "epoch": 1.5852272727272727,
      "grad_norm": 0.4218522310256958,
      "learning_rate": 2.1729055498174224e-05,
      "loss": 0.6591,
      "step": 25110
    },
    {
      "epoch": 1.5858585858585859,
      "grad_norm": 0.47459810972213745,
      "learning_rate": 2.1665466380381438e-05,
      "loss": 0.5698,
      "step": 25120
    },
    {
      "epoch": 1.586489898989899,
      "grad_norm": 0.4616391658782959,
      "learning_rate": 2.160195913849392e-05,
      "loss": 0.4963,
      "step": 25130
    },
    {
      "epoch": 1.5871212121212122,
      "grad_norm": 0.6334791779518127,
      "learning_rate": 2.1538533838889953e-05,
      "loss": 0.4868,
      "step": 25140
    },
    {
      "epoch": 1.5877525252525253,
      "grad_norm": 0.417669415473938,
      "learning_rate": 2.1475190547862167e-05,
      "loss": 0.7707,
      "step": 25150
    },
    {
      "epoch": 1.5883838383838382,
      "grad_norm": 0.4505324959754944,
      "learning_rate": 2.1411929331617553e-05,
      "loss": 0.6337,
      "step": 25160
    },
    {
      "epoch": 1.5890151515151514,
      "grad_norm": 0.4340027868747711,
      "learning_rate": 2.1348750256277216e-05,
      "loss": 0.5572,
      "step": 25170
    },
    {
      "epoch": 1.5896464646464645,
      "grad_norm": 0.4493427276611328,
      "learning_rate": 2.128565338787648e-05,
      "loss": 0.4858,
      "step": 25180
    },
    {
      "epoch": 1.5902777777777777,
      "grad_norm": 0.6487900018692017,
      "learning_rate": 2.1222638792364658e-05,
      "loss": 0.4847,
      "step": 25190
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 0.4169430136680603,
      "learning_rate": 2.1159706535605207e-05,
      "loss": 0.7859,
      "step": 25200
    },
    {
      "epoch": 1.591540404040404,
      "grad_norm": 0.42240747809410095,
      "learning_rate": 2.1096856683375398e-05,
      "loss": 0.6514,
      "step": 25210
    },
    {
      "epoch": 1.5921717171717171,
      "grad_norm": 0.46090957522392273,
      "learning_rate": 2.1034089301366434e-05,
      "loss": 0.5632,
      "step": 25220
    },
    {
      "epoch": 1.5928030303030303,
      "grad_norm": 0.45849260687828064,
      "learning_rate": 2.0971404455183263e-05,
      "loss": 0.5218,
      "step": 25230
    },
    {
      "epoch": 1.5934343434343434,
      "grad_norm": 0.7458134293556213,
      "learning_rate": 2.090880221034467e-05,
      "loss": 0.5037,
      "step": 25240
    },
    {
      "epoch": 1.5940656565656566,
      "grad_norm": 0.4056951403617859,
      "learning_rate": 2.0846282632283022e-05,
      "loss": 0.7348,
      "step": 25250
    },
    {
      "epoch": 1.5946969696969697,
      "grad_norm": 0.46478569507598877,
      "learning_rate": 2.078384578634428e-05,
      "loss": 0.6405,
      "step": 25260
    },
    {
      "epoch": 1.5953282828282829,
      "grad_norm": 0.45266154408454895,
      "learning_rate": 2.0721491737787978e-05,
      "loss": 0.5608,
      "step": 25270
    },
    {
      "epoch": 1.595959595959596,
      "grad_norm": 0.5023328065872192,
      "learning_rate": 2.065922055178704e-05,
      "loss": 0.4919,
      "step": 25280
    },
    {
      "epoch": 1.5965909090909092,
      "grad_norm": 0.7026766538619995,
      "learning_rate": 2.0597032293427887e-05,
      "loss": 0.5054,
      "step": 25290
    },
    {
      "epoch": 1.5972222222222223,
      "grad_norm": 0.425676167011261,
      "learning_rate": 2.053492702771018e-05,
      "loss": 0.8184,
      "step": 25300
    },
    {
      "epoch": 1.5978535353535355,
      "grad_norm": 0.46627727150917053,
      "learning_rate": 2.0472904819546856e-05,
      "loss": 0.6618,
      "step": 25310
    },
    {
      "epoch": 1.5984848484848486,
      "grad_norm": 0.4572855830192566,
      "learning_rate": 2.041096573376402e-05,
      "loss": 0.5555,
      "step": 25320
    },
    {
      "epoch": 1.5991161616161618,
      "grad_norm": 0.461850643157959,
      "learning_rate": 2.0349109835100956e-05,
      "loss": 0.4998,
      "step": 25330
    },
    {
      "epoch": 1.5997474747474747,
      "grad_norm": 0.7245675921440125,
      "learning_rate": 2.0287337188209954e-05,
      "loss": 0.5237,
      "step": 25340
    },
    {
      "epoch": 1.6003787878787878,
      "grad_norm": 0.3831062316894531,
      "learning_rate": 2.0225647857656292e-05,
      "loss": 0.7909,
      "step": 25350
    },
    {
      "epoch": 1.601010101010101,
      "grad_norm": 0.4089537262916565,
      "learning_rate": 2.016404190791814e-05,
      "loss": 0.6408,
      "step": 25360
    },
    {
      "epoch": 1.6016414141414141,
      "grad_norm": 0.4536720812320709,
      "learning_rate": 2.010251940338661e-05,
      "loss": 0.5737,
      "step": 25370
    },
    {
      "epoch": 1.6022727272727273,
      "grad_norm": 0.4523641765117645,
      "learning_rate": 2.004108040836551e-05,
      "loss": 0.5162,
      "step": 25380
    },
    {
      "epoch": 1.6029040404040404,
      "grad_norm": 0.795026957988739,
      "learning_rate": 1.997972498707137e-05,
      "loss": 0.5301,
      "step": 25390
    },
    {
      "epoch": 1.6035353535353534,
      "grad_norm": 0.45874354243278503,
      "learning_rate": 1.9918453203633402e-05,
      "loss": 0.7757,
      "step": 25400
    },
    {
      "epoch": 1.6041666666666665,
      "grad_norm": 0.40785861015319824,
      "learning_rate": 1.985726512209335e-05,
      "loss": 0.6269,
      "step": 25410
    },
    {
      "epoch": 1.6047979797979797,
      "grad_norm": 0.45333734154701233,
      "learning_rate": 1.979616080640556e-05,
      "loss": 0.5564,
      "step": 25420
    },
    {
      "epoch": 1.6054292929292928,
      "grad_norm": 0.4979815185070038,
      "learning_rate": 1.973514032043674e-05,
      "loss": 0.4984,
      "step": 25430
    },
    {
      "epoch": 1.606060606060606,
      "grad_norm": 0.6530413627624512,
      "learning_rate": 1.9674203727966024e-05,
      "loss": 0.4799,
      "step": 25440
    },
    {
      "epoch": 1.6066919191919191,
      "grad_norm": 0.39400148391723633,
      "learning_rate": 1.9613351092684795e-05,
      "loss": 0.7597,
      "step": 25450
    },
    {
      "epoch": 1.6073232323232323,
      "grad_norm": 0.45246535539627075,
      "learning_rate": 1.955258247819681e-05,
      "loss": 0.6459,
      "step": 25460
    },
    {
      "epoch": 1.6079545454545454,
      "grad_norm": 0.47414258122444153,
      "learning_rate": 1.949189794801788e-05,
      "loss": 0.5937,
      "step": 25470
    },
    {
      "epoch": 1.6085858585858586,
      "grad_norm": 0.4870409071445465,
      "learning_rate": 1.9431297565576e-05,
      "loss": 0.5251,
      "step": 25480
    },
    {
      "epoch": 1.6092171717171717,
      "grad_norm": 0.7625123262405396,
      "learning_rate": 1.937078139421118e-05,
      "loss": 0.5045,
      "step": 25490
    },
    {
      "epoch": 1.6098484848484849,
      "grad_norm": 0.4379851520061493,
      "learning_rate": 1.9310349497175405e-05,
      "loss": 0.8123,
      "step": 25500
    },
    {
      "epoch": 1.610479797979798,
      "grad_norm": 0.4331996738910675,
      "learning_rate": 1.9250001937632645e-05,
      "loss": 0.633,
      "step": 25510
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 0.4389835596084595,
      "learning_rate": 1.918973877865864e-05,
      "loss": 0.5888,
      "step": 25520
    },
    {
      "epoch": 1.6117424242424243,
      "grad_norm": 0.49013230204582214,
      "learning_rate": 1.912956008324096e-05,
      "loss": 0.4757,
      "step": 25530
    },
    {
      "epoch": 1.6123737373737375,
      "grad_norm": 0.7425442337989807,
      "learning_rate": 1.9069465914278827e-05,
      "loss": 0.5131,
      "step": 25540
    },
    {
      "epoch": 1.6130050505050506,
      "grad_norm": 0.395367830991745,
      "learning_rate": 1.9009456334583243e-05,
      "loss": 0.7786,
      "step": 25550
    },
    {
      "epoch": 1.6136363636363638,
      "grad_norm": 0.45939525961875916,
      "learning_rate": 1.8949531406876684e-05,
      "loss": 0.6719,
      "step": 25560
    },
    {
      "epoch": 1.614267676767677,
      "grad_norm": 0.4831186830997467,
      "learning_rate": 1.888969119379318e-05,
      "loss": 0.5891,
      "step": 25570
    },
    {
      "epoch": 1.61489898989899,
      "grad_norm": 0.5451868772506714,
      "learning_rate": 1.8829935757878183e-05,
      "loss": 0.4762,
      "step": 25580
    },
    {
      "epoch": 1.615530303030303,
      "grad_norm": 0.8348273038864136,
      "learning_rate": 1.877026516158864e-05,
      "loss": 0.5525,
      "step": 25590
    },
    {
      "epoch": 1.6161616161616161,
      "grad_norm": 0.40289950370788574,
      "learning_rate": 1.8710679467292725e-05,
      "loss": 0.7842,
      "step": 25600
    },
    {
      "epoch": 1.6167929292929293,
      "grad_norm": 0.4535691738128662,
      "learning_rate": 1.8651178737269893e-05,
      "loss": 0.6347,
      "step": 25610
    },
    {
      "epoch": 1.6174242424242424,
      "grad_norm": 0.4126059114933014,
      "learning_rate": 1.8591763033710796e-05,
      "loss": 0.5672,
      "step": 25620
    },
    {
      "epoch": 1.6180555555555556,
      "grad_norm": 0.5136206746101379,
      "learning_rate": 1.8532432418717215e-05,
      "loss": 0.5022,
      "step": 25630
    },
    {
      "epoch": 1.6186868686868687,
      "grad_norm": 0.8367285132408142,
      "learning_rate": 1.8473186954302036e-05,
      "loss": 0.5573,
      "step": 25640
    },
    {
      "epoch": 1.6193181818181817,
      "grad_norm": 0.4055253863334656,
      "learning_rate": 1.8414026702389096e-05,
      "loss": 0.8357,
      "step": 25650
    },
    {
      "epoch": 1.6199494949494948,
      "grad_norm": 0.40358805656433105,
      "learning_rate": 1.8354951724813173e-05,
      "loss": 0.6486,
      "step": 25660
    },
    {
      "epoch": 1.620580808080808,
      "grad_norm": 0.40332603454589844,
      "learning_rate": 1.8295962083319905e-05,
      "loss": 0.5636,
      "step": 25670
    },
    {
      "epoch": 1.621212121212121,
      "grad_norm": 0.4582480788230896,
      "learning_rate": 1.8237057839565807e-05,
      "loss": 0.5279,
      "step": 25680
    },
    {
      "epoch": 1.6218434343434343,
      "grad_norm": 0.7483375072479248,
      "learning_rate": 1.817823905511805e-05,
      "loss": 0.5099,
      "step": 25690
    },
    {
      "epoch": 1.6224747474747474,
      "grad_norm": 0.4171113073825836,
      "learning_rate": 1.8119505791454527e-05,
      "loss": 0.7764,
      "step": 25700
    },
    {
      "epoch": 1.6231060606060606,
      "grad_norm": 0.46834108233451843,
      "learning_rate": 1.8060858109963698e-05,
      "loss": 0.6259,
      "step": 25710
    },
    {
      "epoch": 1.6237373737373737,
      "grad_norm": 0.5033125281333923,
      "learning_rate": 1.800229607194467e-05,
      "loss": 0.5733,
      "step": 25720
    },
    {
      "epoch": 1.6243686868686869,
      "grad_norm": 0.4616455137729645,
      "learning_rate": 1.7943819738606927e-05,
      "loss": 0.5001,
      "step": 25730
    },
    {
      "epoch": 1.625,
      "grad_norm": 0.6900359988212585,
      "learning_rate": 1.7885429171070432e-05,
      "loss": 0.5341,
      "step": 25740
    },
    {
      "epoch": 1.6256313131313131,
      "grad_norm": 0.40040165185928345,
      "learning_rate": 1.7827124430365494e-05,
      "loss": 0.7695,
      "step": 25750
    },
    {
      "epoch": 1.6262626262626263,
      "grad_norm": 0.4512784779071808,
      "learning_rate": 1.776890557743267e-05,
      "loss": 0.6076,
      "step": 25760
    },
    {
      "epoch": 1.6268939393939394,
      "grad_norm": 0.4234769642353058,
      "learning_rate": 1.7710772673122867e-05,
      "loss": 0.5664,
      "step": 25770
    },
    {
      "epoch": 1.6275252525252526,
      "grad_norm": 0.5517420172691345,
      "learning_rate": 1.7652725778197022e-05,
      "loss": 0.5171,
      "step": 25780
    },
    {
      "epoch": 1.6281565656565657,
      "grad_norm": 0.6691545248031616,
      "learning_rate": 1.759476495332626e-05,
      "loss": 0.5064,
      "step": 25790
    },
    {
      "epoch": 1.628787878787879,
      "grad_norm": 0.4226921498775482,
      "learning_rate": 1.7536890259091675e-05,
      "loss": 0.7386,
      "step": 25800
    },
    {
      "epoch": 1.629419191919192,
      "grad_norm": 0.4406505227088928,
      "learning_rate": 1.747910175598443e-05,
      "loss": 0.6348,
      "step": 25810
    },
    {
      "epoch": 1.6300505050505052,
      "grad_norm": 0.4383058547973633,
      "learning_rate": 1.7421399504405534e-05,
      "loss": 0.5422,
      "step": 25820
    },
    {
      "epoch": 1.6306818181818183,
      "grad_norm": 0.4327830970287323,
      "learning_rate": 1.736378356466585e-05,
      "loss": 0.534,
      "step": 25830
    },
    {
      "epoch": 1.6313131313131313,
      "grad_norm": 0.6376233696937561,
      "learning_rate": 1.7306253996986e-05,
      "loss": 0.513,
      "step": 25840
    },
    {
      "epoch": 1.6319444444444444,
      "grad_norm": 0.39082545042037964,
      "learning_rate": 1.7248810861496445e-05,
      "loss": 0.8021,
      "step": 25850
    },
    {
      "epoch": 1.6325757575757576,
      "grad_norm": 0.4170735478401184,
      "learning_rate": 1.719145421823718e-05,
      "loss": 0.6257,
      "step": 25860
    },
    {
      "epoch": 1.6332070707070707,
      "grad_norm": 0.4638461172580719,
      "learning_rate": 1.713418412715786e-05,
      "loss": 0.5588,
      "step": 25870
    },
    {
      "epoch": 1.6338383838383839,
      "grad_norm": 0.4941306412220001,
      "learning_rate": 1.7077000648117637e-05,
      "loss": 0.5005,
      "step": 25880
    },
    {
      "epoch": 1.634469696969697,
      "grad_norm": 0.644615113735199,
      "learning_rate": 1.7019903840885166e-05,
      "loss": 0.4657,
      "step": 25890
    },
    {
      "epoch": 1.63510101010101,
      "grad_norm": 0.40031564235687256,
      "learning_rate": 1.696289376513852e-05,
      "loss": 0.7572,
      "step": 25900
    },
    {
      "epoch": 1.635732323232323,
      "grad_norm": 0.4403163194656372,
      "learning_rate": 1.6905970480465106e-05,
      "loss": 0.6255,
      "step": 25910
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 0.45581746101379395,
      "learning_rate": 1.6849134046361604e-05,
      "loss": 0.5424,
      "step": 25920
    },
    {
      "epoch": 1.6369949494949494,
      "grad_norm": 0.4841029942035675,
      "learning_rate": 1.6792384522233896e-05,
      "loss": 0.5043,
      "step": 25930
    },
    {
      "epoch": 1.6376262626262625,
      "grad_norm": 0.7825595140457153,
      "learning_rate": 1.6735721967397112e-05,
      "loss": 0.4987,
      "step": 25940
    },
    {
      "epoch": 1.6382575757575757,
      "grad_norm": 0.4120021462440491,
      "learning_rate": 1.66791464410754e-05,
      "loss": 0.7519,
      "step": 25950
    },
    {
      "epoch": 1.6388888888888888,
      "grad_norm": 0.46642544865608215,
      "learning_rate": 1.662265800240197e-05,
      "loss": 0.674,
      "step": 25960
    },
    {
      "epoch": 1.639520202020202,
      "grad_norm": 0.4808289110660553,
      "learning_rate": 1.6566256710419015e-05,
      "loss": 0.582,
      "step": 25970
    },
    {
      "epoch": 1.6401515151515151,
      "grad_norm": 0.4016565680503845,
      "learning_rate": 1.650994262407759e-05,
      "loss": 0.4831,
      "step": 25980
    },
    {
      "epoch": 1.6407828282828283,
      "grad_norm": 0.8064765334129333,
      "learning_rate": 1.6453715802237714e-05,
      "loss": 0.5021,
      "step": 25990
    },
    {
      "epoch": 1.6414141414141414,
      "grad_norm": 0.4162544012069702,
      "learning_rate": 1.6397576303668093e-05,
      "loss": 0.7616,
      "step": 26000
    },
    {
      "epoch": 1.6414141414141414,
      "eval_loss": 0.6323121190071106,
      "eval_runtime": 31.6537,
      "eval_samples_per_second": 80.875,
      "eval_steps_per_second": 10.109,
      "step": 26000
    },
    {
      "epoch": 1.6420454545454546,
      "grad_norm": 0.45119911432266235,
      "learning_rate": 1.6341524187046198e-05,
      "loss": 0.6337,
      "step": 26010
    },
    {
      "epoch": 1.6426767676767677,
      "grad_norm": 0.4461151361465454,
      "learning_rate": 1.6285559510958148e-05,
      "loss": 0.5691,
      "step": 26020
    },
    {
      "epoch": 1.6433080808080809,
      "grad_norm": 0.43967461585998535,
      "learning_rate": 1.622968233389872e-05,
      "loss": 0.4948,
      "step": 26030
    },
    {
      "epoch": 1.643939393939394,
      "grad_norm": 0.7374057769775391,
      "learning_rate": 1.617389271427121e-05,
      "loss": 0.5259,
      "step": 26040
    },
    {
      "epoch": 1.6445707070707072,
      "grad_norm": 0.4268018901348114,
      "learning_rate": 1.611819071038736e-05,
      "loss": 0.8011,
      "step": 26050
    },
    {
      "epoch": 1.6452020202020203,
      "grad_norm": 0.4649810194969177,
      "learning_rate": 1.6062576380467364e-05,
      "loss": 0.6146,
      "step": 26060
    },
    {
      "epoch": 1.6458333333333335,
      "grad_norm": 0.4677508771419525,
      "learning_rate": 1.6007049782639827e-05,
      "loss": 0.574,
      "step": 26070
    },
    {
      "epoch": 1.6464646464646466,
      "grad_norm": 0.5116612911224365,
      "learning_rate": 1.595161097494158e-05,
      "loss": 0.4809,
      "step": 26080
    },
    {
      "epoch": 1.6470959595959596,
      "grad_norm": 0.7703385949134827,
      "learning_rate": 1.589626001531772e-05,
      "loss": 0.5022,
      "step": 26090
    },
    {
      "epoch": 1.6477272727272727,
      "grad_norm": 0.40151268243789673,
      "learning_rate": 1.5840996961621545e-05,
      "loss": 0.7352,
      "step": 26100
    },
    {
      "epoch": 1.6483585858585859,
      "grad_norm": 0.394184947013855,
      "learning_rate": 1.5785821871614414e-05,
      "loss": 0.6391,
      "step": 26110
    },
    {
      "epoch": 1.648989898989899,
      "grad_norm": 0.44378772377967834,
      "learning_rate": 1.5730734802965863e-05,
      "loss": 0.5532,
      "step": 26120
    },
    {
      "epoch": 1.6496212121212122,
      "grad_norm": 0.5021860599517822,
      "learning_rate": 1.5675735813253313e-05,
      "loss": 0.5155,
      "step": 26130
    },
    {
      "epoch": 1.6502525252525253,
      "grad_norm": 0.7029537558555603,
      "learning_rate": 1.5620824959962165e-05,
      "loss": 0.5225,
      "step": 26140
    },
    {
      "epoch": 1.6508838383838382,
      "grad_norm": 0.3968351483345032,
      "learning_rate": 1.5566002300485672e-05,
      "loss": 0.8041,
      "step": 26150
    },
    {
      "epoch": 1.6515151515151514,
      "grad_norm": 0.46387699246406555,
      "learning_rate": 1.551126789212499e-05,
      "loss": 0.6826,
      "step": 26160
    },
    {
      "epoch": 1.6521464646464645,
      "grad_norm": 0.45244327187538147,
      "learning_rate": 1.5456621792088967e-05,
      "loss": 0.5768,
      "step": 26170
    },
    {
      "epoch": 1.6527777777777777,
      "grad_norm": 0.5056235790252686,
      "learning_rate": 1.5402064057494124e-05,
      "loss": 0.5056,
      "step": 26180
    },
    {
      "epoch": 1.6534090909090908,
      "grad_norm": 0.7851883769035339,
      "learning_rate": 1.534759474536467e-05,
      "loss": 0.5258,
      "step": 26190
    },
    {
      "epoch": 1.654040404040404,
      "grad_norm": 0.39271995425224304,
      "learning_rate": 1.5293213912632397e-05,
      "loss": 0.8459,
      "step": 26200
    },
    {
      "epoch": 1.6546717171717171,
      "grad_norm": 0.4816075563430786,
      "learning_rate": 1.5238921616136614e-05,
      "loss": 0.654,
      "step": 26210
    },
    {
      "epoch": 1.6553030303030303,
      "grad_norm": 0.46632689237594604,
      "learning_rate": 1.5184717912624059e-05,
      "loss": 0.593,
      "step": 26220
    },
    {
      "epoch": 1.6559343434343434,
      "grad_norm": 0.4375392496585846,
      "learning_rate": 1.5130602858748888e-05,
      "loss": 0.4932,
      "step": 26230
    },
    {
      "epoch": 1.6565656565656566,
      "grad_norm": 0.6660777926445007,
      "learning_rate": 1.5076576511072594e-05,
      "loss": 0.5489,
      "step": 26240
    },
    {
      "epoch": 1.6571969696969697,
      "grad_norm": 0.4116275906562805,
      "learning_rate": 1.5022638926064014e-05,
      "loss": 0.7965,
      "step": 26250
    },
    {
      "epoch": 1.6578282828282829,
      "grad_norm": 0.45720842480659485,
      "learning_rate": 1.4968790160099133e-05,
      "loss": 0.6297,
      "step": 26260
    },
    {
      "epoch": 1.658459595959596,
      "grad_norm": 0.4119005799293518,
      "learning_rate": 1.4915030269461117e-05,
      "loss": 0.5481,
      "step": 26270
    },
    {
      "epoch": 1.6590909090909092,
      "grad_norm": 0.4693313241004944,
      "learning_rate": 1.486135931034024e-05,
      "loss": 0.4831,
      "step": 26280
    },
    {
      "epoch": 1.6597222222222223,
      "grad_norm": 0.7184471487998962,
      "learning_rate": 1.4807777338833883e-05,
      "loss": 0.504,
      "step": 26290
    },
    {
      "epoch": 1.6603535353535355,
      "grad_norm": 0.4211339056491852,
      "learning_rate": 1.4754284410946328e-05,
      "loss": 0.819,
      "step": 26300
    },
    {
      "epoch": 1.6609848484848486,
      "grad_norm": 0.41067278385162354,
      "learning_rate": 1.470088058258885e-05,
      "loss": 0.6592,
      "step": 26310
    },
    {
      "epoch": 1.6616161616161618,
      "grad_norm": 0.4803182780742645,
      "learning_rate": 1.4647565909579552e-05,
      "loss": 0.6045,
      "step": 26320
    },
    {
      "epoch": 1.6622474747474747,
      "grad_norm": 0.4402787387371063,
      "learning_rate": 1.4594340447643373e-05,
      "loss": 0.4993,
      "step": 26330
    },
    {
      "epoch": 1.6628787878787878,
      "grad_norm": 0.7928913831710815,
      "learning_rate": 1.4541204252412034e-05,
      "loss": 0.5155,
      "step": 26340
    },
    {
      "epoch": 1.663510101010101,
      "grad_norm": 0.41848301887512207,
      "learning_rate": 1.4488157379423906e-05,
      "loss": 0.764,
      "step": 26350
    },
    {
      "epoch": 1.6641414141414141,
      "grad_norm": 0.4113439619541168,
      "learning_rate": 1.4435199884124028e-05,
      "loss": 0.6451,
      "step": 26360
    },
    {
      "epoch": 1.6647727272727273,
      "grad_norm": 0.4572337567806244,
      "learning_rate": 1.4382331821863993e-05,
      "loss": 0.5791,
      "step": 26370
    },
    {
      "epoch": 1.6654040404040404,
      "grad_norm": 0.49989691376686096,
      "learning_rate": 1.4329553247901984e-05,
      "loss": 0.5122,
      "step": 26380
    },
    {
      "epoch": 1.6660353535353534,
      "grad_norm": 0.7736592888832092,
      "learning_rate": 1.4276864217402585e-05,
      "loss": 0.5216,
      "step": 26390
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.377758264541626,
      "learning_rate": 1.4224264785436791e-05,
      "loss": 0.7941,
      "step": 26400
    },
    {
      "epoch": 1.6672979797979797,
      "grad_norm": 0.4528137743473053,
      "learning_rate": 1.4171755006981958e-05,
      "loss": 0.6289,
      "step": 26410
    },
    {
      "epoch": 1.6679292929292928,
      "grad_norm": 0.47656673192977905,
      "learning_rate": 1.4119334936921791e-05,
      "loss": 0.5508,
      "step": 26420
    },
    {
      "epoch": 1.668560606060606,
      "grad_norm": 0.44374898076057434,
      "learning_rate": 1.4067004630046154e-05,
      "loss": 0.5199,
      "step": 26430
    },
    {
      "epoch": 1.6691919191919191,
      "grad_norm": 0.810061514377594,
      "learning_rate": 1.4014764141051118e-05,
      "loss": 0.5505,
      "step": 26440
    },
    {
      "epoch": 1.6698232323232323,
      "grad_norm": 0.38165703415870667,
      "learning_rate": 1.396261352453887e-05,
      "loss": 0.7614,
      "step": 26450
    },
    {
      "epoch": 1.6704545454545454,
      "grad_norm": 0.42085856199264526,
      "learning_rate": 1.3910552835017653e-05,
      "loss": 0.6427,
      "step": 26460
    },
    {
      "epoch": 1.6710858585858586,
      "grad_norm": 0.49430570006370544,
      "learning_rate": 1.3858582126901775e-05,
      "loss": 0.5733,
      "step": 26470
    },
    {
      "epoch": 1.6717171717171717,
      "grad_norm": 0.47422918677330017,
      "learning_rate": 1.380670145451144e-05,
      "loss": 0.5252,
      "step": 26480
    },
    {
      "epoch": 1.6723484848484849,
      "grad_norm": 0.6857897639274597,
      "learning_rate": 1.3754910872072735e-05,
      "loss": 0.531,
      "step": 26490
    },
    {
      "epoch": 1.672979797979798,
      "grad_norm": 0.45018309354782104,
      "learning_rate": 1.3703210433717607e-05,
      "loss": 0.822,
      "step": 26500
    },
    {
      "epoch": 1.6736111111111112,
      "grad_norm": 0.40367284417152405,
      "learning_rate": 1.3651600193483816e-05,
      "loss": 0.6395,
      "step": 26510
    },
    {
      "epoch": 1.6742424242424243,
      "grad_norm": 0.4493842124938965,
      "learning_rate": 1.3600080205314792e-05,
      "loss": 0.5777,
      "step": 26520
    },
    {
      "epoch": 1.6748737373737375,
      "grad_norm": 0.48243898153305054,
      "learning_rate": 1.3548650523059648e-05,
      "loss": 0.5276,
      "step": 26530
    },
    {
      "epoch": 1.6755050505050506,
      "grad_norm": 0.6996680498123169,
      "learning_rate": 1.3497311200473117e-05,
      "loss": 0.5011,
      "step": 26540
    },
    {
      "epoch": 1.6761363636363638,
      "grad_norm": 0.37529200315475464,
      "learning_rate": 1.344606229121551e-05,
      "loss": 0.7472,
      "step": 26550
    },
    {
      "epoch": 1.676767676767677,
      "grad_norm": 0.41773155331611633,
      "learning_rate": 1.3394903848852592e-05,
      "loss": 0.6556,
      "step": 26560
    },
    {
      "epoch": 1.67739898989899,
      "grad_norm": 0.43891605734825134,
      "learning_rate": 1.3343835926855597e-05,
      "loss": 0.5413,
      "step": 26570
    },
    {
      "epoch": 1.678030303030303,
      "grad_norm": 0.5056846141815186,
      "learning_rate": 1.3292858578601131e-05,
      "loss": 0.4803,
      "step": 26580
    },
    {
      "epoch": 1.6786616161616161,
      "grad_norm": 0.6681736707687378,
      "learning_rate": 1.3241971857371139e-05,
      "loss": 0.5625,
      "step": 26590
    },
    {
      "epoch": 1.6792929292929293,
      "grad_norm": 0.385825514793396,
      "learning_rate": 1.3191175816352873e-05,
      "loss": 0.745,
      "step": 26600
    },
    {
      "epoch": 1.6799242424242424,
      "grad_norm": 0.43507060408592224,
      "learning_rate": 1.3140470508638769e-05,
      "loss": 0.6735,
      "step": 26610
    },
    {
      "epoch": 1.6805555555555556,
      "grad_norm": 0.4737373888492584,
      "learning_rate": 1.3089855987226417e-05,
      "loss": 0.5888,
      "step": 26620
    },
    {
      "epoch": 1.6811868686868687,
      "grad_norm": 0.43051546812057495,
      "learning_rate": 1.3039332305018526e-05,
      "loss": 0.4824,
      "step": 26630
    },
    {
      "epoch": 1.6818181818181817,
      "grad_norm": 0.7421919107437134,
      "learning_rate": 1.2988899514822906e-05,
      "loss": 0.5099,
      "step": 26640
    },
    {
      "epoch": 1.6824494949494948,
      "grad_norm": 0.4179592728614807,
      "learning_rate": 1.2938557669352314e-05,
      "loss": 0.7905,
      "step": 26650
    },
    {
      "epoch": 1.683080808080808,
      "grad_norm": 0.42550310492515564,
      "learning_rate": 1.288830682122445e-05,
      "loss": 0.6549,
      "step": 26660
    },
    {
      "epoch": 1.683712121212121,
      "grad_norm": 0.45654943585395813,
      "learning_rate": 1.2838147022961944e-05,
      "loss": 0.561,
      "step": 26670
    },
    {
      "epoch": 1.6843434343434343,
      "grad_norm": 0.4755608141422272,
      "learning_rate": 1.278807832699218e-05,
      "loss": 0.4917,
      "step": 26680
    },
    {
      "epoch": 1.6849747474747474,
      "grad_norm": 0.7642814517021179,
      "learning_rate": 1.2738100785647456e-05,
      "loss": 0.4842,
      "step": 26690
    },
    {
      "epoch": 1.6856060606060606,
      "grad_norm": 0.41319528222084045,
      "learning_rate": 1.2688214451164649e-05,
      "loss": 0.7668,
      "step": 26700
    },
    {
      "epoch": 1.6862373737373737,
      "grad_norm": 0.4155384600162506,
      "learning_rate": 1.2638419375685407e-05,
      "loss": 0.6672,
      "step": 26710
    },
    {
      "epoch": 1.6868686868686869,
      "grad_norm": 0.4987005889415741,
      "learning_rate": 1.2588715611255919e-05,
      "loss": 0.5654,
      "step": 26720
    },
    {
      "epoch": 1.6875,
      "grad_norm": 0.4983760416507721,
      "learning_rate": 1.2539103209827008e-05,
      "loss": 0.5101,
      "step": 26730
    },
    {
      "epoch": 1.6881313131313131,
      "grad_norm": 0.7906502485275269,
      "learning_rate": 1.248958222325396e-05,
      "loss": 0.5305,
      "step": 26740
    },
    {
      "epoch": 1.6887626262626263,
      "grad_norm": 0.39079350233078003,
      "learning_rate": 1.2440152703296515e-05,
      "loss": 0.8511,
      "step": 26750
    },
    {
      "epoch": 1.6893939393939394,
      "grad_norm": 0.4676860570907593,
      "learning_rate": 1.2390814701618791e-05,
      "loss": 0.6354,
      "step": 26760
    },
    {
      "epoch": 1.6900252525252526,
      "grad_norm": 0.4641934931278229,
      "learning_rate": 1.2341568269789338e-05,
      "loss": 0.5478,
      "step": 26770
    },
    {
      "epoch": 1.6906565656565657,
      "grad_norm": 0.4562721848487854,
      "learning_rate": 1.2292413459280905e-05,
      "loss": 0.478,
      "step": 26780
    },
    {
      "epoch": 1.691287878787879,
      "grad_norm": 0.7912115454673767,
      "learning_rate": 1.2243350321470493e-05,
      "loss": 0.4905,
      "step": 26790
    },
    {
      "epoch": 1.691919191919192,
      "grad_norm": 0.400313138961792,
      "learning_rate": 1.2194378907639326e-05,
      "loss": 0.7826,
      "step": 26800
    },
    {
      "epoch": 1.6925505050505052,
      "grad_norm": 0.43382975459098816,
      "learning_rate": 1.2145499268972693e-05,
      "loss": 0.6337,
      "step": 26810
    },
    {
      "epoch": 1.6931818181818183,
      "grad_norm": 0.494912713766098,
      "learning_rate": 1.209671145656005e-05,
      "loss": 0.5695,
      "step": 26820
    },
    {
      "epoch": 1.6938131313131313,
      "grad_norm": 0.5176822543144226,
      "learning_rate": 1.2048015521394807e-05,
      "loss": 0.5113,
      "step": 26830
    },
    {
      "epoch": 1.6944444444444444,
      "grad_norm": 0.7297536730766296,
      "learning_rate": 1.1999411514374348e-05,
      "loss": 0.4938,
      "step": 26840
    },
    {
      "epoch": 1.6950757575757576,
      "grad_norm": 0.4145958125591278,
      "learning_rate": 1.1950899486299972e-05,
      "loss": 0.8025,
      "step": 26850
    },
    {
      "epoch": 1.6957070707070707,
      "grad_norm": 0.4301968514919281,
      "learning_rate": 1.1902479487876884e-05,
      "loss": 0.6558,
      "step": 26860
    },
    {
      "epoch": 1.6963383838383839,
      "grad_norm": 0.47069302201271057,
      "learning_rate": 1.1854151569714067e-05,
      "loss": 0.5583,
      "step": 26870
    },
    {
      "epoch": 1.696969696969697,
      "grad_norm": 0.47269487380981445,
      "learning_rate": 1.1805915782324239e-05,
      "loss": 0.4976,
      "step": 26880
    },
    {
      "epoch": 1.69760101010101,
      "grad_norm": 0.7865092158317566,
      "learning_rate": 1.1757772176123848e-05,
      "loss": 0.5144,
      "step": 26890
    },
    {
      "epoch": 1.698232323232323,
      "grad_norm": 0.4122471213340759,
      "learning_rate": 1.1709720801433022e-05,
      "loss": 0.8101,
      "step": 26900
    },
    {
      "epoch": 1.6988636363636362,
      "grad_norm": 0.44088947772979736,
      "learning_rate": 1.1661761708475439e-05,
      "loss": 0.6336,
      "step": 26910
    },
    {
      "epoch": 1.6994949494949494,
      "grad_norm": 0.43687155842781067,
      "learning_rate": 1.1613894947378334e-05,
      "loss": 0.5626,
      "step": 26920
    },
    {
      "epoch": 1.7001262626262625,
      "grad_norm": 0.48923441767692566,
      "learning_rate": 1.1566120568172456e-05,
      "loss": 0.5043,
      "step": 26930
    },
    {
      "epoch": 1.7007575757575757,
      "grad_norm": 0.803626537322998,
      "learning_rate": 1.151843862079195e-05,
      "loss": 0.5202,
      "step": 26940
    },
    {
      "epoch": 1.7013888888888888,
      "grad_norm": 0.3993564248085022,
      "learning_rate": 1.1470849155074425e-05,
      "loss": 0.7522,
      "step": 26950
    },
    {
      "epoch": 1.702020202020202,
      "grad_norm": 0.40212103724479675,
      "learning_rate": 1.1423352220760774e-05,
      "loss": 0.6419,
      "step": 26960
    },
    {
      "epoch": 1.7026515151515151,
      "grad_norm": 0.49444133043289185,
      "learning_rate": 1.1375947867495186e-05,
      "loss": 0.5557,
      "step": 26970
    },
    {
      "epoch": 1.7032828282828283,
      "grad_norm": 0.49678346514701843,
      "learning_rate": 1.1328636144825055e-05,
      "loss": 0.5158,
      "step": 26980
    },
    {
      "epoch": 1.7039141414141414,
      "grad_norm": 0.7584826350212097,
      "learning_rate": 1.1281417102201042e-05,
      "loss": 0.532,
      "step": 26990
    },
    {
      "epoch": 1.7045454545454546,
      "grad_norm": 0.42369577288627625,
      "learning_rate": 1.1234290788976843e-05,
      "loss": 0.7725,
      "step": 27000
    },
    {
      "epoch": 1.7045454545454546,
      "eval_loss": 0.6320201754570007,
      "eval_runtime": 31.6586,
      "eval_samples_per_second": 80.863,
      "eval_steps_per_second": 10.108,
      "step": 27000
    },
    {
      "epoch": 1.7051767676767677,
      "grad_norm": 0.4501471519470215,
      "learning_rate": 1.118725725440929e-05,
      "loss": 0.631,
      "step": 27010
    },
    {
      "epoch": 1.7058080808080809,
      "grad_norm": 0.45992162823677063,
      "learning_rate": 1.1140316547658203e-05,
      "loss": 0.5624,
      "step": 27020
    },
    {
      "epoch": 1.706439393939394,
      "grad_norm": 0.43707793951034546,
      "learning_rate": 1.1093468717786427e-05,
      "loss": 0.4793,
      "step": 27030
    },
    {
      "epoch": 1.7070707070707072,
      "grad_norm": 0.8069823980331421,
      "learning_rate": 1.104671381375968e-05,
      "loss": 0.5091,
      "step": 27040
    },
    {
      "epoch": 1.7077020202020203,
      "grad_norm": 0.40081730484962463,
      "learning_rate": 1.1000051884446593e-05,
      "loss": 0.7575,
      "step": 27050
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 0.46078506112098694,
      "learning_rate": 1.0953482978618601e-05,
      "loss": 0.6344,
      "step": 27060
    },
    {
      "epoch": 1.7089646464646466,
      "grad_norm": 0.4096062481403351,
      "learning_rate": 1.0907007144949877e-05,
      "loss": 0.5604,
      "step": 27070
    },
    {
      "epoch": 1.7095959595959596,
      "grad_norm": 0.49467933177948,
      "learning_rate": 1.0860624432017396e-05,
      "loss": 0.5512,
      "step": 27080
    },
    {
      "epoch": 1.7102272727272727,
      "grad_norm": 0.7642691135406494,
      "learning_rate": 1.0814334888300737e-05,
      "loss": 0.5172,
      "step": 27090
    },
    {
      "epoch": 1.7108585858585859,
      "grad_norm": 0.38865646719932556,
      "learning_rate": 1.0768138562182106e-05,
      "loss": 0.8503,
      "step": 27100
    },
    {
      "epoch": 1.711489898989899,
      "grad_norm": 0.4424746334552765,
      "learning_rate": 1.0722035501946282e-05,
      "loss": 0.6353,
      "step": 27110
    },
    {
      "epoch": 1.7121212121212122,
      "grad_norm": 0.43007683753967285,
      "learning_rate": 1.0676025755780595e-05,
      "loss": 0.5641,
      "step": 27120
    },
    {
      "epoch": 1.7127525252525253,
      "grad_norm": 0.4646677076816559,
      "learning_rate": 1.0630109371774798e-05,
      "loss": 0.4896,
      "step": 27130
    },
    {
      "epoch": 1.7133838383838382,
      "grad_norm": 0.708423912525177,
      "learning_rate": 1.0584286397921084e-05,
      "loss": 0.4897,
      "step": 27140
    },
    {
      "epoch": 1.7140151515151514,
      "grad_norm": 0.39722272753715515,
      "learning_rate": 1.0538556882113981e-05,
      "loss": 0.8224,
      "step": 27150
    },
    {
      "epoch": 1.7146464646464645,
      "grad_norm": 0.4384227693080902,
      "learning_rate": 1.049292087215037e-05,
      "loss": 0.677,
      "step": 27160
    },
    {
      "epoch": 1.7152777777777777,
      "grad_norm": 0.4625227749347687,
      "learning_rate": 1.0447378415729414e-05,
      "loss": 0.5637,
      "step": 27170
    },
    {
      "epoch": 1.7159090909090908,
      "grad_norm": 0.501086413860321,
      "learning_rate": 1.0401929560452438e-05,
      "loss": 0.5128,
      "step": 27180
    },
    {
      "epoch": 1.716540404040404,
      "grad_norm": 0.7050685882568359,
      "learning_rate": 1.035657435382298e-05,
      "loss": 0.515,
      "step": 27190
    },
    {
      "epoch": 1.7171717171717171,
      "grad_norm": 0.40623727440834045,
      "learning_rate": 1.0311312843246646e-05,
      "loss": 0.7756,
      "step": 27200
    },
    {
      "epoch": 1.7178030303030303,
      "grad_norm": 0.44095587730407715,
      "learning_rate": 1.0266145076031186e-05,
      "loss": 0.6421,
      "step": 27210
    },
    {
      "epoch": 1.7184343434343434,
      "grad_norm": 0.4739401042461395,
      "learning_rate": 1.0221071099386303e-05,
      "loss": 0.5909,
      "step": 27220
    },
    {
      "epoch": 1.7190656565656566,
      "grad_norm": 0.44237515330314636,
      "learning_rate": 1.0176090960423701e-05,
      "loss": 0.5096,
      "step": 27230
    },
    {
      "epoch": 1.7196969696969697,
      "grad_norm": 0.7131269574165344,
      "learning_rate": 1.0131204706156972e-05,
      "loss": 0.5209,
      "step": 27240
    },
    {
      "epoch": 1.7203282828282829,
      "grad_norm": 0.4122375547885895,
      "learning_rate": 1.0086412383501643e-05,
      "loss": 0.7972,
      "step": 27250
    },
    {
      "epoch": 1.720959595959596,
      "grad_norm": 0.4439793825149536,
      "learning_rate": 1.0041714039274986e-05,
      "loss": 0.6338,
      "step": 27260
    },
    {
      "epoch": 1.7215909090909092,
      "grad_norm": 0.47115376591682434,
      "learning_rate": 9.997109720196097e-06,
      "loss": 0.5842,
      "step": 27270
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 0.4823412597179413,
      "learning_rate": 9.952599472885781e-06,
      "loss": 0.4855,
      "step": 27280
    },
    {
      "epoch": 1.7228535353535355,
      "grad_norm": 0.7672589421272278,
      "learning_rate": 9.908183343866496e-06,
      "loss": 0.4961,
      "step": 27290
    },
    {
      "epoch": 1.7234848484848486,
      "grad_norm": 0.41927438974380493,
      "learning_rate": 9.86386137956239e-06,
      "loss": 0.7786,
      "step": 27300
    },
    {
      "epoch": 1.7241161616161618,
      "grad_norm": 0.45428693294525146,
      "learning_rate": 9.819633626299118e-06,
      "loss": 0.6317,
      "step": 27310
    },
    {
      "epoch": 1.7247474747474747,
      "grad_norm": 0.4636439383029938,
      "learning_rate": 9.775500130303905e-06,
      "loss": 0.5541,
      "step": 27320
    },
    {
      "epoch": 1.7253787878787878,
      "grad_norm": 0.43414437770843506,
      "learning_rate": 9.731460937705428e-06,
      "loss": 0.4932,
      "step": 27330
    },
    {
      "epoch": 1.726010101010101,
      "grad_norm": 0.7235913872718811,
      "learning_rate": 9.687516094533855e-06,
      "loss": 0.5056,
      "step": 27340
    },
    {
      "epoch": 1.7266414141414141,
      "grad_norm": 0.3940243721008301,
      "learning_rate": 9.643665646720678e-06,
      "loss": 0.7819,
      "step": 27350
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.4458872079849243,
      "learning_rate": 9.599909640098748e-06,
      "loss": 0.6451,
      "step": 27360
    },
    {
      "epoch": 1.7279040404040404,
      "grad_norm": 0.46599650382995605,
      "learning_rate": 9.556248120402201e-06,
      "loss": 0.5821,
      "step": 27370
    },
    {
      "epoch": 1.7285353535353534,
      "grad_norm": 0.47776859998703003,
      "learning_rate": 9.512681133266444e-06,
      "loss": 0.4863,
      "step": 27380
    },
    {
      "epoch": 1.7291666666666665,
      "grad_norm": 0.7007083892822266,
      "learning_rate": 9.46920872422804e-06,
      "loss": 0.5093,
      "step": 27390
    },
    {
      "epoch": 1.7297979797979797,
      "grad_norm": 0.391146183013916,
      "learning_rate": 9.425830938724712e-06,
      "loss": 0.7785,
      "step": 27400
    },
    {
      "epoch": 1.7304292929292928,
      "grad_norm": 0.43038177490234375,
      "learning_rate": 9.382547822095278e-06,
      "loss": 0.6675,
      "step": 27410
    },
    {
      "epoch": 1.731060606060606,
      "grad_norm": 0.446469783782959,
      "learning_rate": 9.339359419579607e-06,
      "loss": 0.5616,
      "step": 27420
    },
    {
      "epoch": 1.7316919191919191,
      "grad_norm": 0.5336118340492249,
      "learning_rate": 9.296265776318592e-06,
      "loss": 0.5279,
      "step": 27430
    },
    {
      "epoch": 1.7323232323232323,
      "grad_norm": 0.7300776839256287,
      "learning_rate": 9.253266937354066e-06,
      "loss": 0.5002,
      "step": 27440
    },
    {
      "epoch": 1.7329545454545454,
      "grad_norm": 0.4059823155403137,
      "learning_rate": 9.210362947628759e-06,
      "loss": 0.8134,
      "step": 27450
    },
    {
      "epoch": 1.7335858585858586,
      "grad_norm": 0.42199838161468506,
      "learning_rate": 9.167553851986265e-06,
      "loss": 0.6446,
      "step": 27460
    },
    {
      "epoch": 1.7342171717171717,
      "grad_norm": 0.4366198182106018,
      "learning_rate": 9.124839695171039e-06,
      "loss": 0.5491,
      "step": 27470
    },
    {
      "epoch": 1.7348484848484849,
      "grad_norm": 0.46978646516799927,
      "learning_rate": 9.082220521828266e-06,
      "loss": 0.5131,
      "step": 27480
    },
    {
      "epoch": 1.735479797979798,
      "grad_norm": 0.7273910045623779,
      "learning_rate": 9.03969637650386e-06,
      "loss": 0.5058,
      "step": 27490
    },
    {
      "epoch": 1.7361111111111112,
      "grad_norm": 0.4074365496635437,
      "learning_rate": 8.997267303644385e-06,
      "loss": 0.7784,
      "step": 27500
    },
    {
      "epoch": 1.7367424242424243,
      "grad_norm": 0.4205987751483917,
      "learning_rate": 8.954933347597071e-06,
      "loss": 0.6313,
      "step": 27510
    },
    {
      "epoch": 1.7373737373737375,
      "grad_norm": 0.43709471821784973,
      "learning_rate": 8.912694552609747e-06,
      "loss": 0.5751,
      "step": 27520
    },
    {
      "epoch": 1.7380050505050506,
      "grad_norm": 0.4505341649055481,
      "learning_rate": 8.870550962830738e-06,
      "loss": 0.5121,
      "step": 27530
    },
    {
      "epoch": 1.7386363636363638,
      "grad_norm": 0.6902543902397156,
      "learning_rate": 8.828502622308864e-06,
      "loss": 0.5201,
      "step": 27540
    },
    {
      "epoch": 1.739267676767677,
      "grad_norm": 0.42152678966522217,
      "learning_rate": 8.786549574993385e-06,
      "loss": 0.77,
      "step": 27550
    },
    {
      "epoch": 1.73989898989899,
      "grad_norm": 0.4583221673965454,
      "learning_rate": 8.744691864734023e-06,
      "loss": 0.6339,
      "step": 27560
    },
    {
      "epoch": 1.740530303030303,
      "grad_norm": 0.42416369915008545,
      "learning_rate": 8.702929535280769e-06,
      "loss": 0.5676,
      "step": 27570
    },
    {
      "epoch": 1.7411616161616161,
      "grad_norm": 0.5075649619102478,
      "learning_rate": 8.661262630283962e-06,
      "loss": 0.5116,
      "step": 27580
    },
    {
      "epoch": 1.7417929292929293,
      "grad_norm": 0.7277026772499084,
      "learning_rate": 8.619691193294188e-06,
      "loss": 0.4937,
      "step": 27590
    },
    {
      "epoch": 1.7424242424242424,
      "grad_norm": 0.42006590962409973,
      "learning_rate": 8.578215267762291e-06,
      "loss": 0.7659,
      "step": 27600
    },
    {
      "epoch": 1.7430555555555556,
      "grad_norm": 0.4274682104587555,
      "learning_rate": 8.536834897039225e-06,
      "loss": 0.6619,
      "step": 27610
    },
    {
      "epoch": 1.7436868686868687,
      "grad_norm": 0.4610104560852051,
      "learning_rate": 8.495550124376106e-06,
      "loss": 0.5521,
      "step": 27620
    },
    {
      "epoch": 1.7443181818181817,
      "grad_norm": 0.46762320399284363,
      "learning_rate": 8.454360992924115e-06,
      "loss": 0.4771,
      "step": 27630
    },
    {
      "epoch": 1.7449494949494948,
      "grad_norm": 0.795552670955658,
      "learning_rate": 8.41326754573447e-06,
      "loss": 0.5364,
      "step": 27640
    },
    {
      "epoch": 1.745580808080808,
      "grad_norm": 0.3863941431045532,
      "learning_rate": 8.372269825758405e-06,
      "loss": 0.7519,
      "step": 27650
    },
    {
      "epoch": 1.746212121212121,
      "grad_norm": 0.45170357823371887,
      "learning_rate": 8.331367875847063e-06,
      "loss": 0.654,
      "step": 27660
    },
    {
      "epoch": 1.7468434343434343,
      "grad_norm": 0.43564528226852417,
      "learning_rate": 8.290561738751501e-06,
      "loss": 0.571,
      "step": 27670
    },
    {
      "epoch": 1.7474747474747474,
      "grad_norm": 0.5204358100891113,
      "learning_rate": 8.249851457122626e-06,
      "loss": 0.5149,
      "step": 27680
    },
    {
      "epoch": 1.7481060606060606,
      "grad_norm": 0.6823309659957886,
      "learning_rate": 8.209237073511178e-06,
      "loss": 0.514,
      "step": 27690
    },
    {
      "epoch": 1.7487373737373737,
      "grad_norm": 0.3842424154281616,
      "learning_rate": 8.168718630367655e-06,
      "loss": 0.7925,
      "step": 27700
    },
    {
      "epoch": 1.7493686868686869,
      "grad_norm": 0.46301087737083435,
      "learning_rate": 8.12829617004225e-06,
      "loss": 0.6325,
      "step": 27710
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.4398267865180969,
      "learning_rate": 8.087969734784839e-06,
      "loss": 0.5596,
      "step": 27720
    },
    {
      "epoch": 1.7506313131313131,
      "grad_norm": 0.522499144077301,
      "learning_rate": 8.047739366744989e-06,
      "loss": 0.5184,
      "step": 27730
    },
    {
      "epoch": 1.7512626262626263,
      "grad_norm": 0.7874104976654053,
      "learning_rate": 8.007605107971804e-06,
      "loss": 0.5466,
      "step": 27740
    },
    {
      "epoch": 1.7518939393939394,
      "grad_norm": 0.38817909359931946,
      "learning_rate": 7.967567000413922e-06,
      "loss": 0.7662,
      "step": 27750
    },
    {
      "epoch": 1.7525252525252526,
      "grad_norm": 0.40292325615882874,
      "learning_rate": 7.927625085919533e-06,
      "loss": 0.6187,
      "step": 27760
    },
    {
      "epoch": 1.7531565656565657,
      "grad_norm": 0.46809983253479004,
      "learning_rate": 7.887779406236217e-06,
      "loss": 0.5431,
      "step": 27770
    },
    {
      "epoch": 1.753787878787879,
      "grad_norm": 0.4773653447628021,
      "learning_rate": 7.848030003011064e-06,
      "loss": 0.4774,
      "step": 27780
    },
    {
      "epoch": 1.754419191919192,
      "grad_norm": 0.7203529477119446,
      "learning_rate": 7.80837691779045e-06,
      "loss": 0.4975,
      "step": 27790
    },
    {
      "epoch": 1.7550505050505052,
      "grad_norm": 0.4183722734451294,
      "learning_rate": 7.76882019202011e-06,
      "loss": 0.7602,
      "step": 27800
    },
    {
      "epoch": 1.7556818181818183,
      "grad_norm": 0.4312719702720642,
      "learning_rate": 7.729359867045038e-06,
      "loss": 0.6795,
      "step": 27810
    },
    {
      "epoch": 1.7563131313131313,
      "grad_norm": 0.464599072933197,
      "learning_rate": 7.689995984109555e-06,
      "loss": 0.5489,
      "step": 27820
    },
    {
      "epoch": 1.7569444444444444,
      "grad_norm": 0.43267297744750977,
      "learning_rate": 7.650728584357069e-06,
      "loss": 0.4855,
      "step": 27830
    },
    {
      "epoch": 1.7575757575757576,
      "grad_norm": 0.7586408853530884,
      "learning_rate": 7.611557708830208e-06,
      "loss": 0.5161,
      "step": 27840
    },
    {
      "epoch": 1.7582070707070707,
      "grad_norm": 0.4017736315727234,
      "learning_rate": 7.572483398470709e-06,
      "loss": 0.786,
      "step": 27850
    },
    {
      "epoch": 1.7588383838383839,
      "grad_norm": 0.4341534674167633,
      "learning_rate": 7.5335056941193246e-06,
      "loss": 0.6271,
      "step": 27860
    },
    {
      "epoch": 1.759469696969697,
      "grad_norm": 0.4621761441230774,
      "learning_rate": 7.494624636515924e-06,
      "loss": 0.5587,
      "step": 27870
    },
    {
      "epoch": 1.76010101010101,
      "grad_norm": 0.5046101808547974,
      "learning_rate": 7.4558402662992855e-06,
      "loss": 0.5015,
      "step": 27880
    },
    {
      "epoch": 1.760732323232323,
      "grad_norm": 0.7068613171577454,
      "learning_rate": 7.417152624007162e-06,
      "loss": 0.5653,
      "step": 27890
    },
    {
      "epoch": 1.7613636363636362,
      "grad_norm": 0.390141099691391,
      "learning_rate": 7.378561750076174e-06,
      "loss": 0.7757,
      "step": 27900
    },
    {
      "epoch": 1.7619949494949494,
      "grad_norm": 0.4378039240837097,
      "learning_rate": 7.340067684841878e-06,
      "loss": 0.6474,
      "step": 27910
    },
    {
      "epoch": 1.7626262626262625,
      "grad_norm": 0.45433107018470764,
      "learning_rate": 7.301670468538546e-06,
      "loss": 0.5717,
      "step": 27920
    },
    {
      "epoch": 1.7632575757575757,
      "grad_norm": 0.5165174603462219,
      "learning_rate": 7.263370141299286e-06,
      "loss": 0.4909,
      "step": 27930
    },
    {
      "epoch": 1.7638888888888888,
      "grad_norm": 0.8406737446784973,
      "learning_rate": 7.225166743155886e-06,
      "loss": 0.5179,
      "step": 27940
    },
    {
      "epoch": 1.764520202020202,
      "grad_norm": 0.3750734031200409,
      "learning_rate": 7.187060314038907e-06,
      "loss": 0.7484,
      "step": 27950
    },
    {
      "epoch": 1.7651515151515151,
      "grad_norm": 0.4174499213695526,
      "learning_rate": 7.149050893777476e-06,
      "loss": 0.6322,
      "step": 27960
    },
    {
      "epoch": 1.7657828282828283,
      "grad_norm": 0.39480653405189514,
      "learning_rate": 7.11113852209937e-06,
      "loss": 0.5649,
      "step": 27970
    },
    {
      "epoch": 1.7664141414141414,
      "grad_norm": 0.49001598358154297,
      "learning_rate": 7.073323238630891e-06,
      "loss": 0.4853,
      "step": 27980
    },
    {
      "epoch": 1.7670454545454546,
      "grad_norm": 0.6703867316246033,
      "learning_rate": 7.03560508289689e-06,
      "loss": 0.4735,
      "step": 27990
    },
    {
      "epoch": 1.7676767676767677,
      "grad_norm": 0.40308552980422974,
      "learning_rate": 6.9979840943207196e-06,
      "loss": 0.7775,
      "step": 28000
    },
    {
      "epoch": 1.7676767676767677,
      "eval_loss": 0.6298761367797852,
      "eval_runtime": 31.6917,
      "eval_samples_per_second": 80.778,
      "eval_steps_per_second": 10.097,
      "step": 28000
    },
    {
      "epoch": 1.7683080808080809,
      "grad_norm": 0.44023093581199646,
      "learning_rate": 6.960460312224126e-06,
      "loss": 0.676,
      "step": 28010
    },
    {
      "epoch": 1.768939393939394,
      "grad_norm": 0.43220528960227966,
      "learning_rate": 6.923033775827304e-06,
      "loss": 0.5406,
      "step": 28020
    },
    {
      "epoch": 1.7695707070707072,
      "grad_norm": 0.48493096232414246,
      "learning_rate": 6.885704524248737e-06,
      "loss": 0.4956,
      "step": 28030
    },
    {
      "epoch": 1.7702020202020203,
      "grad_norm": 0.6971887350082397,
      "learning_rate": 6.848472596505329e-06,
      "loss": 0.5199,
      "step": 28040
    },
    {
      "epoch": 1.7708333333333335,
      "grad_norm": 0.3901382386684418,
      "learning_rate": 6.811338031512149e-06,
      "loss": 0.7647,
      "step": 28050
    },
    {
      "epoch": 1.7714646464646466,
      "grad_norm": 0.4410514831542969,
      "learning_rate": 6.774300868082584e-06,
      "loss": 0.6557,
      "step": 28060
    },
    {
      "epoch": 1.7720959595959596,
      "grad_norm": 0.4074196517467499,
      "learning_rate": 6.7373611449281556e-06,
      "loss": 0.5676,
      "step": 28070
    },
    {
      "epoch": 1.7727272727272727,
      "grad_norm": 0.5008469223976135,
      "learning_rate": 6.700518900658592e-06,
      "loss": 0.485,
      "step": 28080
    },
    {
      "epoch": 1.7733585858585859,
      "grad_norm": 0.8258310556411743,
      "learning_rate": 6.663774173781723e-06,
      "loss": 0.5605,
      "step": 28090
    },
    {
      "epoch": 1.773989898989899,
      "grad_norm": 0.3894161283969879,
      "learning_rate": 6.627127002703415e-06,
      "loss": 0.7731,
      "step": 28100
    },
    {
      "epoch": 1.7746212121212122,
      "grad_norm": 0.4235125780105591,
      "learning_rate": 6.590577425727606e-06,
      "loss": 0.6617,
      "step": 28110
    },
    {
      "epoch": 1.7752525252525253,
      "grad_norm": 0.42444008588790894,
      "learning_rate": 6.554125481056206e-06,
      "loss": 0.5642,
      "step": 28120
    },
    {
      "epoch": 1.7758838383838382,
      "grad_norm": 0.4973350763320923,
      "learning_rate": 6.517771206789125e-06,
      "loss": 0.4882,
      "step": 28130
    },
    {
      "epoch": 1.7765151515151514,
      "grad_norm": 0.7314199209213257,
      "learning_rate": 6.481514640924124e-06,
      "loss": 0.5113,
      "step": 28140
    },
    {
      "epoch": 1.7771464646464645,
      "grad_norm": 0.40155598521232605,
      "learning_rate": 6.445355821356868e-06,
      "loss": 0.759,
      "step": 28150
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.4010210633277893,
      "learning_rate": 6.409294785880848e-06,
      "loss": 0.6543,
      "step": 28160
    },
    {
      "epoch": 1.7784090909090908,
      "grad_norm": 0.4470990002155304,
      "learning_rate": 6.373331572187391e-06,
      "loss": 0.5489,
      "step": 28170
    },
    {
      "epoch": 1.779040404040404,
      "grad_norm": 0.49047964811325073,
      "learning_rate": 6.337466217865528e-06,
      "loss": 0.4853,
      "step": 28180
    },
    {
      "epoch": 1.7796717171717171,
      "grad_norm": 0.7245804071426392,
      "learning_rate": 6.3016987604020195e-06,
      "loss": 0.5258,
      "step": 28190
    },
    {
      "epoch": 1.7803030303030303,
      "grad_norm": 0.3677728772163391,
      "learning_rate": 6.266029237181315e-06,
      "loss": 0.7952,
      "step": 28200
    },
    {
      "epoch": 1.7809343434343434,
      "grad_norm": 0.44206756353378296,
      "learning_rate": 6.23045768548548e-06,
      "loss": 0.6556,
      "step": 28210
    },
    {
      "epoch": 1.7815656565656566,
      "grad_norm": 0.43877777457237244,
      "learning_rate": 6.19498414249422e-06,
      "loss": 0.555,
      "step": 28220
    },
    {
      "epoch": 1.7821969696969697,
      "grad_norm": 0.4774627983570099,
      "learning_rate": 6.159608645284776e-06,
      "loss": 0.5125,
      "step": 28230
    },
    {
      "epoch": 1.7828282828282829,
      "grad_norm": 0.6300804615020752,
      "learning_rate": 6.124331230831892e-06,
      "loss": 0.498,
      "step": 28240
    },
    {
      "epoch": 1.783459595959596,
      "grad_norm": 0.3953890800476074,
      "learning_rate": 6.08915193600782e-06,
      "loss": 0.7982,
      "step": 28250
    },
    {
      "epoch": 1.7840909090909092,
      "grad_norm": 0.44279733300209045,
      "learning_rate": 6.054070797582267e-06,
      "loss": 0.6624,
      "step": 28260
    },
    {
      "epoch": 1.7847222222222223,
      "grad_norm": 0.5138589143753052,
      "learning_rate": 6.019087852222316e-06,
      "loss": 0.586,
      "step": 28270
    },
    {
      "epoch": 1.7853535353535355,
      "grad_norm": 0.5030869841575623,
      "learning_rate": 5.98420313649245e-06,
      "loss": 0.4964,
      "step": 28280
    },
    {
      "epoch": 1.7859848484848486,
      "grad_norm": 0.6562145352363586,
      "learning_rate": 5.949416686854426e-06,
      "loss": 0.4784,
      "step": 28290
    },
    {
      "epoch": 1.7866161616161618,
      "grad_norm": 0.3870893120765686,
      "learning_rate": 5.914728539667369e-06,
      "loss": 0.7595,
      "step": 28300
    },
    {
      "epoch": 1.7872474747474747,
      "grad_norm": 0.42969387769699097,
      "learning_rate": 5.880138731187601e-06,
      "loss": 0.6517,
      "step": 28310
    },
    {
      "epoch": 1.7878787878787878,
      "grad_norm": 0.4473240375518799,
      "learning_rate": 5.84564729756868e-06,
      "loss": 0.5556,
      "step": 28320
    },
    {
      "epoch": 1.788510101010101,
      "grad_norm": 0.462800532579422,
      "learning_rate": 5.811254274861344e-06,
      "loss": 0.5106,
      "step": 28330
    },
    {
      "epoch": 1.7891414141414141,
      "grad_norm": 0.8006094694137573,
      "learning_rate": 5.77695969901344e-06,
      "loss": 0.5442,
      "step": 28340
    },
    {
      "epoch": 1.7897727272727273,
      "grad_norm": 0.42142102122306824,
      "learning_rate": 5.742763605869983e-06,
      "loss": 0.7688,
      "step": 28350
    },
    {
      "epoch": 1.7904040404040404,
      "grad_norm": 0.4214666783809662,
      "learning_rate": 5.708666031172993e-06,
      "loss": 0.6488,
      "step": 28360
    },
    {
      "epoch": 1.7910353535353534,
      "grad_norm": 0.47606027126312256,
      "learning_rate": 5.674667010561541e-06,
      "loss": 0.5725,
      "step": 28370
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 0.49835067987442017,
      "learning_rate": 5.640766579571665e-06,
      "loss": 0.5272,
      "step": 28380
    },
    {
      "epoch": 1.7922979797979797,
      "grad_norm": 0.7807926535606384,
      "learning_rate": 5.606964773636425e-06,
      "loss": 0.5137,
      "step": 28390
    },
    {
      "epoch": 1.7929292929292928,
      "grad_norm": 0.40067899227142334,
      "learning_rate": 5.573261628085702e-06,
      "loss": 0.7736,
      "step": 28400
    },
    {
      "epoch": 1.793560606060606,
      "grad_norm": 0.43387919664382935,
      "learning_rate": 5.53965717814634e-06,
      "loss": 0.6404,
      "step": 28410
    },
    {
      "epoch": 1.7941919191919191,
      "grad_norm": 0.4235251247882843,
      "learning_rate": 5.506151458941955e-06,
      "loss": 0.5203,
      "step": 28420
    },
    {
      "epoch": 1.7948232323232323,
      "grad_norm": 0.47538575530052185,
      "learning_rate": 5.472744505493033e-06,
      "loss": 0.4939,
      "step": 28430
    },
    {
      "epoch": 1.7954545454545454,
      "grad_norm": 0.6538649797439575,
      "learning_rate": 5.439436352716776e-06,
      "loss": 0.4866,
      "step": 28440
    },
    {
      "epoch": 1.7960858585858586,
      "grad_norm": 0.39950206875801086,
      "learning_rate": 5.40622703542717e-06,
      "loss": 0.7789,
      "step": 28450
    },
    {
      "epoch": 1.7967171717171717,
      "grad_norm": 0.43670520186424255,
      "learning_rate": 5.373116588334836e-06,
      "loss": 0.6256,
      "step": 28460
    },
    {
      "epoch": 1.7973484848484849,
      "grad_norm": 0.4729555547237396,
      "learning_rate": 5.340105046047095e-06,
      "loss": 0.5603,
      "step": 28470
    },
    {
      "epoch": 1.797979797979798,
      "grad_norm": 0.5064915418624878,
      "learning_rate": 5.30719244306791e-06,
      "loss": 0.4968,
      "step": 28480
    },
    {
      "epoch": 1.7986111111111112,
      "grad_norm": 0.7252433896064758,
      "learning_rate": 5.274378813797798e-06,
      "loss": 0.5026,
      "step": 28490
    },
    {
      "epoch": 1.7992424242424243,
      "grad_norm": 0.406241238117218,
      "learning_rate": 5.241664192533824e-06,
      "loss": 0.7998,
      "step": 28500
    },
    {
      "epoch": 1.7998737373737375,
      "grad_norm": 0.44739019870758057,
      "learning_rate": 5.209048613469569e-06,
      "loss": 0.669,
      "step": 28510
    },
    {
      "epoch": 1.8005050505050506,
      "grad_norm": 0.4401897192001343,
      "learning_rate": 5.176532110695153e-06,
      "loss": 0.5564,
      "step": 28520
    },
    {
      "epoch": 1.8011363636363638,
      "grad_norm": 0.530987024307251,
      "learning_rate": 5.144114718197057e-06,
      "loss": 0.4977,
      "step": 28530
    },
    {
      "epoch": 1.801767676767677,
      "grad_norm": 0.7594923973083496,
      "learning_rate": 5.111796469858232e-06,
      "loss": 0.4842,
      "step": 28540
    },
    {
      "epoch": 1.80239898989899,
      "grad_norm": 0.4112584590911865,
      "learning_rate": 5.079577399457946e-06,
      "loss": 0.7854,
      "step": 28550
    },
    {
      "epoch": 1.803030303030303,
      "grad_norm": 0.45220401883125305,
      "learning_rate": 5.047457540671873e-06,
      "loss": 0.6622,
      "step": 28560
    },
    {
      "epoch": 1.8036616161616161,
      "grad_norm": 0.4624454379081726,
      "learning_rate": 5.015436927071948e-06,
      "loss": 0.5516,
      "step": 28570
    },
    {
      "epoch": 1.8042929292929293,
      "grad_norm": 0.4781900644302368,
      "learning_rate": 4.983515592126375e-06,
      "loss": 0.518,
      "step": 28580
    },
    {
      "epoch": 1.8049242424242424,
      "grad_norm": 0.7916491627693176,
      "learning_rate": 4.9516935691996e-06,
      "loss": 0.5349,
      "step": 28590
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 0.3836854100227356,
      "learning_rate": 4.919970891552261e-06,
      "loss": 0.7665,
      "step": 28600
    },
    {
      "epoch": 1.8061868686868687,
      "grad_norm": 0.4366767406463623,
      "learning_rate": 4.888347592341181e-06,
      "loss": 0.6794,
      "step": 28610
    },
    {
      "epoch": 1.8068181818181817,
      "grad_norm": 0.4222659766674042,
      "learning_rate": 4.856823704619285e-06,
      "loss": 0.5533,
      "step": 28620
    },
    {
      "epoch": 1.8074494949494948,
      "grad_norm": 0.45776355266571045,
      "learning_rate": 4.825399261335595e-06,
      "loss": 0.4702,
      "step": 28630
    },
    {
      "epoch": 1.808080808080808,
      "grad_norm": 0.7788233757019043,
      "learning_rate": 4.794074295335205e-06,
      "loss": 0.5321,
      "step": 28640
    },
    {
      "epoch": 1.808712121212121,
      "grad_norm": 0.3865162134170532,
      "learning_rate": 4.762848839359235e-06,
      "loss": 0.8104,
      "step": 28650
    },
    {
      "epoch": 1.8093434343434343,
      "grad_norm": 0.4451378285884857,
      "learning_rate": 4.7317229260447906e-06,
      "loss": 0.6593,
      "step": 28660
    },
    {
      "epoch": 1.8099747474747474,
      "grad_norm": 0.4181601107120514,
      "learning_rate": 4.700696587924936e-06,
      "loss": 0.5865,
      "step": 28670
    },
    {
      "epoch": 1.8106060606060606,
      "grad_norm": 0.4753917455673218,
      "learning_rate": 4.669769857428652e-06,
      "loss": 0.5064,
      "step": 28680
    },
    {
      "epoch": 1.8112373737373737,
      "grad_norm": 0.7149508595466614,
      "learning_rate": 4.638942766880805e-06,
      "loss": 0.4951,
      "step": 28690
    },
    {
      "epoch": 1.8118686868686869,
      "grad_norm": 0.41488030552864075,
      "learning_rate": 4.608215348502154e-06,
      "loss": 0.7653,
      "step": 28700
    },
    {
      "epoch": 1.8125,
      "grad_norm": 0.4523298144340515,
      "learning_rate": 4.577587634409219e-06,
      "loss": 0.6511,
      "step": 28710
    },
    {
      "epoch": 1.8131313131313131,
      "grad_norm": 0.49552062153816223,
      "learning_rate": 4.547059656614372e-06,
      "loss": 0.5541,
      "step": 28720
    },
    {
      "epoch": 1.8137626262626263,
      "grad_norm": 0.46407806873321533,
      "learning_rate": 4.5166314470256765e-06,
      "loss": 0.4854,
      "step": 28730
    },
    {
      "epoch": 1.8143939393939394,
      "grad_norm": 0.6594081521034241,
      "learning_rate": 4.4863030374469814e-06,
      "loss": 0.5227,
      "step": 28740
    },
    {
      "epoch": 1.8150252525252526,
      "grad_norm": 0.43310752511024475,
      "learning_rate": 4.456074459577775e-06,
      "loss": 0.762,
      "step": 28750
    },
    {
      "epoch": 1.8156565656565657,
      "grad_norm": 0.4614546000957489,
      "learning_rate": 4.425945745013227e-06,
      "loss": 0.665,
      "step": 28760
    },
    {
      "epoch": 1.816287878787879,
      "grad_norm": 0.4387434720993042,
      "learning_rate": 4.395916925244104e-06,
      "loss": 0.5823,
      "step": 28770
    },
    {
      "epoch": 1.816919191919192,
      "grad_norm": 0.47464632987976074,
      "learning_rate": 4.365988031656798e-06,
      "loss": 0.4981,
      "step": 28780
    },
    {
      "epoch": 1.8175505050505052,
      "grad_norm": 0.7266677618026733,
      "learning_rate": 4.336159095533221e-06,
      "loss": 0.5159,
      "step": 28790
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.4033709168434143,
      "learning_rate": 4.306430148050844e-06,
      "loss": 0.7668,
      "step": 28800
    },
    {
      "epoch": 1.8188131313131313,
      "grad_norm": 0.4734260141849518,
      "learning_rate": 4.276801220282589e-06,
      "loss": 0.6493,
      "step": 28810
    },
    {
      "epoch": 1.8194444444444444,
      "grad_norm": 0.46054303646087646,
      "learning_rate": 4.247272343196851e-06,
      "loss": 0.5779,
      "step": 28820
    },
    {
      "epoch": 1.8200757575757576,
      "grad_norm": 0.4761173725128174,
      "learning_rate": 4.217843547657496e-06,
      "loss": 0.5196,
      "step": 28830
    },
    {
      "epoch": 1.8207070707070707,
      "grad_norm": 0.801598846912384,
      "learning_rate": 4.188514864423709e-06,
      "loss": 0.5326,
      "step": 28840
    },
    {
      "epoch": 1.8213383838383839,
      "grad_norm": 0.3935687243938446,
      "learning_rate": 4.159286324150091e-06,
      "loss": 0.7762,
      "step": 28850
    },
    {
      "epoch": 1.821969696969697,
      "grad_norm": 0.4380428194999695,
      "learning_rate": 4.130157957386549e-06,
      "loss": 0.6454,
      "step": 28860
    },
    {
      "epoch": 1.82260101010101,
      "grad_norm": 0.4899478554725647,
      "learning_rate": 4.1011297945782955e-06,
      "loss": 0.5733,
      "step": 28870
    },
    {
      "epoch": 1.823232323232323,
      "grad_norm": 0.47481539845466614,
      "learning_rate": 4.0722018660658055e-06,
      "loss": 0.4789,
      "step": 28880
    },
    {
      "epoch": 1.8238636363636362,
      "grad_norm": 0.6979573369026184,
      "learning_rate": 4.0433742020848044e-06,
      "loss": 0.5014,
      "step": 28890
    },
    {
      "epoch": 1.8244949494949494,
      "grad_norm": 0.39386940002441406,
      "learning_rate": 4.014646832766167e-06,
      "loss": 0.7503,
      "step": 28900
    },
    {
      "epoch": 1.8251262626262625,
      "grad_norm": 0.4166184067726135,
      "learning_rate": 3.986019788136031e-06,
      "loss": 0.6559,
      "step": 28910
    },
    {
      "epoch": 1.8257575757575757,
      "grad_norm": 0.48478081822395325,
      "learning_rate": 3.9574930981155835e-06,
      "loss": 0.6018,
      "step": 28920
    },
    {
      "epoch": 1.8263888888888888,
      "grad_norm": 0.44756656885147095,
      "learning_rate": 3.929066792521174e-06,
      "loss": 0.4961,
      "step": 28930
    },
    {
      "epoch": 1.827020202020202,
      "grad_norm": 0.7394233345985413,
      "learning_rate": 3.900740901064215e-06,
      "loss": 0.52,
      "step": 28940
    },
    {
      "epoch": 1.8276515151515151,
      "grad_norm": 0.41106539964675903,
      "learning_rate": 3.872515453351133e-06,
      "loss": 0.7645,
      "step": 28950
    },
    {
      "epoch": 1.8282828282828283,
      "grad_norm": 0.45989060401916504,
      "learning_rate": 3.844390478883453e-06,
      "loss": 0.6469,
      "step": 28960
    },
    {
      "epoch": 1.8289141414141414,
      "grad_norm": 0.4680406153202057,
      "learning_rate": 3.816366007057593e-06,
      "loss": 0.5617,
      "step": 28970
    },
    {
      "epoch": 1.8295454545454546,
      "grad_norm": 0.4798475503921509,
      "learning_rate": 3.788442067164977e-06,
      "loss": 0.5167,
      "step": 28980
    },
    {
      "epoch": 1.8301767676767677,
      "grad_norm": 0.7564020156860352,
      "learning_rate": 3.760618688391926e-06,
      "loss": 0.5285,
      "step": 28990
    },
    {
      "epoch": 1.8308080808080809,
      "grad_norm": 0.3695949912071228,
      "learning_rate": 3.732895899819677e-06,
      "loss": 0.7687,
      "step": 29000
    },
    {
      "epoch": 1.8308080808080809,
      "eval_loss": 0.6294780969619751,
      "eval_runtime": 31.9261,
      "eval_samples_per_second": 80.185,
      "eval_steps_per_second": 10.023,
      "step": 29000
    },
    {
      "epoch": 1.831439393939394,
      "grad_norm": 0.42488041520118713,
      "learning_rate": 3.70527373042433e-06,
      "loss": 0.6363,
      "step": 29010
    },
    {
      "epoch": 1.8320707070707072,
      "grad_norm": 0.46133852005004883,
      "learning_rate": 3.6777522090768013e-06,
      "loss": 0.5527,
      "step": 29020
    },
    {
      "epoch": 1.8327020202020203,
      "grad_norm": 0.509753406047821,
      "learning_rate": 3.6503313645427917e-06,
      "loss": 0.5012,
      "step": 29030
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.695665717124939,
      "learning_rate": 3.6230112254828086e-06,
      "loss": 0.5076,
      "step": 29040
    },
    {
      "epoch": 1.8339646464646466,
      "grad_norm": 0.42504647374153137,
      "learning_rate": 3.59579182045211e-06,
      "loss": 0.7832,
      "step": 29050
    },
    {
      "epoch": 1.8345959595959596,
      "grad_norm": 0.4906473159790039,
      "learning_rate": 3.5686731779006165e-06,
      "loss": 0.6398,
      "step": 29060
    },
    {
      "epoch": 1.8352272727272727,
      "grad_norm": 0.41030368208885193,
      "learning_rate": 3.5416553261729657e-06,
      "loss": 0.5348,
      "step": 29070
    },
    {
      "epoch": 1.8358585858585859,
      "grad_norm": 0.4580545723438263,
      "learning_rate": 3.514738293508435e-06,
      "loss": 0.4952,
      "step": 29080
    },
    {
      "epoch": 1.836489898989899,
      "grad_norm": 0.6867746710777283,
      "learning_rate": 3.487922108040942e-06,
      "loss": 0.4889,
      "step": 29090
    },
    {
      "epoch": 1.8371212121212122,
      "grad_norm": 0.43884244561195374,
      "learning_rate": 3.461206797798988e-06,
      "loss": 0.821,
      "step": 29100
    },
    {
      "epoch": 1.8377525252525253,
      "grad_norm": 0.4304676055908203,
      "learning_rate": 3.4345923907056266e-06,
      "loss": 0.6296,
      "step": 29110
    },
    {
      "epoch": 1.8383838383838382,
      "grad_norm": 0.4617955684661865,
      "learning_rate": 3.408078914578461e-06,
      "loss": 0.5543,
      "step": 29120
    },
    {
      "epoch": 1.8390151515151514,
      "grad_norm": 0.5077157616615295,
      "learning_rate": 3.3816663971296013e-06,
      "loss": 0.5139,
      "step": 29130
    },
    {
      "epoch": 1.8396464646464645,
      "grad_norm": 0.7904893159866333,
      "learning_rate": 3.3553548659656207e-06,
      "loss": 0.5244,
      "step": 29140
    },
    {
      "epoch": 1.8402777777777777,
      "grad_norm": 0.419447660446167,
      "learning_rate": 3.329144348587565e-06,
      "loss": 0.7754,
      "step": 29150
    },
    {
      "epoch": 1.8409090909090908,
      "grad_norm": 0.44490954279899597,
      "learning_rate": 3.3030348723908755e-06,
      "loss": 0.6573,
      "step": 29160
    },
    {
      "epoch": 1.841540404040404,
      "grad_norm": 0.45898279547691345,
      "learning_rate": 3.2770264646653783e-06,
      "loss": 0.566,
      "step": 29170
    },
    {
      "epoch": 1.8421717171717171,
      "grad_norm": 0.457150399684906,
      "learning_rate": 3.2511191525953055e-06,
      "loss": 0.5032,
      "step": 29180
    },
    {
      "epoch": 1.8428030303030303,
      "grad_norm": 0.7265349626541138,
      "learning_rate": 3.225312963259186e-06,
      "loss": 0.5103,
      "step": 29190
    },
    {
      "epoch": 1.8434343434343434,
      "grad_norm": 0.3974480926990509,
      "learning_rate": 3.199607923629855e-06,
      "loss": 0.8219,
      "step": 29200
    },
    {
      "epoch": 1.8440656565656566,
      "grad_norm": 0.45630425214767456,
      "learning_rate": 3.1740040605744204e-06,
      "loss": 0.6496,
      "step": 29210
    },
    {
      "epoch": 1.8446969696969697,
      "grad_norm": 0.4365781843662262,
      "learning_rate": 3.148501400854287e-06,
      "loss": 0.5634,
      "step": 29220
    },
    {
      "epoch": 1.8453282828282829,
      "grad_norm": 0.4414352774620056,
      "learning_rate": 3.123099971125032e-06,
      "loss": 0.5106,
      "step": 29230
    },
    {
      "epoch": 1.845959595959596,
      "grad_norm": 0.7539675831794739,
      "learning_rate": 3.0977997979364405e-06,
      "loss": 0.5241,
      "step": 29240
    },
    {
      "epoch": 1.8465909090909092,
      "grad_norm": 0.41672787070274353,
      "learning_rate": 3.072600907732448e-06,
      "loss": 0.7764,
      "step": 29250
    },
    {
      "epoch": 1.8472222222222223,
      "grad_norm": 0.4143024981021881,
      "learning_rate": 3.0475033268511644e-06,
      "loss": 0.6377,
      "step": 29260
    },
    {
      "epoch": 1.8478535353535355,
      "grad_norm": 0.48463237285614014,
      "learning_rate": 3.0225070815247835e-06,
      "loss": 0.5862,
      "step": 29270
    },
    {
      "epoch": 1.8484848484848486,
      "grad_norm": 0.49210500717163086,
      "learning_rate": 2.9976121978795844e-06,
      "loss": 0.5089,
      "step": 29280
    },
    {
      "epoch": 1.8491161616161618,
      "grad_norm": 0.764843225479126,
      "learning_rate": 2.97281870193592e-06,
      "loss": 0.5327,
      "step": 29290
    },
    {
      "epoch": 1.8497474747474747,
      "grad_norm": 0.3943166732788086,
      "learning_rate": 2.9481266196081268e-06,
      "loss": 0.8088,
      "step": 29300
    },
    {
      "epoch": 1.8503787878787878,
      "grad_norm": 0.41932541131973267,
      "learning_rate": 2.9235359767046167e-06,
      "loss": 0.6444,
      "step": 29310
    },
    {
      "epoch": 1.851010101010101,
      "grad_norm": 0.46930429339408875,
      "learning_rate": 2.899046798927707e-06,
      "loss": 0.5464,
      "step": 29320
    },
    {
      "epoch": 1.8516414141414141,
      "grad_norm": 0.45703044533729553,
      "learning_rate": 2.8746591118736897e-06,
      "loss": 0.4936,
      "step": 29330
    },
    {
      "epoch": 1.8522727272727273,
      "grad_norm": 0.7961979508399963,
      "learning_rate": 2.850372941032775e-06,
      "loss": 0.5186,
      "step": 29340
    },
    {
      "epoch": 1.8529040404040404,
      "grad_norm": 0.4272969663143158,
      "learning_rate": 2.826188311789102e-06,
      "loss": 0.7769,
      "step": 29350
    },
    {
      "epoch": 1.8535353535353534,
      "grad_norm": 0.42390429973602295,
      "learning_rate": 2.8021052494206058e-06,
      "loss": 0.6411,
      "step": 29360
    },
    {
      "epoch": 1.8541666666666665,
      "grad_norm": 0.44652530550956726,
      "learning_rate": 2.778123779099129e-06,
      "loss": 0.5778,
      "step": 29370
    },
    {
      "epoch": 1.8547979797979797,
      "grad_norm": 0.4835679531097412,
      "learning_rate": 2.7542439258902984e-06,
      "loss": 0.4928,
      "step": 29380
    },
    {
      "epoch": 1.8554292929292928,
      "grad_norm": 0.6726338267326355,
      "learning_rate": 2.730465714753516e-06,
      "loss": 0.5104,
      "step": 29390
    },
    {
      "epoch": 1.856060606060606,
      "grad_norm": 0.37321093678474426,
      "learning_rate": 2.70678917054199e-06,
      "loss": 0.791,
      "step": 29400
    },
    {
      "epoch": 1.8566919191919191,
      "grad_norm": 0.4406667649745941,
      "learning_rate": 2.683214318002636e-06,
      "loss": 0.6495,
      "step": 29410
    },
    {
      "epoch": 1.8573232323232323,
      "grad_norm": 0.44589632749557495,
      "learning_rate": 2.6597411817760765e-06,
      "loss": 0.5602,
      "step": 29420
    },
    {
      "epoch": 1.8579545454545454,
      "grad_norm": 0.4969729483127594,
      "learning_rate": 2.636369786396631e-06,
      "loss": 0.5005,
      "step": 29430
    },
    {
      "epoch": 1.8585858585858586,
      "grad_norm": 0.8307119011878967,
      "learning_rate": 2.6131001562922918e-06,
      "loss": 0.5061,
      "step": 29440
    },
    {
      "epoch": 1.8592171717171717,
      "grad_norm": 0.387922465801239,
      "learning_rate": 2.589932315784649e-06,
      "loss": 0.7822,
      "step": 29450
    },
    {
      "epoch": 1.8598484848484849,
      "grad_norm": 0.4151994585990906,
      "learning_rate": 2.5668662890889316e-06,
      "loss": 0.6493,
      "step": 29460
    },
    {
      "epoch": 1.860479797979798,
      "grad_norm": 0.4248330891132355,
      "learning_rate": 2.5439021003139328e-06,
      "loss": 0.5676,
      "step": 29470
    },
    {
      "epoch": 1.8611111111111112,
      "grad_norm": 0.44664329290390015,
      "learning_rate": 2.5210397734620305e-06,
      "loss": 0.4658,
      "step": 29480
    },
    {
      "epoch": 1.8617424242424243,
      "grad_norm": 0.643934965133667,
      "learning_rate": 2.4982793324291097e-06,
      "loss": 0.5127,
      "step": 29490
    },
    {
      "epoch": 1.8623737373737375,
      "grad_norm": 0.4048312306404114,
      "learning_rate": 2.4756208010045634e-06,
      "loss": 0.809,
      "step": 29500
    },
    {
      "epoch": 1.8630050505050506,
      "grad_norm": 0.42173245549201965,
      "learning_rate": 2.4530642028712914e-06,
      "loss": 0.6414,
      "step": 29510
    },
    {
      "epoch": 1.8636363636363638,
      "grad_norm": 0.4683539569377899,
      "learning_rate": 2.4306095616056234e-06,
      "loss": 0.5689,
      "step": 29520
    },
    {
      "epoch": 1.864267676767677,
      "grad_norm": 0.5014246106147766,
      "learning_rate": 2.4082569006773526e-06,
      "loss": 0.5187,
      "step": 29530
    },
    {
      "epoch": 1.86489898989899,
      "grad_norm": 0.6608459949493408,
      "learning_rate": 2.3860062434496678e-06,
      "loss": 0.5142,
      "step": 29540
    },
    {
      "epoch": 1.865530303030303,
      "grad_norm": 0.4031886160373688,
      "learning_rate": 2.3638576131791214e-06,
      "loss": 0.7707,
      "step": 29550
    },
    {
      "epoch": 1.8661616161616161,
      "grad_norm": 0.4350784718990326,
      "learning_rate": 2.3418110330156505e-06,
      "loss": 0.6489,
      "step": 29560
    },
    {
      "epoch": 1.8667929292929293,
      "grad_norm": 0.48146748542785645,
      "learning_rate": 2.3198665260025342e-06,
      "loss": 0.5547,
      "step": 29570
    },
    {
      "epoch": 1.8674242424242424,
      "grad_norm": 0.5062400698661804,
      "learning_rate": 2.298024115076347e-06,
      "loss": 0.5491,
      "step": 29580
    },
    {
      "epoch": 1.8680555555555556,
      "grad_norm": 0.6811593174934387,
      "learning_rate": 2.2762838230669715e-06,
      "loss": 0.521,
      "step": 29590
    },
    {
      "epoch": 1.8686868686868687,
      "grad_norm": 0.40277358889579773,
      "learning_rate": 2.2546456726975084e-06,
      "loss": 0.7848,
      "step": 29600
    },
    {
      "epoch": 1.8693181818181817,
      "grad_norm": 0.45602935552597046,
      "learning_rate": 2.2331096865843672e-06,
      "loss": 0.6237,
      "step": 29610
    },
    {
      "epoch": 1.8699494949494948,
      "grad_norm": 0.4286012649536133,
      "learning_rate": 2.21167588723713e-06,
      "loss": 0.5401,
      "step": 29620
    },
    {
      "epoch": 1.870580808080808,
      "grad_norm": 0.4590153396129608,
      "learning_rate": 2.1903442970585665e-06,
      "loss": 0.4975,
      "step": 29630
    },
    {
      "epoch": 1.871212121212121,
      "grad_norm": 0.6932566165924072,
      "learning_rate": 2.1691149383446517e-06,
      "loss": 0.5062,
      "step": 29640
    },
    {
      "epoch": 1.8718434343434343,
      "grad_norm": 0.3985643982887268,
      "learning_rate": 2.1479878332844703e-06,
      "loss": 0.7869,
      "step": 29650
    },
    {
      "epoch": 1.8724747474747474,
      "grad_norm": 0.430909126996994,
      "learning_rate": 2.1269630039602582e-06,
      "loss": 0.6426,
      "step": 29660
    },
    {
      "epoch": 1.8731060606060606,
      "grad_norm": 0.42620399594306946,
      "learning_rate": 2.106040472347348e-06,
      "loss": 0.5912,
      "step": 29670
    },
    {
      "epoch": 1.8737373737373737,
      "grad_norm": 0.44919443130493164,
      "learning_rate": 2.085220260314136e-06,
      "loss": 0.488,
      "step": 29680
    },
    {
      "epoch": 1.8743686868686869,
      "grad_norm": 0.6751399636268616,
      "learning_rate": 2.0645023896220694e-06,
      "loss": 0.5136,
      "step": 29690
    },
    {
      "epoch": 1.875,
      "grad_norm": 0.43590790033340454,
      "learning_rate": 2.0438868819256607e-06,
      "loss": 0.7757,
      "step": 29700
    },
    {
      "epoch": 1.8756313131313131,
      "grad_norm": 0.41690653562545776,
      "learning_rate": 2.0233737587723957e-06,
      "loss": 0.6541,
      "step": 29710
    },
    {
      "epoch": 1.8762626262626263,
      "grad_norm": 0.4637727439403534,
      "learning_rate": 2.002963041602768e-06,
      "loss": 0.5806,
      "step": 29720
    },
    {
      "epoch": 1.8768939393939394,
      "grad_norm": 0.452752947807312,
      "learning_rate": 1.9826547517502124e-06,
      "loss": 0.4931,
      "step": 29730
    },
    {
      "epoch": 1.8775252525252526,
      "grad_norm": 0.7426528930664062,
      "learning_rate": 1.9624489104411502e-06,
      "loss": 0.5139,
      "step": 29740
    },
    {
      "epoch": 1.8781565656565657,
      "grad_norm": 0.4323101043701172,
      "learning_rate": 1.9423455387948653e-06,
      "loss": 0.8307,
      "step": 29750
    },
    {
      "epoch": 1.878787878787879,
      "grad_norm": 0.3905579745769501,
      "learning_rate": 1.9223446578235937e-06,
      "loss": 0.6442,
      "step": 29760
    },
    {
      "epoch": 1.879419191919192,
      "grad_norm": 0.4863211214542389,
      "learning_rate": 1.9024462884324135e-06,
      "loss": 0.5723,
      "step": 29770
    },
    {
      "epoch": 1.8800505050505052,
      "grad_norm": 0.44645950198173523,
      "learning_rate": 1.8826504514192543e-06,
      "loss": 0.4837,
      "step": 29780
    },
    {
      "epoch": 1.8806818181818183,
      "grad_norm": 0.6721382737159729,
      "learning_rate": 1.8629571674748991e-06,
      "loss": 0.5108,
      "step": 29790
    },
    {
      "epoch": 1.8813131313131313,
      "grad_norm": 0.39879903197288513,
      "learning_rate": 1.843366457182938e-06,
      "loss": 0.8278,
      "step": 29800
    },
    {
      "epoch": 1.8819444444444444,
      "grad_norm": 0.4080144464969635,
      "learning_rate": 1.8238783410197358e-06,
      "loss": 0.6303,
      "step": 29810
    },
    {
      "epoch": 1.8825757575757576,
      "grad_norm": 0.4638393223285675,
      "learning_rate": 1.8044928393544325e-06,
      "loss": 0.5804,
      "step": 29820
    },
    {
      "epoch": 1.8832070707070707,
      "grad_norm": 0.48918503522872925,
      "learning_rate": 1.78520997244892e-06,
      "loss": 0.4975,
      "step": 29830
    },
    {
      "epoch": 1.8838383838383839,
      "grad_norm": 0.7266528010368347,
      "learning_rate": 1.7660297604578102e-06,
      "loss": 0.496,
      "step": 29840
    },
    {
      "epoch": 1.884469696969697,
      "grad_norm": 0.3864036202430725,
      "learning_rate": 1.7469522234284108e-06,
      "loss": 0.7898,
      "step": 29850
    },
    {
      "epoch": 1.88510101010101,
      "grad_norm": 0.44709569215774536,
      "learning_rate": 1.7279773813007382e-06,
      "loss": 0.6329,
      "step": 29860
    },
    {
      "epoch": 1.885732323232323,
      "grad_norm": 0.4919479787349701,
      "learning_rate": 1.7091052539074393e-06,
      "loss": 0.574,
      "step": 29870
    },
    {
      "epoch": 1.8863636363636362,
      "grad_norm": 0.47638675570487976,
      "learning_rate": 1.690335860973835e-06,
      "loss": 0.5103,
      "step": 29880
    },
    {
      "epoch": 1.8869949494949494,
      "grad_norm": 0.7733305096626282,
      "learning_rate": 1.6716692221178443e-06,
      "loss": 0.5359,
      "step": 29890
    },
    {
      "epoch": 1.8876262626262625,
      "grad_norm": 0.4178636968135834,
      "learning_rate": 1.6531053568499822e-06,
      "loss": 0.8253,
      "step": 29900
    },
    {
      "epoch": 1.8882575757575757,
      "grad_norm": 0.459340900182724,
      "learning_rate": 1.6346442845733612e-06,
      "loss": 0.6688,
      "step": 29910
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.46522530913352966,
      "learning_rate": 1.6162860245836576e-06,
      "loss": 0.5412,
      "step": 29920
    },
    {
      "epoch": 1.889520202020202,
      "grad_norm": 0.5221881866455078,
      "learning_rate": 1.5980305960690667e-06,
      "loss": 0.4751,
      "step": 29930
    },
    {
      "epoch": 1.8901515151515151,
      "grad_norm": 0.7222245931625366,
      "learning_rate": 1.5798780181103256e-06,
      "loss": 0.536,
      "step": 29940
    },
    {
      "epoch": 1.8907828282828283,
      "grad_norm": 0.4171253740787506,
      "learning_rate": 1.5618283096806351e-06,
      "loss": 0.7361,
      "step": 29950
    },
    {
      "epoch": 1.8914141414141414,
      "grad_norm": 0.4673137664794922,
      "learning_rate": 1.5438814896457155e-06,
      "loss": 0.6292,
      "step": 29960
    },
    {
      "epoch": 1.8920454545454546,
      "grad_norm": 0.3885399401187897,
      "learning_rate": 1.5260375767637504e-06,
      "loss": 0.5569,
      "step": 29970
    },
    {
      "epoch": 1.8926767676767677,
      "grad_norm": 0.47550633549690247,
      "learning_rate": 1.5082965896853096e-06,
      "loss": 0.5316,
      "step": 29980
    },
    {
      "epoch": 1.8933080808080809,
      "grad_norm": 0.8042647838592529,
      "learning_rate": 1.4906585469534495e-06,
      "loss": 0.5378,
      "step": 29990
    },
    {
      "epoch": 1.893939393939394,
      "grad_norm": 0.4180283844470978,
      "learning_rate": 1.4731234670035787e-06,
      "loss": 0.7943,
      "step": 30000
    },
    {
      "epoch": 1.893939393939394,
      "eval_loss": 0.6314722895622253,
      "eval_runtime": 31.7782,
      "eval_samples_per_second": 80.558,
      "eval_steps_per_second": 10.07,
      "step": 30000
    },
    {
      "epoch": 1.8945707070707072,
      "grad_norm": 0.40559959411621094,
      "learning_rate": 1.4556913681635253e-06,
      "loss": 0.6386,
      "step": 30010
    },
    {
      "epoch": 1.8952020202020203,
      "grad_norm": 0.4526718556880951,
      "learning_rate": 1.4383622686534482e-06,
      "loss": 0.5391,
      "step": 30020
    },
    {
      "epoch": 1.8958333333333335,
      "grad_norm": 0.4718783497810364,
      "learning_rate": 1.4211361865858808e-06,
      "loss": 0.4964,
      "step": 30030
    },
    {
      "epoch": 1.8964646464646466,
      "grad_norm": 0.7457193732261658,
      "learning_rate": 1.4040131399656541e-06,
      "loss": 0.5062,
      "step": 30040
    },
    {
      "epoch": 1.8970959595959596,
      "grad_norm": 0.3975883424282074,
      "learning_rate": 1.3869931466899299e-06,
      "loss": 0.7897,
      "step": 30050
    },
    {
      "epoch": 1.8977272727272727,
      "grad_norm": 0.43560877442359924,
      "learning_rate": 1.3700762245481446e-06,
      "loss": 0.6652,
      "step": 30060
    },
    {
      "epoch": 1.8983585858585859,
      "grad_norm": 0.438490629196167,
      "learning_rate": 1.3532623912219987e-06,
      "loss": 0.5454,
      "step": 30070
    },
    {
      "epoch": 1.898989898989899,
      "grad_norm": 0.4728802442550659,
      "learning_rate": 1.336551664285446e-06,
      "loss": 0.499,
      "step": 30080
    },
    {
      "epoch": 1.8996212121212122,
      "grad_norm": 0.748324990272522,
      "learning_rate": 1.3199440612047032e-06,
      "loss": 0.5166,
      "step": 30090
    },
    {
      "epoch": 1.9002525252525253,
      "grad_norm": 0.41088083386421204,
      "learning_rate": 1.3034395993381521e-06,
      "loss": 0.7639,
      "step": 30100
    },
    {
      "epoch": 1.9008838383838382,
      "grad_norm": 0.44066300988197327,
      "learning_rate": 1.2870382959363937e-06,
      "loss": 0.6621,
      "step": 30110
    },
    {
      "epoch": 1.9015151515151514,
      "grad_norm": 0.4411035478115082,
      "learning_rate": 1.270740168142215e-06,
      "loss": 0.5348,
      "step": 30120
    },
    {
      "epoch": 1.9021464646464645,
      "grad_norm": 0.4594660997390747,
      "learning_rate": 1.2545452329905449e-06,
      "loss": 0.5221,
      "step": 30130
    },
    {
      "epoch": 1.9027777777777777,
      "grad_norm": 0.7619085311889648,
      "learning_rate": 1.2384535074084769e-06,
      "loss": 0.524,
      "step": 30140
    },
    {
      "epoch": 1.9034090909090908,
      "grad_norm": 0.4032154977321625,
      "learning_rate": 1.222465008215201e-06,
      "loss": 0.7519,
      "step": 30150
    },
    {
      "epoch": 1.904040404040404,
      "grad_norm": 0.4223882853984833,
      "learning_rate": 1.20657975212205e-06,
      "loss": 0.6383,
      "step": 30160
    },
    {
      "epoch": 1.9046717171717171,
      "grad_norm": 0.4734070599079132,
      "learning_rate": 1.190797755732398e-06,
      "loss": 0.5467,
      "step": 30170
    },
    {
      "epoch": 1.9053030303030303,
      "grad_norm": 0.5212306380271912,
      "learning_rate": 1.1751190355417497e-06,
      "loss": 0.476,
      "step": 30180
    },
    {
      "epoch": 1.9059343434343434,
      "grad_norm": 0.7553642392158508,
      "learning_rate": 1.1595436079376076e-06,
      "loss": 0.5339,
      "step": 30190
    },
    {
      "epoch": 1.9065656565656566,
      "grad_norm": 0.4091821014881134,
      "learning_rate": 1.1440714891995608e-06,
      "loss": 0.7974,
      "step": 30200
    },
    {
      "epoch": 1.9071969696969697,
      "grad_norm": 0.42306020855903625,
      "learning_rate": 1.128702695499173e-06,
      "loss": 0.6442,
      "step": 30210
    },
    {
      "epoch": 1.9078282828282829,
      "grad_norm": 0.4136579930782318,
      "learning_rate": 1.1134372429000506e-06,
      "loss": 0.5578,
      "step": 30220
    },
    {
      "epoch": 1.908459595959596,
      "grad_norm": 0.5210898518562317,
      "learning_rate": 1.0982751473577635e-06,
      "loss": 0.5097,
      "step": 30230
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 0.6559984087944031,
      "learning_rate": 1.0832164247198685e-06,
      "loss": 0.4948,
      "step": 30240
    },
    {
      "epoch": 1.9097222222222223,
      "grad_norm": 0.3896995484828949,
      "learning_rate": 1.068261090725864e-06,
      "loss": 0.8508,
      "step": 30250
    },
    {
      "epoch": 1.9103535353535355,
      "grad_norm": 0.4283027946949005,
      "learning_rate": 1.0534091610071795e-06,
      "loss": 0.6476,
      "step": 30260
    },
    {
      "epoch": 1.9109848484848486,
      "grad_norm": 0.4501132369041443,
      "learning_rate": 1.0386606510871976e-06,
      "loss": 0.5321,
      "step": 30270
    },
    {
      "epoch": 1.9116161616161618,
      "grad_norm": 0.5101898312568665,
      "learning_rate": 1.0240155763811655e-06,
      "loss": 0.499,
      "step": 30280
    },
    {
      "epoch": 1.9122474747474747,
      "grad_norm": 0.6676247715950012,
      "learning_rate": 1.0094739521962383e-06,
      "loss": 0.4973,
      "step": 30290
    },
    {
      "epoch": 1.9128787878787878,
      "grad_norm": 0.4074562191963196,
      "learning_rate": 9.950357937314357e-07,
      "loss": 0.771,
      "step": 30300
    },
    {
      "epoch": 1.913510101010101,
      "grad_norm": 0.4174118936061859,
      "learning_rate": 9.807011160776537e-07,
      "loss": 0.6342,
      "step": 30310
    },
    {
      "epoch": 1.9141414141414141,
      "grad_norm": 0.45577168464660645,
      "learning_rate": 9.664699342176286e-07,
      "loss": 0.5674,
      "step": 30320
    },
    {
      "epoch": 1.9147727272727273,
      "grad_norm": 0.5178308486938477,
      "learning_rate": 9.523422630258849e-07,
      "loss": 0.5021,
      "step": 30330
    },
    {
      "epoch": 1.9154040404040404,
      "grad_norm": 0.7210653424263,
      "learning_rate": 9.383181172687883e-07,
      "loss": 0.5202,
      "step": 30340
    },
    {
      "epoch": 1.9160353535353534,
      "grad_norm": 0.40578797459602356,
      "learning_rate": 9.243975116044912e-07,
      "loss": 0.7737,
      "step": 30350
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.4347330331802368,
      "learning_rate": 9.105804605829327e-07,
      "loss": 0.6501,
      "step": 30360
    },
    {
      "epoch": 1.9172979797979797,
      "grad_norm": 0.43445900082588196,
      "learning_rate": 8.968669786458161e-07,
      "loss": 0.5493,
      "step": 30370
    },
    {
      "epoch": 1.9179292929292928,
      "grad_norm": 0.47736337780952454,
      "learning_rate": 8.832570801265761e-07,
      "loss": 0.4894,
      "step": 30380
    },
    {
      "epoch": 1.918560606060606,
      "grad_norm": 0.7344656586647034,
      "learning_rate": 8.697507792503778e-07,
      "loss": 0.5145,
      "step": 30390
    },
    {
      "epoch": 1.9191919191919191,
      "grad_norm": 0.383739709854126,
      "learning_rate": 8.563480901341514e-07,
      "loss": 0.7923,
      "step": 30400
    },
    {
      "epoch": 1.9198232323232323,
      "grad_norm": 0.44538915157318115,
      "learning_rate": 8.430490267864799e-07,
      "loss": 0.6551,
      "step": 30410
    },
    {
      "epoch": 1.9204545454545454,
      "grad_norm": 0.4443940818309784,
      "learning_rate": 8.298536031076554e-07,
      "loss": 0.5719,
      "step": 30420
    },
    {
      "epoch": 1.9210858585858586,
      "grad_norm": 0.4299946129322052,
      "learning_rate": 8.167618328896342e-07,
      "loss": 0.4851,
      "step": 30430
    },
    {
      "epoch": 1.9217171717171717,
      "grad_norm": 0.755193829536438,
      "learning_rate": 8.037737298160707e-07,
      "loss": 0.5368,
      "step": 30440
    },
    {
      "epoch": 1.9223484848484849,
      "grad_norm": 0.392280250787735,
      "learning_rate": 7.90889307462217e-07,
      "loss": 0.797,
      "step": 30450
    },
    {
      "epoch": 1.922979797979798,
      "grad_norm": 0.42478877305984497,
      "learning_rate": 7.781085792949894e-07,
      "loss": 0.6304,
      "step": 30460
    },
    {
      "epoch": 1.9236111111111112,
      "grad_norm": 0.4315073490142822,
      "learning_rate": 7.654315586729022e-07,
      "loss": 0.5566,
      "step": 30470
    },
    {
      "epoch": 1.9242424242424243,
      "grad_norm": 0.4817598760128021,
      "learning_rate": 7.528582588460787e-07,
      "loss": 0.4957,
      "step": 30480
    },
    {
      "epoch": 1.9248737373737375,
      "grad_norm": 0.6297943592071533,
      "learning_rate": 7.403886929562398e-07,
      "loss": 0.5192,
      "step": 30490
    },
    {
      "epoch": 1.9255050505050506,
      "grad_norm": 0.4133768379688263,
      "learning_rate": 7.280228740366935e-07,
      "loss": 0.7636,
      "step": 30500
    },
    {
      "epoch": 1.9261363636363638,
      "grad_norm": 0.42470741271972656,
      "learning_rate": 7.157608150122896e-07,
      "loss": 0.6562,
      "step": 30510
    },
    {
      "epoch": 1.926767676767677,
      "grad_norm": 0.4229176938533783,
      "learning_rate": 7.036025286994208e-07,
      "loss": 0.5727,
      "step": 30520
    },
    {
      "epoch": 1.92739898989899,
      "grad_norm": 0.4632543921470642,
      "learning_rate": 6.915480278060438e-07,
      "loss": 0.5199,
      "step": 30530
    },
    {
      "epoch": 1.928030303030303,
      "grad_norm": 0.7541932463645935,
      "learning_rate": 6.795973249316245e-07,
      "loss": 0.5263,
      "step": 30540
    },
    {
      "epoch": 1.9286616161616161,
      "grad_norm": 0.3873097896575928,
      "learning_rate": 6.677504325671157e-07,
      "loss": 0.8116,
      "step": 30550
    },
    {
      "epoch": 1.9292929292929293,
      "grad_norm": 0.4217979609966278,
      "learning_rate": 6.560073630950125e-07,
      "loss": 0.6418,
      "step": 30560
    },
    {
      "epoch": 1.9299242424242424,
      "grad_norm": 0.4895303249359131,
      "learning_rate": 6.443681287892522e-07,
      "loss": 0.5929,
      "step": 30570
    },
    {
      "epoch": 1.9305555555555556,
      "grad_norm": 0.5119052529335022,
      "learning_rate": 6.328327418152702e-07,
      "loss": 0.5231,
      "step": 30580
    },
    {
      "epoch": 1.9311868686868687,
      "grad_norm": 0.7151174545288086,
      "learning_rate": 6.214012142299441e-07,
      "loss": 0.5156,
      "step": 30590
    },
    {
      "epoch": 1.9318181818181817,
      "grad_norm": 0.385179340839386,
      "learning_rate": 6.100735579816053e-07,
      "loss": 0.8421,
      "step": 30600
    },
    {
      "epoch": 1.9324494949494948,
      "grad_norm": 0.43208399415016174,
      "learning_rate": 5.988497849099939e-07,
      "loss": 0.648,
      "step": 30610
    },
    {
      "epoch": 1.933080808080808,
      "grad_norm": 0.44236040115356445,
      "learning_rate": 5.87729906746326e-07,
      "loss": 0.5687,
      "step": 30620
    },
    {
      "epoch": 1.933712121212121,
      "grad_norm": 0.47935208678245544,
      "learning_rate": 5.767139351131601e-07,
      "loss": 0.5053,
      "step": 30630
    },
    {
      "epoch": 1.9343434343434343,
      "grad_norm": 0.8037076592445374,
      "learning_rate": 5.65801881524497e-07,
      "loss": 0.5027,
      "step": 30640
    },
    {
      "epoch": 1.9349747474747474,
      "grad_norm": 0.3799731731414795,
      "learning_rate": 5.549937573857023e-07,
      "loss": 0.7464,
      "step": 30650
    },
    {
      "epoch": 1.9356060606060606,
      "grad_norm": 0.4225253164768219,
      "learning_rate": 5.442895739935172e-07,
      "loss": 0.6501,
      "step": 30660
    },
    {
      "epoch": 1.9362373737373737,
      "grad_norm": 0.47945207357406616,
      "learning_rate": 5.336893425360367e-07,
      "loss": 0.5723,
      "step": 30670
    },
    {
      "epoch": 1.9368686868686869,
      "grad_norm": 0.5070646405220032,
      "learning_rate": 5.231930740927316e-07,
      "loss": 0.5317,
      "step": 30680
    },
    {
      "epoch": 1.9375,
      "grad_norm": 0.7028495073318481,
      "learning_rate": 5.128007796343592e-07,
      "loss": 0.5022,
      "step": 30690
    },
    {
      "epoch": 1.9381313131313131,
      "grad_norm": 0.35039258003234863,
      "learning_rate": 5.025124700230533e-07,
      "loss": 0.7538,
      "step": 30700
    },
    {
      "epoch": 1.9387626262626263,
      "grad_norm": 0.40244072675704956,
      "learning_rate": 4.923281560122228e-07,
      "loss": 0.6413,
      "step": 30710
    },
    {
      "epoch": 1.9393939393939394,
      "grad_norm": 0.4542800486087799,
      "learning_rate": 4.822478482466197e-07,
      "loss": 0.5662,
      "step": 30720
    },
    {
      "epoch": 1.9400252525252526,
      "grad_norm": 0.40077152848243713,
      "learning_rate": 4.7227155726224936e-07,
      "loss": 0.5295,
      "step": 30730
    },
    {
      "epoch": 1.9406565656565657,
      "grad_norm": 0.6889774799346924,
      "learning_rate": 4.623992934864263e-07,
      "loss": 0.532,
      "step": 30740
    },
    {
      "epoch": 1.941287878787879,
      "grad_norm": 0.3866966962814331,
      "learning_rate": 4.526310672377077e-07,
      "loss": 0.863,
      "step": 30750
    },
    {
      "epoch": 1.941919191919192,
      "grad_norm": 0.4526979327201843,
      "learning_rate": 4.429668887259375e-07,
      "loss": 0.6642,
      "step": 30760
    },
    {
      "epoch": 1.9425505050505052,
      "grad_norm": 0.4433419108390808,
      "learning_rate": 4.334067680521803e-07,
      "loss": 0.5636,
      "step": 30770
    },
    {
      "epoch": 1.9431818181818183,
      "grad_norm": 0.4515267312526703,
      "learning_rate": 4.239507152087652e-07,
      "loss": 0.4946,
      "step": 30780
    },
    {
      "epoch": 1.9438131313131313,
      "grad_norm": 0.8019356727600098,
      "learning_rate": 4.1459874007924173e-07,
      "loss": 0.5322,
      "step": 30790
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.3804771304130554,
      "learning_rate": 4.053508524383687e-07,
      "loss": 0.7654,
      "step": 30800
    },
    {
      "epoch": 1.9450757575757576,
      "grad_norm": 0.4214995205402374,
      "learning_rate": 3.96207061952103e-07,
      "loss": 0.657,
      "step": 30810
    },
    {
      "epoch": 1.9457070707070707,
      "grad_norm": 0.46435827016830444,
      "learning_rate": 3.871673781776219e-07,
      "loss": 0.5624,
      "step": 30820
    },
    {
      "epoch": 1.9463383838383839,
      "grad_norm": 0.47279274463653564,
      "learning_rate": 3.7823181056327873e-07,
      "loss": 0.5095,
      "step": 30830
    },
    {
      "epoch": 1.946969696969697,
      "grad_norm": 0.6931142210960388,
      "learning_rate": 3.694003684486025e-07,
      "loss": 0.5298,
      "step": 30840
    },
    {
      "epoch": 1.94760101010101,
      "grad_norm": 0.36563289165496826,
      "learning_rate": 3.6067306106427614e-07,
      "loss": 0.7428,
      "step": 30850
    },
    {
      "epoch": 1.948232323232323,
      "grad_norm": 0.4266590178012848,
      "learning_rate": 3.5204989753216955e-07,
      "loss": 0.6637,
      "step": 30860
    },
    {
      "epoch": 1.9488636363636362,
      "grad_norm": 0.45346754789352417,
      "learning_rate": 3.435308868652842e-07,
      "loss": 0.5697,
      "step": 30870
    },
    {
      "epoch": 1.9494949494949494,
      "grad_norm": 0.49114617705345154,
      "learning_rate": 3.3511603796775316e-07,
      "loss": 0.4955,
      "step": 30880
    },
    {
      "epoch": 1.9501262626262625,
      "grad_norm": 0.7328776121139526,
      "learning_rate": 3.2680535963485193e-07,
      "loss": 0.4996,
      "step": 30890
    },
    {
      "epoch": 1.9507575757575757,
      "grad_norm": 0.42184096574783325,
      "learning_rate": 3.185988605529655e-07,
      "loss": 0.7805,
      "step": 30900
    },
    {
      "epoch": 1.9513888888888888,
      "grad_norm": 0.41767755150794983,
      "learning_rate": 3.1049654929959927e-07,
      "loss": 0.6467,
      "step": 30910
    },
    {
      "epoch": 1.952020202020202,
      "grad_norm": 0.45500898361206055,
      "learning_rate": 3.0249843434335677e-07,
      "loss": 0.553,
      "step": 30920
    },
    {
      "epoch": 1.9526515151515151,
      "grad_norm": 0.44200563430786133,
      "learning_rate": 2.946045240439288e-07,
      "loss": 0.4804,
      "step": 30930
    },
    {
      "epoch": 1.9532828282828283,
      "grad_norm": 0.7175806164741516,
      "learning_rate": 2.8681482665210424e-07,
      "loss": 0.5145,
      "step": 30940
    },
    {
      "epoch": 1.9539141414141414,
      "grad_norm": 0.44399186968803406,
      "learning_rate": 2.7912935030972587e-07,
      "loss": 0.7616,
      "step": 30950
    },
    {
      "epoch": 1.9545454545454546,
      "grad_norm": 0.4717651903629303,
      "learning_rate": 2.7154810304973467e-07,
      "loss": 0.6473,
      "step": 30960
    },
    {
      "epoch": 1.9551767676767677,
      "grad_norm": 0.4638075530529022,
      "learning_rate": 2.640710927961032e-07,
      "loss": 0.5517,
      "step": 30970
    },
    {
      "epoch": 1.9558080808080809,
      "grad_norm": 0.4980759620666504,
      "learning_rate": 2.5669832736386903e-07,
      "loss": 0.5208,
      "step": 30980
    },
    {
      "epoch": 1.956439393939394,
      "grad_norm": 0.767947256565094,
      "learning_rate": 2.4942981445910117e-07,
      "loss": 0.5204,
      "step": 30990
    },
    {
      "epoch": 1.9570707070707072,
      "grad_norm": 0.3760986626148224,
      "learning_rate": 2.422655616789116e-07,
      "loss": 0.7674,
      "step": 31000
    },
    {
      "epoch": 1.9570707070707072,
      "eval_loss": 0.6316450834274292,
      "eval_runtime": 31.7632,
      "eval_samples_per_second": 80.596,
      "eval_steps_per_second": 10.075,
      "step": 31000
    },
    {
      "epoch": 1.9577020202020203,
      "grad_norm": 0.42996707558631897,
      "learning_rate": 2.352055765114436e-07,
      "loss": 0.6282,
      "step": 31010
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 0.4699326157569885,
      "learning_rate": 2.282498663358501e-07,
      "loss": 0.5564,
      "step": 31020
    },
    {
      "epoch": 1.9589646464646466,
      "grad_norm": 0.508505642414093,
      "learning_rate": 2.2139843842228225e-07,
      "loss": 0.5056,
      "step": 31030
    },
    {
      "epoch": 1.9595959595959596,
      "grad_norm": 0.752522885799408,
      "learning_rate": 2.146512999319339e-07,
      "loss": 0.521,
      "step": 31040
    },
    {
      "epoch": 1.9602272727272727,
      "grad_norm": 0.40736591815948486,
      "learning_rate": 2.0800845791695277e-07,
      "loss": 0.7941,
      "step": 31050
    },
    {
      "epoch": 1.9608585858585859,
      "grad_norm": 0.46398746967315674,
      "learning_rate": 2.0146991932049608e-07,
      "loss": 0.6435,
      "step": 31060
    },
    {
      "epoch": 1.961489898989899,
      "grad_norm": 0.4520038664340973,
      "learning_rate": 1.9503569097670815e-07,
      "loss": 0.5551,
      "step": 31070
    },
    {
      "epoch": 1.9621212121212122,
      "grad_norm": 0.48279792070388794,
      "learning_rate": 1.8870577961068724e-07,
      "loss": 0.4962,
      "step": 31080
    },
    {
      "epoch": 1.9627525252525253,
      "grad_norm": 0.7197052836418152,
      "learning_rate": 1.8248019183850773e-07,
      "loss": 0.5005,
      "step": 31090
    },
    {
      "epoch": 1.9633838383838382,
      "grad_norm": 0.40251606702804565,
      "learning_rate": 1.7635893416722005e-07,
      "loss": 0.7652,
      "step": 31100
    },
    {
      "epoch": 1.9640151515151514,
      "grad_norm": 0.42865660786628723,
      "learning_rate": 1.7034201299479525e-07,
      "loss": 0.6264,
      "step": 31110
    },
    {
      "epoch": 1.9646464646464645,
      "grad_norm": 0.45447179675102234,
      "learning_rate": 1.6442943461018047e-07,
      "loss": 0.5307,
      "step": 31120
    },
    {
      "epoch": 1.9652777777777777,
      "grad_norm": 0.4909556806087494,
      "learning_rate": 1.5862120519325451e-07,
      "loss": 0.51,
      "step": 31130
    },
    {
      "epoch": 1.9659090909090908,
      "grad_norm": 0.7040495276451111,
      "learning_rate": 1.5291733081481684e-07,
      "loss": 0.5108,
      "step": 31140
    },
    {
      "epoch": 1.966540404040404,
      "grad_norm": 0.39698076248168945,
      "learning_rate": 1.4731781743660966e-07,
      "loss": 0.7902,
      "step": 31150
    },
    {
      "epoch": 1.9671717171717171,
      "grad_norm": 0.4286453127861023,
      "learning_rate": 1.4182267091128464e-07,
      "loss": 0.6234,
      "step": 31160
    },
    {
      "epoch": 1.9678030303030303,
      "grad_norm": 0.4558963179588318,
      "learning_rate": 1.364318969824252e-07,
      "loss": 0.5695,
      "step": 31170
    },
    {
      "epoch": 1.9684343434343434,
      "grad_norm": 0.48042455315589905,
      "learning_rate": 1.311455012845131e-07,
      "loss": 0.4853,
      "step": 31180
    },
    {
      "epoch": 1.9690656565656566,
      "grad_norm": 0.8147392868995667,
      "learning_rate": 1.2596348934291735e-07,
      "loss": 0.4957,
      "step": 31190
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 0.4407426416873932,
      "learning_rate": 1.2088586657393876e-07,
      "loss": 0.8104,
      "step": 31200
    },
    {
      "epoch": 1.9703282828282829,
      "grad_norm": 0.4266372323036194,
      "learning_rate": 1.1591263828474308e-07,
      "loss": 0.6146,
      "step": 31210
    },
    {
      "epoch": 1.970959595959596,
      "grad_norm": 0.4666692316532135,
      "learning_rate": 1.1104380967338346e-07,
      "loss": 0.5544,
      "step": 31220
    },
    {
      "epoch": 1.9715909090909092,
      "grad_norm": 0.5050479173660278,
      "learning_rate": 1.0627938582881136e-07,
      "loss": 0.4758,
      "step": 31230
    },
    {
      "epoch": 1.9722222222222223,
      "grad_norm": 0.6717230677604675,
      "learning_rate": 1.0161937173084335e-07,
      "loss": 0.5086,
      "step": 31240
    },
    {
      "epoch": 1.9728535353535355,
      "grad_norm": 0.4295988082885742,
      "learning_rate": 9.706377225014995e-08,
      "loss": 0.7859,
      "step": 31250
    },
    {
      "epoch": 1.9734848484848486,
      "grad_norm": 0.44067201018333435,
      "learning_rate": 9.261259214828899e-08,
      "loss": 0.6643,
      "step": 31260
    },
    {
      "epoch": 1.9741161616161618,
      "grad_norm": 0.46655726432800293,
      "learning_rate": 8.826583607767225e-08,
      "loss": 0.5816,
      "step": 31270
    },
    {
      "epoch": 1.9747474747474747,
      "grad_norm": 0.42558151483535767,
      "learning_rate": 8.402350858156549e-08,
      "loss": 0.5197,
      "step": 31280
    },
    {
      "epoch": 1.9753787878787878,
      "grad_norm": 0.7522087693214417,
      "learning_rate": 7.988561409408845e-08,
      "loss": 0.5071,
      "step": 31290
    },
    {
      "epoch": 1.976010101010101,
      "grad_norm": 0.4282665252685547,
      "learning_rate": 7.585215694019265e-08,
      "loss": 0.7846,
      "step": 31300
    },
    {
      "epoch": 1.9766414141414141,
      "grad_norm": 0.44660496711730957,
      "learning_rate": 7.192314133568356e-08,
      "loss": 0.6347,
      "step": 31310
    },
    {
      "epoch": 1.9772727272727273,
      "grad_norm": 0.42176851630210876,
      "learning_rate": 6.809857138720954e-08,
      "loss": 0.5788,
      "step": 31320
    },
    {
      "epoch": 1.9779040404040404,
      "grad_norm": 0.524050772190094,
      "learning_rate": 6.437845109223962e-08,
      "loss": 0.5098,
      "step": 31330
    },
    {
      "epoch": 1.9785353535353534,
      "grad_norm": 0.8035215139389038,
      "learning_rate": 6.076278433906346e-08,
      "loss": 0.5181,
      "step": 31340
    },
    {
      "epoch": 1.9791666666666665,
      "grad_norm": 0.41145119071006775,
      "learning_rate": 5.725157490682476e-08,
      "loss": 0.8332,
      "step": 31350
    },
    {
      "epoch": 1.9797979797979797,
      "grad_norm": 0.4076574444770813,
      "learning_rate": 5.3844826465454525e-08,
      "loss": 0.6061,
      "step": 31360
    },
    {
      "epoch": 1.9804292929292928,
      "grad_norm": 0.4908878803253174,
      "learning_rate": 5.054254257572666e-08,
      "loss": 0.5473,
      "step": 31370
    },
    {
      "epoch": 1.981060606060606,
      "grad_norm": 0.4576827585697174,
      "learning_rate": 4.734472668919132e-08,
      "loss": 0.5082,
      "step": 31380
    },
    {
      "epoch": 1.9816919191919191,
      "grad_norm": 0.6710422039031982,
      "learning_rate": 4.425138214826374e-08,
      "loss": 0.5335,
      "step": 31390
    },
    {
      "epoch": 1.9823232323232323,
      "grad_norm": 0.45113110542297363,
      "learning_rate": 4.126251218611321e-08,
      "loss": 0.7716,
      "step": 31400
    },
    {
      "epoch": 1.9829545454545454,
      "grad_norm": 0.4237593412399292,
      "learning_rate": 3.837811992674079e-08,
      "loss": 0.653,
      "step": 31410
    },
    {
      "epoch": 1.9835858585858586,
      "grad_norm": 0.49806904792785645,
      "learning_rate": 3.559820838492378e-08,
      "loss": 0.5657,
      "step": 31420
    },
    {
      "epoch": 1.9842171717171717,
      "grad_norm": 0.47234293818473816,
      "learning_rate": 3.2922780466260184e-08,
      "loss": 0.4903,
      "step": 31430
    },
    {
      "epoch": 1.9848484848484849,
      "grad_norm": 0.8017023205757141,
      "learning_rate": 3.035183896713534e-08,
      "loss": 0.5324,
      "step": 31440
    },
    {
      "epoch": 1.985479797979798,
      "grad_norm": 0.38051027059555054,
      "learning_rate": 2.7885386574699747e-08,
      "loss": 0.809,
      "step": 31450
    },
    {
      "epoch": 1.9861111111111112,
      "grad_norm": 0.43030625581741333,
      "learning_rate": 2.552342586693568e-08,
      "loss": 0.6453,
      "step": 31460
    },
    {
      "epoch": 1.9867424242424243,
      "grad_norm": 0.41081923246383667,
      "learning_rate": 2.326595931255726e-08,
      "loss": 0.5652,
      "step": 31470
    },
    {
      "epoch": 1.9873737373737375,
      "grad_norm": 0.5033016800880432,
      "learning_rate": 2.1112989271099282e-08,
      "loss": 0.5071,
      "step": 31480
    },
    {
      "epoch": 1.9880050505050506,
      "grad_norm": 0.7336419820785522,
      "learning_rate": 1.9064517992872788e-08,
      "loss": 0.4729,
      "step": 31490
    },
    {
      "epoch": 1.9886363636363638,
      "grad_norm": 0.40165871381759644,
      "learning_rate": 1.7120547618942882e-08,
      "loss": 0.7635,
      "step": 31500
    },
    {
      "epoch": 1.989267676767677,
      "grad_norm": 0.41670677065849304,
      "learning_rate": 1.5281080181173136e-08,
      "loss": 0.6433,
      "step": 31510
    },
    {
      "epoch": 1.98989898989899,
      "grad_norm": 0.41474461555480957,
      "learning_rate": 1.3546117602181163e-08,
      "loss": 0.5577,
      "step": 31520
    },
    {
      "epoch": 1.990530303030303,
      "grad_norm": 0.4628988802433014,
      "learning_rate": 1.1915661695360847e-08,
      "loss": 0.492,
      "step": 31530
    },
    {
      "epoch": 1.9911616161616161,
      "grad_norm": 0.7549700140953064,
      "learning_rate": 1.0389714164904529e-08,
      "loss": 0.548,
      "step": 31540
    },
    {
      "epoch": 1.9917929292929293,
      "grad_norm": 0.3780716061592102,
      "learning_rate": 8.968276605714199e-09,
      "loss": 0.7771,
      "step": 31550
    },
    {
      "epoch": 1.9924242424242424,
      "grad_norm": 0.4166505038738251,
      "learning_rate": 7.651350503501409e-09,
      "loss": 0.6319,
      "step": 31560
    },
    {
      "epoch": 1.9930555555555556,
      "grad_norm": 0.4775703549385071,
      "learning_rate": 6.4389372347317675e-09,
      "loss": 0.5559,
      "step": 31570
    },
    {
      "epoch": 1.9936868686868687,
      "grad_norm": 0.46104690432548523,
      "learning_rate": 5.331038066624939e-09,
      "loss": 0.4922,
      "step": 31580
    },
    {
      "epoch": 1.9943181818181817,
      "grad_norm": 0.7905954718589783,
      "learning_rate": 4.327654157176841e-09,
      "loss": 0.5365,
      "step": 31590
    },
    {
      "epoch": 1.9949494949494948,
      "grad_norm": 0.4143497347831726,
      "learning_rate": 3.4287865551152488e-09,
      "loss": 0.7724,
      "step": 31600
    },
    {
      "epoch": 1.995580808080808,
      "grad_norm": 0.46688005328178406,
      "learning_rate": 2.6344361999441904e-09,
      "loss": 0.6418,
      "step": 31610
    },
    {
      "epoch": 1.996212121212121,
      "grad_norm": 0.4990750849246979,
      "learning_rate": 1.944603921943955e-09,
      "loss": 0.5649,
      "step": 31620
    },
    {
      "epoch": 1.9968434343434343,
      "grad_norm": 0.4635232985019684,
      "learning_rate": 1.3592904421155794e-09,
      "loss": 0.5046,
      "step": 31630
    },
    {
      "epoch": 1.9974747474747474,
      "grad_norm": 0.7075760364532471,
      "learning_rate": 8.784963722474615e-10,
      "loss": 0.5328,
      "step": 31640
    },
    {
      "epoch": 1.9981060606060606,
      "grad_norm": 0.383899062871933,
      "learning_rate": 5.022222148487465e-10,
      "loss": 0.7534,
      "step": 31650
    },
    {
      "epoch": 1.9987373737373737,
      "grad_norm": 0.41732728481292725,
      "learning_rate": 2.3046836322704324e-10,
      "loss": 0.6134,
      "step": 31660
    },
    {
      "epoch": 1.9993686868686869,
      "grad_norm": 0.46301987767219543,
      "learning_rate": 6.323510141070798e-11,
      "loss": 0.4962,
      "step": 31670
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.867190957069397,
      "learning_rate": 5.226041932537839e-13,
      "loss": 0.5172,
      "step": 31680
    },
    {
      "epoch": 2.0,
      "step": 31680,
      "total_flos": 1.745584775279149e+18,
      "train_loss": 0.6410706321473676,
      "train_runtime": 40463.1622,
      "train_samples_per_second": 25.054,
      "train_steps_per_second": 0.783
    }
  ],
  "logging_steps": 10,
  "max_steps": 31680,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.745584775279149e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
